{
  "hash": "3e3ea8c071ce7b6fe2a99bb92d141806",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd\"\ndescription: \"backpropagation, from neuron to neural network, from micro grad to pytorch, and more\"\nauthor:\n  - name: \"Tuan Le Khac\"\n    url: https://lktuan.github.io/\ncategories: [til, python, andrej karpathy, nn-z2h, backpropagation, neural networks] \ndate: 06-16-2024\ndate-modified: 06-18-2024\nimage: puppy.jpg\ncode-tools: true\ncode-fold: show\ncode-annotations: hover\ndraft: false\ncss: html/styles.scss\nfig-cap-location: bottom\neditor: visual\n\nformat:\n  html:\n    code-overflow: wrap\n---\n\n**Upfront-note**: There are also greate resources in Vietnamese for learning Backpropagation, for e.g.:\n\n1. Blog [machinelearningcoban](https://machinelearningcoban.com/2017/02/24/mlp/)\n2. Blog [dominhhai](https://dominhhai.github.io/vi/2018/04/nn-bp/)\n\n# MicroGrad from scratch Yayy!\n\n[**üöÄ MicroGrad repo**](https://github.com/karpathy/micrograd)\n[**üî• Video Lecture**](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)\n\n::: {layout-ncol=1}\n![Backpropagation in Neural Networks, photo credit to [GeekforGeek](https://www.geeksforgeeks.org/backpropagation-in-neural-network/)](https://media.geeksforgeeks.org/wp-content/uploads/20240217152156/Frame-13.png){width=100%}\n:::\n\n## intro & micrograd overview - what does your neural network training look like under the hood?\n\nWhat is MicroGrad ‚ùì: a tiny **auto-grad** (automatic gradient) engine, implement of **backpropagation** ~ itertively tune the weight of that nn to minimize the loss function -> improve the accuracy of the neural network. Backpropagation will be the mathematical core of any modern deep neutral network like, say `pytorch`, or `jaxx`.\n\nInstallation: `pip install micrograd`\n\nExample:\n\n::: {#b9df0d96 .cell execution_count=1}\n``` {.python .cell-code}\nfrom micrograd.engine import Value\n\na = Value(-4.0)                                                                     # <1> \nb = Value(2.0)\nc = a + b\nd = a * b + b**3\nc += c + 1\nc += 1 + c + (-a)\nd += d * 2 + (b + a).relu()\nd += 3 * d + (b - a).relu()\ne = c - d\nf = e**2\ng = f / 2.0\ng += 10.0 / f                                                                       # <2>\nprint(f'{g.data:.4f}') # prints 24.7041, the outcome of this forward pass           # <3>\ng.backward()\nprint(f'{a.grad:.4f}') # prints 138.8338, i.e. the numerical value of dg/da         # <4>\nprint(f'{b.grad:.4f}') # prints 645.5773, i.e. the numerical value of dg/db\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n24.7041\n138.8338\n645.5773\n```\n:::\n:::\n\n\n1. Micrograd allows you to build mathematical expressions, in this case `a` and `b` are inputs, wrapped in `Value` object with value equal to `-4.0` and `2.0`, respectively.\n2. `a` and `b` are transformed to `c`, `d`, `e` and eventually `f`, `g`. Mathematical operators are implemented, like `+`, `*`, `**`, even `relu()`.\n3. `Value` object contains `data`, and `grad`.\n4. Call `backpropagation()` process.\n\n## derivative of a simple function with one input\n‚ùìWhat exactly is derivative‚ùì\n\n::: {#f5593771 .cell execution_count=2}\n``` {.python .cell-code}\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n```\n:::\n\n\nA simple quadratic function:\n\n::: {#bad2c0f5 .cell execution_count=3}\n``` {.python .cell-code}\ndef f(x):\n    return 3*x**2 - 4*x + 5\n```\n:::\n\n\n::: {#86db4db7 .cell execution_count=4}\n``` {.python .cell-code}\nf(3.0)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n20.0\n```\n:::\n:::\n\n\nInput also can be an array, we can plot it for visibility.\n\n::: {#09716b95 .cell execution_count=5}\n``` {.python .cell-code}\nxs = np.arange(-5, 5, 0.25)\nys = f(xs)\nplt.plot(xs, ys)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=575 height=411}\n:::\n:::\n\n\nIf we bump up a litle value `h` of `x`, how `f(x)` will response?\n\n::: {#47f820a3 .cell execution_count=6}\n``` {.python .cell-code}\nh = 0.000000000001 # <1>\nx = 3.0\n( f(x+h) - f(x) ) / h\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n14.001244608152774\n```\n:::\n:::\n\n\n1. Change the value of `h` from `0.0001` to be `0.00000...0001` -> the slope value comes to `14` (at the value of `3.0` of `x`).\n\nTry for `x = -3.0`, `x = 5.0`, we get different values of the slope, for `x = 2/3`, the slope is zero. Let's get more complex.\n\n## derivative of a function with multiple inputs\n\n::: {#0d2be48a .cell execution_count=7}\n``` {.python .cell-code}\na = 2.0\nb = -3.0\nc = 10.0\nd = a*b + c\nprint(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4.0\n```\n:::\n:::\n\n\nPut our bump-up element to this multi-variables function:\n\n::: {#c8b804a2 .cell execution_count=8}\n``` {.python .cell-code}\nh = 0.001\n\n# input\na = 2.0\nb = -3.0\nc = 10.0\n\nd1 = a*b + c\na += h  # <1>\nd2 = a*b + c\n\nprint('d1: ', d1)\nprint('d2: ', d2)\nprint('slope: ', (d2 - d1)/h) # <2>\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nd1:  4.0\nd2:  3.997\nslope:  -3.0000000000001137\n```\n:::\n:::\n\n\n1. Do the same for `b`, `c`, we'll get different slopes.\n2. We say given `b = -3.0` and `c = 10.0` are constants, the derivative of `d` at `a = 2.0` is `-3.0`. The rate of which `d` will increase if we scale `a`!\n\n## starting the core `Value` object of micrograd and its visualization\n\nSo we now have some intuitive sense of what is derivative is telling you about the function. We now move to the Neural Networks, which would be massive mathematical expressions. We need some data structures that maintain these expressions, we first declare an object `Value` that holds data.\n\n::: {#b333c6c5 .cell execution_count=9}\n``` {.python .cell-code}\nclass Value:\n    def __init__(self, data, \n                        _children=(), # <3>\n                        _op = '', # <5>\n                        label = ''\n                        ): \n        self.data = data\n        self.grad = 0.0 # <6>\n        self._backward = lambda: None # <7>\n        self._prev = set(_children)\n        self._op = _op\n        self.label =  label\n\n    def __repr__(self) -> str: # a nicer looking for class attributes\n        return f\"Value(data={self.data})\"\n    \n    def __add__(self, other): # <4>\n        other = other if isinstance(other, Value) else Value(other) # turn other to Value object before calculation\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward(): # <8> \n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other) # turn other to Value object before calculation\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n\n        def _backward():\n            self.grad += (1 - t**2) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        \n        # topo order for all children in the graph\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child) \n                topo.append(v)\n        build_topo(self)\n\n        # sequentially apply the chain rules\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n```\n:::\n\n\n3. the connective tissue of this expression. We want to keep these expression graphs, so we need to know and keep pointers about what values produce what other values. `_children` is by default a empty tuple.\n4. as we added `_children`, we also need to point out the father - children relationship in method `__add__` and `__mul__` as well.\n5. we want to know the **operation** between father and child, `_op` is empty string by default, the value `+` and `-` will be added to the operator method respectively.\n6. initially assume that node has no impact to the output.\n7. this backward function basically do nothing at the initial.\n8. implement of backward pass for plus node, `+=` represent the accumulate action (rather than overwrite it), assigne the gradient behaviour for each type of operation, call the `_backward` concurrently with function.\n\nSetting input and expression:\n\n::: {#dbbfa0d2 .cell execution_count=10}\n``` {.python .cell-code}\na = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(10.0, label='c')\n\na + b # <1> \n\na*b + c # <2>\n\n# d = a*b + c rewrite the expression\ne = a*b; e.label = 'e'\nd = e + c; d.label = 'd'\n# d\nf = Value(-2.0, label='f')\nL = d * f; L.label = 'L'\nL\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\nValue(data=-8.0)\n```\n:::\n:::\n\n\n1. which will internally call `a.__add__(b)`\n2. which will internally call `(a.__mul__(b)).__add__(c)`\n\nSo that we can know the children:\n\n::: {#d5fc2d85 .cell execution_count=11}\n``` {.python .cell-code}\nd._prev\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n{Value(data=-6.0), Value(data=10.0)}\n```\n:::\n:::\n\n\nWe can know the operations:\n\n::: {#8e746d76 .cell execution_count=12}\n``` {.python .cell-code}\nd._op\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n'+'\n```\n:::\n:::\n\n\nNow we know exactly how each value came to be by **word** expression and from what other values. These will be quite abit larger, so we need a way to nicely visualize these expressions that we're building out. Below are a-little-scary codes.\n\n::: {#1b6e7a49 .cell execution_count=13}\n``` {.python .cell-code}\nimport os\n\n# Assuming the Graphviz bin directory path is 'C:/Program Files (x86)/Graphviz2.xx/bin'\nos.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz/bin' # add with the code, Gemini instructed me this üò™\n\nfrom graphviz import Digraph\n\ndef trace(root):\n    # build a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v) # <1>\n            for child in v._prev:\n                edges.add((child, v)) # <2>\n                build(child)\n    build(root)\n\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = from left to right\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        # for any value in the graph, create a rectangular ('record') node for it\n        dot.node(name=uid, label=\"{ %s | data %.4f | grad %.4f}\" % (n.label, n.data, n.grad), shape='record') # why is (n.data, ), but not (n.data) ???\n        if n._op:\n            # if this value is a result of some operations, create an op node for it\n            dot.node(name = uid + n._op, label = n._op)\n            # and connect the node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        # connect n1 to the op node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n```\n:::\n\n\n1. This will collect all nodes to the `nodes`.\n2. This will iteratively recursively collect all nodes to the `nodes`, add child and node ralationship information to `edges`.\n\n:::{.column-margin}\nRemember to let [graphviz](https://graphviz.org/download/) installed on your machine, not only Python package, I also run this:\n\n::: {#0ead0492 .cell execution_count=14}\n``` {.python .cell-code}\nimport os\nos.environ[\"PATH\"] += os.pathsep + 'C:\\Program Files (x86)\\Graphviz\\bin\\dot.exe'\n```\n:::\n\n\n::: \n\nNow we can draw üöÄ. \n\n::: {#2df941fe .cell execution_count=15}\n``` {.python .cell-code}\ndraw_dot(d)\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n![](index_files/figure-html/cell-16-output-1.svg){}\n:::\n:::\n\n\nSo far we've build out mathematical expressions using only plus `+` and times `*`, all `Value`s are only scalar.\n\nBack to the `Value` object, we will create 1 more attribute call `label`, make the expression more complicated by adding intermediate value `f`, `d`, out final node will be capital `L`.\n\n# Backpropagation\n\nIn backpropagation, we start at the end and are going to reverse and calculate the gradients along all the intermediate values. What we are actually computing for evert single node here is derivative of that node with respect to `L`.\n\nIn neural nets, `L` represent to a Loss function. And you will be very interested in the derivative of bassically loss function `L` with respect to the **weights** of the neural networks.\n\nWe need to know how are those **leaf nodes** `a`, `b`, `c`, `f` are impacting to the loss function. We call it `grad` and add this attribute to the `Value` object.\n\n## manual backpropagation example #1: simple expression\n\n:::{.column-page}\n\n::: {#e64e9aae .cell execution_count=16}\n``` {.python .cell-code}\ndraw_dot(L)\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n![](index_files/figure-html/cell-17-output-1.svg){}\n:::\n:::\n\n\n:::\n\nLet's do the backpropagation **manually**:\n\n1. First we need to calculate the `dL/dL`, how `L` will response if we change `L` a tiny value `h`. The response simply is `1` so `L.grad = 1.0`.\n2. `F = d * f`, so `dL/dd` -> `(f((x+h)) - f(x))/h = ((d+h)*f - d*f)/h = h*f/h = f = -2.0`. Quite straighforward, so `d.grad = -2.0`.\n3. Similarly, `f.grad = d = 4`.  \n4. Next, for `dL/dc`. We first concern `dd/dc`, we know `d = c + e`. Same with (2) we will soon know `dd/dc = 1.0`, by symmetry `dd/de = 1.0`. Following the **Chain Rules** $h'(x) = f'(g(x))g'(x)$, we have `dL/dc = dL/dd * dd/dc = -2.0 * 1 = -2.0`. [[Chain Rules Wiki](https://en.wikipedia.org/wiki/Chain_rule)]{.aside}\n5. By symmetry, `dL/de = -2.0`.\n6. `dL/da = dL/de * de/da = -2.0 * b = -2.0 * -3.0 = 6.0`.\n7. `dl/db = dL/de * de/db = -2.0 * a = -2.0 * 2.0 = -4.0`.\n\nWe can also create a function for playing around / gradient check, and not messing up the global scope.\n\n::: {#b0ddb825 .cell execution_count=17}\n``` {.python .cell-code}\ndef lol():\n\n    h = 0.0001\n\n    a = Value(2.0, label='a')\n    b = Value(-3.0, label='b')\n    c = Value(10.0, label='c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L1 = L.data\n\n    a = Value(2.0, label='a')\n    b = Value(-3.0, label='b')\n    c = Value(10.0, label='c')\n    c.data += h # dL/dc = -2.0\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    # d.data += h # dL/dd = -2.0\n    f = Value(-2.0 # + h # dL/df = 4.0\n                , label='f') \n    L = d * f; L.label = 'L'\n    L2 = L.data # + h # dL/dL = 1.0\n\n    print((L2 - L1) / h)\n\nlol()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-1.9999999999953388\n```\n:::\n:::\n\n\nSo that is backpropagation ~ just recursively applying the Chain Rules, multiplying local derivatives.\n\n## preview of a single optimization step\n\nWe can change the input that we can control `a, b, c, f` to see 1 step of the optimization of process.\n\n::: {#61c0f1aa .cell execution_count=18}\n``` {.python .cell-code}\na.grad = 6.0\nb.grad = -4.0\nc.grad = -2.0\nf.grad = 4.0\n\na.data += 0.01 * a.grad\nb.data += 0.01 * b.grad\nc.data += 0.01 * c.grad\nf.data += 0.01 * f.grad\n\ne = a * b; e.grad = -2.0; e.label = 'e'\nd = e + c; d.grad = -2.0; d.label = 'd'\nL = d * f\n\nprint(L.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-7.286496\n```\n:::\n:::\n\n\nWe can see the changes, `L` increased a little bit as expected.\n\n:::{.column-page}\n\n::: {#5117ec3b .cell execution_count=19}\n``` {.python .cell-code}\ndraw_dot(L)\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n![](index_files/figure-html/cell-20-output-1.svg){}\n:::\n:::\n\n\n:::\n\n## manual backpropagation example #2: a neuron\n\nAnatomy of neurons, we have:\n\n- `axon` as input $x_0$;\n- `synapse` string as weight $w_0$;\n- information flows into the cell body will be $x_0w_0$;\n- there are multiple inputs $x_iw_i$ flow into the cell body;\n- the cell body has some *bias* itself $b$;\n- the cell body processes all information, the output will flow through an *activation* function ~ which is some kind of a squashing function, like `sigmoid`, `tanh` or something like that;\n\n::: {layout-ncol=1}\n![Neural net Structure with an Activation Function, CS231n Stanford 2017](https://www.researchgate.net/publication/364814302/figure/fig5/AS:11431281092677232@1666928276027/Neural-net-Structure-with-an-Activation-Function-Source-CS231n-Stanford-2017.png){width=100%}\n:::\n\n\nüöÄHow does the `tanh` look like? this hyperbolic function will squash the output to the edge values: `-1.0` or `1.0`.\n\n::: {#1435c01a .cell execution_count=20}\n``` {.python .cell-code}\nplt.plot(np.arange(-5, 5, 0.2), np.tanh(np.arange(-5, 5, 0.2))); plt.grid()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-21-output-1.png){width=590 height=411}\n:::\n:::\n\n\n:::{.column-margin}\nWe first implement [`tanh`](https://en.wikipedia.org/wiki/Hyperbolic_functions) function to our class `Value`.\n\n::: {#c999efd4 .cell execution_count=21}\n``` {.python .cell-code}\ndef tanh(self):\n    x = self.data\n    t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n    out = Value(t, (self, ), 'tanh')\n    return out\n\nValue.tanh = tanh\n```\n:::\n\n\n:::\n\nLet's take a simple example of 2-dimensional neuron with 2 inputs `x1` and `x2`:\n\n::: {#adf855ae .cell execution_count=22}\n``` {.python .cell-code}\n# input x1, x2\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n# weights w1,w2\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n# bias of neuron b\nb = Value(6.88137358, label='b')\n# x1*w1 + x2*w2 + b\nx1w1 = x1*w1; x1w1.label = 'x1w1'\nx2w2 = x2*w2; x2w2.label = 'x2w2'\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\nn = x1w1x2w2 + b; n.label = 'n'\n\no = n.tanh(); o.label = 'o' # not define yet\n```\n:::\n\n\n:::{.column-page}\n\n::: {#b5fc4c81 .cell execution_count=23}\n``` {.python .cell-code}\ndraw_dot(o)\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n![](index_files/figure-html/cell-24-output-1.svg){}\n:::\n:::\n\n\n:::\n\n\n:::{.column-margin}\n\n::: {#050f50a4 .cell execution_count=24}\n``` {.python .cell-code}\no.grad = 1.0\nn.grad = 1.0 - o.data ** 2\nb.grad = n.grad\nx1w1x2w2.grad = n.grad\nx1w1.grad = x1w1x2w2.grad\nx2w2.grad = x1w1x2w2.grad\nx1.grad = w1.data * x1w1.grad\nw1.grad = x1.data * x1w1.grad\nx2.grad = w2.data * x2w2.grad\nw2.grad = x2.data * x2w2.grad\n```\n:::\n\n\n:::\n\nFrom here we will manually calculate the gradient again:\n\n1. `do/do = 1`, that's the base case, so `o.grad = 1.0`.\n2. `o = tanh(n)`, follow that Wiki link (and of course can be easily proof) we have `do/dn = 1 - tanh(x)^2 = 1 - o^2`.\n3. `n = x1w1x2w2 + b`, this is plus node, which gradient will flow to children equally, `do/db = do/dn * dn/db = do/dn * 1`.\n4. By symmertry, `do/dx1w1x2w2 = do/db`.\n5. `do/dx1w1 = do/dx1w1x2w2`.\n6. `do/dx2w2 = do/dx1w1x2w2`.\n7. `do/dx1 = w1 * do/dx1w1`.\n8. `do/dw1 = x1 * do/dx1w1`.\n9. `do/dx2 = w2 * do/dx2w2`.\n10. `do/dw2 = x2 * do/dx2w2`.\n\n:::{.column-page}\n\n::: {#b2a5685c .cell execution_count=25}\n``` {.python .cell-code}\ndraw_dot(o)\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n![](index_files/figure-html/cell-26-output-1.svg){}\n:::\n:::\n\n\n:::\n\n## implementing the backward function for each operation\n\nDoing the backpropagation manually is obviously ridiculous and we are now to put an end to this suffering. We will see how we can implement backward pass a bit more automatically.\n\nWe create `_backward` operation for each operator, implement the Chain Rules. Activate the `_backward` call along with funtion execution.\n\n::: {#724b9351 .cell execution_count=26}\n``` {.python .cell-code}\no.grad = 1.0\n\no._backward()\nn._backward()\nb._backward()\nx1w1x2w2._backward()\nx1w1._backward()\nx2w2._backward()\n```\n:::\n\n\n:::{.column-page}\n\n::: {#7b8ca278 .cell execution_count=27}\n``` {.python .cell-code}\ndraw_dot(o)\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n![](index_files/figure-html/cell-28-output-1.svg){}\n:::\n:::\n\n\n:::\n\nWe still need to call the `_backward` node by node. Now we move to the next step, to implement backward function to whole expression graph.\n\n## implementing the backward function for a whole expression graph\n\nIn short, we need to do everything after each node before we call the backward function itself. For every node, all dependencies, everything that it depends on has to propagate to it before we can continue backpropagation.\n\nThis ordering of graph can be archived using something like [topological sort](https://en.wikipedia.org/wiki/Topological_sorting).\n\n::: {layout-ncol=1}\n![Topological Sort, photo credit to [Claire Lee](https://yuminlee2.medium.com/topological-sort-cf9f8e43af6a)](https://miro.medium.com/v2/resize:fit:720/format:webp/1*0jRSNI2zo30sENk2qlqEvw.png){width=100%}\n:::\n\n:::{.column-margin}\n\n::: {#8e1cf168 .cell execution_count=28}\n``` {.python .cell-code}\n# we first reset the Values\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\nb = Value(6.88137358, label='b')\nx1w1 = x1*w1; x1w1.label = 'x1w1'\nx2w2 = x2*w2; x2w2.label = 'x2w2'\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\nn = x1w1x2w2 + b; n.label = 'n'\no = n.tanh(); o.label = 'o'\n```\n:::\n\n\n:::\n\nBelow is the code:\n\n::: {#701c37a6 .cell execution_count=29}\n``` {.python .cell-code}\ntopo = []\nvisited = set()\n\ndef build_topo(v):\n    if v not in visited:\n        visited.add(v)\n        for child in v._prev:\n            build_topo(child) # recursively look up all children for v\n        topo.append(v)\n\nbuild_topo(o)\ntopo\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n[Value(data=6.88137358),\n Value(data=2.0),\n Value(data=-3.0),\n Value(data=-6.0),\n Value(data=0.0),\n Value(data=1.0),\n Value(data=0.0),\n Value(data=-6.0),\n Value(data=0.88137358),\n Value(data=0.707106777676776)]\n```\n:::\n:::\n\n\nWe implement the topological sort to `backward()` (without underscore) function. Now we can trigger the whole process:\n\n::: {#23bc7fa2 .cell execution_count=30}\n``` {.python .cell-code}\no.backward()\n```\n:::\n\n\n:::{.column-page}\n\n::: {#ffb0088c .cell execution_count=31}\n``` {.python .cell-code}\ndraw_dot(o)\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n![](index_files/figure-html/cell-32-output-1.svg){}\n:::\n:::\n\n\n:::\n\n## fixing a backprop bug when one node is used multiple times ‚õî\n\nThis `a.grad` should be `2.0`.\n\n::: {#6eaa6894 .cell execution_count=32}\n``` {.python .cell-code}\na = Value(3.0, label='a')\nb = a + a; b.label = 'b' # this case self and other are both a, we should not overwrite the gradient, we should accumulate it.\nb.backward()\ndraw_dot(b)\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n![](index_files/figure-html/cell-33-output-1.svg){}\n:::\n:::\n\n\n## breaking up a `tanh`, exercising with more operations\n\nSometime we do operations between `Value` and other, like `int`. We can not do this unless we add below code to `__add__` and `__mul__` operations. Now we can `Value(1.0) + 1.0`, or `Value(2.0) * 2`.\n\n::: {#e91741ed .cell execution_count=33}\n``` {.python .cell-code}\nother = other if isinstance(other, Value) else Value(other)\n```\n:::\n\n\nBut for `2 * Value(2.0)`, which will internally call `2.__mul__(Value(2.0))`, will not work. We add `__rmul__`:\n\n::: {#80439bef .cell execution_count=34}\n``` {.python .cell-code}\ndef __rmul__(self, other): # other * self\n    return self * other\n\nValue.__rmul__ = __rmul__\n```\n:::\n\n\nFor exponential, we add `epx`:\n\n::: {#13022508 .cell execution_count=35}\n``` {.python .cell-code}\ndef exp(self):\n    x = self.data\n    out = Value(math.exp(x), (self, ), 'exp')\n\n    def _backward():\n        self.grad += out.data * out.grad\n    out._backward = _backward\n\n    return out\n\nValue.exp = exp\n```\n:::\n\n\nFor division, we add `__truediv__`:\n\n::: {#b4ac6bfa .cell execution_count=36}\n``` {.python .cell-code}\ndef __truediv__(self, other): # self / other\n    return self * other**(-1)\n\nValue.__truediv__ = __truediv__\n```\n:::\n\n\nFor power, we add `__pow__`:\n\n::: {#b5e05d88 .cell execution_count=37}\n``` {.python .cell-code}\ndef __pow__(self, other): # self ** other\n    assert isinstance(other, (int, float)), \"TypeError: only supporting int/float power for now\"\n    out = Value(self.data**other, (self, ), f'**{other}')\n\n    def _backward():\n        self.grad += other * ( self.data ** (other - 1)) * out.grad\n    out._backward = _backward\n\n    return out\n\nValue.__pow__ = __pow__ \n```\n:::\n\n\nFor subtract, we add `__neg__` and `__sub__`:\n\n::: {#46cb1ab6 .cell execution_count=38}\n``` {.python .cell-code}\ndef __neg__(self): # - self\n    return - self\n\nValue.__neg__ = __neg__ # self - other\n\ndef __sub__(self, other):\n    return self + (-other)\n\nValue.__sub__ = __sub__\n```\n:::\n\n\nNow we are ready to try `tanh` in a different way:\n\n::: {#6164cb90 .cell execution_count=39}\n``` {.python .cell-code}\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\nb = Value(6.88137358, label='b')\nx1w1 = x1*w1; x1w1.label = 'x1w1'\nx2w2 = x2*w2; x2w2.label = 'x2w2'\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\nn = x1w1x2w2 + b; n.label = 'n'\n\ne = (2*n).exp(); e.label = 'e'\no = (e - 1)/(e + 1)\no.label = 'o'\no.backward()\n```\n:::\n\n\n:::{.column-screen}\n\n::: {#548cf0cd .cell execution_count=40}\n``` {.python .cell-code}\ndraw_dot(o)\n```\n\n::: {.cell-output .cell-output-display execution_count=37}\n![](index_files/figure-html/cell-41-output-1.svg){}\n:::\n:::\n\n\n:::\n\n# PyTorch comparison\n\n## doing the same thing but in PyTorch: comparison\n\n::: {#188c2133 .cell execution_count=41}\n``` {.python .cell-code}\nimport torch\n\nx1 = torch.tensor([2.0]).double(); x1.requires_grad = True\nx2 = torch.tensor([0.0]).double(); x2.requires_grad = True\nw1 = torch.tensor([-3.0]).double(); w1.requires_grad = True\nw2 = torch.tensor([1.0]).double(); w2.requires_grad = True\nb = torch.tensor([6.8813735870195432]).double(); b.requires_grad = True\n\nn = x1*w1 + x2*w2 + b\no = torch.tanh(n)\n\nprint(o.data.item())\no.backward()\n\nprint('------------------')\nprint('x1', x1.grad.item())\nprint('w1', w1.grad.item())\nprint('x2', x2.grad.item())\nprint('w2', w2.grad.item())\nprint('------------------')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.7071066904050358\n------------------\nx1 -1.5000003851533106\nw1 1.0000002567688737\nx2 0.5000001283844369\nw2 0.0\n------------------\n```\n:::\n:::\n\n\n# Building the library\n\n## building out a neural net library (multi-layer perceptron) in micrograd\n\nWe are going to build out a two-layer perceptron.\n\n::: {layout-ncol=1}\n![A 3-layer neural network with three inputs, two hidden layers of 4 neurons each and one output layer. Notice that in both cases there are connections (synapses) between neurons across layers, but not within a layer, photo credit to [cs231n](https://cs231n.github.io/neural-networks-1/)](https://cs231n.github.io/assets/nn1/neural_net2.jpeg){width=100%}\n:::\n\n::: {#e0a044d7 .cell execution_count=42}\n``` {.python .cell-code}\nclass Neuron:\n\n    def __init__(self, nin): # <1>\n        self.w = [Value(np.random.uniform(-1,1)) for _ in range(nin)]\n        self.b = Value(np.random.uniform(-1,1))\n    \n    def __call__(self, x): # <2>\n        activation = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n        out = activation.tanh()\n        return out\n\n    def parameters(self):\n        return self.w + [self.b] # list plus list gives you a list\n\nclass Layer:\n\n    def __init__(self, nin, nout):\n        self.neurons = [Neuron(nin) for _ in range(nout)] # <3>\n\n    def __call__(self, x):\n        outs = [n(x) for n in self.neurons]\n        return outs[0] if len(outs) == 1 else outs\n\n    def parameters(self):\n        return [p for neuron in self.neurons for p in neuron.parameters()] # list comprehension\n        # params = []\n        # for neuron in self.neurons:\n        #     ps = neuron.parameters()\n        #     params.extend(ps)\n        # return params\n\nclass MLP:\n\n    def __init__(self, nin, nouts): # <4>\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def parameters(self):\n        return [p for layer in self.layers for p in layer.parameters()] # for neuron in layer.neurons for neuron.parameters()]\n```\n:::\n\n\n1. Number of `input` for the Neuron. `w` is randomly generated for each input, same for `b` which is the bias that controll \"the happiness\".\n2. Object as a function: define the forward pass of the Neuron $\\sum\\limits_{i=1}^{nin} w_ix_i+b$, then squash the output using `tanh`.\n3. A Layer is a list of Neurons, `nout` specifies how many Neurons in the Layer. Each neuron has `nin` inputs ~ nin-D. We just initialize completely independent neurons with this given dimensionality.\n4. A MLP is a sequence of Layers, picture above depicts a 3-layers MLP containing 1 input layer and 3 output layers, we say the size is 4. We sequentially create connection from the input layer to the 1st output layer, 1st output layer to 2nd output layer,...\n\n::: {#ff8ca4dd .cell execution_count=43}\n``` {.python .cell-code}\nnin = 3\nnouts = [2.0, 3.0, -1.0]\n[nin] + nouts\n```\n\n::: {.cell-output .cell-output-display execution_count=40}\n```\n[3, 2.0, 3.0, -1.0]\n```\n:::\n:::\n\n\n::: {#04dc941c .cell execution_count=44}\n``` {.python .cell-code}\nx = [2.0, 3.0]\nn = Neuron(2)\nl = Layer(2, 3)\nn(x)\nl(x)\n```\n\n::: {.cell-output .cell-output-display execution_count=41}\n```\n[Value(data=0.6482171154461839),\n Value(data=-0.9819100896618514),\n Value(data=0.42012274248341236)]\n```\n:::\n:::\n\n\n::: {#8cf5d3a3 .cell execution_count=45}\n``` {.python .cell-code}\nx = [2.0, 3.0, -1.0]\nm = MLP(3, [4, 4, 1]) # a MLP with 3-D input, 3 output layers contains 4, 4, 1 neurons in each layer respectively\nm(x)\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\nValue(data=-0.4705965544324498)\n```\n:::\n:::\n\n\n:::{.column-screen}\n\n::: {#237a863d .cell execution_count=46}\n``` {.python .cell-code}\ndraw_dot(m(x))\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n![](index_files/figure-html/cell-47-output-1.svg){}\n:::\n:::\n\n\n:::\n\n## creating a tiny dataset, writing the loss function\n\nA simple data set, `m()` is the MLP we defined above.\n\n::: {#20ba7355 .cell execution_count=47}\n``` {.python .cell-code}\nxs = [\n    [2.0, 3.0, -1.0],\n    [3.0, -1.0, 0.5],\n    [0.5, 1.0, 1.0],\n    [1.0, 1.0, -1.0]\n]\nys = [1.0, -1.0, -1.0, 1.0] # designed targets\n```\n:::\n\n\nWriting the loss function. [I was unable to sum a list of Value, found the solution [here](https://stackoverflow.com/questions/54153552/unsupported-operand-type-error-when-adding-objects-within-list-using-sum-functio); Edit: I used Numpy random instead of random]{.aside}\n\n::: {#bad2b0d7 .cell execution_count=48}\n``` {.python .cell-code}\nypred = [m(x) for x in xs]\nloss = np.array([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]).sum()\n\nloss\n```\n\n::: {.cell-output .cell-output-display execution_count=45}\n```\nValue(data=3.7846173239470486)\n```\n:::\n:::\n\n\nBackpropagation the `loss`, some magical here:\n\n::: {#88f8a81a .cell execution_count=49}\n``` {.python .cell-code}\nloss.backward()\n```\n:::\n\n\nWe can look into the gradient of weight of the first neuron of the first layer (input layer)\n\n::: {#6bfcc38c .cell execution_count=50}\n``` {.python .cell-code}\nprint('value of 1st neuron in 1st layer: ',m.layers[0].neurons[0].w[0].data)\nprint('grad of 1st neuron in 1st layer: ',m.layers[0].neurons[0].w[0].grad)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nvalue of 1st neuron in 1st layer:  -0.6357735432605731\ngrad of 1st neuron in 1st layer:  -4.1814374695537895\n```\n:::\n:::\n\n\n:::{.column-screen}\n\n::: {#2c9861ac .cell execution_count=51}\n``` {.python .cell-code}\ndraw_dot(loss)\n```\n\n::: {.cell-output .cell-output-display execution_count=48}\n![](index_files/figure-html/cell-52-output-1.svg){}\n:::\n:::\n\n\n:::\n\n## collecting all of the parameters of the neural net\n\nWe aim to produce the fitness `ypred`. `xs` is the data, the input of problem, we can not change it. `ys` is the ground true, can not changes as well. What we can change is the \"paramters\" of each neuron, which is weight `w` and bias `b`.\n\nWe add in to each class a `parameters()` function to collect those. Finally we can get all the paramters of the MLP:\n\n::: {#d4c227bc .cell execution_count=52}\n``` {.python .cell-code}\nlen(m.parameters())\n```\n\n::: {.cell-output .cell-output-display execution_count=49}\n```\n41\n```\n:::\n:::\n\n\n## doing gradient descent optimization manually, training the network\n\nNow we will try to change the paramters to minimize the loss, which means our prediction will be more close to the ground true.\n\n**Forward pass**, calculate the loss:\n\n::: {#6a39b17e .cell execution_count=53}\n``` {.python .cell-code}\nypred = [m(x) for x in xs]\nloss = np.array([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]).sum()\nloss\n```\n\n::: {.cell-output .cell-output-display execution_count=50}\n```\nValue(data=3.7846173239470486)\n```\n:::\n:::\n\n\n**Backward pass**, calculate the parameters:\n\n::: {#799752ef .cell execution_count=54}\n``` {.python .cell-code}\nloss.backward()\n```\n:::\n\n\n**Update the parameters**, change the parameters following opposite direction to reduce the loss:\n\n::: {#d46206a7 .cell execution_count=55}\n``` {.python .cell-code}\nfor p in m.parameters():\n    p.data += -0.01 * p.grad # we want the p.data go on opposite direction of the loss\n```\n:::\n\n\n`0.01` is the learning rate!\n\n**New loss**\n\n::: {#5fb4eec1 .cell execution_count=56}\n``` {.python .cell-code}\nypred = [m(x) for x in xs]\nloss = np.array([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]).sum()\nloss\n```\n\n::: {.cell-output .cell-output-display execution_count=53}\n```\nValue(data=0.885210729994525)\n```\n:::\n:::\n\n\nYeah the loss decreased. In short, the process is:\n\n```{mermaid}\n%%{init: {'theme':'dark'}}%%\nflowchart LR\n\nP1(Updated parameters) -- Forward Pass --> L(Loss)\nL(Loss) -- Backward Pass --> P2(Parameters to update) \nP2(Parameters to update)  -- Update Pamameters --> P1(Updated parameters)\n```\n\n\n**Automate the training loop**:\n\n:::{.callout-warning}\nFor each process, there remained a subtle bug above that we didn't flush the grads before backpropagation. Because we did not overwrite the gradients (remember the `+=`), they kept accumulated. The next action of backward and changing parameters using learning rate and grad (which produce a massive step size) become wrong! We must set the grad to zero before backward pass.\n\n![Common guilties when training the NN](guilty.png)\n:::\n\n::: {#e6a7c5f1 .cell execution_count=57}\n``` {.python .cell-code}\nfor k in range(20):\n    # forward pass:\n    ypred = [m(x) for x in xs]\n    loss = np.array([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]).sum()\n\n    # backward pass:\n    for p in m.parameters():\n        p.grad = 0.0\n    loss.backward()\n\n    # update params:\n    for p in m.parameters():\n        p.data += -0.01 * p.grad\n\n    print(k, loss.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0 0.885210729994525\n1 0.7272070306002337\n2 0.6335880961402813\n3 0.5694033268577418\n4 0.5210728602755308\n5 0.48234433619750416\n6 0.4499798770914163\n7 0.4221446577843293\n8 0.3977190488833604\n9 0.3759751713337349\n10 0.3564130670665513\n11 0.33867257920988014\n12 0.32248348490480044\n13 0.30763598643104695\n14 0.29396252735650596\n15 0.28132615850080067\n16 0.2696128276033427\n17 0.25872609733197516\n18 0.2485834130081963\n19 0.23911338866893608\n```\n:::\n:::\n\n\n::: {#69ae0b38 .cell execution_count=58}\n``` {.python .cell-code}\nypred\n```\n\n::: {.cell-output .cell-output-display execution_count=55}\n```\n[Value(data=0.7723855044989809),\n Value(data=-0.7666895995000865),\n Value(data=-0.7316139699290238),\n Value(data=0.7533418844066474)]\n```\n:::\n:::\n\n\n# Summary\n\n## summary of what we learned, how to go towards modern neural nets\n\n1. What are Neural Nets: they are mathematical expressions, in case of MLP it takes: (1) data as the input, and (2) weights and biases as parameters to build out expression for the forward pass followed by the loss function.\n2. The loss function is kind of measure for the accuracy of predictions. The low loss implies that predicted values are matching our targets and the networks are behaving well.\n3. The process of Gradient Descent is for each step, we calculate the loss (output of the nets), backwarding it to get paramters, then updating data (which we can change - weights and biases) follow the opposite side of the loss (negative grad * learning rate). We'll get a lower loss, and backwarding again and again. This process will find the local minimum of the loss.\n\n## walkthrough of the full code of micrograd on github\n\nSame with which we built today:\n\n- engine: Value\n- nn: Neuron, Layer, MLP, and modulize the zero grad process to class Module\n- test: sanity check - compare the backward with `torch`, also for the forward pass\n- demo: a bit complicated example with `sklearn` dataset, using batch processing when the dataset come large, the loss is slightly different - SVM max-margin loss and using of auto L2 regularization\n- learning rate decay: is a scaled as a function of number of iterations, high at begin and low at the end\n\n## real stuff: diving into PyTorch, finding their backward pass for `tanh`\n\nThese libraries unfortunately grow in size and **entropy**, if you just search for `tanh` it'll give you thousands of results.\n\n## conclusion\n\nThere will be follow up session yeah haha.\n\n## outtakes :\\)\n\nPytorch self-defined autograd.\n\nü§ôüöÄüî•\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}