---
title: "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore"
description: "implement a bigram character-level language model, focus on (1) introducing torch, and (2) the overall framework of language modeling that includes model training, sampling, and the evaluation of a loss"
author:
  - name: "Tuan Le Khac"
    url: https://lktuan.github.io/
categories: [til, python, andrej karpathy, nn-z2h, bigram, neural networks] 
date: 11-15-2024
date-modified: 11-17-2024
image: andrej_new.jpg
draft: false
css: html/styles.scss
fig-cap-location: bottom
editor: visual
format:
  html:
    code-overflow: wrap
    code-tools: true
    code-fold: show
    code-annotations: hover
---

`makemore` on github: <https://github.com/karpathy/makemore>

# PART 1: intro

> `makemore` takes one text file as input, where each line is assumed to be one training thing, and generates more things like it. Under the hood, it is an autoregressive character-level language model, with a wide choice of models from bigrams all the way to a Transformer (exactly as seen in GPT).

## reading and exploring the dataset

```{python}
import pandas as pd

url = "https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt"
words = pd.read_csv(url, header=None).iloc[:, 0].tolist()

print(words[:10])
print(len(words))
```

```{python}
print("No of chars for the shortest word: ", min(len(w) for w in words))
print("No of chars for the longest word: ", max(len(w) for w in words))
```

By looking into (1) the order of characters in individual word, and (2) that pattern for the whole dataset of 32k words, we will try to infer which character is likely to follow a character or chain of characters.

We will first building a `bigrams` languague model - which only works will 2 characters at a time - look at the current character and try to predict the next one. We are just following this local structure!

It's just a simple (and weak) model but a good way to start.

## exploring the `bigrams` in the dataset

```{python}
for w in words[:3]:
  chs = ['<S>'] + list(w) + ['<E>'] # special start and ending token, `list()` will turn all character in word to list
  for ch1, ch2 in zip(chs, chs[1:]):
    print(ch1, ch2)
```

## counting `bigrams` in a python dictionary

In order to learn statistics about what character is more likely to follow another character, the simplest way is `counting`.

```{python}
b = {} # dict to store all pair of character
for w in words[:5]: # do it for first five words
  chs = ['<S>'] + list(w) + ['<E>']
  for ch1, ch2 in zip(chs, chs[1:]):
    bigram = (ch1, ch2)
    b[bigram] = b.get(bigram, 0) + 1
    # print(ch1, ch2)
```

```{python}
sorted(b.items(), key = lambda kv: -kv[1])
```

## counting `bigrams` in a 2D `torch` tensor ("training the model")

Instead of using Python dictionary, we will use `torch` 2D array to store this information.

```{python}
import torch
```

```{python}
a = torch.zeros((3,5), dtype=torch.int32)
a
```

How can we access/assign a value in torch array:

```{python}
a[1:3] = 10
a
```

Now the english alphabet contain 26 characters, we will need to capture the `<S>` and `<E>` also. So it would be 28 x 28 array.

```{python}
N = torch.zeros((28,28), dtype=torch.int32)
```

This will collect all the characters used in our dataset (join all words to a massive string and pass it to a `set()`, which will remove duplicate). With such a large dataset, all the english characters were used.
```{python}
chars = sorted(list(set(''.join(words))))
len(chars) # 26 

# with index
stoi = {s:i for i,s in enumerate(chars)}
stoi['<S>'] = 26
stoi['<E>'] = 27
stoi
```

```{python}
for w in words:
  chs = ['<S>'] + list(w) + ['<E>']
  for ch1, ch2 in zip(chs, chs[1:]):
    ix1 = stoi[ch1]
    ix2 = stoi[ch2]
    N[ix1, ix2] += 1
```

## visualizing the `bigram` tensor

```{python}
itos = {i:s for s, i in stoi.items()}
```

```{python}
import matplotlib.pyplot as plt
%matplotlib inline

plt.figure(figsize=(16,16))
plt.imshow(N, cmap='Blues')
for i in range(28):
  for j in range(28):
    # plot character strings with number of time
    chstr = itos[i] + itos[j]
    plt.text(j, i, chstr, ha="center", va="bottom", color="gray")
    plt.text(j, i, N[i, j].item(), ha="center", va="top", color="gray")
plt.axis('off')
```

## deleting spurious (S) and (E) tokens in favor of a single `.` token

`<S>`, and `<E>` look a bit annoying. let's replace them by simple `.`.

```{python}
N = torch.zeros((27,27), dtype=torch.int32)
stoi = {s:i+1 for i,s in enumerate(chars)}
stoi['.'] = 0
itos = {i:s for s, i in stoi.items()}

for w in words:
  chs = ['.'] + list(w) + ['.']
  for ch1, ch2 in zip(chs, chs[1:]):
    ix1 = stoi[ch1]
    ix2 = stoi[ch2]
    N[ix1, ix2] += 1
```

```{python}
import matplotlib.pyplot as plt
%matplotlib inline

plt.figure(figsize=(16,16))
plt.imshow(N, cmap='Blues')
for i in range(27):
    for j in range(27):
        chstr = itos[i] + itos[j]
        plt.text(j, i, chstr, ha="center", va="bottom", color='gray')
        plt.text(j, i, N[i, j].item(), ha="center", va="top", color='gray')
plt.axis('off')
```

## sampling from the model

Taking the first column of the array.
```{python}
N[0]
```

Column-wise probability.

```{python}
p = N[0].float()
p = p / p.sum()
p
```

Creating random number with Pytorch generator at a state.

```{python}
g = torch.Generator().manual_seed(2147483647)
p_test = torch.rand(3, generator=g)
p_test = p_test / p_test.sum()
```

```{python}
torch.multinomial(p_test, num_samples=100, replacement=True, generator=g)
```

Now back to our data, generate a tensor with 1 value from the `p` vector, it should return `m` ~ index 13 as it's the most common charactor.

```{python}
g = torch.Generator().manual_seed(2147483647)
ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
itos[ix]
```

Let's automate it:

```{python}
g = torch.Generator().manual_seed(214748367)

ix = 0
while True:
  p = N[ix].float()
  p = p / p.sum()
  ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
  print(itos[ix])
  if ix == 0:
    break
```

And more, joining the last result to single word, and make new 10 names:

```{python}
g = torch.Generator().manual_seed(2147483647)

for i in range(10):
  
  out = []
  ix = 0
  while True:
    p = N[ix].float()
    p = p / p.sum()

    # p = torch.ones(27) / 27.0
    # the result look terrible, but compare to an un-trained model for eg p - uncomment to code above, they are still like names.
    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
    out.append(itos[ix])
    if ix == 0:
      break
  print(''.join(out))
```

## efficiency! vectorized normalization of the rows, tensor broadcasting

We just fetching a row of `N` from the counts matrix, and then always do the same things: converting to float, dividing. That's not efficient! We now will optimize this:

```{python}
P = N.float()
# param 1 helps summing horizontally, by rows
# keepdim keeps the dimension the output is still 2D array with 1 column for each row, not a vertical vector entirely
# tensor already support to broadcast the row sum allowing this dividing (keepdim helped not to mess the broadcast)
P /= P.sum(1, keepdim=True)
# inplace operator instead of P = P / P.sum(1, keepdim=True), take care of memory!
```

```{python}
g = torch.Generator().manual_seed(2147483647)

for i in range(10):
  
  out = []
  ix = 0
  while True:
    p = P[ix]
    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
    out.append(itos[ix])
    if ix == 0:
      break
  print(''.join(out))
```

## loss function (the negative log likelihood of the data under our model)

## model smoothing with fake counts

# PART 2: the neural network approach: intro

## creating the bigram dataset for the neural net

## feeding integers into neural nets? one-hot encodings

## the "neural net": one linear layer of neurons implemented with matrix multiplication

## transforming neural net outputs into probabilities: the softmax

## summary, preview to next steps, reference to micrograd

## vectorized loss

## putting everything together

## note 1: one-hot encoding really just selects a row of the next Linear layer's weight matrix

## note 2: model smoothing as regularization loss

## sampling from the neural net

## conclusion

# resources

1. YouTube video lecture: <https://www.youtube.com/watch?v=PaCmpygFfXo>
2. Jupyter notebook files: <https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part1_bigrams.ipynb>
3. `makemore` Github repo: <https://github.com/karpathy/makemore>