---
title: "NN-Z2H Lesson 5: Building makemore part 5 - Becoming a `WaveNet`"
description: "Become swole doge: build a 2-layer MLP and deep dive to how gradients flow backwards with `cross entropy loss`, 2nd linear layer, `tanh`, `batchnorm`, 1st linear layer, and the `embedding table`."
author:
  - name: "Tuan Le Khac"
    url: https://lktuan.github.io/
categories: [til, python, andrej karpathy, nn-z2h, neural networks] 
date: 12-06-2024
date-modified: 12-06-2024
image: b.jpg
draft: true
format:
  html:
    code-overflow: wrap
    code-tools: true
    code-fold: show
    code-annotations: hover
---

::: {.callout-important title="This is not orginal content!"}
This is my study notes / codes along with Andrej Karpathy's "[Neural Networks: Zero to Hero](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)" series.
:::

# intro
## starter code walkthrough
## let’s fix the learning rate plot
## pytorchifying our code: layers, containers, `torch.nn`, fun bugs
# implementing `WaveNet`
## overview: `WaveNet`
## dataset bump the context size to 8
## re-running baseline code on block_size 8
## implementing WaveNet
## training the WaveNet: first pass
## fixing batchnorm1d bug
## re-training `WaveNet` with bug fix
## caling up our `WaveNet`
# conclusions
## experimental harness
## `WaveNet` but with “dilated causal convolutions”
## `torch.nn`
## the development process of building deep neural nets
## going forward
## improve on my loss! how far can we improve a `WaveNet` on this data?
# resources

1. WaveNet 2016 from DeepMind: <https://arxiv.org/abs/1609.03499>;
2. Bengio et al. 2003 MLP LM: <https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf>;
3. Notebook: <https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part5_cnn1.ipynb>