<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tuan Le Khac">
<meta name="dcterms.date" content="2024-06-16">
<meta name="description" content="This is Tuan‚Äôs blog">

<title>Le Khac Tuan - NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../img/rocket_1613268.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="html/styles.scss">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Le Khac Tuan</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../curriculum/index.html"> 
<span class="menu-text">Curriculum</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Â≠¶Ê±âËØ≠ÁöÑÊó•ËÆ∞.html"> 
<span class="menu-text">Â≠¶Ê±âËØ≠ÁöÑÊó•ËÆ∞</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../jiu_jitsu_journal/index.html"> 
<span class="menu-text">Jiu Jitsu Journal</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lktuan"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/tuanlekhac/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.facebook.com/toilatuan.lk/"> <i class="bi bi-facebook" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/Halle4231"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:tuan.lekhac0905@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                  <div>
        <div class="description">
          backpropagation, from neuron to neural network, from micro grad to pytorch, and more
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">til</div>
                <div class="quarto-category">python</div>
                <div class="quarto-category">andrej karpathy</div>
                <div class="quarto-category">nn-z2h</div>
                <div class="quarto-category">backpropagation</div>
                <div class="quarto-category">neural networks</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://lktuan.github.io/">Tuan Le Khac</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 16, 2024</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">June 18, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#micrograd-from-scratch-yayy" id="toc-micrograd-from-scratch-yayy" class="nav-link active" data-scroll-target="#micrograd-from-scratch-yayy">MicroGrad from scratch Yayy!</a>
  <ul class="collapse">
  <li><a href="#intro-micrograd-overview---what-does-your-neural-network-training-look-like-under-the-hood" id="toc-intro-micrograd-overview---what-does-your-neural-network-training-look-like-under-the-hood" class="nav-link" data-scroll-target="#intro-micrograd-overview---what-does-your-neural-network-training-look-like-under-the-hood">intro &amp; micrograd overview - what does your neural network training look like under the hood?</a></li>
  <li><a href="#derivative-of-a-simple-function-with-one-input" id="toc-derivative-of-a-simple-function-with-one-input" class="nav-link" data-scroll-target="#derivative-of-a-simple-function-with-one-input">derivative of a simple function with one input</a></li>
  <li><a href="#derivative-of-a-function-with-multiple-inputs" id="toc-derivative-of-a-function-with-multiple-inputs" class="nav-link" data-scroll-target="#derivative-of-a-function-with-multiple-inputs">derivative of a function with multiple inputs</a></li>
  <li><a href="#starting-the-core-value-object-of-micrograd-and-its-visualization" id="toc-starting-the-core-value-object-of-micrograd-and-its-visualization" class="nav-link" data-scroll-target="#starting-the-core-value-object-of-micrograd-and-its-visualization">starting the core <code>Value</code> object of micrograd and its visualization</a></li>
  </ul></li>
  <li><a href="#backpropagation" id="toc-backpropagation" class="nav-link" data-scroll-target="#backpropagation">Backpropagation</a>
  <ul class="collapse">
  <li><a href="#manual-backpropagation-example-1-simple-expression" id="toc-manual-backpropagation-example-1-simple-expression" class="nav-link" data-scroll-target="#manual-backpropagation-example-1-simple-expression">manual backpropagation example #1: simple expression</a></li>
  <li><a href="#preview-of-a-single-optimization-step" id="toc-preview-of-a-single-optimization-step" class="nav-link" data-scroll-target="#preview-of-a-single-optimization-step">preview of a single optimization step</a></li>
  <li><a href="#manual-backpropagation-example-2-a-neuron" id="toc-manual-backpropagation-example-2-a-neuron" class="nav-link" data-scroll-target="#manual-backpropagation-example-2-a-neuron">manual backpropagation example #2: a neuron</a></li>
  <li><a href="#implementing-the-backward-function-for-each-operation" id="toc-implementing-the-backward-function-for-each-operation" class="nav-link" data-scroll-target="#implementing-the-backward-function-for-each-operation">implementing the backward function for each operation</a></li>
  <li><a href="#implementing-the-backward-function-for-a-whole-expression-graph" id="toc-implementing-the-backward-function-for-a-whole-expression-graph" class="nav-link" data-scroll-target="#implementing-the-backward-function-for-a-whole-expression-graph">implementing the backward function for a whole expression graph</a></li>
  <li><a href="#fixing-a-backprop-bug-when-one-node-is-used-multiple-times" id="toc-fixing-a-backprop-bug-when-one-node-is-used-multiple-times" class="nav-link" data-scroll-target="#fixing-a-backprop-bug-when-one-node-is-used-multiple-times">fixing a backprop bug when one node is used multiple times ‚õî</a></li>
  <li><a href="#breaking-up-a-tanh-exercising-with-more-operations" id="toc-breaking-up-a-tanh-exercising-with-more-operations" class="nav-link" data-scroll-target="#breaking-up-a-tanh-exercising-with-more-operations">breaking up a <code>tanh</code>, exercising with more operations</a></li>
  </ul></li>
  <li><a href="#pytorch-comparison" id="toc-pytorch-comparison" class="nav-link" data-scroll-target="#pytorch-comparison">PyTorch comparison</a>
  <ul class="collapse">
  <li><a href="#doing-the-same-thing-but-in-pytorch-comparison" id="toc-doing-the-same-thing-but-in-pytorch-comparison" class="nav-link" data-scroll-target="#doing-the-same-thing-but-in-pytorch-comparison">doing the same thing but in PyTorch: comparison</a></li>
  </ul></li>
  <li><a href="#building-the-library" id="toc-building-the-library" class="nav-link" data-scroll-target="#building-the-library">Building the library</a>
  <ul class="collapse">
  <li><a href="#building-out-a-neural-net-library-multi-layer-perceptron-in-micrograd" id="toc-building-out-a-neural-net-library-multi-layer-perceptron-in-micrograd" class="nav-link" data-scroll-target="#building-out-a-neural-net-library-multi-layer-perceptron-in-micrograd">building out a neural net library (multi-layer perceptron) in micrograd</a></li>
  <li><a href="#creating-a-tiny-dataset-writing-the-loss-function" id="toc-creating-a-tiny-dataset-writing-the-loss-function" class="nav-link" data-scroll-target="#creating-a-tiny-dataset-writing-the-loss-function">creating a tiny dataset, writing the loss function</a></li>
  <li><a href="#collecting-all-of-the-parameters-of-the-neural-net" id="toc-collecting-all-of-the-parameters-of-the-neural-net" class="nav-link" data-scroll-target="#collecting-all-of-the-parameters-of-the-neural-net">collecting all of the parameters of the neural net</a></li>
  <li><a href="#doing-gradient-descent-optimization-manually-training-the-network" id="toc-doing-gradient-descent-optimization-manually-training-the-network" class="nav-link" data-scroll-target="#doing-gradient-descent-optimization-manually-training-the-network">doing gradient descent optimization manually, training the network</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a>
  <ul class="collapse">
  <li><a href="#summary-of-what-we-learned-how-to-go-towards-modern-neural-nets" id="toc-summary-of-what-we-learned-how-to-go-towards-modern-neural-nets" class="nav-link" data-scroll-target="#summary-of-what-we-learned-how-to-go-towards-modern-neural-nets">summary of what we learned, how to go towards modern neural nets</a></li>
  <li><a href="#walkthrough-of-the-full-code-of-micrograd-on-github" id="toc-walkthrough-of-the-full-code-of-micrograd-on-github" class="nav-link" data-scroll-target="#walkthrough-of-the-full-code-of-micrograd-on-github">walkthrough of the full code of micrograd on github</a></li>
  <li><a href="#real-stuff-diving-into-pytorch-finding-their-backward-pass-for-tanh" id="toc-real-stuff-diving-into-pytorch-finding-their-backward-pass-for-tanh" class="nav-link" data-scroll-target="#real-stuff-diving-into-pytorch-finding-their-backward-pass-for-tanh">real stuff: diving into PyTorch, finding their backward pass for <code>tanh</code></a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">conclusion</a></li>
  <li><a href="#outtakes" id="toc-outtakes" class="nav-link" data-scroll-target="#outtakes">outtakes :)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<div class="callout callout-style-default callout-important callout-titled" title="This is not orginal content!">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This is not orginal content!
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is my study notes / codes along with Andrej Karpathy‚Äôs ‚Äú<a href="https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Neural Networks: Zero to Hero</a>‚Äù series.</p>
</div>
</div>
<p><strong>Upfront-note</strong>: There are also greate resources in Vietnamese for learning Backpropagation, for e.g.:</p>
<ol type="1">
<li>Blog <a href="https://machinelearningcoban.com/2017/02/24/mlp/">machinelearningcoban</a></li>
<li>Blog <a href="https://dominhhai.github.io/vi/2018/04/nn-bp/">dominhhai</a></li>
</ol>
<section id="micrograd-from-scratch-yayy" class="level1 page-columns page-full">
<h1>MicroGrad from scratch Yayy!</h1>
<p><a href="https://github.com/karpathy/micrograd"><strong>üöÄ MicroGrad repo</strong></a> <a href="https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ"><strong>üî• Video Lecture</strong></a></p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20240217152156/Frame-13.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Backpropagation in Neural Networks, photo credit to <a href="https://www.geeksforgeeks.org/backpropagation-in-neural-network/">GeekforGeek</a></figcaption>
</figure>
</div>
</div>
</div>
</div>
<section id="intro-micrograd-overview---what-does-your-neural-network-training-look-like-under-the-hood" class="level2">
<h2 class="anchored" data-anchor-id="intro-micrograd-overview---what-does-your-neural-network-training-look-like-under-the-hood">intro &amp; micrograd overview - what does your neural network training look like under the hood?</h2>
<p>What is MicroGrad ‚ùì: a tiny <strong>auto-grad</strong> (automatic gradient) engine, implement of <strong>backpropagation</strong> ~ itertively tune the weight of that nn to minimize the loss function -&gt; improve the accuracy of the neural network. Backpropagation will be the mathematical core of any modern deep neutral network like, say <code>pytorch</code>, or <code>jaxx</code>.</p>
<p>Installation: <code>pip install micrograd</code></p>
<p>Example:</p>
<div id="3bb769a3" class="cell" data-execution_count="1">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="annotated-cell-1"><pre class="sourceCode numberSource python code-annotation-code number-lines code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-1-1"><a href="#annotated-cell-1-1"></a><span class="im">from</span> micrograd.engine <span class="im">import</span> Value</span>
<span id="annotated-cell-1-2"><a href="#annotated-cell-1-2"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="1">1</button><span id="annotated-cell-1-3" class="code-annotation-target"><a href="#annotated-cell-1-3"></a>a <span class="op">=</span> Value(<span class="op">-</span><span class="fl">4.0</span>)</span>
<span id="annotated-cell-1-4"><a href="#annotated-cell-1-4"></a>b <span class="op">=</span> Value(<span class="fl">2.0</span>)</span>
<span id="annotated-cell-1-5"><a href="#annotated-cell-1-5"></a>c <span class="op">=</span> a <span class="op">+</span> b</span>
<span id="annotated-cell-1-6"><a href="#annotated-cell-1-6"></a>d <span class="op">=</span> a <span class="op">*</span> b <span class="op">+</span> b<span class="op">**</span><span class="dv">3</span></span>
<span id="annotated-cell-1-7"><a href="#annotated-cell-1-7"></a>c <span class="op">+=</span> c <span class="op">+</span> <span class="dv">1</span></span>
<span id="annotated-cell-1-8"><a href="#annotated-cell-1-8"></a>c <span class="op">+=</span> <span class="dv">1</span> <span class="op">+</span> c <span class="op">+</span> (<span class="op">-</span>a)</span>
<span id="annotated-cell-1-9"><a href="#annotated-cell-1-9"></a>d <span class="op">+=</span> d <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> (b <span class="op">+</span> a).relu()</span>
<span id="annotated-cell-1-10"><a href="#annotated-cell-1-10"></a>d <span class="op">+=</span> <span class="dv">3</span> <span class="op">*</span> d <span class="op">+</span> (b <span class="op">-</span> a).relu()</span>
<span id="annotated-cell-1-11"><a href="#annotated-cell-1-11"></a>e <span class="op">=</span> c <span class="op">-</span> d</span>
<span id="annotated-cell-1-12"><a href="#annotated-cell-1-12"></a>f <span class="op">=</span> e<span class="op">**</span><span class="dv">2</span></span>
<span id="annotated-cell-1-13"><a href="#annotated-cell-1-13"></a>g <span class="op">=</span> f <span class="op">/</span> <span class="fl">2.0</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="2">2</button><span id="annotated-cell-1-14" class="code-annotation-target"><a href="#annotated-cell-1-14"></a>g <span class="op">+=</span> <span class="fl">10.0</span> <span class="op">/</span> f</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="3">3</button><span id="annotated-cell-1-15" class="code-annotation-target"><a href="#annotated-cell-1-15"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>g<span class="sc">.</span>data<span class="sc">:.4f}</span><span class="ss">'</span>) <span class="co"># prints 24.7041, the outcome of this forward pass</span></span>
<span id="annotated-cell-1-16"><a href="#annotated-cell-1-16"></a>g.backward()</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="4">4</button><span id="annotated-cell-1-17" class="code-annotation-target"><a href="#annotated-cell-1-17"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>a<span class="sc">.</span>grad<span class="sc">:.4f}</span><span class="ss">'</span>) <span class="co"># prints 138.8338, i.e. the numerical value of dg/da</span></span>
<span id="annotated-cell-1-18"><a href="#annotated-cell-1-18"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>b<span class="sc">.</span>grad<span class="sc">:.4f}</span><span class="ss">'</span>) <span class="co"># prints 645.5773, i.e. the numerical value of dg/db</span></span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-1" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="3" data-code-annotation="1">Micrograd allows you to build mathematical expressions, in this case <code>a</code> and <code>b</code> are inputs, wrapped in <code>Value</code> object with value equal to <code>-4.0</code> and <code>2.0</code>, respectively.</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="14" data-code-annotation="2"><code>a</code> and <code>b</code> are transformed to <code>c</code>, <code>d</code>, <code>e</code> and eventually <code>f</code>, <code>g</code>. Mathematical operators are implemented, like <code>+</code>, <code>*</code>, <code>**</code>, even <code>relu()</code>.</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="15" data-code-annotation="3"><code>Value</code> object contains <code>data</code>, and <code>grad</code>.</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="17" data-code-annotation="4">Call <code>backpropagation()</code> process.</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>24.7041
138.8338
645.5773</code></pre>
</div>
</div>
</section>
<section id="derivative-of-a-simple-function-with-one-input" class="level2">
<h2 class="anchored" data-anchor-id="derivative-of-a-simple-function-with-one-input">derivative of a simple function with one input</h2>
<p>‚ùìWhat exactly is derivative‚ùì</p>
<div id="f98fcd6d" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> math</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>A simple quadratic function:</p>
<div id="dcd4ec3d" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="cf">return</span> <span class="dv">3</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">4</span><span class="op">*</span>x <span class="op">+</span> <span class="dv">5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="188ae8df" class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>f(<span class="fl">3.0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>20.0</code></pre>
</div>
</div>
<p>Input also can be an array, we can plot it for visibility.</p>
<div id="c9b0bd52" class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>xs <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="fl">0.25</span>)</span>
<span id="cb6-2"><a href="#cb6-2"></a>ys <span class="op">=</span> f(xs)</span>
<span id="cb6-3"><a href="#cb6-3"></a>plt.plot(xs, ys)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-6-output-1.png" width="575" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>If we bump up a litle value <code>h</code> of <code>x</code>, how <code>f(x)</code> will response?</p>
<div id="89fe64f9" class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="annotated-cell-6"><pre class="sourceCode numberSource python code-annotation-code number-lines code-with-copy code-annotated"><code class="sourceCode python"><button class="code-annotation-anchor" data-target-cell="annotated-cell-6" data-target-annotation="1">1</button><span id="annotated-cell-6-1" class="code-annotation-target"><a href="#annotated-cell-6-1"></a>h <span class="op">=</span> <span class="fl">0.000000000001</span></span>
<span id="annotated-cell-6-2"><a href="#annotated-cell-6-2"></a>x <span class="op">=</span> <span class="fl">3.0</span></span>
<span id="annotated-cell-6-3"><a href="#annotated-cell-6-3"></a>( f(x<span class="op">+</span>h) <span class="op">-</span> f(x) ) <span class="op">/</span> h</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-6" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-6" data-code-lines="1" data-code-annotation="1">Change the value of <code>h</code> from <code>0.0001</code> to be <code>0.00000...0001</code> -&gt; the slope value comes to <code>14</code> (at the value of <code>3.0</code> of <code>x</code>).</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>14.001244608152774</code></pre>
</div>
</div>
<p>Try for <code>x = -3.0</code>, <code>x = 5.0</code>, we get different values of the slope, for <code>x = 2/3</code>, the slope is zero. Let‚Äôs get more complex.</p>
</section>
<section id="derivative-of-a-function-with-multiple-inputs" class="level2">
<h2 class="anchored" data-anchor-id="derivative-of-a-function-with-multiple-inputs">derivative of a function with multiple inputs</h2>
<div id="d477f02f" class="cell" data-execution_count="7">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>a <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb8-2"><a href="#cb8-2"></a>b <span class="op">=</span> <span class="op">-</span><span class="fl">3.0</span></span>
<span id="cb8-3"><a href="#cb8-3"></a>c <span class="op">=</span> <span class="fl">10.0</span></span>
<span id="cb8-4"><a href="#cb8-4"></a>d <span class="op">=</span> a<span class="op">*</span>b <span class="op">+</span> c</span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="bu">print</span>(d)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>4.0</code></pre>
</div>
</div>
<p>Put our bump-up element to this multi-variables function:</p>
<div id="acf9bf22" class="cell" data-execution_count="8">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="annotated-cell-8"><pre class="sourceCode numberSource python code-annotation-code number-lines code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-8-1"><a href="#annotated-cell-8-1"></a>h <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="annotated-cell-8-2"><a href="#annotated-cell-8-2"></a></span>
<span id="annotated-cell-8-3"><a href="#annotated-cell-8-3"></a><span class="co"># input</span></span>
<span id="annotated-cell-8-4"><a href="#annotated-cell-8-4"></a>a <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="annotated-cell-8-5"><a href="#annotated-cell-8-5"></a>b <span class="op">=</span> <span class="op">-</span><span class="fl">3.0</span></span>
<span id="annotated-cell-8-6"><a href="#annotated-cell-8-6"></a>c <span class="op">=</span> <span class="fl">10.0</span></span>
<span id="annotated-cell-8-7"><a href="#annotated-cell-8-7"></a></span>
<span id="annotated-cell-8-8"><a href="#annotated-cell-8-8"></a>d1 <span class="op">=</span> a<span class="op">*</span>b <span class="op">+</span> c</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="1">1</button><span id="annotated-cell-8-9" class="code-annotation-target"><a href="#annotated-cell-8-9"></a>a <span class="op">+=</span> h</span>
<span id="annotated-cell-8-10"><a href="#annotated-cell-8-10"></a>d2 <span class="op">=</span> a<span class="op">*</span>b <span class="op">+</span> c</span>
<span id="annotated-cell-8-11"><a href="#annotated-cell-8-11"></a></span>
<span id="annotated-cell-8-12"><a href="#annotated-cell-8-12"></a><span class="bu">print</span>(<span class="st">'d1: '</span>, d1)</span>
<span id="annotated-cell-8-13"><a href="#annotated-cell-8-13"></a><span class="bu">print</span>(<span class="st">'d2: '</span>, d2)</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="2">2</button><span id="annotated-cell-8-14" class="code-annotation-target"><a href="#annotated-cell-8-14"></a><span class="bu">print</span>(<span class="st">'slope: '</span>, (d2 <span class="op">-</span> d1)<span class="op">/</span>h)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-8" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="9" data-code-annotation="1">Do the same for <code>b</code>, <code>c</code>, we‚Äôll get different slopes.</span>
</dd>
<dt data-target-cell="annotated-cell-8" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="14" data-code-annotation="2">We say given <code>b = -3.0</code> and <code>c = 10.0</code> are constants, the derivative of <code>d</code> at <code>a = 2.0</code> is <code>-3.0</code>. The rate of which <code>d</code> will increase if we scale <code>a</code>!</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>d1:  4.0
d2:  3.997
slope:  -3.0000000000001137</code></pre>
</div>
</div>
</section>
<section id="starting-the-core-value-object-of-micrograd-and-its-visualization" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="starting-the-core-value-object-of-micrograd-and-its-visualization">starting the core <code>Value</code> object of micrograd and its visualization</h2>
<p>So we now have some intuitive sense of what is derivative is telling you about the function. We now move to the Neural Networks, which would be massive mathematical expressions. We need some data structures that maintain these expressions, we first declare an object <code>Value</code> that holds data.</p>
<div id="d87d8432" class="cell" data-execution_count="9">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="annotated-cell-9"><pre class="sourceCode numberSource python code-annotation-code number-lines code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-9-1"><a href="#annotated-cell-9-1"></a><span class="kw">class</span> Value:</span>
<span id="annotated-cell-9-2"><a href="#annotated-cell-9-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, data, </span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-9" data-target-annotation="3">3</button><span id="annotated-cell-9-3" class="code-annotation-target"><a href="#annotated-cell-9-3"></a>                        _children<span class="op">=</span>(),</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-9" data-target-annotation="5">5</button><span id="annotated-cell-9-4" class="code-annotation-target"><a href="#annotated-cell-9-4"></a>                        _op <span class="op">=</span> <span class="st">''</span>,</span>
<span id="annotated-cell-9-5"><a href="#annotated-cell-9-5"></a>                        label <span class="op">=</span> <span class="st">''</span></span>
<span id="annotated-cell-9-6"><a href="#annotated-cell-9-6"></a>                        ): </span>
<span id="annotated-cell-9-7"><a href="#annotated-cell-9-7"></a>        <span class="va">self</span>.data <span class="op">=</span> data</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-9" data-target-annotation="6">6</button><span id="annotated-cell-9-8" class="code-annotation-target"><a href="#annotated-cell-9-8"></a>        <span class="va">self</span>.grad <span class="op">=</span> <span class="fl">0.0</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-9" data-target-annotation="7">7</button><span id="annotated-cell-9-9" class="code-annotation-target"><a href="#annotated-cell-9-9"></a>        <span class="va">self</span>._backward <span class="op">=</span> <span class="kw">lambda</span>: <span class="va">None</span></span>
<span id="annotated-cell-9-10"><a href="#annotated-cell-9-10"></a>        <span class="va">self</span>._prev <span class="op">=</span> <span class="bu">set</span>(_children)</span>
<span id="annotated-cell-9-11"><a href="#annotated-cell-9-11"></a>        <span class="va">self</span>._op <span class="op">=</span> _op</span>
<span id="annotated-cell-9-12"><a href="#annotated-cell-9-12"></a>        <span class="va">self</span>.label <span class="op">=</span>  label</span>
<span id="annotated-cell-9-13"><a href="#annotated-cell-9-13"></a></span>
<span id="annotated-cell-9-14"><a href="#annotated-cell-9-14"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">str</span>: <span class="co"># a nicer looking for class attributes</span></span>
<span id="annotated-cell-9-15"><a href="#annotated-cell-9-15"></a>        <span class="cf">return</span> <span class="ss">f"Value(data=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>data<span class="sc">}</span><span class="ss">)"</span></span>
<span id="annotated-cell-9-16"><a href="#annotated-cell-9-16"></a>    </span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-9" data-target-annotation="4">4</button><span id="annotated-cell-9-17" class="code-annotation-target"><a href="#annotated-cell-9-17"></a>    <span class="kw">def</span> <span class="fu">__add__</span>(<span class="va">self</span>, other):</span>
<span id="annotated-cell-9-18"><a href="#annotated-cell-9-18"></a>        other <span class="op">=</span> other <span class="cf">if</span> <span class="bu">isinstance</span>(other, Value) <span class="cf">else</span> Value(other) <span class="co"># turn other to Value object before calculation</span></span>
<span id="annotated-cell-9-19"><a href="#annotated-cell-9-19"></a>        out <span class="op">=</span> Value(<span class="va">self</span>.data <span class="op">+</span> other.data, (<span class="va">self</span>, other), <span class="st">'+'</span>)</span>
<span id="annotated-cell-9-20"><a href="#annotated-cell-9-20"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-9" data-target-annotation="8">8</button><span id="annotated-cell-9-21" class="code-annotation-target"><a href="#annotated-cell-9-21"></a>        <span class="kw">def</span> _backward():</span>
<span id="annotated-cell-9-22"><a href="#annotated-cell-9-22"></a>            <span class="va">self</span>.grad <span class="op">+=</span> <span class="fl">1.0</span> <span class="op">*</span> out.grad</span>
<span id="annotated-cell-9-23"><a href="#annotated-cell-9-23"></a>            other.grad <span class="op">+=</span> <span class="fl">1.0</span> <span class="op">*</span> out.grad</span>
<span id="annotated-cell-9-24"><a href="#annotated-cell-9-24"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="annotated-cell-9-25"><a href="#annotated-cell-9-25"></a></span>
<span id="annotated-cell-9-26"><a href="#annotated-cell-9-26"></a>        <span class="cf">return</span> out</span>
<span id="annotated-cell-9-27"><a href="#annotated-cell-9-27"></a></span>
<span id="annotated-cell-9-28"><a href="#annotated-cell-9-28"></a>    <span class="kw">def</span> <span class="fu">__mul__</span>(<span class="va">self</span>, other):</span>
<span id="annotated-cell-9-29"><a href="#annotated-cell-9-29"></a>        other <span class="op">=</span> other <span class="cf">if</span> <span class="bu">isinstance</span>(other, Value) <span class="cf">else</span> Value(other) <span class="co"># turn other to Value object before calculation</span></span>
<span id="annotated-cell-9-30"><a href="#annotated-cell-9-30"></a>        out <span class="op">=</span> Value(<span class="va">self</span>.data <span class="op">*</span> other.data, (<span class="va">self</span>, other), <span class="st">'*'</span>)</span>
<span id="annotated-cell-9-31"><a href="#annotated-cell-9-31"></a></span>
<span id="annotated-cell-9-32"><a href="#annotated-cell-9-32"></a>        <span class="kw">def</span> _backward():</span>
<span id="annotated-cell-9-33"><a href="#annotated-cell-9-33"></a>            <span class="va">self</span>.grad <span class="op">+=</span> other.data <span class="op">*</span> out.grad</span>
<span id="annotated-cell-9-34"><a href="#annotated-cell-9-34"></a>            other.grad <span class="op">+=</span> <span class="va">self</span>.data <span class="op">*</span> out.grad</span>
<span id="annotated-cell-9-35"><a href="#annotated-cell-9-35"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="annotated-cell-9-36"><a href="#annotated-cell-9-36"></a></span>
<span id="annotated-cell-9-37"><a href="#annotated-cell-9-37"></a>        <span class="cf">return</span> out</span>
<span id="annotated-cell-9-38"><a href="#annotated-cell-9-38"></a></span>
<span id="annotated-cell-9-39"><a href="#annotated-cell-9-39"></a>    <span class="kw">def</span> tanh(<span class="va">self</span>):</span>
<span id="annotated-cell-9-40"><a href="#annotated-cell-9-40"></a>        x <span class="op">=</span> <span class="va">self</span>.data</span>
<span id="annotated-cell-9-41"><a href="#annotated-cell-9-41"></a>        t <span class="op">=</span> (math.exp(<span class="dv">2</span><span class="op">*</span>x) <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> (math.exp(<span class="dv">2</span><span class="op">*</span>x) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="annotated-cell-9-42"><a href="#annotated-cell-9-42"></a>        out <span class="op">=</span> Value(t, (<span class="va">self</span>, ), <span class="st">'tanh'</span>)</span>
<span id="annotated-cell-9-43"><a href="#annotated-cell-9-43"></a></span>
<span id="annotated-cell-9-44"><a href="#annotated-cell-9-44"></a>        <span class="kw">def</span> _backward():</span>
<span id="annotated-cell-9-45"><a href="#annotated-cell-9-45"></a>            <span class="va">self</span>.grad <span class="op">+=</span> (<span class="dv">1</span> <span class="op">-</span> t<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> out.grad</span>
<span id="annotated-cell-9-46"><a href="#annotated-cell-9-46"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="annotated-cell-9-47"><a href="#annotated-cell-9-47"></a>        <span class="cf">return</span> out</span>
<span id="annotated-cell-9-48"><a href="#annotated-cell-9-48"></a></span>
<span id="annotated-cell-9-49"><a href="#annotated-cell-9-49"></a>    <span class="kw">def</span> backward(<span class="va">self</span>):</span>
<span id="annotated-cell-9-50"><a href="#annotated-cell-9-50"></a>        </span>
<span id="annotated-cell-9-51"><a href="#annotated-cell-9-51"></a>        <span class="co"># topo order for all children in the graph</span></span>
<span id="annotated-cell-9-52"><a href="#annotated-cell-9-52"></a>        topo <span class="op">=</span> []</span>
<span id="annotated-cell-9-53"><a href="#annotated-cell-9-53"></a>        visited <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="annotated-cell-9-54"><a href="#annotated-cell-9-54"></a>        <span class="kw">def</span> build_topo(v):</span>
<span id="annotated-cell-9-55"><a href="#annotated-cell-9-55"></a>            <span class="cf">if</span> v <span class="kw">not</span> <span class="kw">in</span> visited:</span>
<span id="annotated-cell-9-56"><a href="#annotated-cell-9-56"></a>                visited.add(v)</span>
<span id="annotated-cell-9-57"><a href="#annotated-cell-9-57"></a>                <span class="cf">for</span> child <span class="kw">in</span> v._prev:</span>
<span id="annotated-cell-9-58"><a href="#annotated-cell-9-58"></a>                    build_topo(child) </span>
<span id="annotated-cell-9-59"><a href="#annotated-cell-9-59"></a>                topo.append(v)</span>
<span id="annotated-cell-9-60"><a href="#annotated-cell-9-60"></a>        build_topo(<span class="va">self</span>)</span>
<span id="annotated-cell-9-61"><a href="#annotated-cell-9-61"></a></span>
<span id="annotated-cell-9-62"><a href="#annotated-cell-9-62"></a>        <span class="co"># sequentially apply the chain rules</span></span>
<span id="annotated-cell-9-63"><a href="#annotated-cell-9-63"></a>        <span class="va">self</span>.grad <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="annotated-cell-9-64"><a href="#annotated-cell-9-64"></a>        <span class="cf">for</span> node <span class="kw">in</span> <span class="bu">reversed</span>(topo):</span>
<span id="annotated-cell-9-65"><a href="#annotated-cell-9-65"></a>            node._backward()</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-9" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-9" data-code-lines="3" data-code-annotation="3">the connective tissue of this expression. We want to keep these expression graphs, so we need to know and keep pointers about what values produce what other values. <code>_children</code> is by default a empty tuple.</span>
</dd>
<dt data-target-cell="annotated-cell-9" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-9" data-code-lines="17" data-code-annotation="4">as we added <code>_children</code>, we also need to point out the father - children relationship in method <code>__add__</code> and <code>__mul__</code> as well.</span>
</dd>
<dt data-target-cell="annotated-cell-9" data-target-annotation="5">5</dt>
<dd>
<span data-code-cell="annotated-cell-9" data-code-lines="4" data-code-annotation="5">we want to know the <strong>operation</strong> between father and child, <code>_op</code> is empty string by default, the value <code>+</code> and <code>-</code> will be added to the operator method respectively.</span>
</dd>
<dt data-target-cell="annotated-cell-9" data-target-annotation="6">6</dt>
<dd>
<span data-code-cell="annotated-cell-9" data-code-lines="8" data-code-annotation="6">initially assume that node has no impact to the output.</span>
</dd>
<dt data-target-cell="annotated-cell-9" data-target-annotation="7">7</dt>
<dd>
<span data-code-cell="annotated-cell-9" data-code-lines="9" data-code-annotation="7">this backward function basically do nothing at the initial.</span>
</dd>
<dt data-target-cell="annotated-cell-9" data-target-annotation="8">8</dt>
<dd>
<span data-code-cell="annotated-cell-9" data-code-lines="21" data-code-annotation="8">implement of backward pass for plus node, <code>+=</code> represent the accumulate action (rather than overwrite it), assigne the gradient behaviour for each type of operation, call the <code>_backward</code> concurrently with function.</span>
</dd>
</dl>
</div>
</div>
<p>Setting input and expression:</p>
<div id="7f26cf0e" class="cell" data-execution_count="10">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="annotated-cell-10"><pre class="sourceCode numberSource python code-annotation-code number-lines code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-10-1"><a href="#annotated-cell-10-1"></a>a <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'a'</span>)</span>
<span id="annotated-cell-10-2"><a href="#annotated-cell-10-2"></a>b <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="annotated-cell-10-3"><a href="#annotated-cell-10-3"></a>c <span class="op">=</span> Value(<span class="fl">10.0</span>, label<span class="op">=</span><span class="st">'c'</span>)</span>
<span id="annotated-cell-10-4"><a href="#annotated-cell-10-4"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-10" data-target-annotation="1">1</button><span id="annotated-cell-10-5" class="code-annotation-target"><a href="#annotated-cell-10-5"></a>a <span class="op">+</span> b</span>
<span id="annotated-cell-10-6"><a href="#annotated-cell-10-6"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-10" data-target-annotation="2">2</button><span id="annotated-cell-10-7" class="code-annotation-target"><a href="#annotated-cell-10-7"></a>a<span class="op">*</span>b <span class="op">+</span> c</span>
<span id="annotated-cell-10-8"><a href="#annotated-cell-10-8"></a></span>
<span id="annotated-cell-10-9"><a href="#annotated-cell-10-9"></a><span class="co"># d = a*b + c rewrite the expression</span></span>
<span id="annotated-cell-10-10"><a href="#annotated-cell-10-10"></a>e <span class="op">=</span> a<span class="op">*</span>b<span class="op">;</span> e.label <span class="op">=</span> <span class="st">'e'</span></span>
<span id="annotated-cell-10-11"><a href="#annotated-cell-10-11"></a>d <span class="op">=</span> e <span class="op">+</span> c<span class="op">;</span> d.label <span class="op">=</span> <span class="st">'d'</span></span>
<span id="annotated-cell-10-12"><a href="#annotated-cell-10-12"></a><span class="co"># d</span></span>
<span id="annotated-cell-10-13"><a href="#annotated-cell-10-13"></a>f <span class="op">=</span> Value(<span class="op">-</span><span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'f'</span>)</span>
<span id="annotated-cell-10-14"><a href="#annotated-cell-10-14"></a>L <span class="op">=</span> d <span class="op">*</span> f<span class="op">;</span> L.label <span class="op">=</span> <span class="st">'L'</span></span>
<span id="annotated-cell-10-15"><a href="#annotated-cell-10-15"></a>L</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-10" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-10" data-code-lines="5" data-code-annotation="1">which will internally call <code>a.__add__(b)</code></span>
</dd>
<dt data-target-cell="annotated-cell-10" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-10" data-code-lines="7" data-code-annotation="2">which will internally call <code>(a.__mul__(b)).__add__(c)</code></span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>Value(data=-8.0)</code></pre>
</div>
</div>
<p>So that we can know the children:</p>
<div id="ed1ed56a" class="cell" data-execution_count="11">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>d._prev</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>{Value(data=-6.0), Value(data=10.0)}</code></pre>
</div>
</div>
<p>We can know the operations:</p>
<div id="f973e63a" class="cell" data-execution_count="12">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>d._op</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>'+'</code></pre>
</div>
</div>
<p>Now we know exactly how each value came to be by <strong>word</strong> expression and from what other values. These will be quite abit larger, so we need a way to nicely visualize these expressions that we‚Äôre building out. Below are a-little-scary codes.</p>
<div id="e726ddd4" class="cell" data-execution_count="13">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="annotated-cell-13"><pre class="sourceCode numberSource python code-annotation-code number-lines code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-13-1"><a href="#annotated-cell-13-1"></a><span class="im">import</span> os</span>
<span id="annotated-cell-13-2"><a href="#annotated-cell-13-2"></a></span>
<span id="annotated-cell-13-3"><a href="#annotated-cell-13-3"></a><span class="co"># Assuming the Graphviz bin directory path is 'C:/Program Files (x86)/Graphviz2.xx/bin'</span></span>
<span id="annotated-cell-13-4"><a href="#annotated-cell-13-4"></a>os.environ[<span class="st">"PATH"</span>] <span class="op">+=</span> os.pathsep <span class="op">+</span> <span class="st">'C:/Program Files (x86)/Graphviz/bin'</span> <span class="co"># add with the code, Gemini instructed me this üò™</span></span>
<span id="annotated-cell-13-5"><a href="#annotated-cell-13-5"></a></span>
<span id="annotated-cell-13-6"><a href="#annotated-cell-13-6"></a><span class="im">from</span> graphviz <span class="im">import</span> Digraph</span>
<span id="annotated-cell-13-7"><a href="#annotated-cell-13-7"></a></span>
<span id="annotated-cell-13-8"><a href="#annotated-cell-13-8"></a><span class="kw">def</span> trace(root):</span>
<span id="annotated-cell-13-9"><a href="#annotated-cell-13-9"></a>    <span class="co"># build a set of all nodes and edges in a graph</span></span>
<span id="annotated-cell-13-10"><a href="#annotated-cell-13-10"></a>    nodes, edges <span class="op">=</span> <span class="bu">set</span>(), <span class="bu">set</span>()</span>
<span id="annotated-cell-13-11"><a href="#annotated-cell-13-11"></a>    <span class="kw">def</span> build(v):</span>
<span id="annotated-cell-13-12"><a href="#annotated-cell-13-12"></a>        <span class="cf">if</span> v <span class="kw">not</span> <span class="kw">in</span> nodes:</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-13" data-target-annotation="1">1</button><span id="annotated-cell-13-13" class="code-annotation-target"><a href="#annotated-cell-13-13"></a>            nodes.add(v)</span>
<span id="annotated-cell-13-14"><a href="#annotated-cell-13-14"></a>            <span class="cf">for</span> child <span class="kw">in</span> v._prev:</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-13" data-target-annotation="2">2</button><span id="annotated-cell-13-15" class="code-annotation-target"><a href="#annotated-cell-13-15"></a>                edges.add((child, v))</span>
<span id="annotated-cell-13-16"><a href="#annotated-cell-13-16"></a>                build(child)</span>
<span id="annotated-cell-13-17"><a href="#annotated-cell-13-17"></a>    build(root)</span>
<span id="annotated-cell-13-18"><a href="#annotated-cell-13-18"></a></span>
<span id="annotated-cell-13-19"><a href="#annotated-cell-13-19"></a>    <span class="cf">return</span> nodes, edges</span>
<span id="annotated-cell-13-20"><a href="#annotated-cell-13-20"></a></span>
<span id="annotated-cell-13-21"><a href="#annotated-cell-13-21"></a><span class="kw">def</span> draw_dot(root):</span>
<span id="annotated-cell-13-22"><a href="#annotated-cell-13-22"></a>    dot <span class="op">=</span> Digraph(<span class="bu">format</span><span class="op">=</span><span class="st">'svg'</span>, graph_attr<span class="op">=</span>{<span class="st">'rankdir'</span>: <span class="st">'LR'</span>}) <span class="co"># LR = from left to right</span></span>
<span id="annotated-cell-13-23"><a href="#annotated-cell-13-23"></a>    nodes, edges <span class="op">=</span> trace(root)</span>
<span id="annotated-cell-13-24"><a href="#annotated-cell-13-24"></a>    <span class="cf">for</span> n <span class="kw">in</span> nodes:</span>
<span id="annotated-cell-13-25"><a href="#annotated-cell-13-25"></a>        uid <span class="op">=</span> <span class="bu">str</span>(<span class="bu">id</span>(n))</span>
<span id="annotated-cell-13-26"><a href="#annotated-cell-13-26"></a>        <span class="co"># for any value in the graph, create a rectangular ('record') node for it</span></span>
<span id="annotated-cell-13-27"><a href="#annotated-cell-13-27"></a>        dot.node(name<span class="op">=</span>uid, label<span class="op">=</span><span class="st">"{ </span><span class="sc">%s</span><span class="st"> | data </span><span class="sc">%.4f</span><span class="st"> | grad </span><span class="sc">%.4f</span><span class="st">}"</span> <span class="op">%</span> (n.label, n.data, n.grad), shape<span class="op">=</span><span class="st">'record'</span>) <span class="co"># why is (n.data, ), but not (n.data) ???</span></span>
<span id="annotated-cell-13-28"><a href="#annotated-cell-13-28"></a>        <span class="cf">if</span> n._op:</span>
<span id="annotated-cell-13-29"><a href="#annotated-cell-13-29"></a>            <span class="co"># if this value is a result of some operations, create an op node for it</span></span>
<span id="annotated-cell-13-30"><a href="#annotated-cell-13-30"></a>            dot.node(name <span class="op">=</span> uid <span class="op">+</span> n._op, label <span class="op">=</span> n._op)</span>
<span id="annotated-cell-13-31"><a href="#annotated-cell-13-31"></a>            <span class="co"># and connect the node to it</span></span>
<span id="annotated-cell-13-32"><a href="#annotated-cell-13-32"></a>            dot.edge(uid <span class="op">+</span> n._op, uid)</span>
<span id="annotated-cell-13-33"><a href="#annotated-cell-13-33"></a></span>
<span id="annotated-cell-13-34"><a href="#annotated-cell-13-34"></a>    <span class="cf">for</span> n1, n2 <span class="kw">in</span> edges:</span>
<span id="annotated-cell-13-35"><a href="#annotated-cell-13-35"></a>        <span class="co"># connect n1 to the op node of n2</span></span>
<span id="annotated-cell-13-36"><a href="#annotated-cell-13-36"></a>        dot.edge(<span class="bu">str</span>(<span class="bu">id</span>(n1)), <span class="bu">str</span>(<span class="bu">id</span>(n2)) <span class="op">+</span> n2._op)</span>
<span id="annotated-cell-13-37"><a href="#annotated-cell-13-37"></a></span>
<span id="annotated-cell-13-38"><a href="#annotated-cell-13-38"></a>    <span class="cf">return</span> dot</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-13" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-13" data-code-lines="13" data-code-annotation="1">This will collect all nodes to the <code>nodes</code>.</span>
</dd>
<dt data-target-cell="annotated-cell-13" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-13" data-code-lines="15" data-code-annotation="2">This will iteratively recursively collect all nodes to the <code>nodes</code>, add child and node ralationship information to <code>edges</code>.</span>
</dd>
</dl>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>Remember to let <a href="https://graphviz.org/download/">graphviz</a> installed on your machine, not only Python package, I also run this:</p>
<div id="a3251e5c" class="cell" data-execution_count="14">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="im">import</span> os</span>
<span id="cb16-2"><a href="#cb16-2"></a>os.environ[<span class="st">"PATH"</span>] <span class="op">+=</span> os.pathsep <span class="op">+</span> <span class="st">'C:\Program Files (x86)\Graphviz</span><span class="ch">\b</span><span class="st">in\dot.exe'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</div></div><p>Now we can draw üöÄ.</p>
<div id="b4d58613" class="cell" data-execution_count="15">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>draw_dot(d)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="14">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-16-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>So far we‚Äôve build out mathematical expressions using only plus <code>+</code> and times <code>*</code>, all <code>Value</code>s are only scalar.</p>
<p>Back to the <code>Value</code> object, we will create 1 more attribute call <code>label</code>, make the expression more complicated by adding intermediate value <code>f</code>, <code>d</code>, out final node will be capital <code>L</code>.</p>
</section>
</section>
<section id="backpropagation" class="level1 page-columns page-full">
<h1>Backpropagation</h1>
<p>In backpropagation, we start at the end and are going to reverse and calculate the gradients along all the intermediate values. What we are actually computing for evert single node here is derivative of that node with respect to <code>L</code>.</p>
<p>In neural nets, <code>L</code> represent to a Loss function. And you will be very interested in the derivative of bassically loss function <code>L</code> with respect to the <strong>weights</strong> of the neural networks.</p>
<p>We need to know how are those <strong>leaf nodes</strong> <code>a</code>, <code>b</code>, <code>c</code>, <code>f</code> are impacting to the loss function. We call it <code>grad</code> and add this attribute to the <code>Value</code> object.</p>
<section id="manual-backpropagation-example-1-simple-expression" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="manual-backpropagation-example-1-simple-expression">manual backpropagation example #1: simple expression</h2>
<div class="page-columns page-full">
<div id="0000f751" class="cell page-columns page-full" data-execution_count="16">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>draw_dot(L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display page-columns page-full" data-execution_count="15">
<div class="page-columns page-full">
<figure class="figure page-columns page-full">
<p class="page-columns page-full"><img src="index_files/figure-html/cell-17-output-1.svg" class="img-fluid figure-img column-page"></p>
</figure>
</div>
</div>
</div>
</div>
<p>Let‚Äôs do the backpropagation <strong>manually</strong>:</p>
<ol type="1">
<li>First we need to calculate the <code>dL/dL</code>, how <code>L</code> will response if we change <code>L</code> a tiny value <code>h</code>. The response simply is <code>1</code> so <code>L.grad = 1.0</code>.</li>
<li><code>F = d * f</code>, so <code>dL/dd</code> -&gt; <code>(f((x+h)) - f(x))/h = ((d+h)*f - d*f)/h = h*f/h = f = -2.0</code>. Quite straighforward, so <code>d.grad = -2.0</code>.</li>
<li>Similarly, <code>f.grad = d = 4</code>.<br>
</li>
<li>Next, for <code>dL/dc</code>. We first concern <code>dd/dc</code>, we know <code>d = c + e</code>. Same with (2) we will soon know <code>dd/dc = 1.0</code>, by symmetry <code>dd/de = 1.0</code>. Following the <strong>Chain Rules</strong> <span class="math inline">\(h'(x) = f'(g(x))g'(x)\)</span>, we have <code>dL/dc = dL/dd * dd/dc = -2.0 * 1 = -2.0</code>. </li>
<li>By symmetry, <code>dL/de = -2.0</code>.</li>
<li><code>dL/da = dL/de * de/da = -2.0 * b = -2.0 * -3.0 = 6.0</code>.</li>
<li><code>dl/db = dL/de * de/db = -2.0 * a = -2.0 * 2.0 = -4.0</code>.</li>
</ol>
<div class="no-row-height column-margin column-container"><span class="margin-aside"><a href="https://en.wikipedia.org/wiki/Chain_rule">Chain Rules Wiki</a></span></div><p>We can also create a function for playing around / gradient check, and not messing up the global scope.</p>
<div id="c7194503" class="cell" data-execution_count="17">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="kw">def</span> lol():</span>
<span id="cb19-2"><a href="#cb19-2"></a></span>
<span id="cb19-3"><a href="#cb19-3"></a>    h <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb19-4"><a href="#cb19-4"></a></span>
<span id="cb19-5"><a href="#cb19-5"></a>    a <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'a'</span>)</span>
<span id="cb19-6"><a href="#cb19-6"></a>    b <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb19-7"><a href="#cb19-7"></a>    c <span class="op">=</span> Value(<span class="fl">10.0</span>, label<span class="op">=</span><span class="st">'c'</span>)</span>
<span id="cb19-8"><a href="#cb19-8"></a>    e <span class="op">=</span> a<span class="op">*</span>b<span class="op">;</span> e.label <span class="op">=</span> <span class="st">'e'</span></span>
<span id="cb19-9"><a href="#cb19-9"></a>    d <span class="op">=</span> e <span class="op">+</span> c<span class="op">;</span> d.label <span class="op">=</span> <span class="st">'d'</span></span>
<span id="cb19-10"><a href="#cb19-10"></a>    f <span class="op">=</span> Value(<span class="op">-</span><span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'f'</span>)</span>
<span id="cb19-11"><a href="#cb19-11"></a>    L <span class="op">=</span> d <span class="op">*</span> f<span class="op">;</span> L.label <span class="op">=</span> <span class="st">'L'</span></span>
<span id="cb19-12"><a href="#cb19-12"></a>    L1 <span class="op">=</span> L.data</span>
<span id="cb19-13"><a href="#cb19-13"></a></span>
<span id="cb19-14"><a href="#cb19-14"></a>    a <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'a'</span>)</span>
<span id="cb19-15"><a href="#cb19-15"></a>    b <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb19-16"><a href="#cb19-16"></a>    c <span class="op">=</span> Value(<span class="fl">10.0</span>, label<span class="op">=</span><span class="st">'c'</span>)</span>
<span id="cb19-17"><a href="#cb19-17"></a>    c.data <span class="op">+=</span> h <span class="co"># dL/dc = -2.0</span></span>
<span id="cb19-18"><a href="#cb19-18"></a>    e <span class="op">=</span> a<span class="op">*</span>b<span class="op">;</span> e.label <span class="op">=</span> <span class="st">'e'</span></span>
<span id="cb19-19"><a href="#cb19-19"></a>    d <span class="op">=</span> e <span class="op">+</span> c<span class="op">;</span> d.label <span class="op">=</span> <span class="st">'d'</span></span>
<span id="cb19-20"><a href="#cb19-20"></a>    <span class="co"># d.data += h # dL/dd = -2.0</span></span>
<span id="cb19-21"><a href="#cb19-21"></a>    f <span class="op">=</span> Value(<span class="op">-</span><span class="fl">2.0</span> <span class="co"># + h # dL/df = 4.0</span></span>
<span id="cb19-22"><a href="#cb19-22"></a>                , label<span class="op">=</span><span class="st">'f'</span>) </span>
<span id="cb19-23"><a href="#cb19-23"></a>    L <span class="op">=</span> d <span class="op">*</span> f<span class="op">;</span> L.label <span class="op">=</span> <span class="st">'L'</span></span>
<span id="cb19-24"><a href="#cb19-24"></a>    L2 <span class="op">=</span> L.data <span class="co"># + h # dL/dL = 1.0</span></span>
<span id="cb19-25"><a href="#cb19-25"></a></span>
<span id="cb19-26"><a href="#cb19-26"></a>    <span class="bu">print</span>((L2 <span class="op">-</span> L1) <span class="op">/</span> h)</span>
<span id="cb19-27"><a href="#cb19-27"></a></span>
<span id="cb19-28"><a href="#cb19-28"></a>lol()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>-1.9999999999953388</code></pre>
</div>
</div>
<p>So that is backpropagation ~ just recursively applying the Chain Rules, multiplying local derivatives.</p>
</section>
<section id="preview-of-a-single-optimization-step" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="preview-of-a-single-optimization-step">preview of a single optimization step</h2>
<p>We can change the input that we can control <code>a, b, c, f</code> to see 1 step of the optimization of process.</p>
<div id="7c039380" class="cell" data-execution_count="18">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>a.grad <span class="op">=</span> <span class="fl">6.0</span></span>
<span id="cb21-2"><a href="#cb21-2"></a>b.grad <span class="op">=</span> <span class="op">-</span><span class="fl">4.0</span></span>
<span id="cb21-3"><a href="#cb21-3"></a>c.grad <span class="op">=</span> <span class="op">-</span><span class="fl">2.0</span></span>
<span id="cb21-4"><a href="#cb21-4"></a>f.grad <span class="op">=</span> <span class="fl">4.0</span></span>
<span id="cb21-5"><a href="#cb21-5"></a></span>
<span id="cb21-6"><a href="#cb21-6"></a>a.data <span class="op">+=</span> <span class="fl">0.01</span> <span class="op">*</span> a.grad</span>
<span id="cb21-7"><a href="#cb21-7"></a>b.data <span class="op">+=</span> <span class="fl">0.01</span> <span class="op">*</span> b.grad</span>
<span id="cb21-8"><a href="#cb21-8"></a>c.data <span class="op">+=</span> <span class="fl">0.01</span> <span class="op">*</span> c.grad</span>
<span id="cb21-9"><a href="#cb21-9"></a>f.data <span class="op">+=</span> <span class="fl">0.01</span> <span class="op">*</span> f.grad</span>
<span id="cb21-10"><a href="#cb21-10"></a></span>
<span id="cb21-11"><a href="#cb21-11"></a>e <span class="op">=</span> a <span class="op">*</span> b<span class="op">;</span> e.grad <span class="op">=</span> <span class="op">-</span><span class="fl">2.0</span><span class="op">;</span> e.label <span class="op">=</span> <span class="st">'e'</span></span>
<span id="cb21-12"><a href="#cb21-12"></a>d <span class="op">=</span> e <span class="op">+</span> c<span class="op">;</span> d.grad <span class="op">=</span> <span class="op">-</span><span class="fl">2.0</span><span class="op">;</span> d.label <span class="op">=</span> <span class="st">'d'</span></span>
<span id="cb21-13"><a href="#cb21-13"></a>L <span class="op">=</span> d <span class="op">*</span> f</span>
<span id="cb21-14"><a href="#cb21-14"></a></span>
<span id="cb21-15"><a href="#cb21-15"></a><span class="bu">print</span>(L.data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>-7.286496</code></pre>
</div>
</div>
<p>We can see the changes, <code>L</code> increased a little bit as expected.</p>
<div class="page-columns page-full">
<div id="0b3e10c2" class="cell page-columns page-full" data-execution_count="19">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a>draw_dot(L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display page-columns page-full" data-execution_count="18">
<div class="page-columns page-full">
<figure class="figure page-columns page-full">
<p class="page-columns page-full"><img src="index_files/figure-html/cell-20-output-1.svg" class="img-fluid figure-img column-page"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="manual-backpropagation-example-2-a-neuron" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="manual-backpropagation-example-2-a-neuron">manual backpropagation example #2: a neuron</h2>
<p>Anatomy of neurons, we have:</p>
<ul>
<li><code>axon</code> as input <span class="math inline">\(x_0\)</span>;</li>
<li><code>synapse</code> string as weight <span class="math inline">\(w_0\)</span>;</li>
<li>information flows into the cell body will be <span class="math inline">\(x_0w_0\)</span>;</li>
<li>there are multiple inputs <span class="math inline">\(x_iw_i\)</span> flow into the cell body;</li>
<li>the cell body has some <em>bias</em> itself <span class="math inline">\(b\)</span>;</li>
<li>the cell body processes all information, the output will flow through an <em>activation</em> function ~ which is some kind of a squashing function, like <code>sigmoid</code>, <code>tanh</code> or something like that;</li>
</ul>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.researchgate.net/publication/364814302/figure/fig5/AS:11431281092677232@1666928276027/Neural-net-Structure-with-an-Activation-Function-Source-CS231n-Stanford-2017.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Neural net Structure with an Activation Function, CS231n Stanford 2017</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>üöÄHow does the <code>tanh</code> look like? this hyperbolic function will squash the output to the edge values: <code>-1.0</code> or <code>1.0</code>.</p>
<div id="719e688d" class="cell" data-execution_count="20">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a>plt.plot(np.arange(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="fl">0.2</span>), np.tanh(np.arange(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="fl">0.2</span>)))<span class="op">;</span> plt.grid()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-21-output-1.png" width="590" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>We first implement <a href="https://en.wikipedia.org/wiki/Hyperbolic_functions"><code>tanh</code></a> function to our class <code>Value</code>.</p>
<div id="4a45e192" class="cell" data-execution_count="21">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a><span class="kw">def</span> tanh(<span class="va">self</span>):</span>
<span id="cb25-2"><a href="#cb25-2"></a>    x <span class="op">=</span> <span class="va">self</span>.data</span>
<span id="cb25-3"><a href="#cb25-3"></a>    t <span class="op">=</span> (math.exp(<span class="dv">2</span><span class="op">*</span>x) <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> (math.exp(<span class="dv">2</span><span class="op">*</span>x) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb25-4"><a href="#cb25-4"></a>    out <span class="op">=</span> Value(t, (<span class="va">self</span>, ), <span class="st">'tanh'</span>)</span>
<span id="cb25-5"><a href="#cb25-5"></a>    <span class="cf">return</span> out</span>
<span id="cb25-6"><a href="#cb25-6"></a></span>
<span id="cb25-7"><a href="#cb25-7"></a>Value.tanh <span class="op">=</span> tanh</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</div></div><p>Let‚Äôs take a simple example of 2-dimensional neuron with 2 inputs <code>x1</code> and <code>x2</code>:</p>
<div id="7fca4684" class="cell" data-execution_count="22">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a><span class="co"># input x1, x2</span></span>
<span id="cb26-2"><a href="#cb26-2"></a>x1 <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'x1'</span>)</span>
<span id="cb26-3"><a href="#cb26-3"></a>x2 <span class="op">=</span> Value(<span class="fl">0.0</span>, label<span class="op">=</span><span class="st">'x2'</span>)</span>
<span id="cb26-4"><a href="#cb26-4"></a><span class="co"># weights w1,w2</span></span>
<span id="cb26-5"><a href="#cb26-5"></a>w1 <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'w1'</span>)</span>
<span id="cb26-6"><a href="#cb26-6"></a>w2 <span class="op">=</span> Value(<span class="fl">1.0</span>, label<span class="op">=</span><span class="st">'w2'</span>)</span>
<span id="cb26-7"><a href="#cb26-7"></a><span class="co"># bias of neuron b</span></span>
<span id="cb26-8"><a href="#cb26-8"></a>b <span class="op">=</span> Value(<span class="fl">6.88137358</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb26-9"><a href="#cb26-9"></a><span class="co"># x1*w1 + x2*w2 + b</span></span>
<span id="cb26-10"><a href="#cb26-10"></a>x1w1 <span class="op">=</span> x1<span class="op">*</span>w1<span class="op">;</span> x1w1.label <span class="op">=</span> <span class="st">'x1w1'</span></span>
<span id="cb26-11"><a href="#cb26-11"></a>x2w2 <span class="op">=</span> x2<span class="op">*</span>w2<span class="op">;</span> x2w2.label <span class="op">=</span> <span class="st">'x2w2'</span></span>
<span id="cb26-12"><a href="#cb26-12"></a>x1w1x2w2 <span class="op">=</span> x1w1 <span class="op">+</span> x2w2<span class="op">;</span> x1w1x2w2.label <span class="op">=</span> <span class="st">'x1w1 + x2w2'</span></span>
<span id="cb26-13"><a href="#cb26-13"></a>n <span class="op">=</span> x1w1x2w2 <span class="op">+</span> b<span class="op">;</span> n.label <span class="op">=</span> <span class="st">'n'</span></span>
<span id="cb26-14"><a href="#cb26-14"></a></span>
<span id="cb26-15"><a href="#cb26-15"></a>o <span class="op">=</span> n.tanh()<span class="op">;</span> o.label <span class="op">=</span> <span class="st">'o'</span> <span class="co"># not define yet</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="page-columns page-full">
<div id="753eaa26" class="cell page-columns page-full" data-execution_count="23">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a>draw_dot(o)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display page-columns page-full" data-execution_count="21">
<div class="page-columns page-full">
<figure class="figure page-columns page-full">
<p class="page-columns page-full"><img src="index_files/figure-html/cell-24-output-1.svg" class="img-fluid figure-img column-page"></p>
</figure>
</div>
</div>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div id="63f90b81" class="cell" data-execution_count="24">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a>o.grad <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb28-2"><a href="#cb28-2"></a>n.grad <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> o.data <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb28-3"><a href="#cb28-3"></a>b.grad <span class="op">=</span> n.grad</span>
<span id="cb28-4"><a href="#cb28-4"></a>x1w1x2w2.grad <span class="op">=</span> n.grad</span>
<span id="cb28-5"><a href="#cb28-5"></a>x1w1.grad <span class="op">=</span> x1w1x2w2.grad</span>
<span id="cb28-6"><a href="#cb28-6"></a>x2w2.grad <span class="op">=</span> x1w1x2w2.grad</span>
<span id="cb28-7"><a href="#cb28-7"></a>x1.grad <span class="op">=</span> w1.data <span class="op">*</span> x1w1.grad</span>
<span id="cb28-8"><a href="#cb28-8"></a>w1.grad <span class="op">=</span> x1.data <span class="op">*</span> x1w1.grad</span>
<span id="cb28-9"><a href="#cb28-9"></a>x2.grad <span class="op">=</span> w2.data <span class="op">*</span> x2w2.grad</span>
<span id="cb28-10"><a href="#cb28-10"></a>w2.grad <span class="op">=</span> x2.data <span class="op">*</span> x2w2.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</div></div><p>From here we will manually calculate the gradient again:</p>
<ol type="1">
<li><code>do/do = 1</code>, that‚Äôs the base case, so <code>o.grad = 1.0</code>.</li>
<li><code>o = tanh(n)</code>, follow that Wiki link (and of course can be easily proof) we have <code>do/dn = 1 - tanh(x)^2 = 1 - o^2</code>.</li>
<li><code>n = x1w1x2w2 + b</code>, this is plus node, which gradient will flow to children equally, <code>do/db = do/dn * dn/db = do/dn * 1</code>.</li>
<li>By symmertry, <code>do/dx1w1x2w2 = do/db</code>.</li>
<li><code>do/dx1w1 = do/dx1w1x2w2</code>.</li>
<li><code>do/dx2w2 = do/dx1w1x2w2</code>.</li>
<li><code>do/dx1 = w1 * do/dx1w1</code>.</li>
<li><code>do/dw1 = x1 * do/dx1w1</code>.</li>
<li><code>do/dx2 = w2 * do/dx2w2</code>.</li>
<li><code>do/dw2 = x2 * do/dx2w2</code>.</li>
</ol>
<div class="page-columns page-full">
<div id="f6b67c89" class="cell page-columns page-full" data-execution_count="25">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a>draw_dot(o)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display page-columns page-full" data-execution_count="23">
<div class="page-columns page-full">
<figure class="figure page-columns page-full">
<p class="page-columns page-full"><img src="index_files/figure-html/cell-26-output-1.svg" class="img-fluid figure-img column-page"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="implementing-the-backward-function-for-each-operation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="implementing-the-backward-function-for-each-operation">implementing the backward function for each operation</h2>
<p>Doing the backpropagation manually is obviously ridiculous and we are now to put an end to this suffering. We will see how we can implement backward pass a bit more automatically.</p>
<p>We create <code>_backward</code> operation for each operator, implement the Chain Rules. Activate the <code>_backward</code> call along with funtion execution.</p>
<div id="e7f3ad22" class="cell" data-execution_count="26">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a>o.grad <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb30-2"><a href="#cb30-2"></a></span>
<span id="cb30-3"><a href="#cb30-3"></a>o._backward()</span>
<span id="cb30-4"><a href="#cb30-4"></a>n._backward()</span>
<span id="cb30-5"><a href="#cb30-5"></a>b._backward()</span>
<span id="cb30-6"><a href="#cb30-6"></a>x1w1x2w2._backward()</span>
<span id="cb30-7"><a href="#cb30-7"></a>x1w1._backward()</span>
<span id="cb30-8"><a href="#cb30-8"></a>x2w2._backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="page-columns page-full">
<div id="2311ff06" class="cell page-columns page-full" data-execution_count="27">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a>draw_dot(o)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display page-columns page-full" data-execution_count="25">
<div class="page-columns page-full">
<figure class="figure page-columns page-full">
<p class="page-columns page-full"><img src="index_files/figure-html/cell-28-output-1.svg" class="img-fluid figure-img column-page"></p>
</figure>
</div>
</div>
</div>
</div>
<p>We still need to call the <code>_backward</code> node by node. Now we move to the next step, to implement backward function to whole expression graph.</p>
</section>
<section id="implementing-the-backward-function-for-a-whole-expression-graph" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="implementing-the-backward-function-for-a-whole-expression-graph">implementing the backward function for a whole expression graph</h2>
<p>In short, we need to do everything after each node before we call the backward function itself. For every node, all dependencies, everything that it depends on has to propagate to it before we can continue backpropagation.</p>
<p>This ordering of graph can be archived using something like <a href="https://en.wikipedia.org/wiki/Topological_sorting">topological sort</a>.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*0jRSNI2zo30sENk2qlqEvw.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Topological Sort, photo credit to <a href="https://yuminlee2.medium.com/topological-sort-cf9f8e43af6a">Claire Lee</a></figcaption>
</figure>
</div>
</div>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div id="f3e5ee90" class="cell" data-execution_count="28">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a><span class="co"># we first reset the Values</span></span>
<span id="cb32-2"><a href="#cb32-2"></a>x1 <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'x1'</span>)</span>
<span id="cb32-3"><a href="#cb32-3"></a>x2 <span class="op">=</span> Value(<span class="fl">0.0</span>, label<span class="op">=</span><span class="st">'x2'</span>)</span>
<span id="cb32-4"><a href="#cb32-4"></a>w1 <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'w1'</span>)</span>
<span id="cb32-5"><a href="#cb32-5"></a>w2 <span class="op">=</span> Value(<span class="fl">1.0</span>, label<span class="op">=</span><span class="st">'w2'</span>)</span>
<span id="cb32-6"><a href="#cb32-6"></a>b <span class="op">=</span> Value(<span class="fl">6.88137358</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb32-7"><a href="#cb32-7"></a>x1w1 <span class="op">=</span> x1<span class="op">*</span>w1<span class="op">;</span> x1w1.label <span class="op">=</span> <span class="st">'x1w1'</span></span>
<span id="cb32-8"><a href="#cb32-8"></a>x2w2 <span class="op">=</span> x2<span class="op">*</span>w2<span class="op">;</span> x2w2.label <span class="op">=</span> <span class="st">'x2w2'</span></span>
<span id="cb32-9"><a href="#cb32-9"></a>x1w1x2w2 <span class="op">=</span> x1w1 <span class="op">+</span> x2w2<span class="op">;</span> x1w1x2w2.label <span class="op">=</span> <span class="st">'x1w1 + x2w2'</span></span>
<span id="cb32-10"><a href="#cb32-10"></a>n <span class="op">=</span> x1w1x2w2 <span class="op">+</span> b<span class="op">;</span> n.label <span class="op">=</span> <span class="st">'n'</span></span>
<span id="cb32-11"><a href="#cb32-11"></a>o <span class="op">=</span> n.tanh()<span class="op">;</span> o.label <span class="op">=</span> <span class="st">'o'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</div></div><p>Below is the code:</p>
<div id="c75567f6" class="cell" data-execution_count="29">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a>topo <span class="op">=</span> []</span>
<span id="cb33-2"><a href="#cb33-2"></a>visited <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb33-3"><a href="#cb33-3"></a></span>
<span id="cb33-4"><a href="#cb33-4"></a><span class="kw">def</span> build_topo(v):</span>
<span id="cb33-5"><a href="#cb33-5"></a>    <span class="cf">if</span> v <span class="kw">not</span> <span class="kw">in</span> visited:</span>
<span id="cb33-6"><a href="#cb33-6"></a>        visited.add(v)</span>
<span id="cb33-7"><a href="#cb33-7"></a>        <span class="cf">for</span> child <span class="kw">in</span> v._prev:</span>
<span id="cb33-8"><a href="#cb33-8"></a>            build_topo(child) <span class="co"># recursively look up all children for v</span></span>
<span id="cb33-9"><a href="#cb33-9"></a>        topo.append(v)</span>
<span id="cb33-10"><a href="#cb33-10"></a></span>
<span id="cb33-11"><a href="#cb33-11"></a>build_topo(o)</span>
<span id="cb33-12"><a href="#cb33-12"></a>topo</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>[Value(data=6.88137358),
 Value(data=2.0),
 Value(data=-3.0),
 Value(data=-6.0),
 Value(data=0.0),
 Value(data=1.0),
 Value(data=0.0),
 Value(data=-6.0),
 Value(data=0.88137358),
 Value(data=0.707106777676776)]</code></pre>
</div>
</div>
<p>We implement the topological sort to <code>backward()</code> (without underscore) function. Now we can trigger the whole process:</p>
<div id="04877a79" class="cell" data-execution_count="30">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a>o.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="page-columns page-full">
<div id="e58df73b" class="cell page-columns page-full" data-execution_count="31">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a>draw_dot(o)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display page-columns page-full" data-execution_count="29">
<div class="page-columns page-full">
<figure class="figure page-columns page-full">
<p class="page-columns page-full"><img src="index_files/figure-html/cell-32-output-1.svg" class="img-fluid figure-img column-page"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="fixing-a-backprop-bug-when-one-node-is-used-multiple-times" class="level2">
<h2 class="anchored" data-anchor-id="fixing-a-backprop-bug-when-one-node-is-used-multiple-times">fixing a backprop bug when one node is used multiple times ‚õî</h2>
<p>This <code>a.grad</code> should be <code>2.0</code>.</p>
<div id="b561947c" class="cell" data-execution_count="32">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1"></a>a <span class="op">=</span> Value(<span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'a'</span>)</span>
<span id="cb37-2"><a href="#cb37-2"></a>b <span class="op">=</span> a <span class="op">+</span> a<span class="op">;</span> b.label <span class="op">=</span> <span class="st">'b'</span> <span class="co"># this case self and other are both a, we should not overwrite the gradient, we should accumulate it.</span></span>
<span id="cb37-3"><a href="#cb37-3"></a>b.backward()</span>
<span id="cb37-4"><a href="#cb37-4"></a>draw_dot(b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="30">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-33-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="breaking-up-a-tanh-exercising-with-more-operations" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="breaking-up-a-tanh-exercising-with-more-operations">breaking up a <code>tanh</code>, exercising with more operations</h2>
<p>Sometime we do operations between <code>Value</code> and other, like <code>int</code>. We can not do this unless we add below code to <code>__add__</code> and <code>__mul__</code> operations. Now we can <code>Value(1.0) + 1.0</code>, or <code>Value(2.0) * 2</code>.</p>
<div id="f9f28228" class="cell" data-execution_count="33">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a>other <span class="op">=</span> other <span class="cf">if</span> <span class="bu">isinstance</span>(other, Value) <span class="cf">else</span> Value(other)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>But for <code>2 * Value(2.0)</code>, which will internally call <code>2.__mul__(Value(2.0))</code>, will not work. We add <code>__rmul__</code>:</p>
<div id="65a28a3c" class="cell" data-execution_count="34">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a><span class="kw">def</span> <span class="fu">__rmul__</span>(<span class="va">self</span>, other): <span class="co"># other * self</span></span>
<span id="cb39-2"><a href="#cb39-2"></a>    <span class="cf">return</span> <span class="va">self</span> <span class="op">*</span> other</span>
<span id="cb39-3"><a href="#cb39-3"></a></span>
<span id="cb39-4"><a href="#cb39-4"></a>Value.<span class="fu">__rmul__</span> <span class="op">=</span> <span class="fu">__rmul__</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>For exponential, we add <code>epx</code>:</p>
<div id="5c58e623" class="cell" data-execution_count="35">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a><span class="kw">def</span> exp(<span class="va">self</span>):</span>
<span id="cb40-2"><a href="#cb40-2"></a>    x <span class="op">=</span> <span class="va">self</span>.data</span>
<span id="cb40-3"><a href="#cb40-3"></a>    out <span class="op">=</span> Value(math.exp(x), (<span class="va">self</span>, ), <span class="st">'exp'</span>)</span>
<span id="cb40-4"><a href="#cb40-4"></a></span>
<span id="cb40-5"><a href="#cb40-5"></a>    <span class="kw">def</span> _backward():</span>
<span id="cb40-6"><a href="#cb40-6"></a>        <span class="va">self</span>.grad <span class="op">+=</span> out.data <span class="op">*</span> out.grad</span>
<span id="cb40-7"><a href="#cb40-7"></a>    out._backward <span class="op">=</span> _backward</span>
<span id="cb40-8"><a href="#cb40-8"></a></span>
<span id="cb40-9"><a href="#cb40-9"></a>    <span class="cf">return</span> out</span>
<span id="cb40-10"><a href="#cb40-10"></a></span>
<span id="cb40-11"><a href="#cb40-11"></a>Value.exp <span class="op">=</span> exp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>For division, we add <code>__truediv__</code>:</p>
<div id="fb264fb1" class="cell" data-execution_count="36">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1"></a><span class="kw">def</span> <span class="fu">__truediv__</span>(<span class="va">self</span>, other): <span class="co"># self / other</span></span>
<span id="cb41-2"><a href="#cb41-2"></a>    <span class="cf">return</span> <span class="va">self</span> <span class="op">*</span> other<span class="op">**</span>(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb41-3"><a href="#cb41-3"></a></span>
<span id="cb41-4"><a href="#cb41-4"></a>Value.<span class="fu">__truediv__</span> <span class="op">=</span> <span class="fu">__truediv__</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>For power, we add <code>__pow__</code>:</p>
<div id="42788b33" class="cell" data-execution_count="37">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a><span class="kw">def</span> <span class="fu">__pow__</span>(<span class="va">self</span>, other): <span class="co"># self ** other</span></span>
<span id="cb42-2"><a href="#cb42-2"></a>    <span class="cf">assert</span> <span class="bu">isinstance</span>(other, (<span class="bu">int</span>, <span class="bu">float</span>)), <span class="st">"TypeError: only supporting int/float power for now"</span></span>
<span id="cb42-3"><a href="#cb42-3"></a>    out <span class="op">=</span> Value(<span class="va">self</span>.data<span class="op">**</span>other, (<span class="va">self</span>, ), <span class="ss">f'**</span><span class="sc">{</span>other<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb42-4"><a href="#cb42-4"></a></span>
<span id="cb42-5"><a href="#cb42-5"></a>    <span class="kw">def</span> _backward():</span>
<span id="cb42-6"><a href="#cb42-6"></a>        <span class="va">self</span>.grad <span class="op">+=</span> other <span class="op">*</span> ( <span class="va">self</span>.data <span class="op">**</span> (other <span class="op">-</span> <span class="dv">1</span>)) <span class="op">*</span> out.grad</span>
<span id="cb42-7"><a href="#cb42-7"></a>    out._backward <span class="op">=</span> _backward</span>
<span id="cb42-8"><a href="#cb42-8"></a></span>
<span id="cb42-9"><a href="#cb42-9"></a>    <span class="cf">return</span> out</span>
<span id="cb42-10"><a href="#cb42-10"></a></span>
<span id="cb42-11"><a href="#cb42-11"></a>Value.<span class="fu">__pow__</span> <span class="op">=</span> <span class="fu">__pow__</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>For subtract, we add <code>__neg__</code> and <code>__sub__</code>:</p>
<div id="89c750af" class="cell" data-execution_count="38">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a><span class="kw">def</span> <span class="fu">__neg__</span>(<span class="va">self</span>): <span class="co"># - self</span></span>
<span id="cb43-2"><a href="#cb43-2"></a>    <span class="cf">return</span> <span class="op">-</span> <span class="va">self</span></span>
<span id="cb43-3"><a href="#cb43-3"></a></span>
<span id="cb43-4"><a href="#cb43-4"></a>Value.<span class="fu">__neg__</span> <span class="op">=</span> <span class="fu">__neg__</span> <span class="co"># self - other</span></span>
<span id="cb43-5"><a href="#cb43-5"></a></span>
<span id="cb43-6"><a href="#cb43-6"></a><span class="kw">def</span> <span class="fu">__sub__</span>(<span class="va">self</span>, other):</span>
<span id="cb43-7"><a href="#cb43-7"></a>    <span class="cf">return</span> <span class="va">self</span> <span class="op">+</span> (<span class="op">-</span>other)</span>
<span id="cb43-8"><a href="#cb43-8"></a></span>
<span id="cb43-9"><a href="#cb43-9"></a>Value.<span class="fu">__sub__</span> <span class="op">=</span> <span class="fu">__sub__</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now we are ready to try <code>tanh</code> in a different way:</p>
<div id="f7340b4b" class="cell" data-execution_count="39">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1"></a>x1 <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'x1'</span>)</span>
<span id="cb44-2"><a href="#cb44-2"></a>x2 <span class="op">=</span> Value(<span class="fl">0.0</span>, label<span class="op">=</span><span class="st">'x2'</span>)</span>
<span id="cb44-3"><a href="#cb44-3"></a>w1 <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'w1'</span>)</span>
<span id="cb44-4"><a href="#cb44-4"></a>w2 <span class="op">=</span> Value(<span class="fl">1.0</span>, label<span class="op">=</span><span class="st">'w2'</span>)</span>
<span id="cb44-5"><a href="#cb44-5"></a>b <span class="op">=</span> Value(<span class="fl">6.88137358</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb44-6"><a href="#cb44-6"></a>x1w1 <span class="op">=</span> x1<span class="op">*</span>w1<span class="op">;</span> x1w1.label <span class="op">=</span> <span class="st">'x1w1'</span></span>
<span id="cb44-7"><a href="#cb44-7"></a>x2w2 <span class="op">=</span> x2<span class="op">*</span>w2<span class="op">;</span> x2w2.label <span class="op">=</span> <span class="st">'x2w2'</span></span>
<span id="cb44-8"><a href="#cb44-8"></a>x1w1x2w2 <span class="op">=</span> x1w1 <span class="op">+</span> x2w2<span class="op">;</span> x1w1x2w2.label <span class="op">=</span> <span class="st">'x1w1 + x2w2'</span></span>
<span id="cb44-9"><a href="#cb44-9"></a>n <span class="op">=</span> x1w1x2w2 <span class="op">+</span> b<span class="op">;</span> n.label <span class="op">=</span> <span class="st">'n'</span></span>
<span id="cb44-10"><a href="#cb44-10"></a></span>
<span id="cb44-11"><a href="#cb44-11"></a>e <span class="op">=</span> (<span class="dv">2</span><span class="op">*</span>n).exp()<span class="op">;</span> e.label <span class="op">=</span> <span class="st">'e'</span></span>
<span id="cb44-12"><a href="#cb44-12"></a>o <span class="op">=</span> (e <span class="op">-</span> <span class="dv">1</span>)<span class="op">/</span>(e <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb44-13"><a href="#cb44-13"></a>o.label <span class="op">=</span> <span class="st">'o'</span></span>
<span id="cb44-14"><a href="#cb44-14"></a>o.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="page-columns page-full">
<div id="2bb3d9bd" class="cell page-columns page-full" data-execution_count="40">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1"></a>draw_dot(o)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display page-columns page-full" data-execution_count="37">
<div class="page-columns page-full">
<figure class="figure page-columns page-full">
<p class="page-columns page-full"><img src="index_files/figure-html/cell-41-output-1.svg" class="img-fluid figure-img column-screen"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="pytorch-comparison" class="level1">
<h1>PyTorch comparison</h1>
<section id="doing-the-same-thing-but-in-pytorch-comparison" class="level2">
<h2 class="anchored" data-anchor-id="doing-the-same-thing-but-in-pytorch-comparison">doing the same thing but in PyTorch: comparison</h2>
<div id="a887f767" class="cell" data-execution_count="41">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1"></a><span class="im">import</span> torch</span>
<span id="cb46-2"><a href="#cb46-2"></a></span>
<span id="cb46-3"><a href="#cb46-3"></a>x1 <span class="op">=</span> torch.tensor([<span class="fl">2.0</span>]).double()<span class="op">;</span> x1.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb46-4"><a href="#cb46-4"></a>x2 <span class="op">=</span> torch.tensor([<span class="fl">0.0</span>]).double()<span class="op">;</span> x2.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb46-5"><a href="#cb46-5"></a>w1 <span class="op">=</span> torch.tensor([<span class="op">-</span><span class="fl">3.0</span>]).double()<span class="op">;</span> w1.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb46-6"><a href="#cb46-6"></a>w2 <span class="op">=</span> torch.tensor([<span class="fl">1.0</span>]).double()<span class="op">;</span> w2.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb46-7"><a href="#cb46-7"></a>b <span class="op">=</span> torch.tensor([<span class="fl">6.8813735870195432</span>]).double()<span class="op">;</span> b.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb46-8"><a href="#cb46-8"></a></span>
<span id="cb46-9"><a href="#cb46-9"></a>n <span class="op">=</span> x1<span class="op">*</span>w1 <span class="op">+</span> x2<span class="op">*</span>w2 <span class="op">+</span> b</span>
<span id="cb46-10"><a href="#cb46-10"></a>o <span class="op">=</span> torch.tanh(n)</span>
<span id="cb46-11"><a href="#cb46-11"></a></span>
<span id="cb46-12"><a href="#cb46-12"></a><span class="bu">print</span>(o.data.item())</span>
<span id="cb46-13"><a href="#cb46-13"></a>o.backward()</span>
<span id="cb46-14"><a href="#cb46-14"></a></span>
<span id="cb46-15"><a href="#cb46-15"></a><span class="bu">print</span>(<span class="st">'------------------'</span>)</span>
<span id="cb46-16"><a href="#cb46-16"></a><span class="bu">print</span>(<span class="st">'x1'</span>, x1.grad.item())</span>
<span id="cb46-17"><a href="#cb46-17"></a><span class="bu">print</span>(<span class="st">'w1'</span>, w1.grad.item())</span>
<span id="cb46-18"><a href="#cb46-18"></a><span class="bu">print</span>(<span class="st">'x2'</span>, x2.grad.item())</span>
<span id="cb46-19"><a href="#cb46-19"></a><span class="bu">print</span>(<span class="st">'w2'</span>, w2.grad.item())</span>
<span id="cb46-20"><a href="#cb46-20"></a><span class="bu">print</span>(<span class="st">'------------------'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>0.7071066904050358
------------------
x1 -1.5000003851533106
w1 1.0000002567688737
x2 0.5000001283844369
w2 0.0
------------------</code></pre>
</div>
</div>
</section>
</section>
<section id="building-the-library" class="level1 page-columns page-full">
<h1>Building the library</h1>
<section id="building-out-a-neural-net-library-multi-layer-perceptron-in-micrograd" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="building-out-a-neural-net-library-multi-layer-perceptron-in-micrograd">building out a neural net library (multi-layer perceptron) in micrograd</h2>
<p>We are going to build out a two-layer perceptron.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cs231n.github.io/assets/nn1/neural_net2.jpeg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>A 3-layer neural network with three inputs, two hidden layers of 4 neurons each and one output layer. Notice that in both cases there are connections (synapses) between neurons across layers, but not within a layer, photo credit to <a href="https://cs231n.github.io/neural-networks-1/">cs231n</a></figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="ef2c6b16" class="cell" data-execution_count="42">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="annotated-cell-31"><pre class="sourceCode numberSource python code-annotation-code number-lines code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-31-1"><a href="#annotated-cell-31-1"></a><span class="kw">class</span> Neuron:</span>
<span id="annotated-cell-31-2"><a href="#annotated-cell-31-2"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-31" data-target-annotation="1">1</button><span id="annotated-cell-31-3" class="code-annotation-target"><a href="#annotated-cell-31-3"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, nin):</span>
<span id="annotated-cell-31-4"><a href="#annotated-cell-31-4"></a>        <span class="va">self</span>.w <span class="op">=</span> [Value(np.random.uniform(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(nin)]</span>
<span id="annotated-cell-31-5"><a href="#annotated-cell-31-5"></a>        <span class="va">self</span>.b <span class="op">=</span> Value(np.random.uniform(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="annotated-cell-31-6"><a href="#annotated-cell-31-6"></a>    </span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-31" data-target-annotation="2">2</button><span id="annotated-cell-31-7" class="code-annotation-target"><a href="#annotated-cell-31-7"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="annotated-cell-31-8"><a href="#annotated-cell-31-8"></a>        activation <span class="op">=</span> <span class="bu">sum</span>((wi<span class="op">*</span>xi <span class="cf">for</span> wi, xi <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.w, x)), <span class="va">self</span>.b)</span>
<span id="annotated-cell-31-9"><a href="#annotated-cell-31-9"></a>        out <span class="op">=</span> activation.tanh()</span>
<span id="annotated-cell-31-10"><a href="#annotated-cell-31-10"></a>        <span class="cf">return</span> out</span>
<span id="annotated-cell-31-11"><a href="#annotated-cell-31-11"></a></span>
<span id="annotated-cell-31-12"><a href="#annotated-cell-31-12"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="annotated-cell-31-13"><a href="#annotated-cell-31-13"></a>        <span class="cf">return</span> <span class="va">self</span>.w <span class="op">+</span> [<span class="va">self</span>.b] <span class="co"># list plus list gives you a list</span></span>
<span id="annotated-cell-31-14"><a href="#annotated-cell-31-14"></a></span>
<span id="annotated-cell-31-15"><a href="#annotated-cell-31-15"></a><span class="kw">class</span> Layer:</span>
<span id="annotated-cell-31-16"><a href="#annotated-cell-31-16"></a></span>
<span id="annotated-cell-31-17"><a href="#annotated-cell-31-17"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, nin, nout):</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-31" data-target-annotation="3">3</button><span id="annotated-cell-31-18" class="code-annotation-target"><a href="#annotated-cell-31-18"></a>        <span class="va">self</span>.neurons <span class="op">=</span> [Neuron(nin) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(nout)]</span>
<span id="annotated-cell-31-19"><a href="#annotated-cell-31-19"></a></span>
<span id="annotated-cell-31-20"><a href="#annotated-cell-31-20"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="annotated-cell-31-21"><a href="#annotated-cell-31-21"></a>        outs <span class="op">=</span> [n(x) <span class="cf">for</span> n <span class="kw">in</span> <span class="va">self</span>.neurons]</span>
<span id="annotated-cell-31-22"><a href="#annotated-cell-31-22"></a>        <span class="cf">return</span> outs[<span class="dv">0</span>] <span class="cf">if</span> <span class="bu">len</span>(outs) <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> outs</span>
<span id="annotated-cell-31-23"><a href="#annotated-cell-31-23"></a></span>
<span id="annotated-cell-31-24"><a href="#annotated-cell-31-24"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="annotated-cell-31-25"><a href="#annotated-cell-31-25"></a>        <span class="cf">return</span> [p <span class="cf">for</span> neuron <span class="kw">in</span> <span class="va">self</span>.neurons <span class="cf">for</span> p <span class="kw">in</span> neuron.parameters()] <span class="co"># list comprehension</span></span>
<span id="annotated-cell-31-26"><a href="#annotated-cell-31-26"></a>        <span class="co"># params = []</span></span>
<span id="annotated-cell-31-27"><a href="#annotated-cell-31-27"></a>        <span class="co"># for neuron in self.neurons:</span></span>
<span id="annotated-cell-31-28"><a href="#annotated-cell-31-28"></a>        <span class="co">#     ps = neuron.parameters()</span></span>
<span id="annotated-cell-31-29"><a href="#annotated-cell-31-29"></a>        <span class="co">#     params.extend(ps)</span></span>
<span id="annotated-cell-31-30"><a href="#annotated-cell-31-30"></a>        <span class="co"># return params</span></span>
<span id="annotated-cell-31-31"><a href="#annotated-cell-31-31"></a></span>
<span id="annotated-cell-31-32"><a href="#annotated-cell-31-32"></a><span class="kw">class</span> MLP:</span>
<span id="annotated-cell-31-33"><a href="#annotated-cell-31-33"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-31" data-target-annotation="4">4</button><span id="annotated-cell-31-34" class="code-annotation-target"><a href="#annotated-cell-31-34"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, nin, nouts):</span>
<span id="annotated-cell-31-35"><a href="#annotated-cell-31-35"></a>        sz <span class="op">=</span> [nin] <span class="op">+</span> nouts</span>
<span id="annotated-cell-31-36"><a href="#annotated-cell-31-36"></a>        <span class="va">self</span>.layers <span class="op">=</span> [Layer(sz[i], sz[i<span class="op">+</span><span class="dv">1</span>]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(nouts))]</span>
<span id="annotated-cell-31-37"><a href="#annotated-cell-31-37"></a></span>
<span id="annotated-cell-31-38"><a href="#annotated-cell-31-38"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="annotated-cell-31-39"><a href="#annotated-cell-31-39"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="annotated-cell-31-40"><a href="#annotated-cell-31-40"></a>            x <span class="op">=</span> layer(x)</span>
<span id="annotated-cell-31-41"><a href="#annotated-cell-31-41"></a>        <span class="cf">return</span> x</span>
<span id="annotated-cell-31-42"><a href="#annotated-cell-31-42"></a></span>
<span id="annotated-cell-31-43"><a href="#annotated-cell-31-43"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="annotated-cell-31-44"><a href="#annotated-cell-31-44"></a>        <span class="cf">return</span> [p <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers <span class="cf">for</span> p <span class="kw">in</span> layer.parameters()] <span class="co"># for neuron in layer.neurons for neuron.parameters()]</span></span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-31" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-31" data-code-lines="3" data-code-annotation="1">Number of <code>input</code> for the Neuron. <code>w</code> is randomly generated for each input, same for <code>b</code> which is the bias that controll ‚Äúthe happiness‚Äù.</span>
</dd>
<dt data-target-cell="annotated-cell-31" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-31" data-code-lines="7" data-code-annotation="2">Object as a function: define the forward pass of the Neuron <span class="math inline">\(\sum\limits_{i=1}^{nin} w_ix_i+b\)</span>, then squash the output using <code>tanh</code>.</span>
</dd>
<dt data-target-cell="annotated-cell-31" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-31" data-code-lines="18" data-code-annotation="3">A Layer is a list of Neurons, <code>nout</code> specifies how many Neurons in the Layer. Each neuron has <code>nin</code> inputs ~ nin-D. We just initialize completely independent neurons with this given dimensionality.</span>
</dd>
<dt data-target-cell="annotated-cell-31" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-31" data-code-lines="34" data-code-annotation="4">A MLP is a sequence of Layers, picture above depicts a 3-layers MLP containing 1 input layer and 3 output layers, we say the size is 4. We sequentially create connection from the input layer to the 1st output layer, 1st output layer to 2nd output layer,‚Ä¶</span>
</dd>
</dl>
</div>
</div>
<div id="59a01061" class="cell" data-execution_count="43">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1"></a>nin <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb48-2"><a href="#cb48-2"></a>nouts <span class="op">=</span> [<span class="fl">2.0</span>, <span class="fl">3.0</span>, <span class="op">-</span><span class="fl">1.0</span>]</span>
<span id="cb48-3"><a href="#cb48-3"></a>[nin] <span class="op">+</span> nouts</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>[3, 2.0, 3.0, -1.0]</code></pre>
</div>
</div>
<div id="cd945f86" class="cell" data-execution_count="44">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1"></a>x <span class="op">=</span> [<span class="fl">2.0</span>, <span class="fl">3.0</span>]</span>
<span id="cb50-2"><a href="#cb50-2"></a>n <span class="op">=</span> Neuron(<span class="dv">2</span>)</span>
<span id="cb50-3"><a href="#cb50-3"></a>l <span class="op">=</span> Layer(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb50-4"><a href="#cb50-4"></a>n(x)</span>
<span id="cb50-5"><a href="#cb50-5"></a>l(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>[Value(data=-0.9436737893480257),
 Value(data=0.910851754824634),
 Value(data=0.9819458458197922)]</code></pre>
</div>
</div>
<div id="120384bb" class="cell" data-execution_count="45">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1"></a>x <span class="op">=</span> [<span class="fl">2.0</span>, <span class="fl">3.0</span>, <span class="op">-</span><span class="fl">1.0</span>]</span>
<span id="cb52-2"><a href="#cb52-2"></a>m <span class="op">=</span> MLP(<span class="dv">3</span>, [<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">1</span>]) <span class="co"># a MLP with 3-D input, 3 output layers contains 4, 4, 1 neurons in each layer respectively</span></span>
<span id="cb52-3"><a href="#cb52-3"></a>m(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>Value(data=0.017552562832080985)</code></pre>
</div>
</div>
<div class="page-columns page-full">
<div id="ebada77a" class="cell page-columns page-full" data-execution_count="46">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1"></a>draw_dot(m(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display page-columns page-full" data-execution_count="43">
<div class="page-columns page-full">
<figure class="figure page-columns page-full">
<p class="page-columns page-full"><img src="index_files/figure-html/cell-47-output-1.svg" class="img-fluid figure-img column-screen"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="creating-a-tiny-dataset-writing-the-loss-function" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="creating-a-tiny-dataset-writing-the-loss-function">creating a tiny dataset, writing the loss function</h2>
<p>A simple data set, <code>m()</code> is the MLP we defined above.</p>
<div id="a23c629a" class="cell" data-execution_count="47">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1"></a>xs <span class="op">=</span> [</span>
<span id="cb55-2"><a href="#cb55-2"></a>    [<span class="fl">2.0</span>, <span class="fl">3.0</span>, <span class="op">-</span><span class="fl">1.0</span>],</span>
<span id="cb55-3"><a href="#cb55-3"></a>    [<span class="fl">3.0</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">0.5</span>],</span>
<span id="cb55-4"><a href="#cb55-4"></a>    [<span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">1.0</span>],</span>
<span id="cb55-5"><a href="#cb55-5"></a>    [<span class="fl">1.0</span>, <span class="fl">1.0</span>, <span class="op">-</span><span class="fl">1.0</span>]</span>
<span id="cb55-6"><a href="#cb55-6"></a>]</span>
<span id="cb55-7"><a href="#cb55-7"></a>ys <span class="op">=</span> [<span class="fl">1.0</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">1.0</span>] <span class="co"># designed targets</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="page-columns page-full"><p>Writing the loss function. </p><div class="no-row-height column-margin column-container"><span class="margin-aside">I was unable to sum a list of Value, found the solution <a href="https://stackoverflow.com/questions/54153552/unsupported-operand-type-error-when-adding-objects-within-list-using-sum-functio">here</a>; Edit: I used Numpy random instead of random</span></div></div>
<div id="067973bd" class="cell" data-execution_count="48">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1"></a>ypred <span class="op">=</span> [m(x) <span class="cf">for</span> x <span class="kw">in</span> xs]</span>
<span id="cb56-2"><a href="#cb56-2"></a>loss <span class="op">=</span> np.array([(yout <span class="op">-</span> ygt)<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> ygt, yout <span class="kw">in</span> <span class="bu">zip</span>(ys, ypred)]).<span class="bu">sum</span>()</span>
<span id="cb56-3"><a href="#cb56-3"></a></span>
<span id="cb56-4"><a href="#cb56-4"></a>loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>Value(data=2.2454067439529672)</code></pre>
</div>
</div>
<p>Backpropagation the <code>loss</code>, some magical here:</p>
<div id="72afa9f0" class="cell" data-execution_count="49">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1"></a>loss.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We can look into the gradient of weight of the first neuron of the first layer (input layer)</p>
<div id="844b39e5" class="cell" data-execution_count="50">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1"></a><span class="bu">print</span>(<span class="st">'value of 1st neuron in 1st layer: '</span>,m.layers[<span class="dv">0</span>].neurons[<span class="dv">0</span>].w[<span class="dv">0</span>].data)</span>
<span id="cb59-2"><a href="#cb59-2"></a><span class="bu">print</span>(<span class="st">'grad of 1st neuron in 1st layer: '</span>,m.layers[<span class="dv">0</span>].neurons[<span class="dv">0</span>].w[<span class="dv">0</span>].grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>value of 1st neuron in 1st layer:  0.6122541262352386
grad of 1st neuron in 1st layer:  -0.11311733839375913</code></pre>
</div>
</div>
<div class="page-columns page-full">
<div id="9eb572f7" class="cell page-columns page-full" data-execution_count="51">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1"></a>draw_dot(loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display page-columns page-full" data-execution_count="48">
<div class="page-columns page-full">
<figure class="figure page-columns page-full">
<p class="page-columns page-full"><img src="index_files/figure-html/cell-52-output-1.svg" class="img-fluid figure-img column-screen"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="collecting-all-of-the-parameters-of-the-neural-net" class="level2">
<h2 class="anchored" data-anchor-id="collecting-all-of-the-parameters-of-the-neural-net">collecting all of the parameters of the neural net</h2>
<p>We aim to produce the fitness <code>ypred</code>. <code>xs</code> is the data, the input of problem, we can not change it. <code>ys</code> is the ground true, can not changes as well. What we can change is the ‚Äúparamters‚Äù of each neuron, which is weight <code>w</code> and bias <code>b</code>.</p>
<p>We add in to each class a <code>parameters()</code> function to collect those. Finally we can get all the paramters of the MLP:</p>
<div id="33eeeb44" class="cell" data-execution_count="52">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1"></a><span class="bu">len</span>(m.parameters())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>41</code></pre>
</div>
</div>
</section>
<section id="doing-gradient-descent-optimization-manually-training-the-network" class="level2">
<h2 class="anchored" data-anchor-id="doing-gradient-descent-optimization-manually-training-the-network">doing gradient descent optimization manually, training the network</h2>
<p>Now we will try to change the paramters to minimize the loss, which means our prediction will be more close to the ground true.</p>
<p><strong>Forward pass</strong>, calculate the loss:</p>
<div id="1b9300ef" class="cell" data-execution_count="53">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1"></a>ypred <span class="op">=</span> [m(x) <span class="cf">for</span> x <span class="kw">in</span> xs]</span>
<span id="cb64-2"><a href="#cb64-2"></a>loss <span class="op">=</span> np.array([(yout <span class="op">-</span> ygt)<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> ygt, yout <span class="kw">in</span> <span class="bu">zip</span>(ys, ypred)]).<span class="bu">sum</span>()</span>
<span id="cb64-3"><a href="#cb64-3"></a>loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>Value(data=2.2454067439529672)</code></pre>
</div>
</div>
<p><strong>Backward pass</strong>, calculate the parameters:</p>
<div id="4845a6c1" class="cell" data-execution_count="54">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1"></a>loss.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Update the parameters</strong>, change the parameters following opposite direction to reduce the loss:</p>
<div id="264a598e" class="cell" data-execution_count="55">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1"></a><span class="cf">for</span> p <span class="kw">in</span> m.parameters():</span>
<span id="cb67-2"><a href="#cb67-2"></a>    p.data <span class="op">+=</span> <span class="op">-</span><span class="fl">0.01</span> <span class="op">*</span> p.grad <span class="co"># we want the p.data go on opposite direction of the loss</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><code>0.01</code> is the learning rate!</p>
<p><strong>New loss</strong></p>
<div id="f260bd05" class="cell" data-execution_count="56">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1"></a>ypred <span class="op">=</span> [m(x) <span class="cf">for</span> x <span class="kw">in</span> xs]</span>
<span id="cb68-2"><a href="#cb68-2"></a>loss <span class="op">=</span> np.array([(yout <span class="op">-</span> ygt)<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> ygt, yout <span class="kw">in</span> <span class="bu">zip</span>(ys, ypred)]).<span class="bu">sum</span>()</span>
<span id="cb68-3"><a href="#cb68-3"></a>loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>Value(data=1.8748986906624858)</code></pre>
</div>
</div>
<p>Yeah the loss decreased. In short, the process is:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">%%{init: {'theme':'dark'}}%%
flowchart LR

P1(Updated parameters) -- Forward Pass --&gt; L(Loss)
L(Loss) -- Backward Pass --&gt; P2(Parameters to update) 
P2(Parameters to update)  -- Update Pamameters --&gt; P1(Updated parameters)
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p><strong>Automate the training loop</strong>:</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>For each process, there remained a subtle bug above that we didn‚Äôt flush the grads before backpropagation. Because we did not overwrite the gradients (remember the <code>+=</code>), they kept accumulated. The next action of backward and changing parameters using learning rate and grad (which produce a massive step size) become wrong! We must set the grad to zero before backward pass.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="guilty.png" class="img-fluid figure-img"></p>
<figcaption>Common guilties when training the NN</figcaption>
</figure>
</div>
</div>
</div>
<div id="52f70d45" class="cell" data-execution_count="57">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb70-2"><a href="#cb70-2"></a>    <span class="co"># forward pass:</span></span>
<span id="cb70-3"><a href="#cb70-3"></a>    ypred <span class="op">=</span> [m(x) <span class="cf">for</span> x <span class="kw">in</span> xs]</span>
<span id="cb70-4"><a href="#cb70-4"></a>    loss <span class="op">=</span> np.array([(yout <span class="op">-</span> ygt)<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> ygt, yout <span class="kw">in</span> <span class="bu">zip</span>(ys, ypred)]).<span class="bu">sum</span>()</span>
<span id="cb70-5"><a href="#cb70-5"></a></span>
<span id="cb70-6"><a href="#cb70-6"></a>    <span class="co"># backward pass:</span></span>
<span id="cb70-7"><a href="#cb70-7"></a>    <span class="cf">for</span> p <span class="kw">in</span> m.parameters():</span>
<span id="cb70-8"><a href="#cb70-8"></a>        p.grad <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb70-9"><a href="#cb70-9"></a>    loss.backward()</span>
<span id="cb70-10"><a href="#cb70-10"></a></span>
<span id="cb70-11"><a href="#cb70-11"></a>    <span class="co"># update params:</span></span>
<span id="cb70-12"><a href="#cb70-12"></a>    <span class="cf">for</span> p <span class="kw">in</span> m.parameters():</span>
<span id="cb70-13"><a href="#cb70-13"></a>        p.data <span class="op">+=</span> <span class="op">-</span><span class="fl">0.01</span> <span class="op">*</span> p.grad</span>
<span id="cb70-14"><a href="#cb70-14"></a></span>
<span id="cb70-15"><a href="#cb70-15"></a>    <span class="bu">print</span>(k, loss.data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>0 1.8748986906624858
1 1.7525389208607982
2 1.640329535604251
3 1.5330776503948085
4 1.428451983077621
5 1.3258854458264309
6 1.2258454466617152
7 1.129307611387194
8 1.0373655570775884
9 0.950965398742569
10 0.8707632046017467
11 0.7970871382629623
12 0.729969891292043
13 0.6692148145684491
14 0.6144684747058207
15 0.5652846324996362
16 0.5211741017573215
17 0.48164032652210864
18 0.4462028394628461
19 0.41441137525018645</code></pre>
</div>
</div>
<div id="1770bfd0" class="cell" data-execution_count="58">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1"></a>ypred</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre><code>[Value(data=0.5688075795666488),
 Value(data=-0.8981072657206238),
 Value(data=-0.6253226704952907),
 Value(data=0.721218291010141)]</code></pre>
</div>
</div>
</section>
</section>
<section id="summary" class="level1">
<h1>Summary</h1>
<section id="summary-of-what-we-learned-how-to-go-towards-modern-neural-nets" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-what-we-learned-how-to-go-towards-modern-neural-nets">summary of what we learned, how to go towards modern neural nets</h2>
<ol type="1">
<li>What are Neural Nets: they are mathematical expressions, in case of MLP it takes: (1) data as the input, and (2) weights and biases as parameters to build out expression for the forward pass followed by the loss function.</li>
<li>The loss function is kind of measure for the accuracy of predictions. The low loss implies that predicted values are matching our targets and the networks are behaving well.</li>
<li>The process of Gradient Descent is for each step, we calculate the loss (output of the nets), backwarding it to get paramters, then updating data (which we can change - weights and biases) follow the opposite side of the loss (negative grad * learning rate). We‚Äôll get a lower loss, and backwarding again and again. This process will find the local minimum of the loss.</li>
</ol>
</section>
<section id="walkthrough-of-the-full-code-of-micrograd-on-github" class="level2">
<h2 class="anchored" data-anchor-id="walkthrough-of-the-full-code-of-micrograd-on-github">walkthrough of the full code of micrograd on github</h2>
<p>Same with which we built today:</p>
<ul>
<li>engine: Value</li>
<li>nn: Neuron, Layer, MLP, and modulize the zero grad process to class Module</li>
<li>test: sanity check - compare the backward with <code>torch</code>, also for the forward pass</li>
<li>demo: a bit complicated example with <code>sklearn</code> dataset, using batch processing when the dataset come large, the loss is slightly different - SVM max-margin loss and using of auto L2 regularization</li>
<li>learning rate decay: is a scaled as a function of number of iterations, high at begin and low at the end</li>
</ul>
</section>
<section id="real-stuff-diving-into-pytorch-finding-their-backward-pass-for-tanh" class="level2">
<h2 class="anchored" data-anchor-id="real-stuff-diving-into-pytorch-finding-their-backward-pass-for-tanh">real stuff: diving into PyTorch, finding their backward pass for <code>tanh</code></h2>
<p>These libraries unfortunately grow in size and <strong>entropy</strong>, if you just search for <code>tanh</code> it‚Äôll give you thousands of results.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">conclusion</h2>
<p>There will be follow up session yeah haha.</p>
</section>
<section id="outtakes" class="level2">
<h2 class="anchored" data-anchor-id="outtakes">outtakes :)</h2>
<p>Pytorch self-defined autograd.</p>
<p>ü§ôüöÄüî•</p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/lktuan\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb74" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb74-1"><a href="#cb74-1"></a><span class="co">---</span></span>
<span id="cb74-2"><a href="#cb74-2"></a><span class="an">title:</span><span class="co"> "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd"</span></span>
<span id="cb74-3"><a href="#cb74-3"></a><span class="an">description:</span><span class="co"> "backpropagation, from neuron to neural network, from micro grad to pytorch, and more"</span></span>
<span id="cb74-4"><a href="#cb74-4"></a><span class="an">author:</span></span>
<span id="cb74-5"><a href="#cb74-5"></a><span class="co">  - name: "Tuan Le Khac"</span></span>
<span id="cb74-6"><a href="#cb74-6"></a><span class="co">    url: https://lktuan.github.io/</span></span>
<span id="cb74-7"><a href="#cb74-7"></a><span class="an">categories:</span><span class="co"> [til, python, andrej karpathy, nn-z2h, backpropagation, neural networks] </span></span>
<span id="cb74-8"><a href="#cb74-8"></a><span class="an">date:</span><span class="co"> 06-16-2024</span></span>
<span id="cb74-9"><a href="#cb74-9"></a><span class="an">date-modified:</span><span class="co"> 06-18-2024</span></span>
<span id="cb74-10"><a href="#cb74-10"></a><span class="an">image:</span><span class="co"> puppy.jpg</span></span>
<span id="cb74-11"><a href="#cb74-11"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb74-12"><a href="#cb74-12"></a><span class="an">code-fold:</span><span class="co"> show</span></span>
<span id="cb74-13"><a href="#cb74-13"></a><span class="an">code-annotations:</span><span class="co"> hover</span></span>
<span id="cb74-14"><a href="#cb74-14"></a><span class="an">draft:</span><span class="co"> false</span></span>
<span id="cb74-15"><a href="#cb74-15"></a><span class="an">css:</span><span class="co"> html/styles.scss</span></span>
<span id="cb74-16"><a href="#cb74-16"></a><span class="an">fig-cap-location:</span><span class="co"> bottom</span></span>
<span id="cb74-17"><a href="#cb74-17"></a><span class="an">editor:</span><span class="co"> visual</span></span>
<span id="cb74-18"><a href="#cb74-18"></a></span>
<span id="cb74-19"><a href="#cb74-19"></a><span class="an">format:</span></span>
<span id="cb74-20"><a href="#cb74-20"></a><span class="co">  html:</span></span>
<span id="cb74-21"><a href="#cb74-21"></a><span class="co">    code-overflow: wrap</span></span>
<span id="cb74-22"><a href="#cb74-22"></a><span class="co">---</span></span>
<span id="cb74-23"><a href="#cb74-23"></a></span>
<span id="cb74-24"><a href="#cb74-24"></a>::: {.callout-important title="This is not orginal content!"}</span>
<span id="cb74-25"><a href="#cb74-25"></a>This is my study notes / codes along with Andrej Karpathy's "<span class="co">[</span><span class="ot">Neural Networks: Zero to Hero</span><span class="co">](https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)</span>" series.</span>
<span id="cb74-26"><a href="#cb74-26"></a>:::</span>
<span id="cb74-27"><a href="#cb74-27"></a></span>
<span id="cb74-28"><a href="#cb74-28"></a>**Upfront-note**: There are also greate resources in Vietnamese for learning Backpropagation, for e.g.:</span>
<span id="cb74-29"><a href="#cb74-29"></a></span>
<span id="cb74-30"><a href="#cb74-30"></a><span class="ss">1.  </span>Blog <span class="co">[</span><span class="ot">machinelearningcoban</span><span class="co">](https://machinelearningcoban.com/2017/02/24/mlp/)</span></span>
<span id="cb74-31"><a href="#cb74-31"></a><span class="ss">2.  </span>Blog <span class="co">[</span><span class="ot">dominhhai</span><span class="co">](https://dominhhai.github.io/vi/2018/04/nn-bp/)</span></span>
<span id="cb74-32"><a href="#cb74-32"></a></span>
<span id="cb74-33"><a href="#cb74-33"></a><span class="fu"># MicroGrad from scratch Yayy!</span></span>
<span id="cb74-34"><a href="#cb74-34"></a></span>
<span id="cb74-35"><a href="#cb74-35"></a><span class="co">[</span><span class="ot">**üöÄ MicroGrad repo**</span><span class="co">](https://github.com/karpathy/micrograd)</span> <span class="co">[</span><span class="ot">**üî• Video Lecture**</span><span class="co">](https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)</span></span>
<span id="cb74-36"><a href="#cb74-36"></a></span>
<span id="cb74-37"><a href="#cb74-37"></a>::: {layout-ncol="1"}</span>
<span id="cb74-38"><a href="#cb74-38"></a>!<span class="co">[</span><span class="ot">Backpropagation in Neural Networks, photo credit to [GeekforGeek](https://www.geeksforgeeks.org/backpropagation-in-neural-network/)</span><span class="co">](https://media.geeksforgeeks.org/wp-content/uploads/20240217152156/Frame-13.png)</span>{width="100%"}</span>
<span id="cb74-39"><a href="#cb74-39"></a>:::</span>
<span id="cb74-40"><a href="#cb74-40"></a></span>
<span id="cb74-41"><a href="#cb74-41"></a><span class="fu">## intro &amp; micrograd overview - what does your neural network training look like under the hood?</span></span>
<span id="cb74-42"><a href="#cb74-42"></a></span>
<span id="cb74-43"><a href="#cb74-43"></a>What is MicroGrad ‚ùì: a tiny **auto-grad** (automatic gradient) engine, implement of **backpropagation** \~ itertively tune the weight of that nn to minimize the loss function -<span class="sc">\&gt;</span> improve the accuracy of the neural network. Backpropagation will be the mathematical core of any modern deep neutral network like, say <span class="in">`pytorch`</span>, or <span class="in">`jaxx`</span>.</span>
<span id="cb74-44"><a href="#cb74-44"></a></span>
<span id="cb74-45"><a href="#cb74-45"></a>Installation: <span class="in">`pip install micrograd`</span></span>
<span id="cb74-46"><a href="#cb74-46"></a></span>
<span id="cb74-47"><a href="#cb74-47"></a>Example:</span>
<span id="cb74-48"><a href="#cb74-48"></a></span>
<span id="cb74-51"><a href="#cb74-51"></a><span class="in">```{python}</span></span>
<span id="cb74-52"><a href="#cb74-52"></a><span class="im">from</span> micrograd.engine <span class="im">import</span> Value</span>
<span id="cb74-53"><a href="#cb74-53"></a></span>
<span id="cb74-54"><a href="#cb74-54"></a>a <span class="op">=</span> Value(<span class="op">-</span><span class="fl">4.0</span>)                                                                     <span class="co"># &lt;1&gt; </span></span>
<span id="cb74-55"><a href="#cb74-55"></a>b <span class="op">=</span> Value(<span class="fl">2.0</span>)</span>
<span id="cb74-56"><a href="#cb74-56"></a>c <span class="op">=</span> a <span class="op">+</span> b</span>
<span id="cb74-57"><a href="#cb74-57"></a>d <span class="op">=</span> a <span class="op">*</span> b <span class="op">+</span> b<span class="op">**</span><span class="dv">3</span></span>
<span id="cb74-58"><a href="#cb74-58"></a>c <span class="op">+=</span> c <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb74-59"><a href="#cb74-59"></a>c <span class="op">+=</span> <span class="dv">1</span> <span class="op">+</span> c <span class="op">+</span> (<span class="op">-</span>a)</span>
<span id="cb74-60"><a href="#cb74-60"></a>d <span class="op">+=</span> d <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> (b <span class="op">+</span> a).relu()</span>
<span id="cb74-61"><a href="#cb74-61"></a>d <span class="op">+=</span> <span class="dv">3</span> <span class="op">*</span> d <span class="op">+</span> (b <span class="op">-</span> a).relu()</span>
<span id="cb74-62"><a href="#cb74-62"></a>e <span class="op">=</span> c <span class="op">-</span> d</span>
<span id="cb74-63"><a href="#cb74-63"></a>f <span class="op">=</span> e<span class="op">**</span><span class="dv">2</span></span>
<span id="cb74-64"><a href="#cb74-64"></a>g <span class="op">=</span> f <span class="op">/</span> <span class="fl">2.0</span></span>
<span id="cb74-65"><a href="#cb74-65"></a>g <span class="op">+=</span> <span class="fl">10.0</span> <span class="op">/</span> f                                                                       <span class="co"># &lt;2&gt;</span></span>
<span id="cb74-66"><a href="#cb74-66"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>g<span class="sc">.</span>data<span class="sc">:.4f}</span><span class="ss">'</span>) <span class="co"># prints 24.7041, the outcome of this forward pass           # &lt;3&gt;</span></span>
<span id="cb74-67"><a href="#cb74-67"></a>g.backward()</span>
<span id="cb74-68"><a href="#cb74-68"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>a<span class="sc">.</span>grad<span class="sc">:.4f}</span><span class="ss">'</span>) <span class="co"># prints 138.8338, i.e. the numerical value of dg/da         # &lt;4&gt;</span></span>
<span id="cb74-69"><a href="#cb74-69"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>b<span class="sc">.</span>grad<span class="sc">:.4f}</span><span class="ss">'</span>) <span class="co"># prints 645.5773, i.e. the numerical value of dg/db</span></span>
<span id="cb74-70"><a href="#cb74-70"></a><span class="in">```</span></span>
<span id="cb74-71"><a href="#cb74-71"></a></span>
<span id="cb74-72"><a href="#cb74-72"></a><span class="ss">1.  </span>Micrograd allows you to build mathematical expressions, in this case <span class="in">`a`</span> and <span class="in">`b`</span> are inputs, wrapped in <span class="in">`Value`</span> object with value equal to <span class="in">`-4.0`</span> and <span class="in">`2.0`</span>, respectively.</span>
<span id="cb74-73"><a href="#cb74-73"></a><span class="ss">2.  </span><span class="in">`a`</span> and <span class="in">`b`</span> are transformed to <span class="in">`c`</span>, <span class="in">`d`</span>, <span class="in">`e`</span> and eventually <span class="in">`f`</span>, <span class="in">`g`</span>. Mathematical operators are implemented, like <span class="in">`+`</span>, <span class="in">`*`</span>, <span class="in">`**`</span>, even <span class="in">`relu()`</span>.</span>
<span id="cb74-74"><a href="#cb74-74"></a><span class="ss">3.  </span><span class="in">`Value`</span> object contains <span class="in">`data`</span>, and <span class="in">`grad`</span>.</span>
<span id="cb74-75"><a href="#cb74-75"></a><span class="ss">4.  </span>Call <span class="in">`backpropagation()`</span> process.</span>
<span id="cb74-76"><a href="#cb74-76"></a></span>
<span id="cb74-77"><a href="#cb74-77"></a><span class="fu">## derivative of a simple function with one input</span></span>
<span id="cb74-78"><a href="#cb74-78"></a></span>
<span id="cb74-79"><a href="#cb74-79"></a>‚ùìWhat exactly is derivative‚ùì</span>
<span id="cb74-80"><a href="#cb74-80"></a></span>
<span id="cb74-83"><a href="#cb74-83"></a><span class="in">```{python}</span></span>
<span id="cb74-84"><a href="#cb74-84"></a><span class="im">import</span> math</span>
<span id="cb74-85"><a href="#cb74-85"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb74-86"><a href="#cb74-86"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb74-87"><a href="#cb74-87"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb74-88"><a href="#cb74-88"></a><span class="in">```</span></span>
<span id="cb74-89"><a href="#cb74-89"></a></span>
<span id="cb74-90"><a href="#cb74-90"></a>A simple quadratic function:</span>
<span id="cb74-91"><a href="#cb74-91"></a></span>
<span id="cb74-94"><a href="#cb74-94"></a><span class="in">```{python}</span></span>
<span id="cb74-95"><a href="#cb74-95"></a><span class="kw">def</span> f(x):</span>
<span id="cb74-96"><a href="#cb74-96"></a>    <span class="cf">return</span> <span class="dv">3</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">4</span><span class="op">*</span>x <span class="op">+</span> <span class="dv">5</span></span>
<span id="cb74-97"><a href="#cb74-97"></a><span class="in">```</span></span>
<span id="cb74-98"><a href="#cb74-98"></a></span>
<span id="cb74-101"><a href="#cb74-101"></a><span class="in">```{python}</span></span>
<span id="cb74-102"><a href="#cb74-102"></a>f(<span class="fl">3.0</span>)</span>
<span id="cb74-103"><a href="#cb74-103"></a><span class="in">```</span></span>
<span id="cb74-104"><a href="#cb74-104"></a></span>
<span id="cb74-105"><a href="#cb74-105"></a>Input also can be an array, we can plot it for visibility.</span>
<span id="cb74-106"><a href="#cb74-106"></a></span>
<span id="cb74-109"><a href="#cb74-109"></a><span class="in">```{python}</span></span>
<span id="cb74-110"><a href="#cb74-110"></a>xs <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="fl">0.25</span>)</span>
<span id="cb74-111"><a href="#cb74-111"></a>ys <span class="op">=</span> f(xs)</span>
<span id="cb74-112"><a href="#cb74-112"></a>plt.plot(xs, ys)</span>
<span id="cb74-113"><a href="#cb74-113"></a><span class="in">```</span></span>
<span id="cb74-114"><a href="#cb74-114"></a></span>
<span id="cb74-115"><a href="#cb74-115"></a>If we bump up a litle value <span class="in">`h`</span> of <span class="in">`x`</span>, how <span class="in">`f(x)`</span> will response?</span>
<span id="cb74-116"><a href="#cb74-116"></a></span>
<span id="cb74-119"><a href="#cb74-119"></a><span class="in">```{python}</span></span>
<span id="cb74-120"><a href="#cb74-120"></a>h <span class="op">=</span> <span class="fl">0.000000000001</span> <span class="co"># &lt;1&gt;</span></span>
<span id="cb74-121"><a href="#cb74-121"></a>x <span class="op">=</span> <span class="fl">3.0</span></span>
<span id="cb74-122"><a href="#cb74-122"></a>( f(x<span class="op">+</span>h) <span class="op">-</span> f(x) ) <span class="op">/</span> h</span>
<span id="cb74-123"><a href="#cb74-123"></a><span class="in">```</span></span>
<span id="cb74-124"><a href="#cb74-124"></a></span>
<span id="cb74-125"><a href="#cb74-125"></a><span class="ss">1.  </span>Change the value of <span class="in">`h`</span> from <span class="in">`0.0001`</span> to be <span class="in">`0.00000...0001`</span> -<span class="sc">\&gt;</span> the slope value comes to <span class="in">`14`</span> (at the value of <span class="in">`3.0`</span> of <span class="in">`x`</span>).</span>
<span id="cb74-126"><a href="#cb74-126"></a></span>
<span id="cb74-127"><a href="#cb74-127"></a>Try for <span class="in">`x = -3.0`</span>, <span class="in">`x = 5.0`</span>, we get different values of the slope, for <span class="in">`x = 2/3`</span>, the slope is zero. Let's get more complex.</span>
<span id="cb74-128"><a href="#cb74-128"></a></span>
<span id="cb74-129"><a href="#cb74-129"></a><span class="fu">## derivative of a function with multiple inputs</span></span>
<span id="cb74-130"><a href="#cb74-130"></a></span>
<span id="cb74-133"><a href="#cb74-133"></a><span class="in">```{python}</span></span>
<span id="cb74-134"><a href="#cb74-134"></a>a <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb74-135"><a href="#cb74-135"></a>b <span class="op">=</span> <span class="op">-</span><span class="fl">3.0</span></span>
<span id="cb74-136"><a href="#cb74-136"></a>c <span class="op">=</span> <span class="fl">10.0</span></span>
<span id="cb74-137"><a href="#cb74-137"></a>d <span class="op">=</span> a<span class="op">*</span>b <span class="op">+</span> c</span>
<span id="cb74-138"><a href="#cb74-138"></a><span class="bu">print</span>(d)</span>
<span id="cb74-139"><a href="#cb74-139"></a><span class="in">```</span></span>
<span id="cb74-140"><a href="#cb74-140"></a></span>
<span id="cb74-141"><a href="#cb74-141"></a>Put our bump-up element to this multi-variables function:</span>
<span id="cb74-142"><a href="#cb74-142"></a></span>
<span id="cb74-145"><a href="#cb74-145"></a><span class="in">```{python}</span></span>
<span id="cb74-146"><a href="#cb74-146"></a>h <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb74-147"><a href="#cb74-147"></a></span>
<span id="cb74-148"><a href="#cb74-148"></a><span class="co"># input</span></span>
<span id="cb74-149"><a href="#cb74-149"></a>a <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb74-150"><a href="#cb74-150"></a>b <span class="op">=</span> <span class="op">-</span><span class="fl">3.0</span></span>
<span id="cb74-151"><a href="#cb74-151"></a>c <span class="op">=</span> <span class="fl">10.0</span></span>
<span id="cb74-152"><a href="#cb74-152"></a></span>
<span id="cb74-153"><a href="#cb74-153"></a>d1 <span class="op">=</span> a<span class="op">*</span>b <span class="op">+</span> c</span>
<span id="cb74-154"><a href="#cb74-154"></a>a <span class="op">+=</span> h  <span class="co"># &lt;1&gt;</span></span>
<span id="cb74-155"><a href="#cb74-155"></a>d2 <span class="op">=</span> a<span class="op">*</span>b <span class="op">+</span> c</span>
<span id="cb74-156"><a href="#cb74-156"></a></span>
<span id="cb74-157"><a href="#cb74-157"></a><span class="bu">print</span>(<span class="st">'d1: '</span>, d1)</span>
<span id="cb74-158"><a href="#cb74-158"></a><span class="bu">print</span>(<span class="st">'d2: '</span>, d2)</span>
<span id="cb74-159"><a href="#cb74-159"></a><span class="bu">print</span>(<span class="st">'slope: '</span>, (d2 <span class="op">-</span> d1)<span class="op">/</span>h) <span class="co"># &lt;2&gt;</span></span>
<span id="cb74-160"><a href="#cb74-160"></a><span class="in">```</span></span>
<span id="cb74-161"><a href="#cb74-161"></a></span>
<span id="cb74-162"><a href="#cb74-162"></a><span class="ss">1.  </span>Do the same for <span class="in">`b`</span>, <span class="in">`c`</span>, we'll get different slopes.</span>
<span id="cb74-163"><a href="#cb74-163"></a><span class="ss">2.  </span>We say given <span class="in">`b = -3.0`</span> and <span class="in">`c = 10.0`</span> are constants, the derivative of <span class="in">`d`</span> at <span class="in">`a = 2.0`</span> is <span class="in">`-3.0`</span>. The rate of which <span class="in">`d`</span> will increase if we scale <span class="in">`a`</span>!</span>
<span id="cb74-164"><a href="#cb74-164"></a></span>
<span id="cb74-165"><a href="#cb74-165"></a><span class="fu">## starting the core `Value` object of micrograd and its visualization</span></span>
<span id="cb74-166"><a href="#cb74-166"></a></span>
<span id="cb74-167"><a href="#cb74-167"></a>So we now have some intuitive sense of what is derivative is telling you about the function. We now move to the Neural Networks, which would be massive mathematical expressions. We need some data structures that maintain these expressions, we first declare an object <span class="in">`Value`</span> that holds data.</span>
<span id="cb74-168"><a href="#cb74-168"></a></span>
<span id="cb74-171"><a href="#cb74-171"></a><span class="in">```{python}</span></span>
<span id="cb74-172"><a href="#cb74-172"></a><span class="kw">class</span> Value:</span>
<span id="cb74-173"><a href="#cb74-173"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, data, </span>
<span id="cb74-174"><a href="#cb74-174"></a>                        _children<span class="op">=</span>(), <span class="co"># &lt;3&gt;</span></span>
<span id="cb74-175"><a href="#cb74-175"></a>                        _op <span class="op">=</span> <span class="st">''</span>, <span class="co"># &lt;5&gt;</span></span>
<span id="cb74-176"><a href="#cb74-176"></a>                        label <span class="op">=</span> <span class="st">''</span></span>
<span id="cb74-177"><a href="#cb74-177"></a>                        ): </span>
<span id="cb74-178"><a href="#cb74-178"></a>        <span class="va">self</span>.data <span class="op">=</span> data</span>
<span id="cb74-179"><a href="#cb74-179"></a>        <span class="va">self</span>.grad <span class="op">=</span> <span class="fl">0.0</span> <span class="co"># &lt;6&gt;</span></span>
<span id="cb74-180"><a href="#cb74-180"></a>        <span class="va">self</span>._backward <span class="op">=</span> <span class="kw">lambda</span>: <span class="va">None</span> <span class="co"># &lt;7&gt;</span></span>
<span id="cb74-181"><a href="#cb74-181"></a>        <span class="va">self</span>._prev <span class="op">=</span> <span class="bu">set</span>(_children)</span>
<span id="cb74-182"><a href="#cb74-182"></a>        <span class="va">self</span>._op <span class="op">=</span> _op</span>
<span id="cb74-183"><a href="#cb74-183"></a>        <span class="va">self</span>.label <span class="op">=</span>  label</span>
<span id="cb74-184"><a href="#cb74-184"></a></span>
<span id="cb74-185"><a href="#cb74-185"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">str</span>: <span class="co"># a nicer looking for class attributes</span></span>
<span id="cb74-186"><a href="#cb74-186"></a>        <span class="cf">return</span> <span class="ss">f"Value(data=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>data<span class="sc">}</span><span class="ss">)"</span></span>
<span id="cb74-187"><a href="#cb74-187"></a>    </span>
<span id="cb74-188"><a href="#cb74-188"></a>    <span class="kw">def</span> <span class="fu">__add__</span>(<span class="va">self</span>, other): <span class="co"># &lt;4&gt;</span></span>
<span id="cb74-189"><a href="#cb74-189"></a>        other <span class="op">=</span> other <span class="cf">if</span> <span class="bu">isinstance</span>(other, Value) <span class="cf">else</span> Value(other) <span class="co"># turn other to Value object before calculation</span></span>
<span id="cb74-190"><a href="#cb74-190"></a>        out <span class="op">=</span> Value(<span class="va">self</span>.data <span class="op">+</span> other.data, (<span class="va">self</span>, other), <span class="st">'+'</span>)</span>
<span id="cb74-191"><a href="#cb74-191"></a></span>
<span id="cb74-192"><a href="#cb74-192"></a>        <span class="kw">def</span> _backward(): <span class="co"># &lt;8&gt; </span></span>
<span id="cb74-193"><a href="#cb74-193"></a>            <span class="va">self</span>.grad <span class="op">+=</span> <span class="fl">1.0</span> <span class="op">*</span> out.grad</span>
<span id="cb74-194"><a href="#cb74-194"></a>            other.grad <span class="op">+=</span> <span class="fl">1.0</span> <span class="op">*</span> out.grad</span>
<span id="cb74-195"><a href="#cb74-195"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="cb74-196"><a href="#cb74-196"></a></span>
<span id="cb74-197"><a href="#cb74-197"></a>        <span class="cf">return</span> out</span>
<span id="cb74-198"><a href="#cb74-198"></a></span>
<span id="cb74-199"><a href="#cb74-199"></a>    <span class="kw">def</span> <span class="fu">__mul__</span>(<span class="va">self</span>, other):</span>
<span id="cb74-200"><a href="#cb74-200"></a>        other <span class="op">=</span> other <span class="cf">if</span> <span class="bu">isinstance</span>(other, Value) <span class="cf">else</span> Value(other) <span class="co"># turn other to Value object before calculation</span></span>
<span id="cb74-201"><a href="#cb74-201"></a>        out <span class="op">=</span> Value(<span class="va">self</span>.data <span class="op">*</span> other.data, (<span class="va">self</span>, other), <span class="st">'*'</span>)</span>
<span id="cb74-202"><a href="#cb74-202"></a></span>
<span id="cb74-203"><a href="#cb74-203"></a>        <span class="kw">def</span> _backward():</span>
<span id="cb74-204"><a href="#cb74-204"></a>            <span class="va">self</span>.grad <span class="op">+=</span> other.data <span class="op">*</span> out.grad</span>
<span id="cb74-205"><a href="#cb74-205"></a>            other.grad <span class="op">+=</span> <span class="va">self</span>.data <span class="op">*</span> out.grad</span>
<span id="cb74-206"><a href="#cb74-206"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="cb74-207"><a href="#cb74-207"></a></span>
<span id="cb74-208"><a href="#cb74-208"></a>        <span class="cf">return</span> out</span>
<span id="cb74-209"><a href="#cb74-209"></a></span>
<span id="cb74-210"><a href="#cb74-210"></a>    <span class="kw">def</span> tanh(<span class="va">self</span>):</span>
<span id="cb74-211"><a href="#cb74-211"></a>        x <span class="op">=</span> <span class="va">self</span>.data</span>
<span id="cb74-212"><a href="#cb74-212"></a>        t <span class="op">=</span> (math.exp(<span class="dv">2</span><span class="op">*</span>x) <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> (math.exp(<span class="dv">2</span><span class="op">*</span>x) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb74-213"><a href="#cb74-213"></a>        out <span class="op">=</span> Value(t, (<span class="va">self</span>, ), <span class="st">'tanh'</span>)</span>
<span id="cb74-214"><a href="#cb74-214"></a></span>
<span id="cb74-215"><a href="#cb74-215"></a>        <span class="kw">def</span> _backward():</span>
<span id="cb74-216"><a href="#cb74-216"></a>            <span class="va">self</span>.grad <span class="op">+=</span> (<span class="dv">1</span> <span class="op">-</span> t<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> out.grad</span>
<span id="cb74-217"><a href="#cb74-217"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="cb74-218"><a href="#cb74-218"></a>        <span class="cf">return</span> out</span>
<span id="cb74-219"><a href="#cb74-219"></a></span>
<span id="cb74-220"><a href="#cb74-220"></a>    <span class="kw">def</span> backward(<span class="va">self</span>):</span>
<span id="cb74-221"><a href="#cb74-221"></a>        </span>
<span id="cb74-222"><a href="#cb74-222"></a>        <span class="co"># topo order for all children in the graph</span></span>
<span id="cb74-223"><a href="#cb74-223"></a>        topo <span class="op">=</span> []</span>
<span id="cb74-224"><a href="#cb74-224"></a>        visited <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb74-225"><a href="#cb74-225"></a>        <span class="kw">def</span> build_topo(v):</span>
<span id="cb74-226"><a href="#cb74-226"></a>            <span class="cf">if</span> v <span class="kw">not</span> <span class="kw">in</span> visited:</span>
<span id="cb74-227"><a href="#cb74-227"></a>                visited.add(v)</span>
<span id="cb74-228"><a href="#cb74-228"></a>                <span class="cf">for</span> child <span class="kw">in</span> v._prev:</span>
<span id="cb74-229"><a href="#cb74-229"></a>                    build_topo(child) </span>
<span id="cb74-230"><a href="#cb74-230"></a>                topo.append(v)</span>
<span id="cb74-231"><a href="#cb74-231"></a>        build_topo(<span class="va">self</span>)</span>
<span id="cb74-232"><a href="#cb74-232"></a></span>
<span id="cb74-233"><a href="#cb74-233"></a>        <span class="co"># sequentially apply the chain rules</span></span>
<span id="cb74-234"><a href="#cb74-234"></a>        <span class="va">self</span>.grad <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb74-235"><a href="#cb74-235"></a>        <span class="cf">for</span> node <span class="kw">in</span> <span class="bu">reversed</span>(topo):</span>
<span id="cb74-236"><a href="#cb74-236"></a>            node._backward()</span>
<span id="cb74-237"><a href="#cb74-237"></a><span class="in">```</span></span>
<span id="cb74-238"><a href="#cb74-238"></a></span>
<span id="cb74-239"><a href="#cb74-239"></a><span class="ss">3.  </span>the connective tissue of this expression. We want to keep these expression graphs, so we need to know and keep pointers about what values produce what other values. <span class="in">`_children`</span> is by default a empty tuple.</span>
<span id="cb74-240"><a href="#cb74-240"></a><span class="ss">4.  </span>as we added <span class="in">`_children`</span>, we also need to point out the father - children relationship in method <span class="in">`__add__`</span> and <span class="in">`__mul__`</span> as well.</span>
<span id="cb74-241"><a href="#cb74-241"></a><span class="ss">5.  </span>we want to know the **operation** between father and child, <span class="in">`_op`</span> is empty string by default, the value <span class="in">`+`</span> and <span class="in">`-`</span> will be added to the operator method respectively.</span>
<span id="cb74-242"><a href="#cb74-242"></a><span class="ss">6.  </span>initially assume that node has no impact to the output.</span>
<span id="cb74-243"><a href="#cb74-243"></a><span class="ss">7.  </span>this backward function basically do nothing at the initial.</span>
<span id="cb74-244"><a href="#cb74-244"></a><span class="ss">8.  </span>implement of backward pass for plus node, <span class="in">`+=`</span> represent the accumulate action (rather than overwrite it), assigne the gradient behaviour for each type of operation, call the <span class="in">`_backward`</span> concurrently with function.</span>
<span id="cb74-245"><a href="#cb74-245"></a></span>
<span id="cb74-246"><a href="#cb74-246"></a>Setting input and expression:</span>
<span id="cb74-247"><a href="#cb74-247"></a></span>
<span id="cb74-250"><a href="#cb74-250"></a><span class="in">```{python}</span></span>
<span id="cb74-251"><a href="#cb74-251"></a>a <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'a'</span>)</span>
<span id="cb74-252"><a href="#cb74-252"></a>b <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb74-253"><a href="#cb74-253"></a>c <span class="op">=</span> Value(<span class="fl">10.0</span>, label<span class="op">=</span><span class="st">'c'</span>)</span>
<span id="cb74-254"><a href="#cb74-254"></a></span>
<span id="cb74-255"><a href="#cb74-255"></a>a <span class="op">+</span> b <span class="co"># &lt;1&gt; </span></span>
<span id="cb74-256"><a href="#cb74-256"></a></span>
<span id="cb74-257"><a href="#cb74-257"></a>a<span class="op">*</span>b <span class="op">+</span> c <span class="co"># &lt;2&gt;</span></span>
<span id="cb74-258"><a href="#cb74-258"></a></span>
<span id="cb74-259"><a href="#cb74-259"></a><span class="co"># d = a*b + c rewrite the expression</span></span>
<span id="cb74-260"><a href="#cb74-260"></a>e <span class="op">=</span> a<span class="op">*</span>b<span class="op">;</span> e.label <span class="op">=</span> <span class="st">'e'</span></span>
<span id="cb74-261"><a href="#cb74-261"></a>d <span class="op">=</span> e <span class="op">+</span> c<span class="op">;</span> d.label <span class="op">=</span> <span class="st">'d'</span></span>
<span id="cb74-262"><a href="#cb74-262"></a><span class="co"># d</span></span>
<span id="cb74-263"><a href="#cb74-263"></a>f <span class="op">=</span> Value(<span class="op">-</span><span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'f'</span>)</span>
<span id="cb74-264"><a href="#cb74-264"></a>L <span class="op">=</span> d <span class="op">*</span> f<span class="op">;</span> L.label <span class="op">=</span> <span class="st">'L'</span></span>
<span id="cb74-265"><a href="#cb74-265"></a>L</span>
<span id="cb74-266"><a href="#cb74-266"></a><span class="in">```</span></span>
<span id="cb74-267"><a href="#cb74-267"></a></span>
<span id="cb74-268"><a href="#cb74-268"></a><span class="ss">1.  </span>which will internally call <span class="in">`a.__add__(b)`</span></span>
<span id="cb74-269"><a href="#cb74-269"></a><span class="ss">2.  </span>which will internally call <span class="in">`(a.__mul__(b)).__add__(c)`</span></span>
<span id="cb74-270"><a href="#cb74-270"></a></span>
<span id="cb74-271"><a href="#cb74-271"></a>So that we can know the children:</span>
<span id="cb74-272"><a href="#cb74-272"></a></span>
<span id="cb74-275"><a href="#cb74-275"></a><span class="in">```{python}</span></span>
<span id="cb74-276"><a href="#cb74-276"></a>d._prev</span>
<span id="cb74-277"><a href="#cb74-277"></a><span class="in">```</span></span>
<span id="cb74-278"><a href="#cb74-278"></a></span>
<span id="cb74-279"><a href="#cb74-279"></a>We can know the operations:</span>
<span id="cb74-280"><a href="#cb74-280"></a></span>
<span id="cb74-283"><a href="#cb74-283"></a><span class="in">```{python}</span></span>
<span id="cb74-284"><a href="#cb74-284"></a>d._op</span>
<span id="cb74-285"><a href="#cb74-285"></a><span class="in">```</span></span>
<span id="cb74-286"><a href="#cb74-286"></a></span>
<span id="cb74-287"><a href="#cb74-287"></a>Now we know exactly how each value came to be by **word** expression and from what other values. These will be quite abit larger, so we need a way to nicely visualize these expressions that we're building out. Below are a-little-scary codes.</span>
<span id="cb74-288"><a href="#cb74-288"></a></span>
<span id="cb74-291"><a href="#cb74-291"></a><span class="in">```{python}</span></span>
<span id="cb74-292"><a href="#cb74-292"></a><span class="im">import</span> os</span>
<span id="cb74-293"><a href="#cb74-293"></a></span>
<span id="cb74-294"><a href="#cb74-294"></a><span class="co"># Assuming the Graphviz bin directory path is 'C:/Program Files (x86)/Graphviz2.xx/bin'</span></span>
<span id="cb74-295"><a href="#cb74-295"></a>os.environ[<span class="st">"PATH"</span>] <span class="op">+=</span> os.pathsep <span class="op">+</span> <span class="st">'C:/Program Files (x86)/Graphviz/bin'</span> <span class="co"># add with the code, Gemini instructed me this üò™</span></span>
<span id="cb74-296"><a href="#cb74-296"></a></span>
<span id="cb74-297"><a href="#cb74-297"></a><span class="im">from</span> graphviz <span class="im">import</span> Digraph</span>
<span id="cb74-298"><a href="#cb74-298"></a></span>
<span id="cb74-299"><a href="#cb74-299"></a><span class="kw">def</span> trace(root):</span>
<span id="cb74-300"><a href="#cb74-300"></a>    <span class="co"># build a set of all nodes and edges in a graph</span></span>
<span id="cb74-301"><a href="#cb74-301"></a>    nodes, edges <span class="op">=</span> <span class="bu">set</span>(), <span class="bu">set</span>()</span>
<span id="cb74-302"><a href="#cb74-302"></a>    <span class="kw">def</span> build(v):</span>
<span id="cb74-303"><a href="#cb74-303"></a>        <span class="cf">if</span> v <span class="kw">not</span> <span class="kw">in</span> nodes:</span>
<span id="cb74-304"><a href="#cb74-304"></a>            nodes.add(v) <span class="co"># &lt;1&gt;</span></span>
<span id="cb74-305"><a href="#cb74-305"></a>            <span class="cf">for</span> child <span class="kw">in</span> v._prev:</span>
<span id="cb74-306"><a href="#cb74-306"></a>                edges.add((child, v)) <span class="co"># &lt;2&gt;</span></span>
<span id="cb74-307"><a href="#cb74-307"></a>                build(child)</span>
<span id="cb74-308"><a href="#cb74-308"></a>    build(root)</span>
<span id="cb74-309"><a href="#cb74-309"></a></span>
<span id="cb74-310"><a href="#cb74-310"></a>    <span class="cf">return</span> nodes, edges</span>
<span id="cb74-311"><a href="#cb74-311"></a></span>
<span id="cb74-312"><a href="#cb74-312"></a><span class="kw">def</span> draw_dot(root):</span>
<span id="cb74-313"><a href="#cb74-313"></a>    dot <span class="op">=</span> Digraph(<span class="bu">format</span><span class="op">=</span><span class="st">'svg'</span>, graph_attr<span class="op">=</span>{<span class="st">'rankdir'</span>: <span class="st">'LR'</span>}) <span class="co"># LR = from left to right</span></span>
<span id="cb74-314"><a href="#cb74-314"></a>    nodes, edges <span class="op">=</span> trace(root)</span>
<span id="cb74-315"><a href="#cb74-315"></a>    <span class="cf">for</span> n <span class="kw">in</span> nodes:</span>
<span id="cb74-316"><a href="#cb74-316"></a>        uid <span class="op">=</span> <span class="bu">str</span>(<span class="bu">id</span>(n))</span>
<span id="cb74-317"><a href="#cb74-317"></a>        <span class="co"># for any value in the graph, create a rectangular ('record') node for it</span></span>
<span id="cb74-318"><a href="#cb74-318"></a>        dot.node(name<span class="op">=</span>uid, label<span class="op">=</span><span class="st">"{ </span><span class="sc">%s</span><span class="st"> | data </span><span class="sc">%.4f</span><span class="st"> | grad </span><span class="sc">%.4f</span><span class="st">}"</span> <span class="op">%</span> (n.label, n.data, n.grad), shape<span class="op">=</span><span class="st">'record'</span>) <span class="co"># why is (n.data, ), but not (n.data) ???</span></span>
<span id="cb74-319"><a href="#cb74-319"></a>        <span class="cf">if</span> n._op:</span>
<span id="cb74-320"><a href="#cb74-320"></a>            <span class="co"># if this value is a result of some operations, create an op node for it</span></span>
<span id="cb74-321"><a href="#cb74-321"></a>            dot.node(name <span class="op">=</span> uid <span class="op">+</span> n._op, label <span class="op">=</span> n._op)</span>
<span id="cb74-322"><a href="#cb74-322"></a>            <span class="co"># and connect the node to it</span></span>
<span id="cb74-323"><a href="#cb74-323"></a>            dot.edge(uid <span class="op">+</span> n._op, uid)</span>
<span id="cb74-324"><a href="#cb74-324"></a></span>
<span id="cb74-325"><a href="#cb74-325"></a>    <span class="cf">for</span> n1, n2 <span class="kw">in</span> edges:</span>
<span id="cb74-326"><a href="#cb74-326"></a>        <span class="co"># connect n1 to the op node of n2</span></span>
<span id="cb74-327"><a href="#cb74-327"></a>        dot.edge(<span class="bu">str</span>(<span class="bu">id</span>(n1)), <span class="bu">str</span>(<span class="bu">id</span>(n2)) <span class="op">+</span> n2._op)</span>
<span id="cb74-328"><a href="#cb74-328"></a></span>
<span id="cb74-329"><a href="#cb74-329"></a>    <span class="cf">return</span> dot</span>
<span id="cb74-330"><a href="#cb74-330"></a><span class="in">```</span></span>
<span id="cb74-331"><a href="#cb74-331"></a></span>
<span id="cb74-332"><a href="#cb74-332"></a><span class="ss">1.  </span>This will collect all nodes to the <span class="in">`nodes`</span>.</span>
<span id="cb74-333"><a href="#cb74-333"></a><span class="ss">2.  </span>This will iteratively recursively collect all nodes to the <span class="in">`nodes`</span>, add child and node ralationship information to <span class="in">`edges`</span>.</span>
<span id="cb74-334"><a href="#cb74-334"></a></span>
<span id="cb74-335"><a href="#cb74-335"></a>::: column-margin</span>
<span id="cb74-336"><a href="#cb74-336"></a>Remember to let <span class="co">[</span><span class="ot">graphviz</span><span class="co">](https://graphviz.org/download/)</span> installed on your machine, not only Python package, I also run this:</span>
<span id="cb74-337"><a href="#cb74-337"></a></span>
<span id="cb74-340"><a href="#cb74-340"></a><span class="in">```{python}</span></span>
<span id="cb74-341"><a href="#cb74-341"></a><span class="co">#| eval: false</span></span>
<span id="cb74-342"><a href="#cb74-342"></a><span class="im">import</span> os</span>
<span id="cb74-343"><a href="#cb74-343"></a>os.environ[<span class="st">"PATH"</span>] <span class="op">+=</span> os.pathsep <span class="op">+</span> <span class="st">'C:\Program Files (x86)\Graphviz</span><span class="ch">\b</span><span class="st">in\dot.exe'</span></span>
<span id="cb74-344"><a href="#cb74-344"></a><span class="in">```</span></span>
<span id="cb74-345"><a href="#cb74-345"></a>:::</span>
<span id="cb74-346"><a href="#cb74-346"></a></span>
<span id="cb74-347"><a href="#cb74-347"></a>Now we can draw üöÄ.</span>
<span id="cb74-348"><a href="#cb74-348"></a></span>
<span id="cb74-351"><a href="#cb74-351"></a><span class="in">```{python}</span></span>
<span id="cb74-352"><a href="#cb74-352"></a>draw_dot(d)</span>
<span id="cb74-353"><a href="#cb74-353"></a><span class="in">```</span></span>
<span id="cb74-354"><a href="#cb74-354"></a></span>
<span id="cb74-355"><a href="#cb74-355"></a>So far we've build out mathematical expressions using only plus <span class="in">`+`</span> and times <span class="in">`*`</span>, all <span class="in">`Value`</span>s are only scalar.</span>
<span id="cb74-356"><a href="#cb74-356"></a></span>
<span id="cb74-357"><a href="#cb74-357"></a>Back to the <span class="in">`Value`</span> object, we will create 1 more attribute call <span class="in">`label`</span>, make the expression more complicated by adding intermediate value <span class="in">`f`</span>, <span class="in">`d`</span>, out final node will be capital <span class="in">`L`</span>.</span>
<span id="cb74-358"><a href="#cb74-358"></a></span>
<span id="cb74-359"><a href="#cb74-359"></a><span class="fu"># Backpropagation</span></span>
<span id="cb74-360"><a href="#cb74-360"></a></span>
<span id="cb74-361"><a href="#cb74-361"></a>In backpropagation, we start at the end and are going to reverse and calculate the gradients along all the intermediate values. What we are actually computing for evert single node here is derivative of that node with respect to <span class="in">`L`</span>.</span>
<span id="cb74-362"><a href="#cb74-362"></a></span>
<span id="cb74-363"><a href="#cb74-363"></a>In neural nets, <span class="in">`L`</span> represent to a Loss function. And you will be very interested in the derivative of bassically loss function <span class="in">`L`</span> with respect to the **weights** of the neural networks.</span>
<span id="cb74-364"><a href="#cb74-364"></a></span>
<span id="cb74-365"><a href="#cb74-365"></a>We need to know how are those **leaf nodes** <span class="in">`a`</span>, <span class="in">`b`</span>, <span class="in">`c`</span>, <span class="in">`f`</span> are impacting to the loss function. We call it <span class="in">`grad`</span> and add this attribute to the <span class="in">`Value`</span> object.</span>
<span id="cb74-366"><a href="#cb74-366"></a></span>
<span id="cb74-367"><a href="#cb74-367"></a><span class="fu">## manual backpropagation example #1: simple expression</span></span>
<span id="cb74-368"><a href="#cb74-368"></a></span>
<span id="cb74-369"><a href="#cb74-369"></a>::: column-page</span>
<span id="cb74-372"><a href="#cb74-372"></a><span class="in">```{python}</span></span>
<span id="cb74-373"><a href="#cb74-373"></a>draw_dot(L)</span>
<span id="cb74-374"><a href="#cb74-374"></a><span class="in">```</span></span>
<span id="cb74-375"><a href="#cb74-375"></a>:::</span>
<span id="cb74-376"><a href="#cb74-376"></a></span>
<span id="cb74-377"><a href="#cb74-377"></a>Let's do the backpropagation **manually**:</span>
<span id="cb74-378"><a href="#cb74-378"></a></span>
<span id="cb74-379"><a href="#cb74-379"></a><span class="ss">1.  </span>First we need to calculate the <span class="in">`dL/dL`</span>, how <span class="in">`L`</span> will response if we change <span class="in">`L`</span> a tiny value <span class="in">`h`</span>. The response simply is <span class="in">`1`</span> so <span class="in">`L.grad = 1.0`</span>.</span>
<span id="cb74-380"><a href="#cb74-380"></a><span class="ss">2.  </span><span class="in">`F = d * f`</span>, so <span class="in">`dL/dd`</span> -<span class="sc">\&gt;</span> <span class="in">`(f((x+h)) - f(x))/h = ((d+h)*f - d*f)/h = h*f/h = f = -2.0`</span>. Quite straighforward, so <span class="in">`d.grad = -2.0`</span>.</span>
<span id="cb74-381"><a href="#cb74-381"></a><span class="ss">3.  </span>Similarly, <span class="in">`f.grad = d = 4`</span>.\</span>
<span id="cb74-382"><a href="#cb74-382"></a><span class="ss">4.  </span>Next, for <span class="in">`dL/dc`</span>. We first concern <span class="in">`dd/dc`</span>, we know <span class="in">`d = c + e`</span>. Same with (2) we will soon know <span class="in">`dd/dc = 1.0`</span>, by symmetry <span class="in">`dd/de = 1.0`</span>. Following the **Chain Rules** $h'(x) = f'(g(x))g'(x)$, we have <span class="in">`dL/dc = dL/dd * dd/dc = -2.0 * 1 = -2.0`</span>. [<span class="co">[</span><span class="ot">Chain Rules Wiki</span><span class="co">](https://en.wikipedia.org/wiki/Chain_rule)</span>]{.aside}</span>
<span id="cb74-383"><a href="#cb74-383"></a><span class="ss">5.  </span>By symmetry, <span class="in">`dL/de = -2.0`</span>.</span>
<span id="cb74-384"><a href="#cb74-384"></a><span class="ss">6.  </span><span class="in">`dL/da = dL/de * de/da = -2.0 * b = -2.0 * -3.0 = 6.0`</span>.</span>
<span id="cb74-385"><a href="#cb74-385"></a><span class="ss">7.  </span><span class="in">`dl/db = dL/de * de/db = -2.0 * a = -2.0 * 2.0 = -4.0`</span>.</span>
<span id="cb74-386"><a href="#cb74-386"></a></span>
<span id="cb74-387"><a href="#cb74-387"></a>We can also create a function for playing around / gradient check, and not messing up the global scope.</span>
<span id="cb74-388"><a href="#cb74-388"></a></span>
<span id="cb74-391"><a href="#cb74-391"></a><span class="in">```{python}</span></span>
<span id="cb74-392"><a href="#cb74-392"></a><span class="kw">def</span> lol():</span>
<span id="cb74-393"><a href="#cb74-393"></a></span>
<span id="cb74-394"><a href="#cb74-394"></a>    h <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb74-395"><a href="#cb74-395"></a></span>
<span id="cb74-396"><a href="#cb74-396"></a>    a <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'a'</span>)</span>
<span id="cb74-397"><a href="#cb74-397"></a>    b <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb74-398"><a href="#cb74-398"></a>    c <span class="op">=</span> Value(<span class="fl">10.0</span>, label<span class="op">=</span><span class="st">'c'</span>)</span>
<span id="cb74-399"><a href="#cb74-399"></a>    e <span class="op">=</span> a<span class="op">*</span>b<span class="op">;</span> e.label <span class="op">=</span> <span class="st">'e'</span></span>
<span id="cb74-400"><a href="#cb74-400"></a>    d <span class="op">=</span> e <span class="op">+</span> c<span class="op">;</span> d.label <span class="op">=</span> <span class="st">'d'</span></span>
<span id="cb74-401"><a href="#cb74-401"></a>    f <span class="op">=</span> Value(<span class="op">-</span><span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'f'</span>)</span>
<span id="cb74-402"><a href="#cb74-402"></a>    L <span class="op">=</span> d <span class="op">*</span> f<span class="op">;</span> L.label <span class="op">=</span> <span class="st">'L'</span></span>
<span id="cb74-403"><a href="#cb74-403"></a>    L1 <span class="op">=</span> L.data</span>
<span id="cb74-404"><a href="#cb74-404"></a></span>
<span id="cb74-405"><a href="#cb74-405"></a>    a <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'a'</span>)</span>
<span id="cb74-406"><a href="#cb74-406"></a>    b <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb74-407"><a href="#cb74-407"></a>    c <span class="op">=</span> Value(<span class="fl">10.0</span>, label<span class="op">=</span><span class="st">'c'</span>)</span>
<span id="cb74-408"><a href="#cb74-408"></a>    c.data <span class="op">+=</span> h <span class="co"># dL/dc = -2.0</span></span>
<span id="cb74-409"><a href="#cb74-409"></a>    e <span class="op">=</span> a<span class="op">*</span>b<span class="op">;</span> e.label <span class="op">=</span> <span class="st">'e'</span></span>
<span id="cb74-410"><a href="#cb74-410"></a>    d <span class="op">=</span> e <span class="op">+</span> c<span class="op">;</span> d.label <span class="op">=</span> <span class="st">'d'</span></span>
<span id="cb74-411"><a href="#cb74-411"></a>    <span class="co"># d.data += h # dL/dd = -2.0</span></span>
<span id="cb74-412"><a href="#cb74-412"></a>    f <span class="op">=</span> Value(<span class="op">-</span><span class="fl">2.0</span> <span class="co"># + h # dL/df = 4.0</span></span>
<span id="cb74-413"><a href="#cb74-413"></a>                , label<span class="op">=</span><span class="st">'f'</span>) </span>
<span id="cb74-414"><a href="#cb74-414"></a>    L <span class="op">=</span> d <span class="op">*</span> f<span class="op">;</span> L.label <span class="op">=</span> <span class="st">'L'</span></span>
<span id="cb74-415"><a href="#cb74-415"></a>    L2 <span class="op">=</span> L.data <span class="co"># + h # dL/dL = 1.0</span></span>
<span id="cb74-416"><a href="#cb74-416"></a></span>
<span id="cb74-417"><a href="#cb74-417"></a>    <span class="bu">print</span>((L2 <span class="op">-</span> L1) <span class="op">/</span> h)</span>
<span id="cb74-418"><a href="#cb74-418"></a></span>
<span id="cb74-419"><a href="#cb74-419"></a>lol()</span>
<span id="cb74-420"><a href="#cb74-420"></a><span class="in">```</span></span>
<span id="cb74-421"><a href="#cb74-421"></a></span>
<span id="cb74-422"><a href="#cb74-422"></a>So that is backpropagation \~ just recursively applying the Chain Rules, multiplying local derivatives.</span>
<span id="cb74-423"><a href="#cb74-423"></a></span>
<span id="cb74-424"><a href="#cb74-424"></a><span class="fu">## preview of a single optimization step</span></span>
<span id="cb74-425"><a href="#cb74-425"></a></span>
<span id="cb74-426"><a href="#cb74-426"></a>We can change the input that we can control <span class="in">`a, b, c, f`</span> to see 1 step of the optimization of process.</span>
<span id="cb74-427"><a href="#cb74-427"></a></span>
<span id="cb74-430"><a href="#cb74-430"></a><span class="in">```{python}</span></span>
<span id="cb74-431"><a href="#cb74-431"></a>a.grad <span class="op">=</span> <span class="fl">6.0</span></span>
<span id="cb74-432"><a href="#cb74-432"></a>b.grad <span class="op">=</span> <span class="op">-</span><span class="fl">4.0</span></span>
<span id="cb74-433"><a href="#cb74-433"></a>c.grad <span class="op">=</span> <span class="op">-</span><span class="fl">2.0</span></span>
<span id="cb74-434"><a href="#cb74-434"></a>f.grad <span class="op">=</span> <span class="fl">4.0</span></span>
<span id="cb74-435"><a href="#cb74-435"></a></span>
<span id="cb74-436"><a href="#cb74-436"></a>a.data <span class="op">+=</span> <span class="fl">0.01</span> <span class="op">*</span> a.grad</span>
<span id="cb74-437"><a href="#cb74-437"></a>b.data <span class="op">+=</span> <span class="fl">0.01</span> <span class="op">*</span> b.grad</span>
<span id="cb74-438"><a href="#cb74-438"></a>c.data <span class="op">+=</span> <span class="fl">0.01</span> <span class="op">*</span> c.grad</span>
<span id="cb74-439"><a href="#cb74-439"></a>f.data <span class="op">+=</span> <span class="fl">0.01</span> <span class="op">*</span> f.grad</span>
<span id="cb74-440"><a href="#cb74-440"></a></span>
<span id="cb74-441"><a href="#cb74-441"></a>e <span class="op">=</span> a <span class="op">*</span> b<span class="op">;</span> e.grad <span class="op">=</span> <span class="op">-</span><span class="fl">2.0</span><span class="op">;</span> e.label <span class="op">=</span> <span class="st">'e'</span></span>
<span id="cb74-442"><a href="#cb74-442"></a>d <span class="op">=</span> e <span class="op">+</span> c<span class="op">;</span> d.grad <span class="op">=</span> <span class="op">-</span><span class="fl">2.0</span><span class="op">;</span> d.label <span class="op">=</span> <span class="st">'d'</span></span>
<span id="cb74-443"><a href="#cb74-443"></a>L <span class="op">=</span> d <span class="op">*</span> f</span>
<span id="cb74-444"><a href="#cb74-444"></a></span>
<span id="cb74-445"><a href="#cb74-445"></a><span class="bu">print</span>(L.data)</span>
<span id="cb74-446"><a href="#cb74-446"></a><span class="in">```</span></span>
<span id="cb74-447"><a href="#cb74-447"></a></span>
<span id="cb74-448"><a href="#cb74-448"></a>We can see the changes, <span class="in">`L`</span> increased a little bit as expected.</span>
<span id="cb74-449"><a href="#cb74-449"></a></span>
<span id="cb74-450"><a href="#cb74-450"></a>::: column-page</span>
<span id="cb74-453"><a href="#cb74-453"></a><span class="in">```{python}</span></span>
<span id="cb74-454"><a href="#cb74-454"></a>draw_dot(L)</span>
<span id="cb74-455"><a href="#cb74-455"></a><span class="in">```</span></span>
<span id="cb74-456"><a href="#cb74-456"></a>:::</span>
<span id="cb74-457"><a href="#cb74-457"></a></span>
<span id="cb74-458"><a href="#cb74-458"></a><span class="fu">## manual backpropagation example #2: a neuron</span></span>
<span id="cb74-459"><a href="#cb74-459"></a></span>
<span id="cb74-460"><a href="#cb74-460"></a>Anatomy of neurons, we have:</span>
<span id="cb74-461"><a href="#cb74-461"></a></span>
<span id="cb74-462"><a href="#cb74-462"></a><span class="ss">-   </span><span class="in">`axon`</span> as input $x_0$;</span>
<span id="cb74-463"><a href="#cb74-463"></a><span class="ss">-   </span><span class="in">`synapse`</span> string as weight $w_0$;</span>
<span id="cb74-464"><a href="#cb74-464"></a><span class="ss">-   </span>information flows into the cell body will be $x_0w_0$;</span>
<span id="cb74-465"><a href="#cb74-465"></a><span class="ss">-   </span>there are multiple inputs $x_iw_i$ flow into the cell body;</span>
<span id="cb74-466"><a href="#cb74-466"></a><span class="ss">-   </span>the cell body has some *bias* itself $b$;</span>
<span id="cb74-467"><a href="#cb74-467"></a><span class="ss">-   </span>the cell body processes all information, the output will flow through an *activation* function \~ which is some kind of a squashing function, like <span class="in">`sigmoid`</span>, <span class="in">`tanh`</span> or something like that;</span>
<span id="cb74-468"><a href="#cb74-468"></a></span>
<span id="cb74-469"><a href="#cb74-469"></a>::: {layout-ncol="1"}</span>
<span id="cb74-470"><a href="#cb74-470"></a><span class="al">![Neural net Structure with an Activation Function, CS231n Stanford 2017](https://www.researchgate.net/publication/364814302/figure/fig5/AS:11431281092677232@1666928276027/Neural-net-Structure-with-an-Activation-Function-Source-CS231n-Stanford-2017.png)</span>{width="100%"}</span>
<span id="cb74-471"><a href="#cb74-471"></a>:::</span>
<span id="cb74-472"><a href="#cb74-472"></a></span>
<span id="cb74-473"><a href="#cb74-473"></a>üöÄHow does the <span class="in">`tanh`</span> look like? this hyperbolic function will squash the output to the edge values: <span class="in">`-1.0`</span> or <span class="in">`1.0`</span>.</span>
<span id="cb74-474"><a href="#cb74-474"></a></span>
<span id="cb74-477"><a href="#cb74-477"></a><span class="in">```{python}</span></span>
<span id="cb74-478"><a href="#cb74-478"></a>plt.plot(np.arange(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="fl">0.2</span>), np.tanh(np.arange(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="fl">0.2</span>)))<span class="op">;</span> plt.grid()</span>
<span id="cb74-479"><a href="#cb74-479"></a><span class="in">```</span></span>
<span id="cb74-480"><a href="#cb74-480"></a></span>
<span id="cb74-481"><a href="#cb74-481"></a>::: column-margin</span>
<span id="cb74-482"><a href="#cb74-482"></a>We first implement <span class="co">[</span><span class="ot">`tanh`</span><span class="co">](https://en.wikipedia.org/wiki/Hyperbolic_functions)</span> function to our class <span class="in">`Value`</span>.</span>
<span id="cb74-483"><a href="#cb74-483"></a></span>
<span id="cb74-486"><a href="#cb74-486"></a><span class="in">```{python}</span></span>
<span id="cb74-487"><a href="#cb74-487"></a><span class="co">#| eval: false</span></span>
<span id="cb74-488"><a href="#cb74-488"></a><span class="kw">def</span> tanh(<span class="va">self</span>):</span>
<span id="cb74-489"><a href="#cb74-489"></a>    x <span class="op">=</span> <span class="va">self</span>.data</span>
<span id="cb74-490"><a href="#cb74-490"></a>    t <span class="op">=</span> (math.exp(<span class="dv">2</span><span class="op">*</span>x) <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> (math.exp(<span class="dv">2</span><span class="op">*</span>x) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb74-491"><a href="#cb74-491"></a>    out <span class="op">=</span> Value(t, (<span class="va">self</span>, ), <span class="st">'tanh'</span>)</span>
<span id="cb74-492"><a href="#cb74-492"></a>    <span class="cf">return</span> out</span>
<span id="cb74-493"><a href="#cb74-493"></a></span>
<span id="cb74-494"><a href="#cb74-494"></a>Value.tanh <span class="op">=</span> tanh</span>
<span id="cb74-495"><a href="#cb74-495"></a><span class="in">```</span></span>
<span id="cb74-496"><a href="#cb74-496"></a>:::</span>
<span id="cb74-497"><a href="#cb74-497"></a></span>
<span id="cb74-498"><a href="#cb74-498"></a>Let's take a simple example of 2-dimensional neuron with 2 inputs <span class="in">`x1`</span> and <span class="in">`x2`</span>:</span>
<span id="cb74-499"><a href="#cb74-499"></a></span>
<span id="cb74-502"><a href="#cb74-502"></a><span class="in">```{python}</span></span>
<span id="cb74-503"><a href="#cb74-503"></a><span class="co"># input x1, x2</span></span>
<span id="cb74-504"><a href="#cb74-504"></a>x1 <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'x1'</span>)</span>
<span id="cb74-505"><a href="#cb74-505"></a>x2 <span class="op">=</span> Value(<span class="fl">0.0</span>, label<span class="op">=</span><span class="st">'x2'</span>)</span>
<span id="cb74-506"><a href="#cb74-506"></a><span class="co"># weights w1,w2</span></span>
<span id="cb74-507"><a href="#cb74-507"></a>w1 <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'w1'</span>)</span>
<span id="cb74-508"><a href="#cb74-508"></a>w2 <span class="op">=</span> Value(<span class="fl">1.0</span>, label<span class="op">=</span><span class="st">'w2'</span>)</span>
<span id="cb74-509"><a href="#cb74-509"></a><span class="co"># bias of neuron b</span></span>
<span id="cb74-510"><a href="#cb74-510"></a>b <span class="op">=</span> Value(<span class="fl">6.88137358</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb74-511"><a href="#cb74-511"></a><span class="co"># x1*w1 + x2*w2 + b</span></span>
<span id="cb74-512"><a href="#cb74-512"></a>x1w1 <span class="op">=</span> x1<span class="op">*</span>w1<span class="op">;</span> x1w1.label <span class="op">=</span> <span class="st">'x1w1'</span></span>
<span id="cb74-513"><a href="#cb74-513"></a>x2w2 <span class="op">=</span> x2<span class="op">*</span>w2<span class="op">;</span> x2w2.label <span class="op">=</span> <span class="st">'x2w2'</span></span>
<span id="cb74-514"><a href="#cb74-514"></a>x1w1x2w2 <span class="op">=</span> x1w1 <span class="op">+</span> x2w2<span class="op">;</span> x1w1x2w2.label <span class="op">=</span> <span class="st">'x1w1 + x2w2'</span></span>
<span id="cb74-515"><a href="#cb74-515"></a>n <span class="op">=</span> x1w1x2w2 <span class="op">+</span> b<span class="op">;</span> n.label <span class="op">=</span> <span class="st">'n'</span></span>
<span id="cb74-516"><a href="#cb74-516"></a></span>
<span id="cb74-517"><a href="#cb74-517"></a>o <span class="op">=</span> n.tanh()<span class="op">;</span> o.label <span class="op">=</span> <span class="st">'o'</span> <span class="co"># not define yet</span></span>
<span id="cb74-518"><a href="#cb74-518"></a><span class="in">```</span></span>
<span id="cb74-519"><a href="#cb74-519"></a></span>
<span id="cb74-520"><a href="#cb74-520"></a>::: column-page</span>
<span id="cb74-523"><a href="#cb74-523"></a><span class="in">```{python}</span></span>
<span id="cb74-524"><a href="#cb74-524"></a>draw_dot(o)</span>
<span id="cb74-525"><a href="#cb74-525"></a><span class="in">```</span></span>
<span id="cb74-526"><a href="#cb74-526"></a>:::</span>
<span id="cb74-527"><a href="#cb74-527"></a></span>
<span id="cb74-528"><a href="#cb74-528"></a>::: column-margin</span>
<span id="cb74-531"><a href="#cb74-531"></a><span class="in">```{python}</span></span>
<span id="cb74-532"><a href="#cb74-532"></a>o.grad <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb74-533"><a href="#cb74-533"></a>n.grad <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> o.data <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb74-534"><a href="#cb74-534"></a>b.grad <span class="op">=</span> n.grad</span>
<span id="cb74-535"><a href="#cb74-535"></a>x1w1x2w2.grad <span class="op">=</span> n.grad</span>
<span id="cb74-536"><a href="#cb74-536"></a>x1w1.grad <span class="op">=</span> x1w1x2w2.grad</span>
<span id="cb74-537"><a href="#cb74-537"></a>x2w2.grad <span class="op">=</span> x1w1x2w2.grad</span>
<span id="cb74-538"><a href="#cb74-538"></a>x1.grad <span class="op">=</span> w1.data <span class="op">*</span> x1w1.grad</span>
<span id="cb74-539"><a href="#cb74-539"></a>w1.grad <span class="op">=</span> x1.data <span class="op">*</span> x1w1.grad</span>
<span id="cb74-540"><a href="#cb74-540"></a>x2.grad <span class="op">=</span> w2.data <span class="op">*</span> x2w2.grad</span>
<span id="cb74-541"><a href="#cb74-541"></a>w2.grad <span class="op">=</span> x2.data <span class="op">*</span> x2w2.grad</span>
<span id="cb74-542"><a href="#cb74-542"></a><span class="in">```</span></span>
<span id="cb74-543"><a href="#cb74-543"></a>:::</span>
<span id="cb74-544"><a href="#cb74-544"></a></span>
<span id="cb74-545"><a href="#cb74-545"></a>From here we will manually calculate the gradient again:</span>
<span id="cb74-546"><a href="#cb74-546"></a></span>
<span id="cb74-547"><a href="#cb74-547"></a><span class="ss">1.  </span><span class="in">`do/do = 1`</span>, that's the base case, so <span class="in">`o.grad = 1.0`</span>.</span>
<span id="cb74-548"><a href="#cb74-548"></a><span class="ss">2.  </span><span class="in">`o = tanh(n)`</span>, follow that Wiki link (and of course can be easily proof) we have <span class="in">`do/dn = 1 - tanh(x)^2 = 1 - o^2`</span>.</span>
<span id="cb74-549"><a href="#cb74-549"></a><span class="ss">3.  </span><span class="in">`n = x1w1x2w2 + b`</span>, this is plus node, which gradient will flow to children equally, <span class="in">`do/db = do/dn * dn/db = do/dn * 1`</span>.</span>
<span id="cb74-550"><a href="#cb74-550"></a><span class="ss">4.  </span>By symmertry, <span class="in">`do/dx1w1x2w2 = do/db`</span>.</span>
<span id="cb74-551"><a href="#cb74-551"></a><span class="ss">5.  </span><span class="in">`do/dx1w1 = do/dx1w1x2w2`</span>.</span>
<span id="cb74-552"><a href="#cb74-552"></a><span class="ss">6.  </span><span class="in">`do/dx2w2 = do/dx1w1x2w2`</span>.</span>
<span id="cb74-553"><a href="#cb74-553"></a><span class="ss">7.  </span><span class="in">`do/dx1 = w1 * do/dx1w1`</span>.</span>
<span id="cb74-554"><a href="#cb74-554"></a><span class="ss">8.  </span><span class="in">`do/dw1 = x1 * do/dx1w1`</span>.</span>
<span id="cb74-555"><a href="#cb74-555"></a><span class="ss">9.  </span><span class="in">`do/dx2 = w2 * do/dx2w2`</span>.</span>
<span id="cb74-556"><a href="#cb74-556"></a><span class="ss">10. </span><span class="in">`do/dw2 = x2 * do/dx2w2`</span>.</span>
<span id="cb74-557"><a href="#cb74-557"></a></span>
<span id="cb74-558"><a href="#cb74-558"></a>::: column-page</span>
<span id="cb74-561"><a href="#cb74-561"></a><span class="in">```{python}</span></span>
<span id="cb74-562"><a href="#cb74-562"></a>draw_dot(o)</span>
<span id="cb74-563"><a href="#cb74-563"></a><span class="in">```</span></span>
<span id="cb74-564"><a href="#cb74-564"></a>:::</span>
<span id="cb74-565"><a href="#cb74-565"></a></span>
<span id="cb74-566"><a href="#cb74-566"></a><span class="fu">## implementing the backward function for each operation</span></span>
<span id="cb74-567"><a href="#cb74-567"></a></span>
<span id="cb74-568"><a href="#cb74-568"></a>Doing the backpropagation manually is obviously ridiculous and we are now to put an end to this suffering. We will see how we can implement backward pass a bit more automatically.</span>
<span id="cb74-569"><a href="#cb74-569"></a></span>
<span id="cb74-570"><a href="#cb74-570"></a>We create <span class="in">`_backward`</span> operation for each operator, implement the Chain Rules. Activate the <span class="in">`_backward`</span> call along with funtion execution.</span>
<span id="cb74-571"><a href="#cb74-571"></a></span>
<span id="cb74-574"><a href="#cb74-574"></a><span class="in">```{python}</span></span>
<span id="cb74-575"><a href="#cb74-575"></a>o.grad <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb74-576"><a href="#cb74-576"></a></span>
<span id="cb74-577"><a href="#cb74-577"></a>o._backward()</span>
<span id="cb74-578"><a href="#cb74-578"></a>n._backward()</span>
<span id="cb74-579"><a href="#cb74-579"></a>b._backward()</span>
<span id="cb74-580"><a href="#cb74-580"></a>x1w1x2w2._backward()</span>
<span id="cb74-581"><a href="#cb74-581"></a>x1w1._backward()</span>
<span id="cb74-582"><a href="#cb74-582"></a>x2w2._backward()</span>
<span id="cb74-583"><a href="#cb74-583"></a><span class="in">```</span></span>
<span id="cb74-584"><a href="#cb74-584"></a></span>
<span id="cb74-585"><a href="#cb74-585"></a>::: column-page</span>
<span id="cb74-588"><a href="#cb74-588"></a><span class="in">```{python}</span></span>
<span id="cb74-589"><a href="#cb74-589"></a>draw_dot(o)</span>
<span id="cb74-590"><a href="#cb74-590"></a><span class="in">```</span></span>
<span id="cb74-591"><a href="#cb74-591"></a>:::</span>
<span id="cb74-592"><a href="#cb74-592"></a></span>
<span id="cb74-593"><a href="#cb74-593"></a>We still need to call the <span class="in">`_backward`</span> node by node. Now we move to the next step, to implement backward function to whole expression graph.</span>
<span id="cb74-594"><a href="#cb74-594"></a></span>
<span id="cb74-595"><a href="#cb74-595"></a><span class="fu">## implementing the backward function for a whole expression graph</span></span>
<span id="cb74-596"><a href="#cb74-596"></a></span>
<span id="cb74-597"><a href="#cb74-597"></a>In short, we need to do everything after each node before we call the backward function itself. For every node, all dependencies, everything that it depends on has to propagate to it before we can continue backpropagation.</span>
<span id="cb74-598"><a href="#cb74-598"></a></span>
<span id="cb74-599"><a href="#cb74-599"></a>This ordering of graph can be archived using something like <span class="co">[</span><span class="ot">topological sort</span><span class="co">](https://en.wikipedia.org/wiki/Topological_sorting)</span>.</span>
<span id="cb74-600"><a href="#cb74-600"></a></span>
<span id="cb74-601"><a href="#cb74-601"></a>::: {layout-ncol="1"}</span>
<span id="cb74-602"><a href="#cb74-602"></a>!<span class="co">[</span><span class="ot">Topological Sort, photo credit to [Claire Lee](https://yuminlee2.medium.com/topological-sort-cf9f8e43af6a)</span><span class="co">](https://miro.medium.com/v2/resize:fit:720/format:webp/1*0jRSNI2zo30sENk2qlqEvw.png)</span>{width="100%"}</span>
<span id="cb74-603"><a href="#cb74-603"></a>:::</span>
<span id="cb74-604"><a href="#cb74-604"></a></span>
<span id="cb74-605"><a href="#cb74-605"></a>::: column-margin</span>
<span id="cb74-608"><a href="#cb74-608"></a><span class="in">```{python}</span></span>
<span id="cb74-609"><a href="#cb74-609"></a><span class="co"># we first reset the Values</span></span>
<span id="cb74-610"><a href="#cb74-610"></a>x1 <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'x1'</span>)</span>
<span id="cb74-611"><a href="#cb74-611"></a>x2 <span class="op">=</span> Value(<span class="fl">0.0</span>, label<span class="op">=</span><span class="st">'x2'</span>)</span>
<span id="cb74-612"><a href="#cb74-612"></a>w1 <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'w1'</span>)</span>
<span id="cb74-613"><a href="#cb74-613"></a>w2 <span class="op">=</span> Value(<span class="fl">1.0</span>, label<span class="op">=</span><span class="st">'w2'</span>)</span>
<span id="cb74-614"><a href="#cb74-614"></a>b <span class="op">=</span> Value(<span class="fl">6.88137358</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb74-615"><a href="#cb74-615"></a>x1w1 <span class="op">=</span> x1<span class="op">*</span>w1<span class="op">;</span> x1w1.label <span class="op">=</span> <span class="st">'x1w1'</span></span>
<span id="cb74-616"><a href="#cb74-616"></a>x2w2 <span class="op">=</span> x2<span class="op">*</span>w2<span class="op">;</span> x2w2.label <span class="op">=</span> <span class="st">'x2w2'</span></span>
<span id="cb74-617"><a href="#cb74-617"></a>x1w1x2w2 <span class="op">=</span> x1w1 <span class="op">+</span> x2w2<span class="op">;</span> x1w1x2w2.label <span class="op">=</span> <span class="st">'x1w1 + x2w2'</span></span>
<span id="cb74-618"><a href="#cb74-618"></a>n <span class="op">=</span> x1w1x2w2 <span class="op">+</span> b<span class="op">;</span> n.label <span class="op">=</span> <span class="st">'n'</span></span>
<span id="cb74-619"><a href="#cb74-619"></a>o <span class="op">=</span> n.tanh()<span class="op">;</span> o.label <span class="op">=</span> <span class="st">'o'</span></span>
<span id="cb74-620"><a href="#cb74-620"></a><span class="in">```</span></span>
<span id="cb74-621"><a href="#cb74-621"></a>:::</span>
<span id="cb74-622"><a href="#cb74-622"></a></span>
<span id="cb74-623"><a href="#cb74-623"></a>Below is the code:</span>
<span id="cb74-624"><a href="#cb74-624"></a></span>
<span id="cb74-627"><a href="#cb74-627"></a><span class="in">```{python}</span></span>
<span id="cb74-628"><a href="#cb74-628"></a>topo <span class="op">=</span> []</span>
<span id="cb74-629"><a href="#cb74-629"></a>visited <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb74-630"><a href="#cb74-630"></a></span>
<span id="cb74-631"><a href="#cb74-631"></a><span class="kw">def</span> build_topo(v):</span>
<span id="cb74-632"><a href="#cb74-632"></a>    <span class="cf">if</span> v <span class="kw">not</span> <span class="kw">in</span> visited:</span>
<span id="cb74-633"><a href="#cb74-633"></a>        visited.add(v)</span>
<span id="cb74-634"><a href="#cb74-634"></a>        <span class="cf">for</span> child <span class="kw">in</span> v._prev:</span>
<span id="cb74-635"><a href="#cb74-635"></a>            build_topo(child) <span class="co"># recursively look up all children for v</span></span>
<span id="cb74-636"><a href="#cb74-636"></a>        topo.append(v)</span>
<span id="cb74-637"><a href="#cb74-637"></a></span>
<span id="cb74-638"><a href="#cb74-638"></a>build_topo(o)</span>
<span id="cb74-639"><a href="#cb74-639"></a>topo</span>
<span id="cb74-640"><a href="#cb74-640"></a><span class="in">```</span></span>
<span id="cb74-641"><a href="#cb74-641"></a></span>
<span id="cb74-642"><a href="#cb74-642"></a>We implement the topological sort to <span class="in">`backward()`</span> (without underscore) function. Now we can trigger the whole process:</span>
<span id="cb74-643"><a href="#cb74-643"></a></span>
<span id="cb74-646"><a href="#cb74-646"></a><span class="in">```{python}</span></span>
<span id="cb74-647"><a href="#cb74-647"></a>o.backward()</span>
<span id="cb74-648"><a href="#cb74-648"></a><span class="in">```</span></span>
<span id="cb74-649"><a href="#cb74-649"></a></span>
<span id="cb74-650"><a href="#cb74-650"></a>::: column-page</span>
<span id="cb74-653"><a href="#cb74-653"></a><span class="in">```{python}</span></span>
<span id="cb74-654"><a href="#cb74-654"></a>draw_dot(o)</span>
<span id="cb74-655"><a href="#cb74-655"></a><span class="in">```</span></span>
<span id="cb74-656"><a href="#cb74-656"></a>:::</span>
<span id="cb74-657"><a href="#cb74-657"></a></span>
<span id="cb74-658"><a href="#cb74-658"></a><span class="fu">## fixing a backprop bug when one node is used multiple times ‚õî</span></span>
<span id="cb74-659"><a href="#cb74-659"></a></span>
<span id="cb74-660"><a href="#cb74-660"></a>This <span class="in">`a.grad`</span> should be <span class="in">`2.0`</span>.</span>
<span id="cb74-661"><a href="#cb74-661"></a></span>
<span id="cb74-664"><a href="#cb74-664"></a><span class="in">```{python}</span></span>
<span id="cb74-665"><a href="#cb74-665"></a>a <span class="op">=</span> Value(<span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'a'</span>)</span>
<span id="cb74-666"><a href="#cb74-666"></a>b <span class="op">=</span> a <span class="op">+</span> a<span class="op">;</span> b.label <span class="op">=</span> <span class="st">'b'</span> <span class="co"># this case self and other are both a, we should not overwrite the gradient, we should accumulate it.</span></span>
<span id="cb74-667"><a href="#cb74-667"></a>b.backward()</span>
<span id="cb74-668"><a href="#cb74-668"></a>draw_dot(b)</span>
<span id="cb74-669"><a href="#cb74-669"></a><span class="in">```</span></span>
<span id="cb74-670"><a href="#cb74-670"></a></span>
<span id="cb74-671"><a href="#cb74-671"></a><span class="fu">## breaking up a `tanh`, exercising with more operations</span></span>
<span id="cb74-672"><a href="#cb74-672"></a></span>
<span id="cb74-673"><a href="#cb74-673"></a>Sometime we do operations between <span class="in">`Value`</span> and other, like <span class="in">`int`</span>. We can not do this unless we add below code to <span class="in">`__add__`</span> and <span class="in">`__mul__`</span> operations. Now we can <span class="in">`Value(1.0) + 1.0`</span>, or <span class="in">`Value(2.0) * 2`</span>.</span>
<span id="cb74-674"><a href="#cb74-674"></a></span>
<span id="cb74-677"><a href="#cb74-677"></a><span class="in">```{python}</span></span>
<span id="cb74-678"><a href="#cb74-678"></a><span class="co">#| eval: false</span></span>
<span id="cb74-679"><a href="#cb74-679"></a>other <span class="op">=</span> other <span class="cf">if</span> <span class="bu">isinstance</span>(other, Value) <span class="cf">else</span> Value(other)</span>
<span id="cb74-680"><a href="#cb74-680"></a><span class="in">```</span></span>
<span id="cb74-681"><a href="#cb74-681"></a></span>
<span id="cb74-682"><a href="#cb74-682"></a>But for <span class="in">`2 * Value(2.0)`</span>, which will internally call <span class="in">`2.__mul__(Value(2.0))`</span>, will not work. We add <span class="in">`__rmul__`</span>:</span>
<span id="cb74-683"><a href="#cb74-683"></a></span>
<span id="cb74-686"><a href="#cb74-686"></a><span class="in">```{python}</span></span>
<span id="cb74-687"><a href="#cb74-687"></a><span class="kw">def</span> <span class="fu">__rmul__</span>(<span class="va">self</span>, other): <span class="co"># other * self</span></span>
<span id="cb74-688"><a href="#cb74-688"></a>    <span class="cf">return</span> <span class="va">self</span> <span class="op">*</span> other</span>
<span id="cb74-689"><a href="#cb74-689"></a></span>
<span id="cb74-690"><a href="#cb74-690"></a>Value.<span class="fu">__rmul__</span> <span class="op">=</span> <span class="fu">__rmul__</span></span>
<span id="cb74-691"><a href="#cb74-691"></a><span class="in">```</span></span>
<span id="cb74-692"><a href="#cb74-692"></a></span>
<span id="cb74-693"><a href="#cb74-693"></a>For exponential, we add <span class="in">`epx`</span>:</span>
<span id="cb74-694"><a href="#cb74-694"></a></span>
<span id="cb74-697"><a href="#cb74-697"></a><span class="in">```{python}</span></span>
<span id="cb74-698"><a href="#cb74-698"></a><span class="kw">def</span> exp(<span class="va">self</span>):</span>
<span id="cb74-699"><a href="#cb74-699"></a>    x <span class="op">=</span> <span class="va">self</span>.data</span>
<span id="cb74-700"><a href="#cb74-700"></a>    out <span class="op">=</span> Value(math.exp(x), (<span class="va">self</span>, ), <span class="st">'exp'</span>)</span>
<span id="cb74-701"><a href="#cb74-701"></a></span>
<span id="cb74-702"><a href="#cb74-702"></a>    <span class="kw">def</span> _backward():</span>
<span id="cb74-703"><a href="#cb74-703"></a>        <span class="va">self</span>.grad <span class="op">+=</span> out.data <span class="op">*</span> out.grad</span>
<span id="cb74-704"><a href="#cb74-704"></a>    out._backward <span class="op">=</span> _backward</span>
<span id="cb74-705"><a href="#cb74-705"></a></span>
<span id="cb74-706"><a href="#cb74-706"></a>    <span class="cf">return</span> out</span>
<span id="cb74-707"><a href="#cb74-707"></a></span>
<span id="cb74-708"><a href="#cb74-708"></a>Value.exp <span class="op">=</span> exp</span>
<span id="cb74-709"><a href="#cb74-709"></a><span class="in">```</span></span>
<span id="cb74-710"><a href="#cb74-710"></a></span>
<span id="cb74-711"><a href="#cb74-711"></a>For division, we add <span class="in">`__truediv__`</span>:</span>
<span id="cb74-712"><a href="#cb74-712"></a></span>
<span id="cb74-715"><a href="#cb74-715"></a><span class="in">```{python}</span></span>
<span id="cb74-716"><a href="#cb74-716"></a><span class="kw">def</span> <span class="fu">__truediv__</span>(<span class="va">self</span>, other): <span class="co"># self / other</span></span>
<span id="cb74-717"><a href="#cb74-717"></a>    <span class="cf">return</span> <span class="va">self</span> <span class="op">*</span> other<span class="op">**</span>(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb74-718"><a href="#cb74-718"></a></span>
<span id="cb74-719"><a href="#cb74-719"></a>Value.<span class="fu">__truediv__</span> <span class="op">=</span> <span class="fu">__truediv__</span></span>
<span id="cb74-720"><a href="#cb74-720"></a><span class="in">```</span></span>
<span id="cb74-721"><a href="#cb74-721"></a></span>
<span id="cb74-722"><a href="#cb74-722"></a>For power, we add <span class="in">`__pow__`</span>:</span>
<span id="cb74-723"><a href="#cb74-723"></a></span>
<span id="cb74-726"><a href="#cb74-726"></a><span class="in">```{python}</span></span>
<span id="cb74-727"><a href="#cb74-727"></a><span class="kw">def</span> <span class="fu">__pow__</span>(<span class="va">self</span>, other): <span class="co"># self ** other</span></span>
<span id="cb74-728"><a href="#cb74-728"></a>    <span class="cf">assert</span> <span class="bu">isinstance</span>(other, (<span class="bu">int</span>, <span class="bu">float</span>)), <span class="st">"TypeError: only supporting int/float power for now"</span></span>
<span id="cb74-729"><a href="#cb74-729"></a>    out <span class="op">=</span> Value(<span class="va">self</span>.data<span class="op">**</span>other, (<span class="va">self</span>, ), <span class="ss">f'**</span><span class="sc">{</span>other<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb74-730"><a href="#cb74-730"></a></span>
<span id="cb74-731"><a href="#cb74-731"></a>    <span class="kw">def</span> _backward():</span>
<span id="cb74-732"><a href="#cb74-732"></a>        <span class="va">self</span>.grad <span class="op">+=</span> other <span class="op">*</span> ( <span class="va">self</span>.data <span class="op">**</span> (other <span class="op">-</span> <span class="dv">1</span>)) <span class="op">*</span> out.grad</span>
<span id="cb74-733"><a href="#cb74-733"></a>    out._backward <span class="op">=</span> _backward</span>
<span id="cb74-734"><a href="#cb74-734"></a></span>
<span id="cb74-735"><a href="#cb74-735"></a>    <span class="cf">return</span> out</span>
<span id="cb74-736"><a href="#cb74-736"></a></span>
<span id="cb74-737"><a href="#cb74-737"></a>Value.<span class="fu">__pow__</span> <span class="op">=</span> <span class="fu">__pow__</span> </span>
<span id="cb74-738"><a href="#cb74-738"></a><span class="in">```</span></span>
<span id="cb74-739"><a href="#cb74-739"></a></span>
<span id="cb74-740"><a href="#cb74-740"></a>For subtract, we add <span class="in">`__neg__`</span> and <span class="in">`__sub__`</span>:</span>
<span id="cb74-741"><a href="#cb74-741"></a></span>
<span id="cb74-744"><a href="#cb74-744"></a><span class="in">```{python}</span></span>
<span id="cb74-745"><a href="#cb74-745"></a><span class="kw">def</span> <span class="fu">__neg__</span>(<span class="va">self</span>): <span class="co"># - self</span></span>
<span id="cb74-746"><a href="#cb74-746"></a>    <span class="cf">return</span> <span class="op">-</span> <span class="va">self</span></span>
<span id="cb74-747"><a href="#cb74-747"></a></span>
<span id="cb74-748"><a href="#cb74-748"></a>Value.<span class="fu">__neg__</span> <span class="op">=</span> <span class="fu">__neg__</span> <span class="co"># self - other</span></span>
<span id="cb74-749"><a href="#cb74-749"></a></span>
<span id="cb74-750"><a href="#cb74-750"></a><span class="kw">def</span> <span class="fu">__sub__</span>(<span class="va">self</span>, other):</span>
<span id="cb74-751"><a href="#cb74-751"></a>    <span class="cf">return</span> <span class="va">self</span> <span class="op">+</span> (<span class="op">-</span>other)</span>
<span id="cb74-752"><a href="#cb74-752"></a></span>
<span id="cb74-753"><a href="#cb74-753"></a>Value.<span class="fu">__sub__</span> <span class="op">=</span> <span class="fu">__sub__</span></span>
<span id="cb74-754"><a href="#cb74-754"></a><span class="in">```</span></span>
<span id="cb74-755"><a href="#cb74-755"></a></span>
<span id="cb74-756"><a href="#cb74-756"></a>Now we are ready to try <span class="in">`tanh`</span> in a different way:</span>
<span id="cb74-757"><a href="#cb74-757"></a></span>
<span id="cb74-760"><a href="#cb74-760"></a><span class="in">```{python}</span></span>
<span id="cb74-761"><a href="#cb74-761"></a>x1 <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'x1'</span>)</span>
<span id="cb74-762"><a href="#cb74-762"></a>x2 <span class="op">=</span> Value(<span class="fl">0.0</span>, label<span class="op">=</span><span class="st">'x2'</span>)</span>
<span id="cb74-763"><a href="#cb74-763"></a>w1 <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'w1'</span>)</span>
<span id="cb74-764"><a href="#cb74-764"></a>w2 <span class="op">=</span> Value(<span class="fl">1.0</span>, label<span class="op">=</span><span class="st">'w2'</span>)</span>
<span id="cb74-765"><a href="#cb74-765"></a>b <span class="op">=</span> Value(<span class="fl">6.88137358</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb74-766"><a href="#cb74-766"></a>x1w1 <span class="op">=</span> x1<span class="op">*</span>w1<span class="op">;</span> x1w1.label <span class="op">=</span> <span class="st">'x1w1'</span></span>
<span id="cb74-767"><a href="#cb74-767"></a>x2w2 <span class="op">=</span> x2<span class="op">*</span>w2<span class="op">;</span> x2w2.label <span class="op">=</span> <span class="st">'x2w2'</span></span>
<span id="cb74-768"><a href="#cb74-768"></a>x1w1x2w2 <span class="op">=</span> x1w1 <span class="op">+</span> x2w2<span class="op">;</span> x1w1x2w2.label <span class="op">=</span> <span class="st">'x1w1 + x2w2'</span></span>
<span id="cb74-769"><a href="#cb74-769"></a>n <span class="op">=</span> x1w1x2w2 <span class="op">+</span> b<span class="op">;</span> n.label <span class="op">=</span> <span class="st">'n'</span></span>
<span id="cb74-770"><a href="#cb74-770"></a></span>
<span id="cb74-771"><a href="#cb74-771"></a>e <span class="op">=</span> (<span class="dv">2</span><span class="op">*</span>n).exp()<span class="op">;</span> e.label <span class="op">=</span> <span class="st">'e'</span></span>
<span id="cb74-772"><a href="#cb74-772"></a>o <span class="op">=</span> (e <span class="op">-</span> <span class="dv">1</span>)<span class="op">/</span>(e <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb74-773"><a href="#cb74-773"></a>o.label <span class="op">=</span> <span class="st">'o'</span></span>
<span id="cb74-774"><a href="#cb74-774"></a>o.backward()</span>
<span id="cb74-775"><a href="#cb74-775"></a><span class="in">```</span></span>
<span id="cb74-776"><a href="#cb74-776"></a></span>
<span id="cb74-777"><a href="#cb74-777"></a>::: column-screen</span>
<span id="cb74-780"><a href="#cb74-780"></a><span class="in">```{python}</span></span>
<span id="cb74-781"><a href="#cb74-781"></a>draw_dot(o)</span>
<span id="cb74-782"><a href="#cb74-782"></a><span class="in">```</span></span>
<span id="cb74-783"><a href="#cb74-783"></a>:::</span>
<span id="cb74-784"><a href="#cb74-784"></a></span>
<span id="cb74-785"><a href="#cb74-785"></a><span class="fu"># PyTorch comparison</span></span>
<span id="cb74-786"><a href="#cb74-786"></a></span>
<span id="cb74-787"><a href="#cb74-787"></a><span class="fu">## doing the same thing but in PyTorch: comparison</span></span>
<span id="cb74-788"><a href="#cb74-788"></a></span>
<span id="cb74-791"><a href="#cb74-791"></a><span class="in">```{python}</span></span>
<span id="cb74-792"><a href="#cb74-792"></a><span class="im">import</span> torch</span>
<span id="cb74-793"><a href="#cb74-793"></a></span>
<span id="cb74-794"><a href="#cb74-794"></a>x1 <span class="op">=</span> torch.tensor([<span class="fl">2.0</span>]).double()<span class="op">;</span> x1.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb74-795"><a href="#cb74-795"></a>x2 <span class="op">=</span> torch.tensor([<span class="fl">0.0</span>]).double()<span class="op">;</span> x2.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb74-796"><a href="#cb74-796"></a>w1 <span class="op">=</span> torch.tensor([<span class="op">-</span><span class="fl">3.0</span>]).double()<span class="op">;</span> w1.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb74-797"><a href="#cb74-797"></a>w2 <span class="op">=</span> torch.tensor([<span class="fl">1.0</span>]).double()<span class="op">;</span> w2.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb74-798"><a href="#cb74-798"></a>b <span class="op">=</span> torch.tensor([<span class="fl">6.8813735870195432</span>]).double()<span class="op">;</span> b.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb74-799"><a href="#cb74-799"></a></span>
<span id="cb74-800"><a href="#cb74-800"></a>n <span class="op">=</span> x1<span class="op">*</span>w1 <span class="op">+</span> x2<span class="op">*</span>w2 <span class="op">+</span> b</span>
<span id="cb74-801"><a href="#cb74-801"></a>o <span class="op">=</span> torch.tanh(n)</span>
<span id="cb74-802"><a href="#cb74-802"></a></span>
<span id="cb74-803"><a href="#cb74-803"></a><span class="bu">print</span>(o.data.item())</span>
<span id="cb74-804"><a href="#cb74-804"></a>o.backward()</span>
<span id="cb74-805"><a href="#cb74-805"></a></span>
<span id="cb74-806"><a href="#cb74-806"></a><span class="bu">print</span>(<span class="st">'------------------'</span>)</span>
<span id="cb74-807"><a href="#cb74-807"></a><span class="bu">print</span>(<span class="st">'x1'</span>, x1.grad.item())</span>
<span id="cb74-808"><a href="#cb74-808"></a><span class="bu">print</span>(<span class="st">'w1'</span>, w1.grad.item())</span>
<span id="cb74-809"><a href="#cb74-809"></a><span class="bu">print</span>(<span class="st">'x2'</span>, x2.grad.item())</span>
<span id="cb74-810"><a href="#cb74-810"></a><span class="bu">print</span>(<span class="st">'w2'</span>, w2.grad.item())</span>
<span id="cb74-811"><a href="#cb74-811"></a><span class="bu">print</span>(<span class="st">'------------------'</span>)</span>
<span id="cb74-812"><a href="#cb74-812"></a><span class="in">```</span></span>
<span id="cb74-813"><a href="#cb74-813"></a></span>
<span id="cb74-814"><a href="#cb74-814"></a><span class="fu"># Building the library</span></span>
<span id="cb74-815"><a href="#cb74-815"></a></span>
<span id="cb74-816"><a href="#cb74-816"></a><span class="fu">## building out a neural net library (multi-layer perceptron) in micrograd</span></span>
<span id="cb74-817"><a href="#cb74-817"></a></span>
<span id="cb74-818"><a href="#cb74-818"></a>We are going to build out a two-layer perceptron.</span>
<span id="cb74-819"><a href="#cb74-819"></a></span>
<span id="cb74-820"><a href="#cb74-820"></a>::: {layout-ncol="1"}</span>
<span id="cb74-821"><a href="#cb74-821"></a>!<span class="co">[</span><span class="ot">A 3-layer neural network with three inputs, two hidden layers of 4 neurons each and one output layer. Notice that in both cases there are connections (synapses) between neurons across layers, but not within a layer, photo credit to [cs231n](https://cs231n.github.io/neural-networks-1/)</span><span class="co">](https://cs231n.github.io/assets/nn1/neural_net2.jpeg)</span>{width="100%"}</span>
<span id="cb74-822"><a href="#cb74-822"></a>:::</span>
<span id="cb74-823"><a href="#cb74-823"></a></span>
<span id="cb74-826"><a href="#cb74-826"></a><span class="in">```{python}</span></span>
<span id="cb74-827"><a href="#cb74-827"></a><span class="kw">class</span> Neuron:</span>
<span id="cb74-828"><a href="#cb74-828"></a></span>
<span id="cb74-829"><a href="#cb74-829"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, nin): <span class="co"># &lt;1&gt;</span></span>
<span id="cb74-830"><a href="#cb74-830"></a>        <span class="va">self</span>.w <span class="op">=</span> [Value(np.random.uniform(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(nin)]</span>
<span id="cb74-831"><a href="#cb74-831"></a>        <span class="va">self</span>.b <span class="op">=</span> Value(np.random.uniform(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb74-832"><a href="#cb74-832"></a>    </span>
<span id="cb74-833"><a href="#cb74-833"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x): <span class="co"># &lt;2&gt;</span></span>
<span id="cb74-834"><a href="#cb74-834"></a>        activation <span class="op">=</span> <span class="bu">sum</span>((wi<span class="op">*</span>xi <span class="cf">for</span> wi, xi <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.w, x)), <span class="va">self</span>.b)</span>
<span id="cb74-835"><a href="#cb74-835"></a>        out <span class="op">=</span> activation.tanh()</span>
<span id="cb74-836"><a href="#cb74-836"></a>        <span class="cf">return</span> out</span>
<span id="cb74-837"><a href="#cb74-837"></a></span>
<span id="cb74-838"><a href="#cb74-838"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb74-839"><a href="#cb74-839"></a>        <span class="cf">return</span> <span class="va">self</span>.w <span class="op">+</span> [<span class="va">self</span>.b] <span class="co"># list plus list gives you a list</span></span>
<span id="cb74-840"><a href="#cb74-840"></a></span>
<span id="cb74-841"><a href="#cb74-841"></a><span class="kw">class</span> Layer:</span>
<span id="cb74-842"><a href="#cb74-842"></a></span>
<span id="cb74-843"><a href="#cb74-843"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, nin, nout):</span>
<span id="cb74-844"><a href="#cb74-844"></a>        <span class="va">self</span>.neurons <span class="op">=</span> [Neuron(nin) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(nout)] <span class="co"># &lt;3&gt;</span></span>
<span id="cb74-845"><a href="#cb74-845"></a></span>
<span id="cb74-846"><a href="#cb74-846"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb74-847"><a href="#cb74-847"></a>        outs <span class="op">=</span> [n(x) <span class="cf">for</span> n <span class="kw">in</span> <span class="va">self</span>.neurons]</span>
<span id="cb74-848"><a href="#cb74-848"></a>        <span class="cf">return</span> outs[<span class="dv">0</span>] <span class="cf">if</span> <span class="bu">len</span>(outs) <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> outs</span>
<span id="cb74-849"><a href="#cb74-849"></a></span>
<span id="cb74-850"><a href="#cb74-850"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb74-851"><a href="#cb74-851"></a>        <span class="cf">return</span> [p <span class="cf">for</span> neuron <span class="kw">in</span> <span class="va">self</span>.neurons <span class="cf">for</span> p <span class="kw">in</span> neuron.parameters()] <span class="co"># list comprehension</span></span>
<span id="cb74-852"><a href="#cb74-852"></a>        <span class="co"># params = []</span></span>
<span id="cb74-853"><a href="#cb74-853"></a>        <span class="co"># for neuron in self.neurons:</span></span>
<span id="cb74-854"><a href="#cb74-854"></a>        <span class="co">#     ps = neuron.parameters()</span></span>
<span id="cb74-855"><a href="#cb74-855"></a>        <span class="co">#     params.extend(ps)</span></span>
<span id="cb74-856"><a href="#cb74-856"></a>        <span class="co"># return params</span></span>
<span id="cb74-857"><a href="#cb74-857"></a></span>
<span id="cb74-858"><a href="#cb74-858"></a><span class="kw">class</span> MLP:</span>
<span id="cb74-859"><a href="#cb74-859"></a></span>
<span id="cb74-860"><a href="#cb74-860"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, nin, nouts): <span class="co"># &lt;4&gt;</span></span>
<span id="cb74-861"><a href="#cb74-861"></a>        sz <span class="op">=</span> [nin] <span class="op">+</span> nouts</span>
<span id="cb74-862"><a href="#cb74-862"></a>        <span class="va">self</span>.layers <span class="op">=</span> [Layer(sz[i], sz[i<span class="op">+</span><span class="dv">1</span>]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(nouts))]</span>
<span id="cb74-863"><a href="#cb74-863"></a></span>
<span id="cb74-864"><a href="#cb74-864"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb74-865"><a href="#cb74-865"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb74-866"><a href="#cb74-866"></a>            x <span class="op">=</span> layer(x)</span>
<span id="cb74-867"><a href="#cb74-867"></a>        <span class="cf">return</span> x</span>
<span id="cb74-868"><a href="#cb74-868"></a></span>
<span id="cb74-869"><a href="#cb74-869"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb74-870"><a href="#cb74-870"></a>        <span class="cf">return</span> [p <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers <span class="cf">for</span> p <span class="kw">in</span> layer.parameters()] <span class="co"># for neuron in layer.neurons for neuron.parameters()]</span></span>
<span id="cb74-871"><a href="#cb74-871"></a><span class="in">```</span></span>
<span id="cb74-872"><a href="#cb74-872"></a></span>
<span id="cb74-873"><a href="#cb74-873"></a><span class="ss">1.  </span>Number of <span class="in">`input`</span> for the Neuron. <span class="in">`w`</span> is randomly generated for each input, same for <span class="in">`b`</span> which is the bias that controll "the happiness".</span>
<span id="cb74-874"><a href="#cb74-874"></a><span class="ss">2.  </span>Object as a function: define the forward pass of the Neuron $\sum\limits_{i=1}^{nin} w_ix_i+b$, then squash the output using <span class="in">`tanh`</span>.</span>
<span id="cb74-875"><a href="#cb74-875"></a><span class="ss">3.  </span>A Layer is a list of Neurons, <span class="in">`nout`</span> specifies how many Neurons in the Layer. Each neuron has <span class="in">`nin`</span> inputs \~ nin-D. We just initialize completely independent neurons with this given dimensionality.</span>
<span id="cb74-876"><a href="#cb74-876"></a><span class="ss">4.  </span>A MLP is a sequence of Layers, picture above depicts a 3-layers MLP containing 1 input layer and 3 output layers, we say the size is 4. We sequentially create connection from the input layer to the 1st output layer, 1st output layer to 2nd output layer,...</span>
<span id="cb74-877"><a href="#cb74-877"></a></span>
<span id="cb74-880"><a href="#cb74-880"></a><span class="in">```{python}</span></span>
<span id="cb74-881"><a href="#cb74-881"></a>nin <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb74-882"><a href="#cb74-882"></a>nouts <span class="op">=</span> [<span class="fl">2.0</span>, <span class="fl">3.0</span>, <span class="op">-</span><span class="fl">1.0</span>]</span>
<span id="cb74-883"><a href="#cb74-883"></a>[nin] <span class="op">+</span> nouts</span>
<span id="cb74-884"><a href="#cb74-884"></a><span class="in">```</span></span>
<span id="cb74-885"><a href="#cb74-885"></a></span>
<span id="cb74-888"><a href="#cb74-888"></a><span class="in">```{python}</span></span>
<span id="cb74-889"><a href="#cb74-889"></a>x <span class="op">=</span> [<span class="fl">2.0</span>, <span class="fl">3.0</span>]</span>
<span id="cb74-890"><a href="#cb74-890"></a>n <span class="op">=</span> Neuron(<span class="dv">2</span>)</span>
<span id="cb74-891"><a href="#cb74-891"></a>l <span class="op">=</span> Layer(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb74-892"><a href="#cb74-892"></a>n(x)</span>
<span id="cb74-893"><a href="#cb74-893"></a>l(x)</span>
<span id="cb74-894"><a href="#cb74-894"></a><span class="in">```</span></span>
<span id="cb74-895"><a href="#cb74-895"></a></span>
<span id="cb74-898"><a href="#cb74-898"></a><span class="in">```{python}</span></span>
<span id="cb74-899"><a href="#cb74-899"></a>x <span class="op">=</span> [<span class="fl">2.0</span>, <span class="fl">3.0</span>, <span class="op">-</span><span class="fl">1.0</span>]</span>
<span id="cb74-900"><a href="#cb74-900"></a>m <span class="op">=</span> MLP(<span class="dv">3</span>, [<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">1</span>]) <span class="co"># a MLP with 3-D input, 3 output layers contains 4, 4, 1 neurons in each layer respectively</span></span>
<span id="cb74-901"><a href="#cb74-901"></a>m(x)</span>
<span id="cb74-902"><a href="#cb74-902"></a><span class="in">```</span></span>
<span id="cb74-903"><a href="#cb74-903"></a></span>
<span id="cb74-904"><a href="#cb74-904"></a>::: column-screen</span>
<span id="cb74-907"><a href="#cb74-907"></a><span class="in">```{python}</span></span>
<span id="cb74-908"><a href="#cb74-908"></a>draw_dot(m(x))</span>
<span id="cb74-909"><a href="#cb74-909"></a><span class="in">```</span></span>
<span id="cb74-910"><a href="#cb74-910"></a>:::</span>
<span id="cb74-911"><a href="#cb74-911"></a></span>
<span id="cb74-912"><a href="#cb74-912"></a><span class="fu">## creating a tiny dataset, writing the loss function</span></span>
<span id="cb74-913"><a href="#cb74-913"></a></span>
<span id="cb74-914"><a href="#cb74-914"></a>A simple data set, <span class="in">`m()`</span> is the MLP we defined above.</span>
<span id="cb74-915"><a href="#cb74-915"></a></span>
<span id="cb74-918"><a href="#cb74-918"></a><span class="in">```{python}</span></span>
<span id="cb74-919"><a href="#cb74-919"></a>xs <span class="op">=</span> [</span>
<span id="cb74-920"><a href="#cb74-920"></a>    [<span class="fl">2.0</span>, <span class="fl">3.0</span>, <span class="op">-</span><span class="fl">1.0</span>],</span>
<span id="cb74-921"><a href="#cb74-921"></a>    [<span class="fl">3.0</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">0.5</span>],</span>
<span id="cb74-922"><a href="#cb74-922"></a>    [<span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">1.0</span>],</span>
<span id="cb74-923"><a href="#cb74-923"></a>    [<span class="fl">1.0</span>, <span class="fl">1.0</span>, <span class="op">-</span><span class="fl">1.0</span>]</span>
<span id="cb74-924"><a href="#cb74-924"></a>]</span>
<span id="cb74-925"><a href="#cb74-925"></a>ys <span class="op">=</span> [<span class="fl">1.0</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">1.0</span>] <span class="co"># designed targets</span></span>
<span id="cb74-926"><a href="#cb74-926"></a><span class="in">```</span></span>
<span id="cb74-927"><a href="#cb74-927"></a></span>
<span id="cb74-928"><a href="#cb74-928"></a>Writing the loss function. <span class="co">[</span><span class="ot">I was unable to sum a list of Value, found the solution [here](https://stackoverflow.com/questions/54153552/unsupported-operand-type-error-when-adding-objects-within-list-using-sum-functio); Edit: I used Numpy random instead of random</span><span class="co">]</span>{.aside}</span>
<span id="cb74-929"><a href="#cb74-929"></a></span>
<span id="cb74-932"><a href="#cb74-932"></a><span class="in">```{python}</span></span>
<span id="cb74-933"><a href="#cb74-933"></a>ypred <span class="op">=</span> [m(x) <span class="cf">for</span> x <span class="kw">in</span> xs]</span>
<span id="cb74-934"><a href="#cb74-934"></a>loss <span class="op">=</span> np.array([(yout <span class="op">-</span> ygt)<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> ygt, yout <span class="kw">in</span> <span class="bu">zip</span>(ys, ypred)]).<span class="bu">sum</span>()</span>
<span id="cb74-935"><a href="#cb74-935"></a></span>
<span id="cb74-936"><a href="#cb74-936"></a>loss</span>
<span id="cb74-937"><a href="#cb74-937"></a><span class="in">```</span></span>
<span id="cb74-938"><a href="#cb74-938"></a></span>
<span id="cb74-939"><a href="#cb74-939"></a>Backpropagation the <span class="in">`loss`</span>, some magical here:</span>
<span id="cb74-940"><a href="#cb74-940"></a></span>
<span id="cb74-943"><a href="#cb74-943"></a><span class="in">```{python}</span></span>
<span id="cb74-944"><a href="#cb74-944"></a>loss.backward()</span>
<span id="cb74-945"><a href="#cb74-945"></a><span class="in">```</span></span>
<span id="cb74-946"><a href="#cb74-946"></a></span>
<span id="cb74-947"><a href="#cb74-947"></a>We can look into the gradient of weight of the first neuron of the first layer (input layer)</span>
<span id="cb74-948"><a href="#cb74-948"></a></span>
<span id="cb74-951"><a href="#cb74-951"></a><span class="in">```{python}</span></span>
<span id="cb74-952"><a href="#cb74-952"></a><span class="bu">print</span>(<span class="st">'value of 1st neuron in 1st layer: '</span>,m.layers[<span class="dv">0</span>].neurons[<span class="dv">0</span>].w[<span class="dv">0</span>].data)</span>
<span id="cb74-953"><a href="#cb74-953"></a><span class="bu">print</span>(<span class="st">'grad of 1st neuron in 1st layer: '</span>,m.layers[<span class="dv">0</span>].neurons[<span class="dv">0</span>].w[<span class="dv">0</span>].grad)</span>
<span id="cb74-954"><a href="#cb74-954"></a><span class="in">```</span></span>
<span id="cb74-955"><a href="#cb74-955"></a></span>
<span id="cb74-956"><a href="#cb74-956"></a>::: column-screen</span>
<span id="cb74-959"><a href="#cb74-959"></a><span class="in">```{python}</span></span>
<span id="cb74-960"><a href="#cb74-960"></a>draw_dot(loss)</span>
<span id="cb74-961"><a href="#cb74-961"></a><span class="in">```</span></span>
<span id="cb74-962"><a href="#cb74-962"></a>:::</span>
<span id="cb74-963"><a href="#cb74-963"></a></span>
<span id="cb74-964"><a href="#cb74-964"></a><span class="fu">## collecting all of the parameters of the neural net</span></span>
<span id="cb74-965"><a href="#cb74-965"></a></span>
<span id="cb74-966"><a href="#cb74-966"></a>We aim to produce the fitness <span class="in">`ypred`</span>. <span class="in">`xs`</span> is the data, the input of problem, we can not change it. <span class="in">`ys`</span> is the ground true, can not changes as well. What we can change is the "paramters" of each neuron, which is weight <span class="in">`w`</span> and bias <span class="in">`b`</span>.</span>
<span id="cb74-967"><a href="#cb74-967"></a></span>
<span id="cb74-968"><a href="#cb74-968"></a>We add in to each class a <span class="in">`parameters()`</span> function to collect those. Finally we can get all the paramters of the MLP:</span>
<span id="cb74-969"><a href="#cb74-969"></a></span>
<span id="cb74-972"><a href="#cb74-972"></a><span class="in">```{python}</span></span>
<span id="cb74-973"><a href="#cb74-973"></a><span class="bu">len</span>(m.parameters())</span>
<span id="cb74-974"><a href="#cb74-974"></a><span class="in">```</span></span>
<span id="cb74-975"><a href="#cb74-975"></a></span>
<span id="cb74-976"><a href="#cb74-976"></a><span class="fu">## doing gradient descent optimization manually, training the network</span></span>
<span id="cb74-977"><a href="#cb74-977"></a></span>
<span id="cb74-978"><a href="#cb74-978"></a>Now we will try to change the paramters to minimize the loss, which means our prediction will be more close to the ground true.</span>
<span id="cb74-979"><a href="#cb74-979"></a></span>
<span id="cb74-980"><a href="#cb74-980"></a>**Forward pass**, calculate the loss:</span>
<span id="cb74-981"><a href="#cb74-981"></a></span>
<span id="cb74-984"><a href="#cb74-984"></a><span class="in">```{python}</span></span>
<span id="cb74-985"><a href="#cb74-985"></a>ypred <span class="op">=</span> [m(x) <span class="cf">for</span> x <span class="kw">in</span> xs]</span>
<span id="cb74-986"><a href="#cb74-986"></a>loss <span class="op">=</span> np.array([(yout <span class="op">-</span> ygt)<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> ygt, yout <span class="kw">in</span> <span class="bu">zip</span>(ys, ypred)]).<span class="bu">sum</span>()</span>
<span id="cb74-987"><a href="#cb74-987"></a>loss</span>
<span id="cb74-988"><a href="#cb74-988"></a><span class="in">```</span></span>
<span id="cb74-989"><a href="#cb74-989"></a></span>
<span id="cb74-990"><a href="#cb74-990"></a>**Backward pass**, calculate the parameters:</span>
<span id="cb74-991"><a href="#cb74-991"></a></span>
<span id="cb74-994"><a href="#cb74-994"></a><span class="in">```{python}</span></span>
<span id="cb74-995"><a href="#cb74-995"></a>loss.backward()</span>
<span id="cb74-996"><a href="#cb74-996"></a><span class="in">```</span></span>
<span id="cb74-997"><a href="#cb74-997"></a></span>
<span id="cb74-998"><a href="#cb74-998"></a>**Update the parameters**, change the parameters following opposite direction to reduce the loss:</span>
<span id="cb74-999"><a href="#cb74-999"></a></span>
<span id="cb74-1002"><a href="#cb74-1002"></a><span class="in">```{python}</span></span>
<span id="cb74-1003"><a href="#cb74-1003"></a><span class="cf">for</span> p <span class="kw">in</span> m.parameters():</span>
<span id="cb74-1004"><a href="#cb74-1004"></a>    p.data <span class="op">+=</span> <span class="op">-</span><span class="fl">0.01</span> <span class="op">*</span> p.grad <span class="co"># we want the p.data go on opposite direction of the loss</span></span>
<span id="cb74-1005"><a href="#cb74-1005"></a><span class="in">```</span></span>
<span id="cb74-1006"><a href="#cb74-1006"></a></span>
<span id="cb74-1007"><a href="#cb74-1007"></a><span class="in">`0.01`</span> is the learning rate!</span>
<span id="cb74-1008"><a href="#cb74-1008"></a></span>
<span id="cb74-1009"><a href="#cb74-1009"></a>**New loss**</span>
<span id="cb74-1010"><a href="#cb74-1010"></a></span>
<span id="cb74-1013"><a href="#cb74-1013"></a><span class="in">```{python}</span></span>
<span id="cb74-1014"><a href="#cb74-1014"></a>ypred <span class="op">=</span> [m(x) <span class="cf">for</span> x <span class="kw">in</span> xs]</span>
<span id="cb74-1015"><a href="#cb74-1015"></a>loss <span class="op">=</span> np.array([(yout <span class="op">-</span> ygt)<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> ygt, yout <span class="kw">in</span> <span class="bu">zip</span>(ys, ypred)]).<span class="bu">sum</span>()</span>
<span id="cb74-1016"><a href="#cb74-1016"></a>loss</span>
<span id="cb74-1017"><a href="#cb74-1017"></a><span class="in">```</span></span>
<span id="cb74-1018"><a href="#cb74-1018"></a></span>
<span id="cb74-1019"><a href="#cb74-1019"></a>Yeah the loss decreased. In short, the process is:</span>
<span id="cb74-1020"><a href="#cb74-1020"></a></span>
<span id="cb74-1023"><a href="#cb74-1023"></a><span class="in">```{mermaid}</span></span>
<span id="cb74-1024"><a href="#cb74-1024"></a><span class="in">%%{init: {'theme':'dark'}}%%</span></span>
<span id="cb74-1025"><a href="#cb74-1025"></a><span class="in">flowchart LR</span></span>
<span id="cb74-1026"><a href="#cb74-1026"></a></span>
<span id="cb74-1027"><a href="#cb74-1027"></a><span class="in">P1(Updated parameters) -- Forward Pass --&gt; L(Loss)</span></span>
<span id="cb74-1028"><a href="#cb74-1028"></a><span class="in">L(Loss) -- Backward Pass --&gt; P2(Parameters to update) </span></span>
<span id="cb74-1029"><a href="#cb74-1029"></a><span class="in">P2(Parameters to update)  -- Update Pamameters --&gt; P1(Updated parameters)</span></span>
<span id="cb74-1030"><a href="#cb74-1030"></a><span class="in">```</span></span>
<span id="cb74-1031"><a href="#cb74-1031"></a></span>
<span id="cb74-1032"><a href="#cb74-1032"></a>**Automate the training loop**:</span>
<span id="cb74-1033"><a href="#cb74-1033"></a></span>
<span id="cb74-1034"><a href="#cb74-1034"></a>::: callout-warning</span>
<span id="cb74-1035"><a href="#cb74-1035"></a>For each process, there remained a subtle bug above that we didn't flush the grads before backpropagation. Because we did not overwrite the gradients (remember the <span class="in">`+=`</span>), they kept accumulated. The next action of backward and changing parameters using learning rate and grad (which produce a massive step size) become wrong! We must set the grad to zero before backward pass.</span>
<span id="cb74-1036"><a href="#cb74-1036"></a></span>
<span id="cb74-1037"><a href="#cb74-1037"></a><span class="al">![Common guilties when training the NN](guilty.png)</span></span>
<span id="cb74-1038"><a href="#cb74-1038"></a>:::</span>
<span id="cb74-1039"><a href="#cb74-1039"></a></span>
<span id="cb74-1042"><a href="#cb74-1042"></a><span class="in">```{python}</span></span>
<span id="cb74-1043"><a href="#cb74-1043"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb74-1044"><a href="#cb74-1044"></a>    <span class="co"># forward pass:</span></span>
<span id="cb74-1045"><a href="#cb74-1045"></a>    ypred <span class="op">=</span> [m(x) <span class="cf">for</span> x <span class="kw">in</span> xs]</span>
<span id="cb74-1046"><a href="#cb74-1046"></a>    loss <span class="op">=</span> np.array([(yout <span class="op">-</span> ygt)<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> ygt, yout <span class="kw">in</span> <span class="bu">zip</span>(ys, ypred)]).<span class="bu">sum</span>()</span>
<span id="cb74-1047"><a href="#cb74-1047"></a></span>
<span id="cb74-1048"><a href="#cb74-1048"></a>    <span class="co"># backward pass:</span></span>
<span id="cb74-1049"><a href="#cb74-1049"></a>    <span class="cf">for</span> p <span class="kw">in</span> m.parameters():</span>
<span id="cb74-1050"><a href="#cb74-1050"></a>        p.grad <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb74-1051"><a href="#cb74-1051"></a>    loss.backward()</span>
<span id="cb74-1052"><a href="#cb74-1052"></a></span>
<span id="cb74-1053"><a href="#cb74-1053"></a>    <span class="co"># update params:</span></span>
<span id="cb74-1054"><a href="#cb74-1054"></a>    <span class="cf">for</span> p <span class="kw">in</span> m.parameters():</span>
<span id="cb74-1055"><a href="#cb74-1055"></a>        p.data <span class="op">+=</span> <span class="op">-</span><span class="fl">0.01</span> <span class="op">*</span> p.grad</span>
<span id="cb74-1056"><a href="#cb74-1056"></a></span>
<span id="cb74-1057"><a href="#cb74-1057"></a>    <span class="bu">print</span>(k, loss.data)</span>
<span id="cb74-1058"><a href="#cb74-1058"></a><span class="in">```</span></span>
<span id="cb74-1059"><a href="#cb74-1059"></a></span>
<span id="cb74-1062"><a href="#cb74-1062"></a><span class="in">```{python}</span></span>
<span id="cb74-1063"><a href="#cb74-1063"></a>ypred</span>
<span id="cb74-1064"><a href="#cb74-1064"></a><span class="in">```</span></span>
<span id="cb74-1065"><a href="#cb74-1065"></a></span>
<span id="cb74-1066"><a href="#cb74-1066"></a><span class="fu"># Summary</span></span>
<span id="cb74-1067"><a href="#cb74-1067"></a></span>
<span id="cb74-1068"><a href="#cb74-1068"></a><span class="fu">## summary of what we learned, how to go towards modern neural nets</span></span>
<span id="cb74-1069"><a href="#cb74-1069"></a></span>
<span id="cb74-1070"><a href="#cb74-1070"></a><span class="ss">1.  </span>What are Neural Nets: they are mathematical expressions, in case of MLP it takes: (1) data as the input, and (2) weights and biases as parameters to build out expression for the forward pass followed by the loss function.</span>
<span id="cb74-1071"><a href="#cb74-1071"></a><span class="ss">2.  </span>The loss function is kind of measure for the accuracy of predictions. The low loss implies that predicted values are matching our targets and the networks are behaving well.</span>
<span id="cb74-1072"><a href="#cb74-1072"></a><span class="ss">3.  </span>The process of Gradient Descent is for each step, we calculate the loss (output of the nets), backwarding it to get paramters, then updating data (which we can change - weights and biases) follow the opposite side of the loss (negative grad <span class="sc">\*</span> learning rate). We'll get a lower loss, and backwarding again and again. This process will find the local minimum of the loss.</span>
<span id="cb74-1073"><a href="#cb74-1073"></a></span>
<span id="cb74-1074"><a href="#cb74-1074"></a><span class="fu">## walkthrough of the full code of micrograd on github</span></span>
<span id="cb74-1075"><a href="#cb74-1075"></a></span>
<span id="cb74-1076"><a href="#cb74-1076"></a>Same with which we built today:</span>
<span id="cb74-1077"><a href="#cb74-1077"></a></span>
<span id="cb74-1078"><a href="#cb74-1078"></a><span class="ss">-   </span>engine: Value</span>
<span id="cb74-1079"><a href="#cb74-1079"></a><span class="ss">-   </span>nn: Neuron, Layer, MLP, and modulize the zero grad process to class Module</span>
<span id="cb74-1080"><a href="#cb74-1080"></a><span class="ss">-   </span>test: sanity check - compare the backward with <span class="in">`torch`</span>, also for the forward pass</span>
<span id="cb74-1081"><a href="#cb74-1081"></a><span class="ss">-   </span>demo: a bit complicated example with <span class="in">`sklearn`</span> dataset, using batch processing when the dataset come large, the loss is slightly different - SVM max-margin loss and using of auto L2 regularization</span>
<span id="cb74-1082"><a href="#cb74-1082"></a><span class="ss">-   </span>learning rate decay: is a scaled as a function of number of iterations, high at begin and low at the end</span>
<span id="cb74-1083"><a href="#cb74-1083"></a></span>
<span id="cb74-1084"><a href="#cb74-1084"></a><span class="fu">## real stuff: diving into PyTorch, finding their backward pass for `tanh`</span></span>
<span id="cb74-1085"><a href="#cb74-1085"></a></span>
<span id="cb74-1086"><a href="#cb74-1086"></a>These libraries unfortunately grow in size and **entropy**, if you just search for <span class="in">`tanh`</span> it'll give you thousands of results.</span>
<span id="cb74-1087"><a href="#cb74-1087"></a></span>
<span id="cb74-1088"><a href="#cb74-1088"></a><span class="fu">## conclusion</span></span>
<span id="cb74-1089"><a href="#cb74-1089"></a></span>
<span id="cb74-1090"><a href="#cb74-1090"></a>There will be follow up session yeah haha.</span>
<span id="cb74-1091"><a href="#cb74-1091"></a></span>
<span id="cb74-1092"><a href="#cb74-1092"></a><span class="fu">## outtakes :)</span></span>
<span id="cb74-1093"><a href="#cb74-1093"></a></span>
<span id="cb74-1094"><a href="#cb74-1094"></a>Pytorch self-defined autograd.</span>
<span id="cb74-1095"><a href="#cb74-1095"></a></span>
<span id="cb74-1096"><a href="#cb74-1096"></a>ü§ôüöÄüî•</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block"><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"><i class="fa-brands fa-creative-commons" aria-label="creative-commons"></i></a> 2023-2024 Le Khac Tuan</span></p>
</div>   
    <div class="nav-footer-center">
<p><span class="faux-block">Designed with <i class="fa-solid fa-heart" aria-label="heart"></i></span></p>
</div>
    <div class="nav-footer-right">
<p><span class="faux-block">Made with <a href="https://quarto.org/">Quarto</a></span></p>
</div>
  </div>
</footer>




</body></html>