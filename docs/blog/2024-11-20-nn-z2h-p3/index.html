<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tuan Le Khac">
<meta name="dcterms.date" content="2024-11-20">
<meta name="description" content="This is Tuan’s blog">

<title>NN-Z2H Lesson 3: Building makemore part 2 - MLP – Le Khac Tuan</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../img/rocket_1613268.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-8e54dfbe729680b42f22c627ac27a053.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-81b089a3b74ed4cf194f083418e7130b.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-89cb849060b93fd12025e4f44aaa3f02.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-27750d9d09892f3d6a52d1fd788c41c6.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Le Khac Tuan</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../curriculum/index.html"> 
<span class="menu-text">Curriculum</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../学汉语的日记.html"> 
<span class="menu-text">学汉语的日记</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../jiu_jitsu_journal/index.html"> 
<span class="menu-text">Jiu Jitsu Journal</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lktuan"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/tuanlekhac/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.facebook.com/toilatuan.lk/"> <i class="bi bi-facebook" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/Halle4231"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/@tuan_lekhac"> <i class="bi bi-youtube" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:tuan.lekhac0905@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">NN-Z2H Lesson 3: Building makemore part 2 - MLP</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                  <div>
        <div class="description">
          implement a multilayer perceptron (MLP) character-level language model, introduce model training, learning rate tuning, hyperparameters, evaluation, train/dev/test splits, under/overfitting, etc.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">til</div>
                <div class="quarto-category">python</div>
                <div class="quarto-category">andrej karpathy</div>
                <div class="quarto-category">nn-z2h</div>
                <div class="quarto-category">neural networks</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://lktuan.github.io/">Tuan Le Khac</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 20, 2024</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">November 20, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#part-1-intro-to-mlp" id="toc-part-1-intro-to-mlp" class="nav-link active" data-scroll-target="#part-1-intro-to-mlp">PART 1: intro to MLP</a>
  <ul class="collapse">
  <li><a href="#bengio-et-al.-2003-mlp-language-model-paper-walkthrough" id="toc-bengio-et-al.-2003-mlp-language-model-paper-walkthrough" class="nav-link" data-scroll-target="#bengio-et-al.-2003-mlp-language-model-paper-walkthrough">Bengio et al.&nbsp;2003 (MLP language model) paper walkthrough</a>
  <ul class="collapse">
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology">Methodology:</a></li>
  <li><a href="#neural-architecture" id="toc-neural-architecture" class="nav-link" data-scroll-target="#neural-architecture">Neural architecture:</a></li>
  </ul></li>
  <li><a href="#re-building-our-training-dataset" id="toc-re-building-our-training-dataset" class="nav-link" data-scroll-target="#re-building-our-training-dataset">(re-)building our training dataset</a></li>
  <li><a href="#implementing-the-embedding-lookup-table" id="toc-implementing-the-embedding-lookup-table" class="nav-link" data-scroll-target="#implementing-the-embedding-lookup-table">implementing the embedding lookup table</a></li>
  <li><a href="#implementing-the-hidden-layer-internals-of-torch.tensor-storage-views" id="toc-implementing-the-hidden-layer-internals-of-torch.tensor-storage-views" class="nav-link" data-scroll-target="#implementing-the-hidden-layer-internals-of-torch.tensor-storage-views">implementing the hidden layer + internals of <code>torch.Tensor</code>: <code>storage</code>, <code>views</code></a></li>
  <li><a href="#implementing-the-output-layer" id="toc-implementing-the-output-layer" class="nav-link" data-scroll-target="#implementing-the-output-layer">implementing the output layer</a></li>
  <li><a href="#implementing-the-negative-log-likelihood-loss" id="toc-implementing-the-negative-log-likelihood-loss" class="nav-link" data-scroll-target="#implementing-the-negative-log-likelihood-loss">implementing the negative log likelihood loss</a></li>
  <li><a href="#summary-of-the-full-network" id="toc-summary-of-the-full-network" class="nav-link" data-scroll-target="#summary-of-the-full-network">summary of the full network</a></li>
  </ul></li>
  <li><a href="#part-2-intro-to-many-basics-of-machine-learning" id="toc-part-2-intro-to-many-basics-of-machine-learning" class="nav-link" data-scroll-target="#part-2-intro-to-many-basics-of-machine-learning">PART 2: intro to many basics of machine learning</a>
  <ul class="collapse">
  <li><a href="#introducing-f.cross_entropy-and-why" id="toc-introducing-f.cross_entropy-and-why" class="nav-link" data-scroll-target="#introducing-f.cross_entropy-and-why">introducing <code>F.cross_entropy</code> and why</a></li>
  <li><a href="#implementing-the-training-loop-overfitting-one-batch" id="toc-implementing-the-training-loop-overfitting-one-batch" class="nav-link" data-scroll-target="#implementing-the-training-loop-overfitting-one-batch">implementing the training loop, overfitting one batch</a></li>
  <li><a href="#training-on-the-full-dataset-minibatches" id="toc-training-on-the-full-dataset-minibatches" class="nav-link" data-scroll-target="#training-on-the-full-dataset-minibatches">training on the full dataset, minibatches</a></li>
  <li><a href="#finding-a-good-initial-learning-rate" id="toc-finding-a-good-initial-learning-rate" class="nav-link" data-scroll-target="#finding-a-good-initial-learning-rate">finding a good initial learning rate</a></li>
  <li><a href="#splitting-up-the-dataset-into-trainvaltest-splits-and-why" id="toc-splitting-up-the-dataset-into-trainvaltest-splits-and-why" class="nav-link" data-scroll-target="#splitting-up-the-dataset-into-trainvaltest-splits-and-why">splitting up the dataset into train/val/test splits and why</a></li>
  <li><a href="#visualizing-the-loss-character-embeddings" id="toc-visualizing-the-loss-character-embeddings" class="nav-link" data-scroll-target="#visualizing-the-loss-character-embeddings">visualizing the loss, character embeddings</a></li>
  <li><a href="#experiment-larger-hidden-layer-larger-embedding-size" id="toc-experiment-larger-hidden-layer-larger-embedding-size" class="nav-link" data-scroll-target="#experiment-larger-hidden-layer-larger-embedding-size">experiment: larger hidden layer, larger embedding size</a></li>
  <li><a href="#summary-of-our-final-code-conclusion" id="toc-summary-of-our-final-code-conclusion" class="nav-link" data-scroll-target="#summary-of-our-final-code-conclusion">summary of our final code, conclusion</a></li>
  <li><a href="#sampling-from-the-model" id="toc-sampling-from-the-model" class="nav-link" data-scroll-target="#sampling-from-the-model">sampling from the model</a></li>
  <li><a href="#google-collab-new-notebook-advertisement" id="toc-google-collab-new-notebook-advertisement" class="nav-link" data-scroll-target="#google-collab-new-notebook-advertisement">google collab (new!!) notebook advertisement</a></li>
  </ul></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">resources</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="callout callout-style-default callout-important callout-titled" title="This is not orginal content!">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This is not orginal content!
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is my study notes / codes along with Andrej Karpathy’s “<a href="https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Neural Networks: Zero to Hero</a>” series.</p>
<p><em>Edit 1st June, 25</em>: I changed the loop to 5k only, for the shake of time saving when re-rendering this post!</p>
</div>
</div>
<p>In the previous lecture, we built a simple <code>bigram</code> character-level language model, using 2 different approaches that are (1) count, and (2) 1 layer neural network. They produced the same (and both poor - since the context is 1 character only) result but the neural network option offers more flexibility so that we can complexify our model to get better performance.</p>
<p>In this lecture we are going to implement 20-years ago neural probabilistic language model by <em>Bengio et al.&nbsp;(2003)</em>.</p>
<section id="part-1-intro-to-mlp" class="level1">
<h1>PART 1: intro to MLP</h1>
<section id="bengio-et-al.-2003-mlp-language-model-paper-walkthrough" class="level2">
<h2 class="anchored" data-anchor-id="bengio-et-al.-2003-mlp-language-model-paper-walkthrough">Bengio et al.&nbsp;2003 (MLP language model) paper walkthrough</h2>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<p><strong>Problem Statement</strong>:</p>
<ul>
<li>Traditional n-gram language models suffer from the <em>curse of dimensionality</em>: they can’t effectively generalize to word sequences not seen in training data;</li>
<li>The core issue is treating words as atomic units with no <em>inherent similarity</em> to each other;</li>
<li>For example, if we’ve seen “dog is eating” in training but never “cat is eating”, n-gram models can’t leverage the similarity between “dog” and “cat”;</li>
<li>This leads to poor probability estimates for rare or unseen word sequences.</li>
</ul>
<p><strong>Solution</strong>:</p>
<ul>
<li>Learn a <em>distributed representation</em> (embedding) for each word in a continuous vector space where similar words are close to each other;</li>
<li>Use a neural network architecture with:
<ul>
<li>Input layer: concatenated embeddings of n-1 previous words;</li>
<li>Hidden layer: dense neural network with <code>tanh</code> activation;</li>
<li>Output layer: softmax over entire vocabulary to predict next word probability.</li>
</ul></li>
</ul>
<p><strong>The model simultaneously learns</strong>:</p>
<ul>
<li>Word feature vectors (embeddings) that capture <em>semantic/syntactic word similarities</em>;</li>
<li>Neural network parameters that combine these features to estimate probability distributions.</li>
</ul>
<p><strong>Key advantages</strong>:</p>
<ul>
<li>Words with similar meanings get similar feature vectors, enabling better <em>generalization</em>;</li>
<li>The probability function is smooth with respect to word embeddings, so similar words yield <em>similar predictions</em>;</li>
<li>Can generalize to <em>unseen sequences</em> by leveraging learned word similarities.</li>
</ul>
</section>
<section id="methodology" class="level3">
<h3 class="anchored" data-anchor-id="methodology">Methodology:</h3>
<ul>
<li><p>Traditional Problem:</p>
<ul>
<li>In n-gram models, each word sequence of length n is a separate parameter;</li>
<li>For vocabulary size <span class="math inline">\(|V|\)</span>, need <span class="math inline">\(|V|^n\)</span> parameters;</li>
<li>Most sequences never appear in training, leading to poor generalization;</li>
</ul></li>
<li><p>Solution via <strong>Distributed Representation</strong>:</p>
<ul>
<li>Each word mapped to a dense vector in <span class="math inline">\(R^m\)</span> (typically m=50-100);</li>
<li>Similar words get similar vectors through training;</li>
<li>Probability function is smooth w.r.t these vectors;</li>
<li>Key benefit: If “dog” and “cat” have similar vectors, model can generalize from “dog is eating” to “cat is eating”;</li>
<li>Number of parameters reduces to <span class="math inline">\(O(|V|×m + m×h + h×|V|)\)</span>, where <span class="math inline">\(h\)</span> is hidden layer size;</li>
<li>This is much smaller than <span class="math inline">\(|V|^n\)</span> and allows better generalization;</li>
</ul></li>
</ul>
</section>
<section id="neural-architecture" class="level3">
<h3 class="anchored" data-anchor-id="neural-architecture">Neural architecture:</h3>
<p><strong>Input Layer</strong>:</p>
<ul>
<li>Takes <span class="math inline">\(n-1\)</span> previous words (context window);</li>
<li>Each word i mapped to vector <span class="math inline">\(C(i) ∈ R^m\)</span> via lookup table;</li>
<li>Concatenates these vectors: <span class="math inline">\(x = [C(wₜ₋ₙ₊₁), ..., C(wₜ₋₁)]\)</span>;</li>
<li><span class="math inline">\(x\)</span> dimension is <span class="math inline">\((n-1)×m\)</span>;</li>
</ul>
<p><strong>Hidden Layer</strong>:</p>
<ul>
<li>Dense layer with tanh activation;</li>
<li>Computation: <span class="math inline">\(h = tanh(d + Hx)\)</span>;</li>
<li><span class="math inline">\(H\)</span> is weight matrix, <span class="math inline">\(d\)</span> is bias vector;</li>
<li>Maps concatenated context to hidden representation;</li>
</ul>
<p><strong>Output Layer</strong>:</p>
<ul>
<li>Computes probability distribution over all words;</li>
<li><span class="math inline">\(y = b + Wx + Uh\)</span>;</li>
<li>Softmax activation: <span class="math inline">\(P(wₜ|context) = exp(yᵢ)/Σⱼexp(yⱼ)\)</span>;</li>
<li><span class="math inline">\(W\)</span> provides “shortcut” connections from input to output;</li>
<li>Direct connection helps learn simpler patterns;</li>
</ul>
<p><strong>Training</strong>:</p>
<ul>
<li>Maximizes log-likelihood of training data;</li>
<li>Uses stochastic gradient descent;</li>
<li>Learns both word vectors <span class="math inline">\(C(i)\)</span> and neural network parameters <span class="math inline">\((H, d, W, U, b)\)</span>;</li>
<li>Word vectors capture similarities as they help predict similar contexts;</li>
<li>Can initialize word vectors randomly or with pretrained vectors.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="NLM_Bengio_etal.png" class="img-fluid figure-img"></p>
<figcaption>Neural Language Model proposed by (Bengio et al., 2003). C(i) is the i th word embedding.</figcaption>
</figure>
</div>
</section>
</section>
<section id="re-building-our-training-dataset" class="level2">
<h2 class="anchored" data-anchor-id="re-building-our-training-dataset">(re-)building our training dataset</h2>
<p>Loading library, reading data, building dictionary:</p>
<div id="fdd3087a" class="cell" data-execution_count="1">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="e2965745" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2"></a></span>
<span id="cb2-3"><a href="#cb2-3"></a>url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt"</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>words <span class="op">=</span> pd.read_csv(url, header<span class="op">=</span><span class="va">None</span>).iloc[:, <span class="dv">0</span>].tolist()</span>
<span id="cb2-5"><a href="#cb2-5"></a>words[:<span class="dv">8</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']</code></pre>
</div>
</div>
<div id="b0c6b45d" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="bu">len</span>(words)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>32033</code></pre>
</div>
</div>
<div id="ca40b4e4" class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># build the vocabulary of characters and mapping to/from integer</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(<span class="st">''</span>.join(words))))</span>
<span id="cb6-3"><a href="#cb6-3"></a>stoi <span class="op">=</span> {s:i<span class="op">+</span><span class="dv">1</span> <span class="cf">for</span> i, s <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb6-4"><a href="#cb6-4"></a>stoi[<span class="st">'.'</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-5"><a href="#cb6-5"></a></span>
<span id="cb6-6"><a href="#cb6-6"></a>itos <span class="op">=</span> {i: s <span class="cf">for</span> s, i <span class="kw">in</span> stoi.items()}</span>
<span id="cb6-7"><a href="#cb6-7"></a>itos</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>{1: 'a',
 2: 'b',
 3: 'c',
 4: 'd',
 5: 'e',
 6: 'f',
 7: 'g',
 8: 'h',
 9: 'i',
 10: 'j',
 11: 'k',
 12: 'l',
 13: 'm',
 14: 'n',
 15: 'o',
 16: 'p',
 17: 'q',
 18: 'r',
 19: 's',
 20: 't',
 21: 'u',
 22: 'v',
 23: 'w',
 24: 'x',
 25: 'y',
 26: 'z',
 0: '.'}</code></pre>
</div>
</div>
<p>Building the dataset:</p>
<div id="c2d0e12d" class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>block_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># the context length: how many characters do we take to predict the next one?</span></span>
<span id="cb8-2"><a href="#cb8-2"></a>X, Y <span class="op">=</span> [], []</span>
<span id="cb8-3"><a href="#cb8-3"></a></span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="cf">for</span> w <span class="kw">in</span> words[:<span class="dv">5</span>]:</span>
<span id="cb8-5"><a href="#cb8-5"></a>    <span class="bu">print</span>(w)</span>
<span id="cb8-6"><a href="#cb8-6"></a>    context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size <span class="co"># 0 so context will be padded by '.'</span></span>
<span id="cb8-7"><a href="#cb8-7"></a>    <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb8-8"><a href="#cb8-8"></a>        ix <span class="op">=</span> stoi[ch]</span>
<span id="cb8-9"><a href="#cb8-9"></a>        X.append(context)</span>
<span id="cb8-10"><a href="#cb8-10"></a>        Y.append(ix)</span>
<span id="cb8-11"><a href="#cb8-11"></a>        <span class="bu">print</span>(<span class="st">''</span>.join(itos[i] <span class="cf">for</span> i <span class="kw">in</span> context), <span class="st">'-----&gt;'</span>, itos[ix] )</span>
<span id="cb8-12"><a href="#cb8-12"></a>        context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix] <span class="co"># rolling to the next one</span></span>
<span id="cb8-13"><a href="#cb8-13"></a></span>
<span id="cb8-14"><a href="#cb8-14"></a>X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb8-15"><a href="#cb8-15"></a>Y <span class="op">=</span> torch.tensor(Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>emma
... -----&gt; e
..e -----&gt; m
.em -----&gt; m
emm -----&gt; a
mma -----&gt; .
olivia
... -----&gt; o
..o -----&gt; l
.ol -----&gt; i
oli -----&gt; v
liv -----&gt; i
ivi -----&gt; a
via -----&gt; .
ava
... -----&gt; a
..a -----&gt; v
.av -----&gt; a
ava -----&gt; .
isabella
... -----&gt; i
..i -----&gt; s
.is -----&gt; a
isa -----&gt; b
sab -----&gt; e
abe -----&gt; l
bel -----&gt; l
ell -----&gt; a
lla -----&gt; .
sophia
... -----&gt; s
..s -----&gt; o
.so -----&gt; p
sop -----&gt; h
oph -----&gt; i
phi -----&gt; a
hia -----&gt; .</code></pre>
</div>
</div>
<div id="f58ee224" class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>X.shape, X.dtype, Y.shape, Y.dtype</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)</code></pre>
</div>
</div>
</section>
<section id="implementing-the-embedding-lookup-table" class="level2">
<h2 class="anchored" data-anchor-id="implementing-the-embedding-lookup-table">implementing the embedding lookup table</h2>
<p>In the paper they cram 17k word into as-low-as-possible 30 dimensions space, for our data, we just cram words into 2D space.</p>
<div id="42d14a1a" class="cell" data-execution_count="7">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>C <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We can access the element of <code>torch.tensor</code> by:</p>
<div id="5acc903f" class="cell" data-execution_count="8">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a>C[<span class="dv">5</span>] <span class="co"># can be integer, list [5, 6, 7], or torch.tensor([5,6,7])</span></span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="co"># &gt; tensor([1.0825, 0.2010])</span></span>
<span id="cb13-3"><a href="#cb13-3"></a></span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="co"># or</span></span>
<span id="cb13-5"><a href="#cb13-5"></a></span>
<span id="cb13-6"><a href="#cb13-6"></a>F.one_hot(torch.tensor(<span class="dv">5</span>), num_classes<span class="op">=</span><span class="dv">27</span>).<span class="bu">float</span>() <span class="op">@</span> C</span>
<span id="cb13-7"><a href="#cb13-7"></a><span class="co"># produce identical result, remember torch.tensor() infer long dtype int64, so we need to cast to float</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>tensor([0.8894, 1.9139])</code></pre>
</div>
</div>
<p>…but in this lecture accessing by <code>C[5]</code> would be sufficient. We can even access using a more than 1 dimension tensor:</p>
<div id="489e2022" class="cell" data-execution_count="9">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="bu">print</span>(C[X].shape)</span>
<span id="cb15-2"><a href="#cb15-2"></a><span class="bu">print</span>(X[<span class="dv">13</span>, <span class="dv">2</span>]) <span class="co"># integer 1 for 13rd index of 2nd dimension</span></span>
<span id="cb15-3"><a href="#cb15-3"></a><span class="bu">print</span>(C[X][<span class="dv">13</span>,<span class="dv">2</span>]) <span class="co"># will be the embedding of that element</span></span>
<span id="cb15-4"><a href="#cb15-4"></a><span class="bu">print</span>(C[<span class="dv">1</span>]) <span class="co"># so C[X][13,2] = C[1]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([32, 3, 2])
tensor(1)
tensor([ 0.3933, -0.0305])
tensor([ 0.3933, -0.0305])</code></pre>
</div>
</div>
<p>PyTorch is great for embedding words:</p>
<div id="6a42dac6" class="cell" data-execution_count="10">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>emb <span class="op">=</span> C[X]</span>
<span id="cb17-2"><a href="#cb17-2"></a>emb.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>torch.Size([32, 3, 2])</code></pre>
</div>
</div>
<p>We’ve compeleted the first layer with <code>context</code> and lookup table!</p>
</section>
<section id="implementing-the-hidden-layer-internals-of-torch.tensor-storage-views" class="level2">
<h2 class="anchored" data-anchor-id="implementing-the-hidden-layer-internals-of-torch.tensor-storage-views">implementing the hidden layer + internals of <code>torch.Tensor</code>: <code>storage</code>, <code>views</code></h2>
<div id="f9346419" class="cell" data-execution_count="11">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="co"># input of tanh layer will be 6 (3 words in context x 2 dimensions)</span></span>
<span id="cb19-2"><a href="#cb19-2"></a><span class="co"># and the number or neurons is up to us - let's set it 100</span></span>
<span id="cb19-3"><a href="#cb19-3"></a>W1 <span class="op">=</span> torch.randn((<span class="dv">6</span>, <span class="dv">100</span>))</span>
<span id="cb19-4"><a href="#cb19-4"></a>b1 <span class="op">=</span> torch.randn(<span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now we need to do something like <code>emb @ W1 + b1</code>, but <code>emb.shape</code> is <code>[32, 3, 2]</code> and <code>W1.shape</code> is <code>[6, 100]</code>. We need to somehow concatnate/transform:</p>
<div id="c18f9ed8" class="cell" data-execution_count="12">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a><span class="co"># emb[:, 0, :] is tensor for each input in the 3-words context, shape is [32, 2]</span></span>
<span id="cb20-2"><a href="#cb20-2"></a><span class="co"># cat 3 of them using the 2nd dimension (index 1) -&gt; so we set dim = 1</span></span>
<span id="cb20-3"><a href="#cb20-3"></a>torch.cat([emb[:, <span class="dv">0</span>, :], emb[:, <span class="dv">1</span>, :], emb[:, <span class="dv">2</span>, :]], dim<span class="op">=</span><span class="dv">1</span>).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>torch.Size([32, 6])</code></pre>
</div>
</div>
<p>However this code does not change dynamically when we change the block size. We will be using <code>torch.unbind()</code></p>
<div id="c5da2fd1" class="cell" data-execution_count="13">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="co"># this is good!</span></span>
<span id="cb22-2"><a href="#cb22-2"></a>torch.cat(torch.unbind(emb, <span class="dv">1</span>), <span class="dv">1</span>).shape</span>
<span id="cb22-3"><a href="#cb22-3"></a><span class="co"># new memory for storage is created, so it is not efficient</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>torch.Size([32, 6])</code></pre>
</div>
</div>
<p>This works, but we have a better and more efficient way to do this. Since:</p>
<ul>
<li>every <code>torch.Tensor</code> have <code>.storage()</code> which is one-dimensional vector tensor;</li>
<li>when we call <code>.view()</code>, we instruct how this vector tensor is interpreted;</li>
<li>no memory is being changed/copied/moved/or created. the storage is identical.</li>
</ul>
<p>Readmore: <a href="http://blog.ezyang.com/2019/05/pytorch-internals/" class="uri">http://blog.ezyang.com/2019/05/pytorch-internals/</a></p>
<p>So this hidden layer can be declared:</p>
<div id="073fb56b" class="cell" data-execution_count="14">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="co"># instead or 32 we can write emb.shape[1], or -1 (whatever fitted)</span></span>
<span id="cb24-2"><a href="#cb24-2"></a>h <span class="op">=</span> emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1</span>
<span id="cb24-3"><a href="#cb24-3"></a>h.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>torch.Size([32, 100])</code></pre>
</div>
</div>
<p>Notice that in the final operation, <code>b1</code> will be broadcasted.</p>
</section>
<section id="implementing-the-output-layer" class="level2">
<h2 class="anchored" data-anchor-id="implementing-the-output-layer">implementing the output layer</h2>
<div id="2fe17834" class="cell" data-execution_count="15">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a>W2 <span class="op">=</span> torch.randn((<span class="dv">100</span>, <span class="dv">27</span>))</span>
<span id="cb26-2"><a href="#cb26-2"></a>b2 <span class="op">=</span> torch.randn(<span class="dv">27</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In Deep Learning, people use <code>logits</code> for what raw output that range from negative inf to positive inf.</p>
<div id="9a5f0b69" class="cell" data-execution_count="16">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="3d055c67" class="cell" data-execution_count="17">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a>logits.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>torch.Size([32, 27])</code></pre>
</div>
</div>
<p>Now we need to exponentiate it and get the probability.</p>
<div id="cdee3279" class="cell" data-execution_count="18">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a>counts <span class="op">=</span> logits.exp()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="f50dcece" class="cell" data-execution_count="19">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a>probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="571595fd" class="cell" data-execution_count="20">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a>probs.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>torch.Size([32, 27])</code></pre>
</div>
</div>
<p>Every row of <code>probs</code> has sum of 1.</p>
<div id="96c01118" class="cell" data-execution_count="21">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a>probs[<span class="dv">0</span>].<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>tensor(1.)</code></pre>
</div>
</div>
<p>And this is the <code>probs</code> of each ground true <code>Y</code> in current output of the neural nets:</p>
<div id="1a92c30a" class="cell" data-execution_count="22">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a>probs[torch.arange(<span class="dv">32</span>), Y]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>tensor([4.7061e-24, 3.7041e-35, 4.8793e-07, 7.2092e-24, 3.7791e-13, 1.0648e-35,
        6.9894e-13, 3.9922e-36, 5.4665e-26, 4.1032e-37, 1.1009e-11, 1.6808e-12,
        3.1990e-13, 1.2050e-29, 3.0691e-12, 7.4121e-09, 1.7415e-12, 5.6369e-30,
        1.2058e-32, 6.7942e-15, 1.8272e-36, 2.1324e-16, 0.0000e+00, 9.5874e-30,
        8.1249e-17, 1.0161e-18, 0.0000e+00, 3.1831e-13, 0.0000e+00, 6.8864e-19,
        2.5463e-16, 5.0601e-13])</code></pre>
</div>
</div>
<p>Result is not good as we’ve not trained the network yet!</p>
</section>
<section id="implementing-the-negative-log-likelihood-loss" class="level2">
<h2 class="anchored" data-anchor-id="implementing-the-negative-log-likelihood-loss">implementing the negative log likelihood loss</h2>
<p>We define the negative log likelihood as:</p>
<div id="e8fb2462" class="cell" data-execution_count="23">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a>loss <span class="op">=</span> <span class="op">-</span> probs[torch.arange(<span class="dv">32</span>), Y].log().mean()</span>
<span id="cb38-2"><a href="#cb38-2"></a>loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>tensor(inf)</code></pre>
</div>
</div>
</section>
<section id="summary-of-the-full-network" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-the-full-network">summary of the full network</h2>
<p>Dataset:</p>
<div id="6ed72068" class="cell" data-execution_count="24">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a>X.shape, Y.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>(torch.Size([32, 3]), torch.Size([32]))</code></pre>
</div>
</div>
<p>Neural network layers:</p>
<div id="cfe31779" class="cell" data-execution_count="25">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb42-2"><a href="#cb42-2"></a>C <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">2</span>), generator<span class="op">=</span>g)</span>
<span id="cb42-3"><a href="#cb42-3"></a>W1 <span class="op">=</span> torch.randn((<span class="dv">6</span>, <span class="dv">100</span>), generator<span class="op">=</span>g)</span>
<span id="cb42-4"><a href="#cb42-4"></a>b1 <span class="op">=</span> torch.randn(<span class="dv">100</span>, generator<span class="op">=</span>g)</span>
<span id="cb42-5"><a href="#cb42-5"></a>W2 <span class="op">=</span> torch.randn((<span class="dv">100</span>, <span class="dv">27</span>), generator<span class="op">=</span>g)</span>
<span id="cb42-6"><a href="#cb42-6"></a>b2 <span class="op">=</span> torch.randn(<span class="dv">27</span>, generator<span class="op">=</span>g)</span>
<span id="cb42-7"><a href="#cb42-7"></a></span>
<span id="cb42-8"><a href="#cb42-8"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Size of the network:</p>
<div id="06c91191" class="cell" data-execution_count="26">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a><span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>3481</code></pre>
</div>
</div>
<p>Constructing forward pass:</p>
<div id="dde18326" class="cell" data-execution_count="27">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1"></a>emb <span class="op">=</span> C[X] <span class="co"># (32, 3, 2)</span></span>
<span id="cb45-2"><a href="#cb45-2"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb45-3"><a href="#cb45-3"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb45-4"><a href="#cb45-4"></a>counts <span class="op">=</span> logits.exp()</span>
<span id="cb45-5"><a href="#cb45-5"></a>probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb45-6"><a href="#cb45-6"></a>loss <span class="op">=</span> <span class="op">-</span> probs[torch.arange(<span class="dv">32</span>), Y].log().mean()</span>
<span id="cb45-7"><a href="#cb45-7"></a>loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>tensor(17.7697)</code></pre>
</div>
</div>
</section>
</section>
<section id="part-2-intro-to-many-basics-of-machine-learning" class="level1">
<h1>PART 2: intro to many basics of machine learning</h1>
<section id="introducing-f.cross_entropy-and-why" class="level2">
<h2 class="anchored" data-anchor-id="introducing-f.cross_entropy-and-why">introducing <code>F.cross_entropy</code> and why</h2>
<p>We re-define loss:</p>
<div id="2bc16c81" class="cell" data-execution_count="28">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1"></a>loss <span class="op">=</span> F.cross_entropy(logits, Y)</span>
<span id="cb47-2"><a href="#cb47-2"></a>loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>tensor(17.7697)</code></pre>
</div>
</div>
<p>Why?</p>
<ul>
<li>Pytorch will create more intermediate tensor for every assignment: <code>counts</code>, <code>probs</code> -&gt; more memory;</li>
<li>Backward pass will be more optimized, because the expressions are much analytically and mathematically interpreted;</li>
<li>Cross entropy can be significantly &amp; numerically well behaved (for eg when we exponentiate a large positive number we got inf, PyTorch cross entropy will calculate the max of set and subtract it - which will not impact the exp result)</li>
</ul>
</section>
<section id="implementing-the-training-loop-overfitting-one-batch" class="level2">
<h2 class="anchored" data-anchor-id="implementing-the-training-loop-overfitting-one-batch">implementing the training loop, overfitting one batch</h2>
<p>So the forward pass, backward pass, and update loop will be implemented as below:</p>
<div id="0233325e" class="cell" data-execution_count="29">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb49-2"><a href="#cb49-2"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="d1dfc450" class="cell" data-execution_count="30">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb50-2"><a href="#cb50-2"></a>    <span class="co"># forward pass:</span></span>
<span id="cb50-3"><a href="#cb50-3"></a>    emb <span class="op">=</span> C[X] <span class="co"># (32, 3, 2)</span></span>
<span id="cb50-4"><a href="#cb50-4"></a>    h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb50-5"><a href="#cb50-5"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb50-6"><a href="#cb50-6"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Y)</span>
<span id="cb50-7"><a href="#cb50-7"></a>    <span class="bu">print</span>(loss.item())</span>
<span id="cb50-8"><a href="#cb50-8"></a>    <span class="co"># backward pass:</span></span>
<span id="cb50-9"><a href="#cb50-9"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb50-10"><a href="#cb50-10"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb50-11"><a href="#cb50-11"></a>    loss.backward()</span>
<span id="cb50-12"><a href="#cb50-12"></a>    <span class="co"># update</span></span>
<span id="cb50-13"><a href="#cb50-13"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb50-14"><a href="#cb50-14"></a>        p.data <span class="op">+=</span> <span class="op">-</span><span class="fl">0.1</span> <span class="op">*</span> p.grad</span>
<span id="cb50-15"><a href="#cb50-15"></a></span>
<span id="cb50-16"><a href="#cb50-16"></a><span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>17.76971435546875
13.656400680541992
11.298768997192383
9.452457427978516
7.984262466430664
6.891322135925293
6.100014686584473
5.452037334442139
4.8981523513793945
4.414664268493652
4.414664268493652</code></pre>
</div>
</div>
<p>We are fitting 32 examples to a neural nets of 3481 params, so it’s super easy to be overfitting. We got a low final loss, but it would never be 0, because the output can varry for the same input, for eg, <code>...</code>.</p>
<div id="5ad23bfa" class="cell" data-execution_count="31">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1"></a>logits.<span class="bu">max</span>(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>torch.return_types.max(
values=tensor([10.7865, 12.2558, 17.3982, 13.2739, 10.6965, 10.7865,  9.5145,  9.0495,
        14.0280, 11.8378,  9.9038, 15.4187, 10.7865, 10.1476,  9.8372, 11.7660,
        10.7865, 10.0029,  9.2940,  9.6824, 11.4241,  9.4885,  8.1164,  9.5176,
        12.6383, 10.7865, 10.6021, 11.0822,  6.3617, 17.3157, 12.4544,  8.1669],
       grad_fn=&lt;MaxBackward0&gt;),
indices=tensor([ 1,  8,  9,  0, 15,  1, 17,  2,  9,  9,  2,  0,  1, 15,  1,  0,  1, 19,
         1,  1, 16, 10, 26,  9,  0,  1, 15, 16,  3,  9, 19,  1]))</code></pre>
</div>
</div>
</section>
<section id="training-on-the-full-dataset-minibatches" class="level2">
<h2 class="anchored" data-anchor-id="training-on-the-full-dataset-minibatches">training on the full dataset, minibatches</h2>
<p>We can deploy our code to all the dataset, un-fold the below code block to see full code.</p>
<div id="abf23143" class="cell" data-execution_count="32">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1"></a>block_size <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb54-2"><a href="#cb54-2"></a>X, Y <span class="op">=</span> [], []</span>
<span id="cb54-3"><a href="#cb54-3"></a></span>
<span id="cb54-4"><a href="#cb54-4"></a><span class="co"># Dataset</span></span>
<span id="cb54-5"><a href="#cb54-5"></a><span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb54-6"><a href="#cb54-6"></a>    <span class="co"># print(w)</span></span>
<span id="cb54-7"><a href="#cb54-7"></a>    context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size</span>
<span id="cb54-8"><a href="#cb54-8"></a>    <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb54-9"><a href="#cb54-9"></a>        ix <span class="op">=</span> stoi[ch]</span>
<span id="cb54-10"><a href="#cb54-10"></a>        X.append(context)</span>
<span id="cb54-11"><a href="#cb54-11"></a>        Y.append(ix)</span>
<span id="cb54-12"><a href="#cb54-12"></a>        <span class="co"># print(''.join(itos[i] for i in context), '-----&gt;', itos[ix] )</span></span>
<span id="cb54-13"><a href="#cb54-13"></a>        context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix] <span class="co"># rolling to the next one</span></span>
<span id="cb54-14"><a href="#cb54-14"></a></span>
<span id="cb54-15"><a href="#cb54-15"></a><span class="co"># Input and ground true</span></span>
<span id="cb54-16"><a href="#cb54-16"></a>X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb54-17"><a href="#cb54-17"></a>Y <span class="op">=</span> torch.tensor(Y)</span>
<span id="cb54-18"><a href="#cb54-18"></a><span class="bu">print</span>(<span class="st">"Data size"</span>, X.shape, Y.shape)</span>
<span id="cb54-19"><a href="#cb54-19"></a></span>
<span id="cb54-20"><a href="#cb54-20"></a><span class="co"># Lookup table</span></span>
<span id="cb54-21"><a href="#cb54-21"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb54-22"><a href="#cb54-22"></a>C <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">2</span>), generator<span class="op">=</span>g)</span>
<span id="cb54-23"><a href="#cb54-23"></a>emb <span class="op">=</span> C[X] <span class="co"># (32, 3, 2)</span></span>
<span id="cb54-24"><a href="#cb54-24"></a></span>
<span id="cb54-25"><a href="#cb54-25"></a><span class="co"># Layer 1 - tanh</span></span>
<span id="cb54-26"><a href="#cb54-26"></a>W1 <span class="op">=</span> torch.randn((<span class="dv">6</span>, <span class="dv">100</span>), generator<span class="op">=</span>g)</span>
<span id="cb54-27"><a href="#cb54-27"></a>b1 <span class="op">=</span> torch.randn(<span class="dv">100</span>, generator<span class="op">=</span>g)</span>
<span id="cb54-28"><a href="#cb54-28"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb54-29"><a href="#cb54-29"></a></span>
<span id="cb54-30"><a href="#cb54-30"></a><span class="co"># Layer 2 - softmax</span></span>
<span id="cb54-31"><a href="#cb54-31"></a>W2 <span class="op">=</span> torch.randn((<span class="dv">100</span>, <span class="dv">27</span>), generator<span class="op">=</span>g)</span>
<span id="cb54-32"><a href="#cb54-32"></a>b2 <span class="op">=</span> torch.randn(<span class="dv">27</span>, generator<span class="op">=</span>g)</span>
<span id="cb54-33"><a href="#cb54-33"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb54-34"><a href="#cb54-34"></a>loss <span class="op">=</span> F.cross_entropy(logits, Y)</span>
<span id="cb54-35"><a href="#cb54-35"></a></span>
<span id="cb54-36"><a href="#cb54-36"></a><span class="co"># All params</span></span>
<span id="cb54-37"><a href="#cb54-37"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb54-38"><a href="#cb54-38"></a><span class="bu">print</span>(<span class="st">"No of params: "</span>, <span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span>
<span id="cb54-39"><a href="#cb54-39"></a></span>
<span id="cb54-40"><a href="#cb54-40"></a><span class="co"># Pre-training</span></span>
<span id="cb54-41"><a href="#cb54-41"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb54-42"><a href="#cb54-42"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Data size torch.Size([228146, 3]) torch.Size([228146])
No of params:  3481</code></pre>
</div>
</div>
<p>We notice that it takes a bit long time for each training in the loop. In practice, we will perform the forward/backward passes and update parameters for a small batch of the dataset. The minibatch construction is added/modified for lines of code with <code>#👈</code>.</p>
<p>Read more: <a href="https://nttuan8.com/bai-10-cac-ky-thuat-co-ban-trong-deep-learning/" class="uri">https://nttuan8.com/bai-10-cac-ky-thuat-co-ban-trong-deep-learning/</a></p>
<div id="73e0e9b7" class="cell" data-execution_count="33">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1"></a><span class="co"># Training</span></span>
<span id="cb56-2"><a href="#cb56-2"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10000</span>):</span>
<span id="cb56-3"><a href="#cb56-3"></a>    <span class="co"># minibatch construct                                           #👈</span></span>
<span id="cb56-4"><a href="#cb56-4"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, X.shape[<span class="dv">0</span>], (<span class="dv">32</span>,))                        <span class="co">#👈</span></span>
<span id="cb56-5"><a href="#cb56-5"></a></span>
<span id="cb56-6"><a href="#cb56-6"></a>    <span class="co"># forward pass:</span></span>
<span id="cb56-7"><a href="#cb56-7"></a>    emb <span class="op">=</span> C[X[ix]] <span class="co"># (32, 3, 2)                                     #👈</span></span>
<span id="cb56-8"><a href="#cb56-8"></a>    h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb56-9"><a href="#cb56-9"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb56-10"><a href="#cb56-10"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Y[ix])                           <span class="co">#👈</span></span>
<span id="cb56-11"><a href="#cb56-11"></a>    <span class="cf">if</span> _ <span class="op">&gt;=</span> <span class="dv">9990</span>: <span class="bu">print</span>(<span class="ss">f"___after running </span><span class="sc">{</span>_<span class="sc">}</span><span class="ss"> time: "</span>, loss.item())</span>
<span id="cb56-12"><a href="#cb56-12"></a>    <span class="co"># backward pass:</span></span>
<span id="cb56-13"><a href="#cb56-13"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb56-14"><a href="#cb56-14"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb56-15"><a href="#cb56-15"></a>    loss.backward()</span>
<span id="cb56-16"><a href="#cb56-16"></a>    <span class="co"># update</span></span>
<span id="cb56-17"><a href="#cb56-17"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb56-18"><a href="#cb56-18"></a>        p.data <span class="op">+=</span> <span class="op">-</span><span class="fl">0.1</span> <span class="op">*</span> p.grad</span>
<span id="cb56-19"><a href="#cb56-19"></a></span>
<span id="cb56-20"><a href="#cb56-20"></a><span class="bu">print</span>(<span class="st">"final minibatch loss: "</span>, loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>___after running 9990 time:  2.3651716709136963
___after running 9991 time:  2.547208786010742
___after running 9992 time:  2.886200189590454
___after running 9993 time:  2.6920998096466064
___after running 9994 time:  2.448667526245117
___after running 9995 time:  2.080944299697876
___after running 9996 time:  2.4017341136932373
___after running 9997 time:  2.379610061645508
___after running 9998 time:  2.3751413822174072
___after running 9999 time:  2.551131010055542
final minibatch loss:  2.551131010055542</code></pre>
</div>
</div>
<p>The <code>loss</code> decrease much much better, although the direction of gradient might be not correct direction. But it is good enough for an approximation. Notice the loss for a minibatch is not the loss of whole dataset.</p>
<div id="74c06f3e" class="cell" data-execution_count="34">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1"></a>emb <span class="op">=</span> C[X] <span class="co"># (32, 3, 2)</span></span>
<span id="cb58-2"><a href="#cb58-2"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb58-3"><a href="#cb58-3"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb58-4"><a href="#cb58-4"></a>loss <span class="op">=</span> F.cross_entropy(logits, Y)</span>
<span id="cb58-5"><a href="#cb58-5"></a>loss.item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>2.4879841804504395</code></pre>
</div>
</div>
<p>We archived 2.39 loss for final minibatch and 2.5 on overall network.</p>
</section>
<section id="finding-a-good-initial-learning-rate" class="level2">
<h2 class="anchored" data-anchor-id="finding-a-good-initial-learning-rate">finding a good initial learning rate</h2>
<p>Now we’re continuing the optimization, let’s focus on how much we update the data from the gradient <code>p.data += -0.1 * p.grad</code>. We do not know if we step too little or too much.</p>
<p>We can create 1000 learning rates to use along with the training loop and see which one offers more stable convergence.</p>
<div id="42082ac7" class="cell" data-execution_count="35">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1"></a>lre <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">1000</span>)</span>
<span id="cb60-2"><a href="#cb60-2"></a>lrs <span class="op">=</span> <span class="dv">10</span><span class="op">**</span>lre</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Reset the code:</p>
<div id="6a80f664" class="cell" data-execution_count="36">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1"></a>block_size <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb61-2"><a href="#cb61-2"></a>X, Y <span class="op">=</span> [], []</span>
<span id="cb61-3"><a href="#cb61-3"></a></span>
<span id="cb61-4"><a href="#cb61-4"></a><span class="co"># Dataset</span></span>
<span id="cb61-5"><a href="#cb61-5"></a><span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb61-6"><a href="#cb61-6"></a>    <span class="co"># print(w)</span></span>
<span id="cb61-7"><a href="#cb61-7"></a>    context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size</span>
<span id="cb61-8"><a href="#cb61-8"></a>    <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb61-9"><a href="#cb61-9"></a>        ix <span class="op">=</span> stoi[ch]</span>
<span id="cb61-10"><a href="#cb61-10"></a>        X.append(context)</span>
<span id="cb61-11"><a href="#cb61-11"></a>        Y.append(ix)</span>
<span id="cb61-12"><a href="#cb61-12"></a>        <span class="co"># print(''.join(itos[i] for i in context), '-----&gt;', itos[ix] )</span></span>
<span id="cb61-13"><a href="#cb61-13"></a>        context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix] <span class="co"># rolling to the next one</span></span>
<span id="cb61-14"><a href="#cb61-14"></a></span>
<span id="cb61-15"><a href="#cb61-15"></a><span class="co"># Input and ground true</span></span>
<span id="cb61-16"><a href="#cb61-16"></a>X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb61-17"><a href="#cb61-17"></a>Y <span class="op">=</span> torch.tensor(Y)</span>
<span id="cb61-18"><a href="#cb61-18"></a><span class="bu">print</span>(<span class="st">"Data size"</span>, X.shape, Y.shape)</span>
<span id="cb61-19"><a href="#cb61-19"></a></span>
<span id="cb61-20"><a href="#cb61-20"></a><span class="co"># Lookup table</span></span>
<span id="cb61-21"><a href="#cb61-21"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb61-22"><a href="#cb61-22"></a>C <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">2</span>), generator<span class="op">=</span>g)</span>
<span id="cb61-23"><a href="#cb61-23"></a>emb <span class="op">=</span> C[X] <span class="co"># (32, 3, 2)</span></span>
<span id="cb61-24"><a href="#cb61-24"></a></span>
<span id="cb61-25"><a href="#cb61-25"></a><span class="co"># Layer 1 - tanh</span></span>
<span id="cb61-26"><a href="#cb61-26"></a>W1 <span class="op">=</span> torch.randn((<span class="dv">6</span>, <span class="dv">100</span>), generator<span class="op">=</span>g)</span>
<span id="cb61-27"><a href="#cb61-27"></a>b1 <span class="op">=</span> torch.randn(<span class="dv">100</span>, generator<span class="op">=</span>g)</span>
<span id="cb61-28"><a href="#cb61-28"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb61-29"><a href="#cb61-29"></a></span>
<span id="cb61-30"><a href="#cb61-30"></a><span class="co"># Layer 2 - softmax</span></span>
<span id="cb61-31"><a href="#cb61-31"></a>W2 <span class="op">=</span> torch.randn((<span class="dv">100</span>, <span class="dv">27</span>), generator<span class="op">=</span>g)</span>
<span id="cb61-32"><a href="#cb61-32"></a>b2 <span class="op">=</span> torch.randn(<span class="dv">27</span>, generator<span class="op">=</span>g)</span>
<span id="cb61-33"><a href="#cb61-33"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb61-34"><a href="#cb61-34"></a>loss <span class="op">=</span> F.cross_entropy(logits, Y)</span>
<span id="cb61-35"><a href="#cb61-35"></a></span>
<span id="cb61-36"><a href="#cb61-36"></a><span class="co"># All params</span></span>
<span id="cb61-37"><a href="#cb61-37"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb61-38"><a href="#cb61-38"></a><span class="bu">print</span>(<span class="st">"No of params: "</span>, <span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span>
<span id="cb61-39"><a href="#cb61-39"></a></span>
<span id="cb61-40"><a href="#cb61-40"></a><span class="co"># Pre-training</span></span>
<span id="cb61-41"><a href="#cb61-41"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb61-42"><a href="#cb61-42"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Data size torch.Size([228146, 3]) torch.Size([228146])
No of params:  3481</code></pre>
</div>
</div>
<p>Training and tracking stats:</p>
<div id="7eff55ae" class="cell" data-execution_count="37">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1"></a>lri <span class="op">=</span> []</span>
<span id="cb63-2"><a href="#cb63-2"></a>lossi <span class="op">=</span> []</span>
<span id="cb63-3"><a href="#cb63-3"></a></span>
<span id="cb63-4"><a href="#cb63-4"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb63-5"><a href="#cb63-5"></a>    <span class="co"># minibatch construct</span></span>
<span id="cb63-6"><a href="#cb63-6"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, X.shape[<span class="dv">0</span>], (<span class="dv">32</span>,))</span>
<span id="cb63-7"><a href="#cb63-7"></a>    <span class="co"># forward pass:</span></span>
<span id="cb63-8"><a href="#cb63-8"></a>    emb <span class="op">=</span> C[X[ix]] <span class="co"># (32, 3, 2)</span></span>
<span id="cb63-9"><a href="#cb63-9"></a>    h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb63-10"><a href="#cb63-10"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb63-11"><a href="#cb63-11"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Y[ix])</span>
<span id="cb63-12"><a href="#cb63-12"></a>    <span class="co"># backward pass:</span></span>
<span id="cb63-13"><a href="#cb63-13"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb63-14"><a href="#cb63-14"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb63-15"><a href="#cb63-15"></a>    loss.backward()</span>
<span id="cb63-16"><a href="#cb63-16"></a>    <span class="co"># update</span></span>
<span id="cb63-17"><a href="#cb63-17"></a>    lr <span class="op">=</span> lrs[i]</span>
<span id="cb63-18"><a href="#cb63-18"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb63-19"><a href="#cb63-19"></a>        p.data <span class="op">+=</span> <span class="op">-</span> lr <span class="op">*</span> p.grad</span>
<span id="cb63-20"><a href="#cb63-20"></a></span>
<span id="cb63-21"><a href="#cb63-21"></a>    <span class="co"># track stats</span></span>
<span id="cb63-22"><a href="#cb63-22"></a>    lri.append(lre[i])</span>
<span id="cb63-23"><a href="#cb63-23"></a>    lossi.append(loss.item())</span>
<span id="cb63-24"><a href="#cb63-24"></a></span>
<span id="cb63-25"><a href="#cb63-25"></a>loss.item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>9.962945938110352</code></pre>
</div>
</div>
<p>Plotting, we see a good exponential element of learning rate turn out to be around <code>-1</code>.</p>
<p><span class="math inline">\(10^{-1}\)</span> is <code>0.1</code> so our initial guess seems good.</p>
<div id="ecfc32b9" class="cell" data-execution_count="38">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1"></a>plt.plot(lri, lossi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-39-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="splitting-up-the-dataset-into-trainvaltest-splits-and-why" class="level2">
<h2 class="anchored" data-anchor-id="splitting-up-the-dataset-into-trainvaltest-splits-and-why">splitting up the dataset into train/val/test splits and why</h2>
<p>Now we can keep lengthening the training loop to continue decreasing loss. We can try some techniques like change the learning rate to <code>0.001</code> after 20k, 30k loops of training with <code>0.1</code>.</p>
<p>But it will come to be overfitting when we try to keep training or increase the size of network to achieve a lower loss. The model just memorizing our training set verbatim, so if we try to sample from the model it just gives us the same thing in the dataset. Or if we calculate the loss on another dataset, it might be very high.</p>
<p>So another industry standard is we will split the data set into 3 pieces: (1) training set; (2) dev/validation set; and (3) test set, they can be 80% - 10% - 10% roughly and respectively.</p>
<ol type="1">
<li>Training split: train the parameters;</li>
<li>Dev/validation split: train the hyperparamerters (size of hidden layer, size of embedding, streng of regularization, etc);</li>
<li>Test split: evaluate the performance of the model at the end, we only work on this a very very few times, otherwise we learn from it and repeat overfitting.</li>
</ol>
<p>We are going to implement this train/dev/test splits:</p>
<div id="33815b87" class="cell" data-execution_count="39">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1"></a><span class="co"># build the dataset</span></span>
<span id="cb66-2"><a href="#cb66-2"></a><span class="kw">def</span> buid_dataset(words):</span>
<span id="cb66-3"><a href="#cb66-3"></a>    block_size <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb66-4"><a href="#cb66-4"></a>    X, Y <span class="op">=</span> [], []</span>
<span id="cb66-5"><a href="#cb66-5"></a></span>
<span id="cb66-6"><a href="#cb66-6"></a>    <span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb66-7"><a href="#cb66-7"></a>        context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size</span>
<span id="cb66-8"><a href="#cb66-8"></a>        <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb66-9"><a href="#cb66-9"></a>            ix <span class="op">=</span> stoi[ch]</span>
<span id="cb66-10"><a href="#cb66-10"></a>            X.append(context)</span>
<span id="cb66-11"><a href="#cb66-11"></a>            Y.append(ix)</span>
<span id="cb66-12"><a href="#cb66-12"></a>            context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix]</span>
<span id="cb66-13"><a href="#cb66-13"></a></span>
<span id="cb66-14"><a href="#cb66-14"></a>    X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb66-15"><a href="#cb66-15"></a>    Y <span class="op">=</span> torch.tensor(Y)</span>
<span id="cb66-16"><a href="#cb66-16"></a>    <span class="bu">print</span>(X.shape, Y.shape)</span>
<span id="cb66-17"><a href="#cb66-17"></a>    <span class="cf">return</span> X, Y</span>
<span id="cb66-18"><a href="#cb66-18"></a></span>
<span id="cb66-19"><a href="#cb66-19"></a><span class="im">import</span> random</span>
<span id="cb66-20"><a href="#cb66-20"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb66-21"><a href="#cb66-21"></a>random.shuffle(words)</span>
<span id="cb66-22"><a href="#cb66-22"></a>n1 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb66-23"><a href="#cb66-23"></a>n2 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb66-24"><a href="#cb66-24"></a></span>
<span id="cb66-25"><a href="#cb66-25"></a>Xtr, Ytr <span class="op">=</span> buid_dataset(words[:n1])</span>
<span id="cb66-26"><a href="#cb66-26"></a>Xdev, Ydev <span class="op">=</span> buid_dataset(words[n1:n2])</span>
<span id="cb66-27"><a href="#cb66-27"></a>Xte, Yte <span class="op">=</span> buid_dataset(words[n2:])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([182625, 3]) torch.Size([182625])
torch.Size([22655, 3]) torch.Size([22655])
torch.Size([22866, 3]) torch.Size([22866])</code></pre>
</div>
</div>
<p>Now we’re already to train on splits of the dataset, but let’s hold on as we are talking abount overfitting. As discussed, overfitting also come from using a complex (too many parameters) for a small data set.</p>
<p>Our dataset has roughly 228k records, while the size of network is only 3.4k. So we are still underfitting, let’s continue to complexify our neural networks.</p>
<p>2 things to consider here:</p>
<ul>
<li>the size of tanh - hidden layer; and</li>
<li>dimensions of embedding space.</li>
</ul>
</section>
<section id="visualizing-the-loss-character-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-the-loss-character-embeddings">visualizing the loss, character embeddings</h2>
<p>First we want to see: - how the loss decrease with 200k training loop with current network setting, learning rate decay to 0.01 after first 100k; and - how the current character embeddings recognize the similarity between characters in (2D) space.</p>
<p>Training on the <code>Xtr</code>, <code>Ytr</code>:</p>
<div id="33e1a4c4" class="cell" data-execution_count="40">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1"></a><span class="co"># Lookup table</span></span>
<span id="cb68-2"><a href="#cb68-2"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb68-3"><a href="#cb68-3"></a>C <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">2</span>), generator<span class="op">=</span>g)</span>
<span id="cb68-4"><a href="#cb68-4"></a></span>
<span id="cb68-5"><a href="#cb68-5"></a><span class="co"># Layer 1 - tanh</span></span>
<span id="cb68-6"><a href="#cb68-6"></a>W1 <span class="op">=</span> torch.randn((<span class="dv">6</span>, <span class="dv">100</span>), generator<span class="op">=</span>g)</span>
<span id="cb68-7"><a href="#cb68-7"></a>b1 <span class="op">=</span> torch.randn(<span class="dv">100</span>, generator<span class="op">=</span>g)</span>
<span id="cb68-8"><a href="#cb68-8"></a></span>
<span id="cb68-9"><a href="#cb68-9"></a><span class="co"># Layer 2 - softmax</span></span>
<span id="cb68-10"><a href="#cb68-10"></a>W2 <span class="op">=</span> torch.randn((<span class="dv">100</span>, <span class="dv">27</span>), generator<span class="op">=</span>g)</span>
<span id="cb68-11"><a href="#cb68-11"></a>b2 <span class="op">=</span> torch.randn(<span class="dv">27</span>, generator<span class="op">=</span>g)</span>
<span id="cb68-12"><a href="#cb68-12"></a></span>
<span id="cb68-13"><a href="#cb68-13"></a><span class="co"># All params</span></span>
<span id="cb68-14"><a href="#cb68-14"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb68-15"><a href="#cb68-15"></a><span class="bu">print</span>(<span class="st">"No of params: "</span>, <span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span>
<span id="cb68-16"><a href="#cb68-16"></a></span>
<span id="cb68-17"><a href="#cb68-17"></a><span class="co"># Pre-training</span></span>
<span id="cb68-18"><a href="#cb68-18"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb68-19"><a href="#cb68-19"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb68-20"><a href="#cb68-20"></a></span>
<span id="cb68-21"><a href="#cb68-21"></a><span class="co"># Stats holders</span></span>
<span id="cb68-22"><a href="#cb68-22"></a>lossi <span class="op">=</span> []</span>
<span id="cb68-23"><a href="#cb68-23"></a>stepi <span class="op">=</span> []</span>
<span id="cb68-24"><a href="#cb68-24"></a></span>
<span id="cb68-25"><a href="#cb68-25"></a><span class="co"># Training on Xtr, Ytr</span></span>
<span id="cb68-26"><a href="#cb68-26"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5_000</span>): <span class="co">#200_000</span></span>
<span id="cb68-27"><a href="#cb68-27"></a>    <span class="co"># minibatch construct</span></span>
<span id="cb68-28"><a href="#cb68-28"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (<span class="dv">32</span>,))                       <span class="co">#👈</span></span>
<span id="cb68-29"><a href="#cb68-29"></a>    <span class="co"># forward pass:</span></span>
<span id="cb68-30"><a href="#cb68-30"></a>    emb <span class="op">=</span> C[Xtr[ix]] <span class="co"># (32, 3, 2)                                    #👈</span></span>
<span id="cb68-31"><a href="#cb68-31"></a>    h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb68-32"><a href="#cb68-32"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb68-33"><a href="#cb68-33"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Ytr[ix])                          <span class="co">#👈</span></span>
<span id="cb68-34"><a href="#cb68-34"></a>    <span class="co"># backward pass:</span></span>
<span id="cb68-35"><a href="#cb68-35"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb68-36"><a href="#cb68-36"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb68-37"><a href="#cb68-37"></a>    loss.backward()</span>
<span id="cb68-38"><a href="#cb68-38"></a>    <span class="co"># update</span></span>
<span id="cb68-39"><a href="#cb68-39"></a>    lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;=</span> <span class="dv">100_000</span> <span class="cf">else</span> <span class="fl">0.01</span>                                <span class="co">#👈</span></span>
<span id="cb68-40"><a href="#cb68-40"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb68-41"><a href="#cb68-41"></a>        p.data <span class="op">+=</span> <span class="op">-</span> lr <span class="op">*</span> p.grad</span>
<span id="cb68-42"><a href="#cb68-42"></a></span>
<span id="cb68-43"><a href="#cb68-43"></a>    <span class="co"># track stats</span></span>
<span id="cb68-44"><a href="#cb68-44"></a>    lossi.append(loss.item())</span>
<span id="cb68-45"><a href="#cb68-45"></a>    stepi.append(i)</span>
<span id="cb68-46"><a href="#cb68-46"></a></span>
<span id="cb68-47"><a href="#cb68-47"></a><span class="bu">print</span>(<span class="st">"Loss on minibatch: "</span>, loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>No of params:  3481
Loss on minibatch:  2.652956485748291</code></pre>
</div>
</div>
<p>Loss on whole training dataset:</p>
<div id="5559d90b" class="cell" data-execution_count="41">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1"></a>emb <span class="op">=</span> C[Xtr] <span class="co"># (32, 3, 2)</span></span>
<span id="cb70-2"><a href="#cb70-2"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb70-3"><a href="#cb70-3"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb70-4"><a href="#cb70-4"></a>loss <span class="op">=</span> F.cross_entropy(logits, Ytr)</span>
<span id="cb70-5"><a href="#cb70-5"></a>loss.item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>2.5263497829437256</code></pre>
</div>
</div>
<p>Loss on dev/validation dataset, it’s not much different from loss on training as the model is still underfitting, it still generalizes thing:</p>
<div id="8b2cc082" class="cell" data-execution_count="42">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1"></a>emb <span class="op">=</span> C[Xdev] <span class="co"># (32, 3, 2)</span></span>
<span id="cb72-2"><a href="#cb72-2"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb72-3"><a href="#cb72-3"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb72-4"><a href="#cb72-4"></a>loss <span class="op">=</span> F.cross_entropy(logits, Ydev)</span>
<span id="cb72-5"><a href="#cb72-5"></a>loss.item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>2.520705461502075</code></pre>
</div>
</div>
<p>Visualizing loss, we can see the loss shaking significantly as the batch size still small - 32.</p>
<div id="bd285cd6" class="cell" data-execution_count="43">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1"></a>plt.plot(stepi, lossi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-44-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Visualizing the character embeddings, we can see the model can cluster for eg. vowels a, e, i, o, u.</p>
<div id="a23eb13a" class="cell" data-execution_count="44">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb75-2"><a href="#cb75-2"></a>plt.scatter(C[:,<span class="dv">0</span>].data, C[:, <span class="dv">1</span>].data, s<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb75-3"><a href="#cb75-3"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(C.shape[<span class="dv">0</span>]):</span>
<span id="cb75-4"><a href="#cb75-4"></a>    plt.text(C[i,<span class="dv">0</span>].item(),C[i,<span class="dv">1</span>].item(), itos[i], ha<span class="op">=</span><span class="st">"center"</span>, va<span class="op">=</span><span class="st">"center"</span>, color<span class="op">=</span><span class="st">"white"</span>)</span>
<span id="cb75-5"><a href="#cb75-5"></a>plt.grid(<span class="st">'minor'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-45-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="experiment-larger-hidden-layer-larger-embedding-size" class="level2">
<h2 class="anchored" data-anchor-id="experiment-larger-hidden-layer-larger-embedding-size">experiment: larger hidden layer, larger embedding size</h2>
<p>Now we can experiment a larger hidden layer (300), and larger embedding_size (10). Below is the whole code:</p>
<div id="015476ba" class="cell" data-execution_count="45">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1"></a><span class="co"># hyper-parameters</span></span>
<span id="cb76-2"><a href="#cb76-2"></a>block_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># number of chracters / inputs to predict the nextone</span></span>
<span id="cb76-3"><a href="#cb76-3"></a>no_chars <span class="op">=</span> <span class="dv">27</span> <span class="co"># number of possible chracters, include '.'</span></span>
<span id="cb76-4"><a href="#cb76-4"></a>emb_size <span class="op">=</span> <span class="dv">10</span> <span class="co"># no of dimensions of the embedding space.</span></span>
<span id="cb76-5"><a href="#cb76-5"></a>hidden_size <span class="op">=</span> <span class="dv">300</span> <span class="co"># size of the hidden - tanh layer</span></span>
<span id="cb76-6"><a href="#cb76-6"></a>batch_size <span class="op">=</span> <span class="dv">32</span> <span class="co"># minibatch size for training, 2, 4, 8, 16, 32, 64, etc</span></span>
<span id="cb76-7"><a href="#cb76-7"></a></span>
<span id="cb76-8"><a href="#cb76-8"></a><span class="co"># build the dataset</span></span>
<span id="cb76-9"><a href="#cb76-9"></a><span class="kw">def</span> buid_dataset(words):</span>
<span id="cb76-10"><a href="#cb76-10"></a></span>
<span id="cb76-11"><a href="#cb76-11"></a>    X, Y <span class="op">=</span> [], []</span>
<span id="cb76-12"><a href="#cb76-12"></a></span>
<span id="cb76-13"><a href="#cb76-13"></a>    <span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb76-14"><a href="#cb76-14"></a>        context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size</span>
<span id="cb76-15"><a href="#cb76-15"></a>        <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb76-16"><a href="#cb76-16"></a>            ix <span class="op">=</span> stoi[ch]</span>
<span id="cb76-17"><a href="#cb76-17"></a>            X.append(context)</span>
<span id="cb76-18"><a href="#cb76-18"></a>            Y.append(ix)</span>
<span id="cb76-19"><a href="#cb76-19"></a>            context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix]</span>
<span id="cb76-20"><a href="#cb76-20"></a></span>
<span id="cb76-21"><a href="#cb76-21"></a>    X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb76-22"><a href="#cb76-22"></a>    Y <span class="op">=</span> torch.tensor(Y)</span>
<span id="cb76-23"><a href="#cb76-23"></a>    <span class="bu">print</span>(X.shape, Y.shape)</span>
<span id="cb76-24"><a href="#cb76-24"></a>    <span class="cf">return</span> X, Y</span>
<span id="cb76-25"><a href="#cb76-25"></a></span>
<span id="cb76-26"><a href="#cb76-26"></a><span class="im">import</span> random</span>
<span id="cb76-27"><a href="#cb76-27"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb76-28"><a href="#cb76-28"></a>random.shuffle(words)</span>
<span id="cb76-29"><a href="#cb76-29"></a>n1 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb76-30"><a href="#cb76-30"></a>n2 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb76-31"><a href="#cb76-31"></a></span>
<span id="cb76-32"><a href="#cb76-32"></a><span class="co"># 80 - 10 - 10 splits</span></span>
<span id="cb76-33"><a href="#cb76-33"></a>Xtr, Ytr <span class="op">=</span> buid_dataset(words[:n1])</span>
<span id="cb76-34"><a href="#cb76-34"></a>Xdev, Ydev <span class="op">=</span> buid_dataset(words[n1:n2])</span>
<span id="cb76-35"><a href="#cb76-35"></a>Xte, Yte <span class="op">=</span> buid_dataset(words[n2:])</span>
<span id="cb76-36"><a href="#cb76-36"></a></span>
<span id="cb76-37"><a href="#cb76-37"></a><span class="co"># Lookup table - 10 dimensional space</span></span>
<span id="cb76-38"><a href="#cb76-38"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>) <span class="co"># for reproductivity</span></span>
<span id="cb76-39"><a href="#cb76-39"></a>C <span class="op">=</span> torch.randn((no_chars, emb_size), generator<span class="op">=</span>g)</span>
<span id="cb76-40"><a href="#cb76-40"></a></span>
<span id="cb76-41"><a href="#cb76-41"></a><span class="co"># Layer 1 - tanh - 300 neurons</span></span>
<span id="cb76-42"><a href="#cb76-42"></a>W1 <span class="op">=</span> torch.randn((block_size <span class="op">*</span> emb_size, hidden_size), generator<span class="op">=</span>g)</span>
<span id="cb76-43"><a href="#cb76-43"></a>b1 <span class="op">=</span> torch.randn(hidden_size, generator<span class="op">=</span>g)</span>
<span id="cb76-44"><a href="#cb76-44"></a></span>
<span id="cb76-45"><a href="#cb76-45"></a><span class="co"># Layer 2 - softmax</span></span>
<span id="cb76-46"><a href="#cb76-46"></a>W2 <span class="op">=</span> torch.randn((hidden_size, no_chars), generator<span class="op">=</span>g)</span>
<span id="cb76-47"><a href="#cb76-47"></a>b2 <span class="op">=</span> torch.randn(no_chars, generator<span class="op">=</span>g)</span>
<span id="cb76-48"><a href="#cb76-48"></a></span>
<span id="cb76-49"><a href="#cb76-49"></a><span class="co"># All params</span></span>
<span id="cb76-50"><a href="#cb76-50"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb76-51"><a href="#cb76-51"></a><span class="bu">print</span>(<span class="st">"No of params: "</span>, <span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span>
<span id="cb76-52"><a href="#cb76-52"></a></span>
<span id="cb76-53"><a href="#cb76-53"></a><span class="co"># Pre-training</span></span>
<span id="cb76-54"><a href="#cb76-54"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb76-55"><a href="#cb76-55"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb76-56"><a href="#cb76-56"></a></span>
<span id="cb76-57"><a href="#cb76-57"></a><span class="co"># Stats holders</span></span>
<span id="cb76-58"><a href="#cb76-58"></a>lossi <span class="op">=</span> []</span>
<span id="cb76-59"><a href="#cb76-59"></a>stepi <span class="op">=</span> []</span>
<span id="cb76-60"><a href="#cb76-60"></a></span>
<span id="cb76-61"><a href="#cb76-61"></a><span class="co"># Training on Xtr, Ytr</span></span>
<span id="cb76-62"><a href="#cb76-62"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5_000</span>): <span class="co">#200_000</span></span>
<span id="cb76-63"><a href="#cb76-63"></a>    <span class="co"># minibatch construct</span></span>
<span id="cb76-64"><a href="#cb76-64"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,))</span>
<span id="cb76-65"><a href="#cb76-65"></a>    <span class="co"># forward pass:</span></span>
<span id="cb76-66"><a href="#cb76-66"></a>    emb <span class="op">=</span> C[Xtr[ix]]</span>
<span id="cb76-67"><a href="#cb76-67"></a>    h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, block_size <span class="op">*</span> emb_size) <span class="op">@</span> W1 <span class="op">+</span> b1)</span>
<span id="cb76-68"><a href="#cb76-68"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb76-69"><a href="#cb76-69"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Ytr[ix])</span>
<span id="cb76-70"><a href="#cb76-70"></a>    <span class="co"># backward pass:</span></span>
<span id="cb76-71"><a href="#cb76-71"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb76-72"><a href="#cb76-72"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb76-73"><a href="#cb76-73"></a>    loss.backward()</span>
<span id="cb76-74"><a href="#cb76-74"></a>    <span class="co"># update</span></span>
<span id="cb76-75"><a href="#cb76-75"></a>    lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;=</span> <span class="dv">100_000</span> <span class="cf">else</span> <span class="fl">0.01</span></span>
<span id="cb76-76"><a href="#cb76-76"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb76-77"><a href="#cb76-77"></a>        p.data <span class="op">+=</span> <span class="op">-</span> lr <span class="op">*</span> p.grad</span>
<span id="cb76-78"><a href="#cb76-78"></a></span>
<span id="cb76-79"><a href="#cb76-79"></a>    <span class="co"># track stats</span></span>
<span id="cb76-80"><a href="#cb76-80"></a>    lossi.append(loss.item())</span>
<span id="cb76-81"><a href="#cb76-81"></a>    stepi.append(i)</span>
<span id="cb76-82"><a href="#cb76-82"></a></span>
<span id="cb76-83"><a href="#cb76-83"></a><span class="bu">print</span>(<span class="st">"Loss on minibatch: "</span>, loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([182580, 3]) torch.Size([182580])
torch.Size([22767, 3]) torch.Size([22767])
torch.Size([22799, 3]) torch.Size([22799])
No of params:  17697
Loss on minibatch:  3.161036252975464</code></pre>
</div>
</div>
<div id="0ad21115" class="cell" data-execution_count="46">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1"></a>emb <span class="op">=</span> C[Xtr]</span>
<span id="cb78-2"><a href="#cb78-2"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, block_size <span class="op">*</span> emb_size) <span class="op">@</span> W1 <span class="op">+</span> b1)</span>
<span id="cb78-3"><a href="#cb78-3"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb78-4"><a href="#cb78-4"></a>loss <span class="op">=</span> F.cross_entropy(logits, Ytr)</span>
<span id="cb78-5"><a href="#cb78-5"></a><span class="bu">print</span>(<span class="st">"Loss on whole training set: "</span>, loss.item())</span>
<span id="cb78-6"><a href="#cb78-6"></a></span>
<span id="cb78-7"><a href="#cb78-7"></a>emb <span class="op">=</span> C[Xdev]</span>
<span id="cb78-8"><a href="#cb78-8"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, block_size <span class="op">*</span> emb_size) <span class="op">@</span> W1 <span class="op">+</span> b1)</span>
<span id="cb78-9"><a href="#cb78-9"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb78-10"><a href="#cb78-10"></a>loss <span class="op">=</span> F.cross_entropy(logits, Ydev)</span>
<span id="cb78-11"><a href="#cb78-11"></a><span class="bu">print</span>(<span class="st">"Loss on dev/validation set: "</span>, loss.item())</span>
<span id="cb78-12"><a href="#cb78-12"></a></span>
<span id="cb78-13"><a href="#cb78-13"></a>emb <span class="op">=</span> C[Xte]</span>
<span id="cb78-14"><a href="#cb78-14"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, block_size <span class="op">*</span> emb_size) <span class="op">@</span> W1 <span class="op">+</span> b1)</span>
<span id="cb78-15"><a href="#cb78-15"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb78-16"><a href="#cb78-16"></a>loss <span class="op">=</span> F.cross_entropy(logits, Yte)</span>
<span id="cb78-17"><a href="#cb78-17"></a><span class="bu">print</span>(<span class="st">"Loss on test set: "</span>, loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on whole training set:  2.955504894256592
Loss on dev/validation set:  2.96545672416687
Loss on test set:  2.9667985439300537</code></pre>
</div>
</div>
</section>
<section id="summary-of-our-final-code-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-our-final-code-conclusion">summary of our final code, conclusion</h2>
<div id="19e6a5ba" class="cell" data-execution_count="47">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1"></a>plt.plot(stepi, lossi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-48-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="989d29e3" class="cell" data-execution_count="48">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb81-2"><a href="#cb81-2"></a>plt.scatter(C[:,<span class="dv">0</span>].data, C[:, <span class="dv">1</span>].data, s<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb81-3"><a href="#cb81-3"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(C.shape[<span class="dv">0</span>]):</span>
<span id="cb81-4"><a href="#cb81-4"></a>    plt.text(C[i,<span class="dv">0</span>].item(),C[i,<span class="dv">1</span>].item(), itos[i], ha<span class="op">=</span><span class="st">"center"</span>, va<span class="op">=</span><span class="st">"center"</span>, color<span class="op">=</span><span class="st">"white"</span>)</span>
<span id="cb81-5"><a href="#cb81-5"></a>plt.grid(<span class="st">'minor'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-49-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can see the loss on validation set and test set are quite similar as we are not try different scenarios to calibrate/tune hyperparamters much. So they both have the same suprise to the model training by <code>Xtr</code>.</p>
<p>We still have rooms for improvement!</p>
</section>
<section id="sampling-from-the-model" class="level2">
<h2 class="anchored" data-anchor-id="sampling-from-the-model">sampling from the model</h2>
<p>But our networks now can generate more name-like name!</p>
<div id="78e22062" class="cell" data-execution_count="49">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span> <span class="op">+</span> <span class="dv">10</span>)</span>
<span id="cb82-2"><a href="#cb82-2"></a></span>
<span id="cb82-3"><a href="#cb82-3"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb82-4"><a href="#cb82-4"></a></span>
<span id="cb82-5"><a href="#cb82-5"></a>    out <span class="op">=</span> []</span>
<span id="cb82-6"><a href="#cb82-6"></a>    context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size <span class="co"># initialize with all ...</span></span>
<span id="cb82-7"><a href="#cb82-7"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb82-8"><a href="#cb82-8"></a>      emb <span class="op">=</span> C[torch.tensor([context])] <span class="co"># (1,block_size,d)</span></span>
<span id="cb82-9"><a href="#cb82-9"></a>      h <span class="op">=</span> torch.tanh(emb.view(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">@</span> W1 <span class="op">+</span> b1)</span>
<span id="cb82-10"><a href="#cb82-10"></a>      logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb82-11"><a href="#cb82-11"></a>      probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb82-12"><a href="#cb82-12"></a>      ix <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>, generator<span class="op">=</span>g).item()</span>
<span id="cb82-13"><a href="#cb82-13"></a>      context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix]</span>
<span id="cb82-14"><a href="#cb82-14"></a>      out.append(ix)</span>
<span id="cb82-15"><a href="#cb82-15"></a>      <span class="cf">if</span> ix <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb82-16"><a href="#cb82-16"></a>        <span class="cf">break</span></span>
<span id="cb82-17"><a href="#cb82-17"></a></span>
<span id="cb82-18"><a href="#cb82-18"></a>    <span class="bu">print</span>(<span class="st">''</span>.join(itos[i] <span class="cf">for</span> i <span class="kw">in</span> out))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>dra.
aisya.
zhyes.
ddyn.
kai.
eavin.
dadra.
kai.
chadielie.
sailiea.
daeis.
doanaileatzivila.
adi.
sadhersahiah.
saila.
rajayslen.
kula.
uyr.
yada.
kyryle.</code></pre>
</div>
</div>
</section>
<section id="google-collab-new-notebook-advertisement" class="level2">
<h2 class="anchored" data-anchor-id="google-collab-new-notebook-advertisement">google collab (new!!) notebook advertisement</h2>
<p>Colab link: <a href="https://colab.research.google.com/drive/1YIfmkftLrz6MPTOO9Vwqrop2Q5llHIGK?usp=sharing" class="uri">https://colab.research.google.com/drive/1YIfmkftLrz6MPTOO9Vwqrop2Q5llHIGK?usp=sharing</a></p>
<p>Thanks Andrej!</p>
</section>
</section>
<section id="resources" class="level1">
<h1>resources</h1>
<ol type="1">
<li><a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"><strong>A Neural Probabilistic Language Model</strong>, Bengio et al.&nbsp;(2003)</a></li>
<li><a href="https://www.youtube.com/watch?v=TCH_1BHY58I">Video lecturer</a></li>
<li><a href="https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part2_mlp.ipynb">Notebook</a></li>
<li><a href="https://github.com/karpathy/makemore"><code>makemore</code> on Github</a></li>
<li><a href="https://pytorch.org/docs/main/tensors.html"><code>torch.Tensor()</code> documentation</a></li>
</ol>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/lktuan\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb84" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb84-1"><a href="#cb84-1"></a><span class="co">---</span></span>
<span id="cb84-2"><a href="#cb84-2"></a><span class="an">title:</span><span class="co"> "NN-Z2H Lesson 3: Building makemore part 2 - MLP"</span></span>
<span id="cb84-3"><a href="#cb84-3"></a><span class="an">description:</span><span class="co"> "implement a multilayer perceptron (MLP) character-level language model, introduce model training, learning rate tuning, hyperparameters, evaluation, train/dev/test splits, under/overfitting, etc."</span></span>
<span id="cb84-4"><a href="#cb84-4"></a><span class="an">author:</span></span>
<span id="cb84-5"><a href="#cb84-5"></a><span class="co">  - name: "Tuan Le Khac"</span></span>
<span id="cb84-6"><a href="#cb84-6"></a><span class="co">    url: https://lktuan.github.io/</span></span>
<span id="cb84-7"><a href="#cb84-7"></a><span class="an">categories:</span><span class="co"> [til, python, andrej karpathy, nn-z2h, neural networks]</span></span>
<span id="cb84-8"><a href="#cb84-8"></a><span class="an">date:</span><span class="co"> 11-20-2024</span></span>
<span id="cb84-9"><a href="#cb84-9"></a><span class="an">date-modified:</span><span class="co"> 11-20-2024</span></span>
<span id="cb84-10"><a href="#cb84-10"></a><span class="an">image:</span><span class="co"> NLM_Bengio_etal.png</span></span>
<span id="cb84-11"><a href="#cb84-11"></a><span class="an">draft:</span><span class="co"> false</span></span>
<span id="cb84-12"><a href="#cb84-12"></a><span class="co"># editor: visual</span></span>
<span id="cb84-13"><a href="#cb84-13"></a><span class="an">format:</span></span>
<span id="cb84-14"><a href="#cb84-14"></a><span class="co">  html:</span></span>
<span id="cb84-15"><a href="#cb84-15"></a><span class="co">    code-overflow: wrap</span></span>
<span id="cb84-16"><a href="#cb84-16"></a><span class="co">    code-tools: true</span></span>
<span id="cb84-17"><a href="#cb84-17"></a><span class="co">    code-fold: show</span></span>
<span id="cb84-18"><a href="#cb84-18"></a><span class="co">    code-annotations: hover</span></span>
<span id="cb84-19"><a href="#cb84-19"></a><span class="co">---</span></span>
<span id="cb84-20"><a href="#cb84-20"></a></span>
<span id="cb84-21"><a href="#cb84-21"></a>::: {.callout-important title="This is not orginal content!"}</span>
<span id="cb84-22"><a href="#cb84-22"></a>This is my study notes / codes along with Andrej Karpathy's "<span class="co">[</span><span class="ot">Neural Networks: Zero to Hero</span><span class="co">](https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)</span>" series.</span>
<span id="cb84-23"><a href="#cb84-23"></a></span>
<span id="cb84-24"><a href="#cb84-24"></a>*Edit 1st June, 25*: I changed the loop to 5k only, for the shake of time saving when re-rendering this post!</span>
<span id="cb84-25"><a href="#cb84-25"></a>:::</span>
<span id="cb84-26"><a href="#cb84-26"></a></span>
<span id="cb84-27"><a href="#cb84-27"></a>In the previous lecture, we built a simple <span class="in">`bigram`</span> character-level language model, using 2 different approaches that are (1) count, and (2) 1 layer neural network. They produced the same (and both poor - since the context is 1 character only) result but the neural network option offers more flexibility so that we can complexify our model to get better performance.</span>
<span id="cb84-28"><a href="#cb84-28"></a></span>
<span id="cb84-29"><a href="#cb84-29"></a>In this lecture we are going to implement 20-years ago neural probabilistic language model by *Bengio et al. (2003)*.</span>
<span id="cb84-30"><a href="#cb84-30"></a></span>
<span id="cb84-31"><a href="#cb84-31"></a><span class="fu"># PART 1: intro to MLP</span></span>
<span id="cb84-32"><a href="#cb84-32"></a></span>
<span id="cb84-33"><a href="#cb84-33"></a><span class="fu">## Bengio et al. 2003 (MLP language model) paper walkthrough</span></span>
<span id="cb84-34"><a href="#cb84-34"></a></span>
<span id="cb84-35"><a href="#cb84-35"></a><span class="fu">### Summary</span></span>
<span id="cb84-36"><a href="#cb84-36"></a></span>
<span id="cb84-37"><a href="#cb84-37"></a>**Problem Statement**:</span>
<span id="cb84-38"><a href="#cb84-38"></a></span>
<span id="cb84-39"><a href="#cb84-39"></a><span class="ss">-   </span>Traditional n-gram language models suffer from the *curse of dimensionality*: they can't effectively generalize to word sequences not seen in training data;</span>
<span id="cb84-40"><a href="#cb84-40"></a><span class="ss">-   </span>The core issue is treating words as atomic units with no *inherent similarity* to each other;</span>
<span id="cb84-41"><a href="#cb84-41"></a><span class="ss">-   </span>For example, if we've seen "dog is eating" in training but never "cat is eating", n-gram models can't leverage the similarity between "dog" and "cat";</span>
<span id="cb84-42"><a href="#cb84-42"></a><span class="ss">-   </span>This leads to poor probability estimates for rare or unseen word sequences.</span>
<span id="cb84-43"><a href="#cb84-43"></a></span>
<span id="cb84-44"><a href="#cb84-44"></a>**Solution**:</span>
<span id="cb84-45"><a href="#cb84-45"></a></span>
<span id="cb84-46"><a href="#cb84-46"></a><span class="ss">-   </span>Learn a *distributed representation* (embedding) for each word in a continuous vector space where similar words are close to each other;</span>
<span id="cb84-47"><a href="#cb84-47"></a><span class="ss">-   </span>Use a neural network architecture with:</span>
<span id="cb84-48"><a href="#cb84-48"></a><span class="ss">    -   </span>Input layer: concatenated embeddings of n-1 previous words;</span>
<span id="cb84-49"><a href="#cb84-49"></a><span class="ss">    -   </span>Hidden layer: dense neural network with <span class="in">`tanh`</span> activation;</span>
<span id="cb84-50"><a href="#cb84-50"></a><span class="ss">    -   </span>Output layer: softmax over entire vocabulary to predict next word probability.</span>
<span id="cb84-51"><a href="#cb84-51"></a></span>
<span id="cb84-52"><a href="#cb84-52"></a>**The model simultaneously learns**:</span>
<span id="cb84-53"><a href="#cb84-53"></a></span>
<span id="cb84-54"><a href="#cb84-54"></a><span class="ss">-   </span>Word feature vectors (embeddings) that capture *semantic/syntactic word similarities*;</span>
<span id="cb84-55"><a href="#cb84-55"></a><span class="ss">-   </span>Neural network parameters that combine these features to estimate probability distributions.</span>
<span id="cb84-56"><a href="#cb84-56"></a></span>
<span id="cb84-57"><a href="#cb84-57"></a>**Key advantages**:</span>
<span id="cb84-58"><a href="#cb84-58"></a></span>
<span id="cb84-59"><a href="#cb84-59"></a><span class="ss">-   </span>Words with similar meanings get similar feature vectors, enabling better *generalization*;</span>
<span id="cb84-60"><a href="#cb84-60"></a><span class="ss">-   </span>The probability function is smooth with respect to word embeddings, so similar words yield *similar predictions*;</span>
<span id="cb84-61"><a href="#cb84-61"></a><span class="ss">-   </span>Can generalize to *unseen sequences* by leveraging learned word similarities.</span>
<span id="cb84-62"><a href="#cb84-62"></a></span>
<span id="cb84-63"><a href="#cb84-63"></a><span class="fu">### Methodology:</span></span>
<span id="cb84-64"><a href="#cb84-64"></a></span>
<span id="cb84-65"><a href="#cb84-65"></a><span class="ss">-   </span>Traditional Problem:</span>
<span id="cb84-66"><a href="#cb84-66"></a></span>
<span id="cb84-67"><a href="#cb84-67"></a><span class="ss">    -   </span>In n-gram models, each word sequence of length n is a separate parameter;</span>
<span id="cb84-68"><a href="#cb84-68"></a><span class="ss">    -   </span>For vocabulary size $|V|$, need $|V|^n$ parameters;</span>
<span id="cb84-69"><a href="#cb84-69"></a><span class="ss">    -   </span>Most sequences never appear in training, leading to poor generalization;</span>
<span id="cb84-70"><a href="#cb84-70"></a></span>
<span id="cb84-71"><a href="#cb84-71"></a><span class="ss">-   </span>Solution via **Distributed Representation**:</span>
<span id="cb84-72"><a href="#cb84-72"></a></span>
<span id="cb84-73"><a href="#cb84-73"></a><span class="ss">    -   </span>Each word mapped to a dense vector in $R^m$ (typically m=50-100);</span>
<span id="cb84-74"><a href="#cb84-74"></a><span class="ss">    -   </span>Similar words get similar vectors through training;</span>
<span id="cb84-75"><a href="#cb84-75"></a><span class="ss">    -   </span>Probability function is smooth w.r.t these vectors;</span>
<span id="cb84-76"><a href="#cb84-76"></a><span class="ss">    -   </span>Key benefit: If "dog" and "cat" have similar vectors, model can generalize from "dog is eating" to "cat is eating";</span>
<span id="cb84-77"><a href="#cb84-77"></a><span class="ss">    -   </span>Number of parameters reduces to $O(|V|×m + m×h + h×|V|)$, where $h$ is hidden layer size;</span>
<span id="cb84-78"><a href="#cb84-78"></a><span class="ss">    -   </span>This is much smaller than $|V|^n$ and allows better generalization;</span>
<span id="cb84-79"><a href="#cb84-79"></a></span>
<span id="cb84-80"><a href="#cb84-80"></a><span class="fu">### Neural architecture:</span></span>
<span id="cb84-81"><a href="#cb84-81"></a></span>
<span id="cb84-82"><a href="#cb84-82"></a>**Input Layer**:</span>
<span id="cb84-83"><a href="#cb84-83"></a></span>
<span id="cb84-84"><a href="#cb84-84"></a><span class="ss">-   </span>Takes $n-1$ previous words (context window);</span>
<span id="cb84-85"><a href="#cb84-85"></a><span class="ss">-   </span>Each word i mapped to vector $C(i) ∈ R^m$ via lookup table;</span>
<span id="cb84-86"><a href="#cb84-86"></a><span class="ss">-   </span>Concatenates these vectors: $x = <span class="co">[</span><span class="ot">C(wₜ₋ₙ₊₁), ..., C(wₜ₋₁)</span><span class="co">]</span>$;</span>
<span id="cb84-87"><a href="#cb84-87"></a><span class="ss">-   </span>$x$ dimension is $(n-1)×m$;</span>
<span id="cb84-88"><a href="#cb84-88"></a></span>
<span id="cb84-89"><a href="#cb84-89"></a>**Hidden Layer**:</span>
<span id="cb84-90"><a href="#cb84-90"></a></span>
<span id="cb84-91"><a href="#cb84-91"></a><span class="ss">-   </span>Dense layer with tanh activation;</span>
<span id="cb84-92"><a href="#cb84-92"></a><span class="ss">-   </span>Computation: $h = tanh(d + Hx)$;</span>
<span id="cb84-93"><a href="#cb84-93"></a><span class="ss">-   </span>$H$ is weight matrix, $d$ is bias vector;</span>
<span id="cb84-94"><a href="#cb84-94"></a><span class="ss">-   </span>Maps concatenated context to hidden representation;</span>
<span id="cb84-95"><a href="#cb84-95"></a></span>
<span id="cb84-96"><a href="#cb84-96"></a>**Output Layer**:</span>
<span id="cb84-97"><a href="#cb84-97"></a></span>
<span id="cb84-98"><a href="#cb84-98"></a><span class="ss">-   </span>Computes probability distribution over all words;</span>
<span id="cb84-99"><a href="#cb84-99"></a><span class="ss">-   </span>$y = b + Wx + Uh$;</span>
<span id="cb84-100"><a href="#cb84-100"></a><span class="ss">-   </span>Softmax activation: $P(wₜ|context) = exp(yᵢ)/Σⱼexp(yⱼ)$;</span>
<span id="cb84-101"><a href="#cb84-101"></a><span class="ss">-   </span>$W$ provides "shortcut" connections from input to output;</span>
<span id="cb84-102"><a href="#cb84-102"></a><span class="ss">-   </span>Direct connection helps learn simpler patterns;</span>
<span id="cb84-103"><a href="#cb84-103"></a></span>
<span id="cb84-104"><a href="#cb84-104"></a>**Training**:</span>
<span id="cb84-105"><a href="#cb84-105"></a></span>
<span id="cb84-106"><a href="#cb84-106"></a><span class="ss">-   </span>Maximizes log-likelihood of training data;</span>
<span id="cb84-107"><a href="#cb84-107"></a><span class="ss">-   </span>Uses stochastic gradient descent;</span>
<span id="cb84-108"><a href="#cb84-108"></a><span class="ss">-   </span>Learns both word vectors $C(i)$ and neural network parameters $(H, d, W, U, b)$;</span>
<span id="cb84-109"><a href="#cb84-109"></a><span class="ss">-   </span>Word vectors capture similarities as they help predict similar contexts;</span>
<span id="cb84-110"><a href="#cb84-110"></a><span class="ss">-   </span>Can initialize word vectors randomly or with pretrained vectors.</span>
<span id="cb84-111"><a href="#cb84-111"></a></span>
<span id="cb84-112"><a href="#cb84-112"></a><span class="al">![Neural Language Model proposed by (Bengio et al., 2003). C(i) is the i th word embedding.](NLM_Bengio_etal.png)</span></span>
<span id="cb84-113"><a href="#cb84-113"></a></span>
<span id="cb84-114"><a href="#cb84-114"></a><span class="fu">## (re-)building our training dataset</span></span>
<span id="cb84-115"><a href="#cb84-115"></a></span>
<span id="cb84-116"><a href="#cb84-116"></a>Loading library, reading data, building dictionary:</span>
<span id="cb84-117"><a href="#cb84-117"></a></span>
<span id="cb84-120"><a href="#cb84-120"></a><span class="in">```{python}</span></span>
<span id="cb84-121"><a href="#cb84-121"></a><span class="im">import</span> torch</span>
<span id="cb84-122"><a href="#cb84-122"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb84-123"><a href="#cb84-123"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb84-124"><a href="#cb84-124"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb84-125"><a href="#cb84-125"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb84-126"><a href="#cb84-126"></a><span class="in">```</span></span>
<span id="cb84-127"><a href="#cb84-127"></a></span>
<span id="cb84-130"><a href="#cb84-130"></a><span class="in">```{python}</span></span>
<span id="cb84-131"><a href="#cb84-131"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb84-132"><a href="#cb84-132"></a></span>
<span id="cb84-133"><a href="#cb84-133"></a>url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt"</span></span>
<span id="cb84-134"><a href="#cb84-134"></a>words <span class="op">=</span> pd.read_csv(url, header<span class="op">=</span><span class="va">None</span>).iloc[:, <span class="dv">0</span>].tolist()</span>
<span id="cb84-135"><a href="#cb84-135"></a>words[:<span class="dv">8</span>]</span>
<span id="cb84-136"><a href="#cb84-136"></a><span class="in">```</span></span>
<span id="cb84-137"><a href="#cb84-137"></a></span>
<span id="cb84-140"><a href="#cb84-140"></a><span class="in">```{python}</span></span>
<span id="cb84-141"><a href="#cb84-141"></a><span class="bu">len</span>(words)</span>
<span id="cb84-142"><a href="#cb84-142"></a><span class="in">```</span></span>
<span id="cb84-143"><a href="#cb84-143"></a></span>
<span id="cb84-146"><a href="#cb84-146"></a><span class="in">```{python}</span></span>
<span id="cb84-147"><a href="#cb84-147"></a><span class="co"># build the vocabulary of characters and mapping to/from integer</span></span>
<span id="cb84-148"><a href="#cb84-148"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(<span class="st">''</span>.join(words))))</span>
<span id="cb84-149"><a href="#cb84-149"></a>stoi <span class="op">=</span> {s:i<span class="op">+</span><span class="dv">1</span> <span class="cf">for</span> i, s <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb84-150"><a href="#cb84-150"></a>stoi[<span class="st">'.'</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb84-151"><a href="#cb84-151"></a></span>
<span id="cb84-152"><a href="#cb84-152"></a>itos <span class="op">=</span> {i: s <span class="cf">for</span> s, i <span class="kw">in</span> stoi.items()}</span>
<span id="cb84-153"><a href="#cb84-153"></a>itos</span>
<span id="cb84-154"><a href="#cb84-154"></a><span class="in">```</span></span>
<span id="cb84-155"><a href="#cb84-155"></a></span>
<span id="cb84-156"><a href="#cb84-156"></a>Building the dataset:</span>
<span id="cb84-157"><a href="#cb84-157"></a></span>
<span id="cb84-160"><a href="#cb84-160"></a><span class="in">```{python}</span></span>
<span id="cb84-161"><a href="#cb84-161"></a>block_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># the context length: how many characters do we take to predict the next one?</span></span>
<span id="cb84-162"><a href="#cb84-162"></a>X, Y <span class="op">=</span> [], []</span>
<span id="cb84-163"><a href="#cb84-163"></a></span>
<span id="cb84-164"><a href="#cb84-164"></a><span class="cf">for</span> w <span class="kw">in</span> words[:<span class="dv">5</span>]:</span>
<span id="cb84-165"><a href="#cb84-165"></a>    <span class="bu">print</span>(w)</span>
<span id="cb84-166"><a href="#cb84-166"></a>    context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size <span class="co"># 0 so context will be padded by '.'</span></span>
<span id="cb84-167"><a href="#cb84-167"></a>    <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb84-168"><a href="#cb84-168"></a>        ix <span class="op">=</span> stoi[ch]</span>
<span id="cb84-169"><a href="#cb84-169"></a>        X.append(context)</span>
<span id="cb84-170"><a href="#cb84-170"></a>        Y.append(ix)</span>
<span id="cb84-171"><a href="#cb84-171"></a>        <span class="bu">print</span>(<span class="st">''</span>.join(itos[i] <span class="cf">for</span> i <span class="kw">in</span> context), <span class="st">'-----&gt;'</span>, itos[ix] )</span>
<span id="cb84-172"><a href="#cb84-172"></a>        context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix] <span class="co"># rolling to the next one</span></span>
<span id="cb84-173"><a href="#cb84-173"></a></span>
<span id="cb84-174"><a href="#cb84-174"></a>X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb84-175"><a href="#cb84-175"></a>Y <span class="op">=</span> torch.tensor(Y)</span>
<span id="cb84-176"><a href="#cb84-176"></a><span class="in">```</span></span>
<span id="cb84-177"><a href="#cb84-177"></a></span>
<span id="cb84-180"><a href="#cb84-180"></a><span class="in">```{python}</span></span>
<span id="cb84-181"><a href="#cb84-181"></a>X.shape, X.dtype, Y.shape, Y.dtype</span>
<span id="cb84-182"><a href="#cb84-182"></a><span class="in">```</span></span>
<span id="cb84-183"><a href="#cb84-183"></a></span>
<span id="cb84-184"><a href="#cb84-184"></a><span class="fu">## implementing the embedding lookup table</span></span>
<span id="cb84-185"><a href="#cb84-185"></a></span>
<span id="cb84-186"><a href="#cb84-186"></a>In the paper they cram 17k word into as-low-as-possible 30 dimensions space, for our data, we just cram words into 2D space.</span>
<span id="cb84-187"><a href="#cb84-187"></a></span>
<span id="cb84-190"><a href="#cb84-190"></a><span class="in">```{python}</span></span>
<span id="cb84-191"><a href="#cb84-191"></a>C <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">2</span>))</span>
<span id="cb84-192"><a href="#cb84-192"></a><span class="in">```</span></span>
<span id="cb84-193"><a href="#cb84-193"></a></span>
<span id="cb84-194"><a href="#cb84-194"></a>We can access the element of <span class="in">`torch.tensor`</span> by:</span>
<span id="cb84-195"><a href="#cb84-195"></a></span>
<span id="cb84-198"><a href="#cb84-198"></a><span class="in">```{python}</span></span>
<span id="cb84-199"><a href="#cb84-199"></a>C[<span class="dv">5</span>] <span class="co"># can be integer, list [5, 6, 7], or torch.tensor([5,6,7])</span></span>
<span id="cb84-200"><a href="#cb84-200"></a><span class="co"># &gt; tensor([1.0825, 0.2010])</span></span>
<span id="cb84-201"><a href="#cb84-201"></a></span>
<span id="cb84-202"><a href="#cb84-202"></a><span class="co"># or</span></span>
<span id="cb84-203"><a href="#cb84-203"></a></span>
<span id="cb84-204"><a href="#cb84-204"></a>F.one_hot(torch.tensor(<span class="dv">5</span>), num_classes<span class="op">=</span><span class="dv">27</span>).<span class="bu">float</span>() <span class="op">@</span> C</span>
<span id="cb84-205"><a href="#cb84-205"></a><span class="co"># produce identical result, remember torch.tensor() infer long dtype int64, so we need to cast to float</span></span>
<span id="cb84-206"><a href="#cb84-206"></a><span class="in">```</span></span>
<span id="cb84-207"><a href="#cb84-207"></a></span>
<span id="cb84-208"><a href="#cb84-208"></a>...but in this lecture accessing by <span class="in">`C[5]`</span> would be sufficient. We can even access using a more than 1 dimension tensor:</span>
<span id="cb84-209"><a href="#cb84-209"></a></span>
<span id="cb84-212"><a href="#cb84-212"></a><span class="in">```{python}</span></span>
<span id="cb84-213"><a href="#cb84-213"></a><span class="bu">print</span>(C[X].shape)</span>
<span id="cb84-214"><a href="#cb84-214"></a><span class="bu">print</span>(X[<span class="dv">13</span>, <span class="dv">2</span>]) <span class="co"># integer 1 for 13rd index of 2nd dimension</span></span>
<span id="cb84-215"><a href="#cb84-215"></a><span class="bu">print</span>(C[X][<span class="dv">13</span>,<span class="dv">2</span>]) <span class="co"># will be the embedding of that element</span></span>
<span id="cb84-216"><a href="#cb84-216"></a><span class="bu">print</span>(C[<span class="dv">1</span>]) <span class="co"># so C[X][13,2] = C[1]</span></span>
<span id="cb84-217"><a href="#cb84-217"></a><span class="in">```</span></span>
<span id="cb84-218"><a href="#cb84-218"></a></span>
<span id="cb84-219"><a href="#cb84-219"></a>PyTorch is great for embedding words:</span>
<span id="cb84-220"><a href="#cb84-220"></a></span>
<span id="cb84-223"><a href="#cb84-223"></a><span class="in">```{python}</span></span>
<span id="cb84-224"><a href="#cb84-224"></a>emb <span class="op">=</span> C[X]</span>
<span id="cb84-225"><a href="#cb84-225"></a>emb.shape</span>
<span id="cb84-226"><a href="#cb84-226"></a><span class="in">```</span></span>
<span id="cb84-227"><a href="#cb84-227"></a></span>
<span id="cb84-228"><a href="#cb84-228"></a>We've compeleted the first layer with <span class="in">`context`</span> and lookup table!</span>
<span id="cb84-229"><a href="#cb84-229"></a></span>
<span id="cb84-230"><a href="#cb84-230"></a><span class="fu">## implementing the hidden layer + internals of `torch.Tensor`: `storage`, `views`</span></span>
<span id="cb84-231"><a href="#cb84-231"></a></span>
<span id="cb84-234"><a href="#cb84-234"></a><span class="in">```{python}</span></span>
<span id="cb84-235"><a href="#cb84-235"></a><span class="co"># input of tanh layer will be 6 (3 words in context x 2 dimensions)</span></span>
<span id="cb84-236"><a href="#cb84-236"></a><span class="co"># and the number or neurons is up to us - let's set it 100</span></span>
<span id="cb84-237"><a href="#cb84-237"></a>W1 <span class="op">=</span> torch.randn((<span class="dv">6</span>, <span class="dv">100</span>))</span>
<span id="cb84-238"><a href="#cb84-238"></a>b1 <span class="op">=</span> torch.randn(<span class="dv">100</span>)</span>
<span id="cb84-239"><a href="#cb84-239"></a><span class="in">```</span></span>
<span id="cb84-240"><a href="#cb84-240"></a></span>
<span id="cb84-241"><a href="#cb84-241"></a>Now we need to do something like <span class="in">`emb @ W1 + b1`</span>, but <span class="in">`emb.shape`</span> is <span class="in">`[32, 3, 2]`</span> and <span class="in">`W1.shape`</span> is <span class="in">`[6, 100]`</span>. We need to somehow concatnate/transform:</span>
<span id="cb84-242"><a href="#cb84-242"></a></span>
<span id="cb84-245"><a href="#cb84-245"></a><span class="in">```{python}</span></span>
<span id="cb84-246"><a href="#cb84-246"></a><span class="co"># emb[:, 0, :] is tensor for each input in the 3-words context, shape is [32, 2]</span></span>
<span id="cb84-247"><a href="#cb84-247"></a><span class="co"># cat 3 of them using the 2nd dimension (index 1) -&gt; so we set dim = 1</span></span>
<span id="cb84-248"><a href="#cb84-248"></a>torch.cat([emb[:, <span class="dv">0</span>, :], emb[:, <span class="dv">1</span>, :], emb[:, <span class="dv">2</span>, :]], dim<span class="op">=</span><span class="dv">1</span>).shape</span>
<span id="cb84-249"><a href="#cb84-249"></a><span class="in">```</span></span>
<span id="cb84-250"><a href="#cb84-250"></a></span>
<span id="cb84-251"><a href="#cb84-251"></a>However this code does not change dynamically when we change the block size. We will be using <span class="in">`torch.unbind()`</span></span>
<span id="cb84-252"><a href="#cb84-252"></a></span>
<span id="cb84-255"><a href="#cb84-255"></a><span class="in">```{python}</span></span>
<span id="cb84-256"><a href="#cb84-256"></a><span class="co"># this is good!</span></span>
<span id="cb84-257"><a href="#cb84-257"></a>torch.cat(torch.unbind(emb, <span class="dv">1</span>), <span class="dv">1</span>).shape</span>
<span id="cb84-258"><a href="#cb84-258"></a><span class="co"># new memory for storage is created, so it is not efficient</span></span>
<span id="cb84-259"><a href="#cb84-259"></a><span class="in">```</span></span>
<span id="cb84-260"><a href="#cb84-260"></a></span>
<span id="cb84-261"><a href="#cb84-261"></a>This works, but we have a better and more efficient way to do this. Since:</span>
<span id="cb84-262"><a href="#cb84-262"></a></span>
<span id="cb84-263"><a href="#cb84-263"></a><span class="ss">-   </span>every <span class="in">`torch.Tensor`</span> have <span class="in">`.storage()`</span> which is one-dimensional vector tensor;</span>
<span id="cb84-264"><a href="#cb84-264"></a><span class="ss">-   </span>when we call <span class="in">`.view()`</span>, we instruct how this vector tensor is interpreted;</span>
<span id="cb84-265"><a href="#cb84-265"></a><span class="ss">-   </span>no memory is being changed/copied/moved/or created. the storage is identical.</span>
<span id="cb84-266"><a href="#cb84-266"></a></span>
<span id="cb84-267"><a href="#cb84-267"></a>Readmore: <span class="ot">&lt;http://blog.ezyang.com/2019/05/pytorch-internals/&gt;</span></span>
<span id="cb84-268"><a href="#cb84-268"></a></span>
<span id="cb84-269"><a href="#cb84-269"></a>So this hidden layer can be declared:</span>
<span id="cb84-270"><a href="#cb84-270"></a></span>
<span id="cb84-273"><a href="#cb84-273"></a><span class="in">```{python}</span></span>
<span id="cb84-274"><a href="#cb84-274"></a><span class="co"># instead or 32 we can write emb.shape[1], or -1 (whatever fitted)</span></span>
<span id="cb84-275"><a href="#cb84-275"></a>h <span class="op">=</span> emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1</span>
<span id="cb84-276"><a href="#cb84-276"></a>h.shape</span>
<span id="cb84-277"><a href="#cb84-277"></a><span class="in">```</span></span>
<span id="cb84-278"><a href="#cb84-278"></a></span>
<span id="cb84-279"><a href="#cb84-279"></a>Notice that in the final operation, <span class="in">`b1`</span> will be broadcasted.</span>
<span id="cb84-280"><a href="#cb84-280"></a></span>
<span id="cb84-281"><a href="#cb84-281"></a><span class="fu">## implementing the output layer</span></span>
<span id="cb84-282"><a href="#cb84-282"></a></span>
<span id="cb84-285"><a href="#cb84-285"></a><span class="in">```{python}</span></span>
<span id="cb84-286"><a href="#cb84-286"></a>W2 <span class="op">=</span> torch.randn((<span class="dv">100</span>, <span class="dv">27</span>))</span>
<span id="cb84-287"><a href="#cb84-287"></a>b2 <span class="op">=</span> torch.randn(<span class="dv">27</span>)</span>
<span id="cb84-288"><a href="#cb84-288"></a><span class="in">```</span></span>
<span id="cb84-289"><a href="#cb84-289"></a></span>
<span id="cb84-290"><a href="#cb84-290"></a>In Deep Learning, people use <span class="in">`logits`</span> for what raw output that range from negative inf to positive inf.</span>
<span id="cb84-291"><a href="#cb84-291"></a></span>
<span id="cb84-294"><a href="#cb84-294"></a><span class="in">```{python}</span></span>
<span id="cb84-295"><a href="#cb84-295"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb84-296"><a href="#cb84-296"></a><span class="in">```</span></span>
<span id="cb84-297"><a href="#cb84-297"></a></span>
<span id="cb84-300"><a href="#cb84-300"></a><span class="in">```{python}</span></span>
<span id="cb84-301"><a href="#cb84-301"></a>logits.shape</span>
<span id="cb84-302"><a href="#cb84-302"></a><span class="in">```</span></span>
<span id="cb84-303"><a href="#cb84-303"></a></span>
<span id="cb84-304"><a href="#cb84-304"></a>Now we need to exponentiate it and get the probability.</span>
<span id="cb84-305"><a href="#cb84-305"></a></span>
<span id="cb84-308"><a href="#cb84-308"></a><span class="in">```{python}</span></span>
<span id="cb84-309"><a href="#cb84-309"></a>counts <span class="op">=</span> logits.exp()</span>
<span id="cb84-310"><a href="#cb84-310"></a><span class="in">```</span></span>
<span id="cb84-311"><a href="#cb84-311"></a></span>
<span id="cb84-314"><a href="#cb84-314"></a><span class="in">```{python}</span></span>
<span id="cb84-315"><a href="#cb84-315"></a>probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb84-316"><a href="#cb84-316"></a><span class="in">```</span></span>
<span id="cb84-317"><a href="#cb84-317"></a></span>
<span id="cb84-320"><a href="#cb84-320"></a><span class="in">```{python}</span></span>
<span id="cb84-321"><a href="#cb84-321"></a>probs.shape</span>
<span id="cb84-322"><a href="#cb84-322"></a><span class="in">```</span></span>
<span id="cb84-323"><a href="#cb84-323"></a></span>
<span id="cb84-324"><a href="#cb84-324"></a>Every row of <span class="in">`probs`</span> has sum of 1.</span>
<span id="cb84-325"><a href="#cb84-325"></a></span>
<span id="cb84-328"><a href="#cb84-328"></a><span class="in">```{python}</span></span>
<span id="cb84-329"><a href="#cb84-329"></a>probs[<span class="dv">0</span>].<span class="bu">sum</span>()</span>
<span id="cb84-330"><a href="#cb84-330"></a><span class="in">```</span></span>
<span id="cb84-331"><a href="#cb84-331"></a></span>
<span id="cb84-332"><a href="#cb84-332"></a>And this is the <span class="in">`probs`</span> of each ground true <span class="in">`Y`</span> in current output of the neural nets:</span>
<span id="cb84-333"><a href="#cb84-333"></a></span>
<span id="cb84-336"><a href="#cb84-336"></a><span class="in">```{python}</span></span>
<span id="cb84-337"><a href="#cb84-337"></a>probs[torch.arange(<span class="dv">32</span>), Y]</span>
<span id="cb84-338"><a href="#cb84-338"></a><span class="in">```</span></span>
<span id="cb84-339"><a href="#cb84-339"></a></span>
<span id="cb84-340"><a href="#cb84-340"></a>Result is not good as we've not trained the network yet!</span>
<span id="cb84-341"><a href="#cb84-341"></a></span>
<span id="cb84-342"><a href="#cb84-342"></a><span class="fu">## implementing the negative log likelihood loss</span></span>
<span id="cb84-343"><a href="#cb84-343"></a></span>
<span id="cb84-344"><a href="#cb84-344"></a>We define the negative log likelihood as:</span>
<span id="cb84-345"><a href="#cb84-345"></a></span>
<span id="cb84-348"><a href="#cb84-348"></a><span class="in">```{python}</span></span>
<span id="cb84-349"><a href="#cb84-349"></a>loss <span class="op">=</span> <span class="op">-</span> probs[torch.arange(<span class="dv">32</span>), Y].log().mean()</span>
<span id="cb84-350"><a href="#cb84-350"></a>loss</span>
<span id="cb84-351"><a href="#cb84-351"></a><span class="in">```</span></span>
<span id="cb84-352"><a href="#cb84-352"></a></span>
<span id="cb84-353"><a href="#cb84-353"></a><span class="fu">## summary of the full network</span></span>
<span id="cb84-354"><a href="#cb84-354"></a></span>
<span id="cb84-355"><a href="#cb84-355"></a>Dataset:</span>
<span id="cb84-356"><a href="#cb84-356"></a></span>
<span id="cb84-359"><a href="#cb84-359"></a><span class="in">```{python}</span></span>
<span id="cb84-360"><a href="#cb84-360"></a>X.shape, Y.shape</span>
<span id="cb84-361"><a href="#cb84-361"></a><span class="in">```</span></span>
<span id="cb84-362"><a href="#cb84-362"></a></span>
<span id="cb84-363"><a href="#cb84-363"></a>Neural network layers:</span>
<span id="cb84-364"><a href="#cb84-364"></a></span>
<span id="cb84-367"><a href="#cb84-367"></a><span class="in">```{python}</span></span>
<span id="cb84-368"><a href="#cb84-368"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb84-369"><a href="#cb84-369"></a>C <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">2</span>), generator<span class="op">=</span>g)</span>
<span id="cb84-370"><a href="#cb84-370"></a>W1 <span class="op">=</span> torch.randn((<span class="dv">6</span>, <span class="dv">100</span>), generator<span class="op">=</span>g)</span>
<span id="cb84-371"><a href="#cb84-371"></a>b1 <span class="op">=</span> torch.randn(<span class="dv">100</span>, generator<span class="op">=</span>g)</span>
<span id="cb84-372"><a href="#cb84-372"></a>W2 <span class="op">=</span> torch.randn((<span class="dv">100</span>, <span class="dv">27</span>), generator<span class="op">=</span>g)</span>
<span id="cb84-373"><a href="#cb84-373"></a>b2 <span class="op">=</span> torch.randn(<span class="dv">27</span>, generator<span class="op">=</span>g)</span>
<span id="cb84-374"><a href="#cb84-374"></a></span>
<span id="cb84-375"><a href="#cb84-375"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb84-376"><a href="#cb84-376"></a><span class="in">```</span></span>
<span id="cb84-377"><a href="#cb84-377"></a></span>
<span id="cb84-378"><a href="#cb84-378"></a>Size of the network:</span>
<span id="cb84-379"><a href="#cb84-379"></a></span>
<span id="cb84-382"><a href="#cb84-382"></a><span class="in">```{python}</span></span>
<span id="cb84-383"><a href="#cb84-383"></a><span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters)</span>
<span id="cb84-384"><a href="#cb84-384"></a><span class="in">```</span></span>
<span id="cb84-385"><a href="#cb84-385"></a></span>
<span id="cb84-386"><a href="#cb84-386"></a>Constructing forward pass:</span>
<span id="cb84-387"><a href="#cb84-387"></a></span>
<span id="cb84-390"><a href="#cb84-390"></a><span class="in">```{python}</span></span>
<span id="cb84-391"><a href="#cb84-391"></a>emb <span class="op">=</span> C[X] <span class="co"># (32, 3, 2)</span></span>
<span id="cb84-392"><a href="#cb84-392"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb84-393"><a href="#cb84-393"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb84-394"><a href="#cb84-394"></a>counts <span class="op">=</span> logits.exp()</span>
<span id="cb84-395"><a href="#cb84-395"></a>probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb84-396"><a href="#cb84-396"></a>loss <span class="op">=</span> <span class="op">-</span> probs[torch.arange(<span class="dv">32</span>), Y].log().mean()</span>
<span id="cb84-397"><a href="#cb84-397"></a>loss</span>
<span id="cb84-398"><a href="#cb84-398"></a><span class="in">```</span></span>
<span id="cb84-399"><a href="#cb84-399"></a></span>
<span id="cb84-400"><a href="#cb84-400"></a><span class="fu"># PART 2: intro to many basics of machine learning</span></span>
<span id="cb84-401"><a href="#cb84-401"></a></span>
<span id="cb84-402"><a href="#cb84-402"></a><span class="fu">## introducing `F.cross_entropy` and why</span></span>
<span id="cb84-403"><a href="#cb84-403"></a></span>
<span id="cb84-404"><a href="#cb84-404"></a>We re-define loss:</span>
<span id="cb84-405"><a href="#cb84-405"></a></span>
<span id="cb84-408"><a href="#cb84-408"></a><span class="in">```{python}</span></span>
<span id="cb84-409"><a href="#cb84-409"></a>loss <span class="op">=</span> F.cross_entropy(logits, Y)</span>
<span id="cb84-410"><a href="#cb84-410"></a>loss</span>
<span id="cb84-411"><a href="#cb84-411"></a><span class="in">```</span></span>
<span id="cb84-412"><a href="#cb84-412"></a></span>
<span id="cb84-413"><a href="#cb84-413"></a>Why?</span>
<span id="cb84-414"><a href="#cb84-414"></a></span>
<span id="cb84-415"><a href="#cb84-415"></a><span class="ss">-   </span>Pytorch will create more intermediate tensor for every assignment: <span class="in">`counts`</span>, <span class="in">`probs`</span> -<span class="sc">\&gt;</span> more memory;</span>
<span id="cb84-416"><a href="#cb84-416"></a><span class="ss">-   </span>Backward pass will be more optimized, because the expressions are much analytically and mathematically interpreted;</span>
<span id="cb84-417"><a href="#cb84-417"></a><span class="ss">-   </span>Cross entropy can be significantly &amp; numerically well behaved (for eg when we exponentiate a large positive number we got inf, PyTorch cross entropy will calculate the max of set and subtract it - which will not impact the exp result)</span>
<span id="cb84-418"><a href="#cb84-418"></a></span>
<span id="cb84-419"><a href="#cb84-419"></a><span class="fu">## implementing the training loop, overfitting one batch</span></span>
<span id="cb84-420"><a href="#cb84-420"></a></span>
<span id="cb84-421"><a href="#cb84-421"></a>So the forward pass, backward pass, and update loop will be implemented as below:</span>
<span id="cb84-422"><a href="#cb84-422"></a></span>
<span id="cb84-425"><a href="#cb84-425"></a><span class="in">```{python}</span></span>
<span id="cb84-426"><a href="#cb84-426"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb84-427"><a href="#cb84-427"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb84-428"><a href="#cb84-428"></a><span class="in">```</span></span>
<span id="cb84-429"><a href="#cb84-429"></a></span>
<span id="cb84-432"><a href="#cb84-432"></a><span class="in">```{python}</span></span>
<span id="cb84-433"><a href="#cb84-433"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb84-434"><a href="#cb84-434"></a>    <span class="co"># forward pass:</span></span>
<span id="cb84-435"><a href="#cb84-435"></a>    emb <span class="op">=</span> C[X] <span class="co"># (32, 3, 2)</span></span>
<span id="cb84-436"><a href="#cb84-436"></a>    h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb84-437"><a href="#cb84-437"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb84-438"><a href="#cb84-438"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Y)</span>
<span id="cb84-439"><a href="#cb84-439"></a>    <span class="bu">print</span>(loss.item())</span>
<span id="cb84-440"><a href="#cb84-440"></a>    <span class="co"># backward pass:</span></span>
<span id="cb84-441"><a href="#cb84-441"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb84-442"><a href="#cb84-442"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb84-443"><a href="#cb84-443"></a>    loss.backward()</span>
<span id="cb84-444"><a href="#cb84-444"></a>    <span class="co"># update</span></span>
<span id="cb84-445"><a href="#cb84-445"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb84-446"><a href="#cb84-446"></a>        p.data <span class="op">+=</span> <span class="op">-</span><span class="fl">0.1</span> <span class="op">*</span> p.grad</span>
<span id="cb84-447"><a href="#cb84-447"></a></span>
<span id="cb84-448"><a href="#cb84-448"></a><span class="bu">print</span>(loss.item())</span>
<span id="cb84-449"><a href="#cb84-449"></a><span class="in">```</span></span>
<span id="cb84-450"><a href="#cb84-450"></a></span>
<span id="cb84-451"><a href="#cb84-451"></a>We are fitting 32 examples to a neural nets of 3481 params, so it's super easy to be overfitting. We got a low final loss, but it would never be 0, because the output can varry for the same input, for eg, <span class="in">`...`</span>.</span>
<span id="cb84-452"><a href="#cb84-452"></a></span>
<span id="cb84-455"><a href="#cb84-455"></a><span class="in">```{python}</span></span>
<span id="cb84-456"><a href="#cb84-456"></a>logits.<span class="bu">max</span>(<span class="dv">1</span>)</span>
<span id="cb84-457"><a href="#cb84-457"></a><span class="in">```</span></span>
<span id="cb84-458"><a href="#cb84-458"></a></span>
<span id="cb84-459"><a href="#cb84-459"></a><span class="fu">## training on the full dataset, minibatches</span></span>
<span id="cb84-460"><a href="#cb84-460"></a></span>
<span id="cb84-461"><a href="#cb84-461"></a>We can deploy our code to all the dataset, un-fold the below code block to see full code.</span>
<span id="cb84-462"><a href="#cb84-462"></a></span>
<span id="cb84-465"><a href="#cb84-465"></a><span class="in">```{python}</span></span>
<span id="cb84-466"><a href="#cb84-466"></a><span class="co">#| code-fold: true</span></span>
<span id="cb84-467"><a href="#cb84-467"></a>block_size <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb84-468"><a href="#cb84-468"></a>X, Y <span class="op">=</span> [], []</span>
<span id="cb84-469"><a href="#cb84-469"></a></span>
<span id="cb84-470"><a href="#cb84-470"></a><span class="co"># Dataset</span></span>
<span id="cb84-471"><a href="#cb84-471"></a><span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb84-472"><a href="#cb84-472"></a>    <span class="co"># print(w)</span></span>
<span id="cb84-473"><a href="#cb84-473"></a>    context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size</span>
<span id="cb84-474"><a href="#cb84-474"></a>    <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb84-475"><a href="#cb84-475"></a>        ix <span class="op">=</span> stoi[ch]</span>
<span id="cb84-476"><a href="#cb84-476"></a>        X.append(context)</span>
<span id="cb84-477"><a href="#cb84-477"></a>        Y.append(ix)</span>
<span id="cb84-478"><a href="#cb84-478"></a>        <span class="co"># print(''.join(itos[i] for i in context), '-----&gt;', itos[ix] )</span></span>
<span id="cb84-479"><a href="#cb84-479"></a>        context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix] <span class="co"># rolling to the next one</span></span>
<span id="cb84-480"><a href="#cb84-480"></a></span>
<span id="cb84-481"><a href="#cb84-481"></a><span class="co"># Input and ground true</span></span>
<span id="cb84-482"><a href="#cb84-482"></a>X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb84-483"><a href="#cb84-483"></a>Y <span class="op">=</span> torch.tensor(Y)</span>
<span id="cb84-484"><a href="#cb84-484"></a><span class="bu">print</span>(<span class="st">"Data size"</span>, X.shape, Y.shape)</span>
<span id="cb84-485"><a href="#cb84-485"></a></span>
<span id="cb84-486"><a href="#cb84-486"></a><span class="co"># Lookup table</span></span>
<span id="cb84-487"><a href="#cb84-487"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb84-488"><a href="#cb84-488"></a>C <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">2</span>), generator<span class="op">=</span>g)</span>
<span id="cb84-489"><a href="#cb84-489"></a>emb <span class="op">=</span> C[X] <span class="co"># (32, 3, 2)</span></span>
<span id="cb84-490"><a href="#cb84-490"></a></span>
<span id="cb84-491"><a href="#cb84-491"></a><span class="co"># Layer 1 - tanh</span></span>
<span id="cb84-492"><a href="#cb84-492"></a>W1 <span class="op">=</span> torch.randn((<span class="dv">6</span>, <span class="dv">100</span>), generator<span class="op">=</span>g)</span>
<span id="cb84-493"><a href="#cb84-493"></a>b1 <span class="op">=</span> torch.randn(<span class="dv">100</span>, generator<span class="op">=</span>g)</span>
<span id="cb84-494"><a href="#cb84-494"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb84-495"><a href="#cb84-495"></a></span>
<span id="cb84-496"><a href="#cb84-496"></a><span class="co"># Layer 2 - softmax</span></span>
<span id="cb84-497"><a href="#cb84-497"></a>W2 <span class="op">=</span> torch.randn((<span class="dv">100</span>, <span class="dv">27</span>), generator<span class="op">=</span>g)</span>
<span id="cb84-498"><a href="#cb84-498"></a>b2 <span class="op">=</span> torch.randn(<span class="dv">27</span>, generator<span class="op">=</span>g)</span>
<span id="cb84-499"><a href="#cb84-499"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb84-500"><a href="#cb84-500"></a>loss <span class="op">=</span> F.cross_entropy(logits, Y)</span>
<span id="cb84-501"><a href="#cb84-501"></a></span>
<span id="cb84-502"><a href="#cb84-502"></a><span class="co"># All params</span></span>
<span id="cb84-503"><a href="#cb84-503"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb84-504"><a href="#cb84-504"></a><span class="bu">print</span>(<span class="st">"No of params: "</span>, <span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span>
<span id="cb84-505"><a href="#cb84-505"></a></span>
<span id="cb84-506"><a href="#cb84-506"></a><span class="co"># Pre-training</span></span>
<span id="cb84-507"><a href="#cb84-507"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb84-508"><a href="#cb84-508"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb84-509"><a href="#cb84-509"></a><span class="in">```</span></span>
<span id="cb84-510"><a href="#cb84-510"></a></span>
<span id="cb84-511"><a href="#cb84-511"></a>We notice that it takes a bit long time for each training in the loop. In practice, we will perform the forward/backward passes and update parameters for a small batch of the dataset. The minibatch construction is added/modified for lines of code with <span class="in">`#👈`</span>.</span>
<span id="cb84-512"><a href="#cb84-512"></a></span>
<span id="cb84-513"><a href="#cb84-513"></a>Read more: <span class="ot">&lt;https://nttuan8.com/bai-10-cac-ky-thuat-co-ban-trong-deep-learning/&gt;</span></span>
<span id="cb84-514"><a href="#cb84-514"></a></span>
<span id="cb84-517"><a href="#cb84-517"></a><span class="in">```{python}</span></span>
<span id="cb84-518"><a href="#cb84-518"></a><span class="co"># Training</span></span>
<span id="cb84-519"><a href="#cb84-519"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10000</span>):</span>
<span id="cb84-520"><a href="#cb84-520"></a>    <span class="co"># minibatch construct                                           #👈</span></span>
<span id="cb84-521"><a href="#cb84-521"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, X.shape[<span class="dv">0</span>], (<span class="dv">32</span>,))                        <span class="co">#👈</span></span>
<span id="cb84-522"><a href="#cb84-522"></a></span>
<span id="cb84-523"><a href="#cb84-523"></a>    <span class="co"># forward pass:</span></span>
<span id="cb84-524"><a href="#cb84-524"></a>    emb <span class="op">=</span> C[X[ix]] <span class="co"># (32, 3, 2)                                     #👈</span></span>
<span id="cb84-525"><a href="#cb84-525"></a>    h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb84-526"><a href="#cb84-526"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb84-527"><a href="#cb84-527"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Y[ix])                           <span class="co">#👈</span></span>
<span id="cb84-528"><a href="#cb84-528"></a>    <span class="cf">if</span> _ <span class="op">&gt;=</span> <span class="dv">9990</span>: <span class="bu">print</span>(<span class="ss">f"___after running </span><span class="sc">{</span>_<span class="sc">}</span><span class="ss"> time: "</span>, loss.item())</span>
<span id="cb84-529"><a href="#cb84-529"></a>    <span class="co"># backward pass:</span></span>
<span id="cb84-530"><a href="#cb84-530"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb84-531"><a href="#cb84-531"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb84-532"><a href="#cb84-532"></a>    loss.backward()</span>
<span id="cb84-533"><a href="#cb84-533"></a>    <span class="co"># update</span></span>
<span id="cb84-534"><a href="#cb84-534"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb84-535"><a href="#cb84-535"></a>        p.data <span class="op">+=</span> <span class="op">-</span><span class="fl">0.1</span> <span class="op">*</span> p.grad</span>
<span id="cb84-536"><a href="#cb84-536"></a></span>
<span id="cb84-537"><a href="#cb84-537"></a><span class="bu">print</span>(<span class="st">"final minibatch loss: "</span>, loss.item())</span>
<span id="cb84-538"><a href="#cb84-538"></a><span class="in">```</span></span>
<span id="cb84-539"><a href="#cb84-539"></a></span>
<span id="cb84-540"><a href="#cb84-540"></a>The <span class="in">`loss`</span> decrease much much better, although the direction of gradient might be not correct direction. But it is good enough for an approximation. Notice the loss for a minibatch is not the loss of whole dataset.</span>
<span id="cb84-541"><a href="#cb84-541"></a></span>
<span id="cb84-544"><a href="#cb84-544"></a><span class="in">```{python}</span></span>
<span id="cb84-545"><a href="#cb84-545"></a>emb <span class="op">=</span> C[X] <span class="co"># (32, 3, 2)</span></span>
<span id="cb84-546"><a href="#cb84-546"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb84-547"><a href="#cb84-547"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb84-548"><a href="#cb84-548"></a>loss <span class="op">=</span> F.cross_entropy(logits, Y)</span>
<span id="cb84-549"><a href="#cb84-549"></a>loss.item()</span>
<span id="cb84-550"><a href="#cb84-550"></a><span class="in">```</span></span>
<span id="cb84-551"><a href="#cb84-551"></a></span>
<span id="cb84-552"><a href="#cb84-552"></a>We archived 2.39 loss for final minibatch and 2.5 on overall network.</span>
<span id="cb84-553"><a href="#cb84-553"></a></span>
<span id="cb84-554"><a href="#cb84-554"></a><span class="fu">## finding a good initial learning rate</span></span>
<span id="cb84-555"><a href="#cb84-555"></a></span>
<span id="cb84-556"><a href="#cb84-556"></a>Now we're continuing the optimization, let's focus on how much we update the data from the gradient <span class="in">`p.data += -0.1 * p.grad`</span>. We do not know if we step too little or too much.</span>
<span id="cb84-557"><a href="#cb84-557"></a></span>
<span id="cb84-558"><a href="#cb84-558"></a>We can create 1000 learning rates to use along with the training loop and see which one offers more stable convergence.</span>
<span id="cb84-559"><a href="#cb84-559"></a></span>
<span id="cb84-562"><a href="#cb84-562"></a><span class="in">```{python}</span></span>
<span id="cb84-563"><a href="#cb84-563"></a>lre <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">1000</span>)</span>
<span id="cb84-564"><a href="#cb84-564"></a>lrs <span class="op">=</span> <span class="dv">10</span><span class="op">**</span>lre</span>
<span id="cb84-565"><a href="#cb84-565"></a><span class="in">```</span></span>
<span id="cb84-566"><a href="#cb84-566"></a></span>
<span id="cb84-567"><a href="#cb84-567"></a>Reset the code:</span>
<span id="cb84-568"><a href="#cb84-568"></a></span>
<span id="cb84-571"><a href="#cb84-571"></a><span class="in">```{python}</span></span>
<span id="cb84-572"><a href="#cb84-572"></a><span class="co">#| code-fold: true</span></span>
<span id="cb84-573"><a href="#cb84-573"></a>block_size <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb84-574"><a href="#cb84-574"></a>X, Y <span class="op">=</span> [], []</span>
<span id="cb84-575"><a href="#cb84-575"></a></span>
<span id="cb84-576"><a href="#cb84-576"></a><span class="co"># Dataset</span></span>
<span id="cb84-577"><a href="#cb84-577"></a><span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb84-578"><a href="#cb84-578"></a>    <span class="co"># print(w)</span></span>
<span id="cb84-579"><a href="#cb84-579"></a>    context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size</span>
<span id="cb84-580"><a href="#cb84-580"></a>    <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb84-581"><a href="#cb84-581"></a>        ix <span class="op">=</span> stoi[ch]</span>
<span id="cb84-582"><a href="#cb84-582"></a>        X.append(context)</span>
<span id="cb84-583"><a href="#cb84-583"></a>        Y.append(ix)</span>
<span id="cb84-584"><a href="#cb84-584"></a>        <span class="co"># print(''.join(itos[i] for i in context), '-----&gt;', itos[ix] )</span></span>
<span id="cb84-585"><a href="#cb84-585"></a>        context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix] <span class="co"># rolling to the next one</span></span>
<span id="cb84-586"><a href="#cb84-586"></a></span>
<span id="cb84-587"><a href="#cb84-587"></a><span class="co"># Input and ground true</span></span>
<span id="cb84-588"><a href="#cb84-588"></a>X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb84-589"><a href="#cb84-589"></a>Y <span class="op">=</span> torch.tensor(Y)</span>
<span id="cb84-590"><a href="#cb84-590"></a><span class="bu">print</span>(<span class="st">"Data size"</span>, X.shape, Y.shape)</span>
<span id="cb84-591"><a href="#cb84-591"></a></span>
<span id="cb84-592"><a href="#cb84-592"></a><span class="co"># Lookup table</span></span>
<span id="cb84-593"><a href="#cb84-593"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb84-594"><a href="#cb84-594"></a>C <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">2</span>), generator<span class="op">=</span>g)</span>
<span id="cb84-595"><a href="#cb84-595"></a>emb <span class="op">=</span> C[X] <span class="co"># (32, 3, 2)</span></span>
<span id="cb84-596"><a href="#cb84-596"></a></span>
<span id="cb84-597"><a href="#cb84-597"></a><span class="co"># Layer 1 - tanh</span></span>
<span id="cb84-598"><a href="#cb84-598"></a>W1 <span class="op">=</span> torch.randn((<span class="dv">6</span>, <span class="dv">100</span>), generator<span class="op">=</span>g)</span>
<span id="cb84-599"><a href="#cb84-599"></a>b1 <span class="op">=</span> torch.randn(<span class="dv">100</span>, generator<span class="op">=</span>g)</span>
<span id="cb84-600"><a href="#cb84-600"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb84-601"><a href="#cb84-601"></a></span>
<span id="cb84-602"><a href="#cb84-602"></a><span class="co"># Layer 2 - softmax</span></span>
<span id="cb84-603"><a href="#cb84-603"></a>W2 <span class="op">=</span> torch.randn((<span class="dv">100</span>, <span class="dv">27</span>), generator<span class="op">=</span>g)</span>
<span id="cb84-604"><a href="#cb84-604"></a>b2 <span class="op">=</span> torch.randn(<span class="dv">27</span>, generator<span class="op">=</span>g)</span>
<span id="cb84-605"><a href="#cb84-605"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb84-606"><a href="#cb84-606"></a>loss <span class="op">=</span> F.cross_entropy(logits, Y)</span>
<span id="cb84-607"><a href="#cb84-607"></a></span>
<span id="cb84-608"><a href="#cb84-608"></a><span class="co"># All params</span></span>
<span id="cb84-609"><a href="#cb84-609"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb84-610"><a href="#cb84-610"></a><span class="bu">print</span>(<span class="st">"No of params: "</span>, <span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span>
<span id="cb84-611"><a href="#cb84-611"></a></span>
<span id="cb84-612"><a href="#cb84-612"></a><span class="co"># Pre-training</span></span>
<span id="cb84-613"><a href="#cb84-613"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb84-614"><a href="#cb84-614"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb84-615"><a href="#cb84-615"></a><span class="in">```</span></span>
<span id="cb84-616"><a href="#cb84-616"></a></span>
<span id="cb84-617"><a href="#cb84-617"></a>Training and tracking stats:</span>
<span id="cb84-618"><a href="#cb84-618"></a></span>
<span id="cb84-621"><a href="#cb84-621"></a><span class="in">```{python}</span></span>
<span id="cb84-622"><a href="#cb84-622"></a>lri <span class="op">=</span> []</span>
<span id="cb84-623"><a href="#cb84-623"></a>lossi <span class="op">=</span> []</span>
<span id="cb84-624"><a href="#cb84-624"></a></span>
<span id="cb84-625"><a href="#cb84-625"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb84-626"><a href="#cb84-626"></a>    <span class="co"># minibatch construct</span></span>
<span id="cb84-627"><a href="#cb84-627"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, X.shape[<span class="dv">0</span>], (<span class="dv">32</span>,))</span>
<span id="cb84-628"><a href="#cb84-628"></a>    <span class="co"># forward pass:</span></span>
<span id="cb84-629"><a href="#cb84-629"></a>    emb <span class="op">=</span> C[X[ix]] <span class="co"># (32, 3, 2)</span></span>
<span id="cb84-630"><a href="#cb84-630"></a>    h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb84-631"><a href="#cb84-631"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb84-632"><a href="#cb84-632"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Y[ix])</span>
<span id="cb84-633"><a href="#cb84-633"></a>    <span class="co"># backward pass:</span></span>
<span id="cb84-634"><a href="#cb84-634"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb84-635"><a href="#cb84-635"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb84-636"><a href="#cb84-636"></a>    loss.backward()</span>
<span id="cb84-637"><a href="#cb84-637"></a>    <span class="co"># update</span></span>
<span id="cb84-638"><a href="#cb84-638"></a>    lr <span class="op">=</span> lrs[i]</span>
<span id="cb84-639"><a href="#cb84-639"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb84-640"><a href="#cb84-640"></a>        p.data <span class="op">+=</span> <span class="op">-</span> lr <span class="op">*</span> p.grad</span>
<span id="cb84-641"><a href="#cb84-641"></a></span>
<span id="cb84-642"><a href="#cb84-642"></a>    <span class="co"># track stats</span></span>
<span id="cb84-643"><a href="#cb84-643"></a>    lri.append(lre[i])</span>
<span id="cb84-644"><a href="#cb84-644"></a>    lossi.append(loss.item())</span>
<span id="cb84-645"><a href="#cb84-645"></a></span>
<span id="cb84-646"><a href="#cb84-646"></a>loss.item()</span>
<span id="cb84-647"><a href="#cb84-647"></a><span class="in">```</span></span>
<span id="cb84-648"><a href="#cb84-648"></a></span>
<span id="cb84-649"><a href="#cb84-649"></a>Plotting, we see a good exponential element of learning rate turn out to be around <span class="in">`-1`</span>.</span>
<span id="cb84-650"><a href="#cb84-650"></a></span>
<span id="cb84-651"><a href="#cb84-651"></a>$10^{-1}$ is <span class="in">`0.1`</span> so our initial guess seems good.</span>
<span id="cb84-652"><a href="#cb84-652"></a></span>
<span id="cb84-655"><a href="#cb84-655"></a><span class="in">```{python}</span></span>
<span id="cb84-656"><a href="#cb84-656"></a>plt.plot(lri, lossi)</span>
<span id="cb84-657"><a href="#cb84-657"></a><span class="in">```</span></span>
<span id="cb84-658"><a href="#cb84-658"></a></span>
<span id="cb84-659"><a href="#cb84-659"></a><span class="fu">## splitting up the dataset into train/val/test splits and why</span></span>
<span id="cb84-660"><a href="#cb84-660"></a></span>
<span id="cb84-661"><a href="#cb84-661"></a>Now we can keep lengthening the training loop to continue decreasing loss. We can try some techniques like change the learning rate to <span class="in">`0.001`</span> after 20k, 30k loops of training with <span class="in">`0.1`</span>.</span>
<span id="cb84-662"><a href="#cb84-662"></a></span>
<span id="cb84-663"><a href="#cb84-663"></a>But it will come to be overfitting when we try to keep training or increase the size of network to achieve a lower loss. The model just memorizing our training set verbatim, so if we try to sample from the model it just gives us the same thing in the dataset. Or if we calculate the loss on another dataset, it might be very high.</span>
<span id="cb84-664"><a href="#cb84-664"></a></span>
<span id="cb84-665"><a href="#cb84-665"></a>So another industry standard is we will split the data set into 3 pieces: (1) training set; (2) dev/validation set; and (3) test set, they can be 80% - 10% - 10% roughly and respectively.</span>
<span id="cb84-666"><a href="#cb84-666"></a></span>
<span id="cb84-667"><a href="#cb84-667"></a><span class="ss">1.  </span>Training split: train the parameters;</span>
<span id="cb84-668"><a href="#cb84-668"></a><span class="ss">2.  </span>Dev/validation split: train the hyperparamerters (size of hidden layer, size of embedding, streng of regularization, etc);</span>
<span id="cb84-669"><a href="#cb84-669"></a><span class="ss">3.  </span>Test split: evaluate the performance of the model at the end, we only work on this a very very few times, otherwise we learn from it and repeat overfitting.</span>
<span id="cb84-670"><a href="#cb84-670"></a></span>
<span id="cb84-671"><a href="#cb84-671"></a>We are going to implement this train/dev/test splits:</span>
<span id="cb84-672"><a href="#cb84-672"></a></span>
<span id="cb84-675"><a href="#cb84-675"></a><span class="in">```{python}</span></span>
<span id="cb84-676"><a href="#cb84-676"></a><span class="co"># build the dataset</span></span>
<span id="cb84-677"><a href="#cb84-677"></a><span class="kw">def</span> buid_dataset(words):</span>
<span id="cb84-678"><a href="#cb84-678"></a>    block_size <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb84-679"><a href="#cb84-679"></a>    X, Y <span class="op">=</span> [], []</span>
<span id="cb84-680"><a href="#cb84-680"></a></span>
<span id="cb84-681"><a href="#cb84-681"></a>    <span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb84-682"><a href="#cb84-682"></a>        context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size</span>
<span id="cb84-683"><a href="#cb84-683"></a>        <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb84-684"><a href="#cb84-684"></a>            ix <span class="op">=</span> stoi[ch]</span>
<span id="cb84-685"><a href="#cb84-685"></a>            X.append(context)</span>
<span id="cb84-686"><a href="#cb84-686"></a>            Y.append(ix)</span>
<span id="cb84-687"><a href="#cb84-687"></a>            context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix]</span>
<span id="cb84-688"><a href="#cb84-688"></a></span>
<span id="cb84-689"><a href="#cb84-689"></a>    X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb84-690"><a href="#cb84-690"></a>    Y <span class="op">=</span> torch.tensor(Y)</span>
<span id="cb84-691"><a href="#cb84-691"></a>    <span class="bu">print</span>(X.shape, Y.shape)</span>
<span id="cb84-692"><a href="#cb84-692"></a>    <span class="cf">return</span> X, Y</span>
<span id="cb84-693"><a href="#cb84-693"></a></span>
<span id="cb84-694"><a href="#cb84-694"></a><span class="im">import</span> random</span>
<span id="cb84-695"><a href="#cb84-695"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb84-696"><a href="#cb84-696"></a>random.shuffle(words)</span>
<span id="cb84-697"><a href="#cb84-697"></a>n1 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb84-698"><a href="#cb84-698"></a>n2 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb84-699"><a href="#cb84-699"></a></span>
<span id="cb84-700"><a href="#cb84-700"></a>Xtr, Ytr <span class="op">=</span> buid_dataset(words[:n1])</span>
<span id="cb84-701"><a href="#cb84-701"></a>Xdev, Ydev <span class="op">=</span> buid_dataset(words[n1:n2])</span>
<span id="cb84-702"><a href="#cb84-702"></a>Xte, Yte <span class="op">=</span> buid_dataset(words[n2:])</span>
<span id="cb84-703"><a href="#cb84-703"></a><span class="in">```</span></span>
<span id="cb84-704"><a href="#cb84-704"></a></span>
<span id="cb84-705"><a href="#cb84-705"></a>Now we're already to train on splits of the dataset, but let's hold on as we are talking abount overfitting. As discussed, overfitting also come from using a complex (too many parameters) for a small data set.</span>
<span id="cb84-706"><a href="#cb84-706"></a></span>
<span id="cb84-707"><a href="#cb84-707"></a>Our dataset has roughly 228k records, while the size of network is only 3.4k. So we are still underfitting, let's continue to complexify our neural networks.</span>
<span id="cb84-708"><a href="#cb84-708"></a></span>
<span id="cb84-709"><a href="#cb84-709"></a>2 things to consider here:</span>
<span id="cb84-710"><a href="#cb84-710"></a></span>
<span id="cb84-711"><a href="#cb84-711"></a><span class="ss">-   </span>the size of tanh - hidden layer; and</span>
<span id="cb84-712"><a href="#cb84-712"></a><span class="ss">-   </span>dimensions of embedding space.</span>
<span id="cb84-713"><a href="#cb84-713"></a></span>
<span id="cb84-714"><a href="#cb84-714"></a><span class="fu">## visualizing the loss, character embeddings</span></span>
<span id="cb84-715"><a href="#cb84-715"></a></span>
<span id="cb84-716"><a href="#cb84-716"></a>First we want to see: - how the loss decrease with 200k training loop with current network setting, learning rate decay to 0.01 after first 100k; and - how the current character embeddings recognize the similarity between characters in (2D) space.</span>
<span id="cb84-717"><a href="#cb84-717"></a></span>
<span id="cb84-718"><a href="#cb84-718"></a>Training on the <span class="in">`Xtr`</span>, <span class="in">`Ytr`</span>:</span>
<span id="cb84-719"><a href="#cb84-719"></a></span>
<span id="cb84-722"><a href="#cb84-722"></a><span class="in">```{python}</span></span>
<span id="cb84-723"><a href="#cb84-723"></a><span class="co"># Lookup table</span></span>
<span id="cb84-724"><a href="#cb84-724"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb84-725"><a href="#cb84-725"></a>C <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">2</span>), generator<span class="op">=</span>g)</span>
<span id="cb84-726"><a href="#cb84-726"></a></span>
<span id="cb84-727"><a href="#cb84-727"></a><span class="co"># Layer 1 - tanh</span></span>
<span id="cb84-728"><a href="#cb84-728"></a>W1 <span class="op">=</span> torch.randn((<span class="dv">6</span>, <span class="dv">100</span>), generator<span class="op">=</span>g)</span>
<span id="cb84-729"><a href="#cb84-729"></a>b1 <span class="op">=</span> torch.randn(<span class="dv">100</span>, generator<span class="op">=</span>g)</span>
<span id="cb84-730"><a href="#cb84-730"></a></span>
<span id="cb84-731"><a href="#cb84-731"></a><span class="co"># Layer 2 - softmax</span></span>
<span id="cb84-732"><a href="#cb84-732"></a>W2 <span class="op">=</span> torch.randn((<span class="dv">100</span>, <span class="dv">27</span>), generator<span class="op">=</span>g)</span>
<span id="cb84-733"><a href="#cb84-733"></a>b2 <span class="op">=</span> torch.randn(<span class="dv">27</span>, generator<span class="op">=</span>g)</span>
<span id="cb84-734"><a href="#cb84-734"></a></span>
<span id="cb84-735"><a href="#cb84-735"></a><span class="co"># All params</span></span>
<span id="cb84-736"><a href="#cb84-736"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb84-737"><a href="#cb84-737"></a><span class="bu">print</span>(<span class="st">"No of params: "</span>, <span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span>
<span id="cb84-738"><a href="#cb84-738"></a></span>
<span id="cb84-739"><a href="#cb84-739"></a><span class="co"># Pre-training</span></span>
<span id="cb84-740"><a href="#cb84-740"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb84-741"><a href="#cb84-741"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb84-742"><a href="#cb84-742"></a></span>
<span id="cb84-743"><a href="#cb84-743"></a><span class="co"># Stats holders</span></span>
<span id="cb84-744"><a href="#cb84-744"></a>lossi <span class="op">=</span> []</span>
<span id="cb84-745"><a href="#cb84-745"></a>stepi <span class="op">=</span> []</span>
<span id="cb84-746"><a href="#cb84-746"></a></span>
<span id="cb84-747"><a href="#cb84-747"></a><span class="co"># Training on Xtr, Ytr</span></span>
<span id="cb84-748"><a href="#cb84-748"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5_000</span>): <span class="co">#200_000</span></span>
<span id="cb84-749"><a href="#cb84-749"></a>    <span class="co"># minibatch construct</span></span>
<span id="cb84-750"><a href="#cb84-750"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (<span class="dv">32</span>,))                       <span class="co">#👈</span></span>
<span id="cb84-751"><a href="#cb84-751"></a>    <span class="co"># forward pass:</span></span>
<span id="cb84-752"><a href="#cb84-752"></a>    emb <span class="op">=</span> C[Xtr[ix]] <span class="co"># (32, 3, 2)                                    #👈</span></span>
<span id="cb84-753"><a href="#cb84-753"></a>    h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb84-754"><a href="#cb84-754"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb84-755"><a href="#cb84-755"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Ytr[ix])                          <span class="co">#👈</span></span>
<span id="cb84-756"><a href="#cb84-756"></a>    <span class="co"># backward pass:</span></span>
<span id="cb84-757"><a href="#cb84-757"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb84-758"><a href="#cb84-758"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb84-759"><a href="#cb84-759"></a>    loss.backward()</span>
<span id="cb84-760"><a href="#cb84-760"></a>    <span class="co"># update</span></span>
<span id="cb84-761"><a href="#cb84-761"></a>    lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;=</span> <span class="dv">100_000</span> <span class="cf">else</span> <span class="fl">0.01</span>                                <span class="co">#👈</span></span>
<span id="cb84-762"><a href="#cb84-762"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb84-763"><a href="#cb84-763"></a>        p.data <span class="op">+=</span> <span class="op">-</span> lr <span class="op">*</span> p.grad</span>
<span id="cb84-764"><a href="#cb84-764"></a></span>
<span id="cb84-765"><a href="#cb84-765"></a>    <span class="co"># track stats</span></span>
<span id="cb84-766"><a href="#cb84-766"></a>    lossi.append(loss.item())</span>
<span id="cb84-767"><a href="#cb84-767"></a>    stepi.append(i)</span>
<span id="cb84-768"><a href="#cb84-768"></a></span>
<span id="cb84-769"><a href="#cb84-769"></a><span class="bu">print</span>(<span class="st">"Loss on minibatch: "</span>, loss.item())</span>
<span id="cb84-770"><a href="#cb84-770"></a><span class="in">```</span></span>
<span id="cb84-771"><a href="#cb84-771"></a></span>
<span id="cb84-772"><a href="#cb84-772"></a>Loss on whole training dataset:</span>
<span id="cb84-773"><a href="#cb84-773"></a></span>
<span id="cb84-776"><a href="#cb84-776"></a><span class="in">```{python}</span></span>
<span id="cb84-777"><a href="#cb84-777"></a>emb <span class="op">=</span> C[Xtr] <span class="co"># (32, 3, 2)</span></span>
<span id="cb84-778"><a href="#cb84-778"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb84-779"><a href="#cb84-779"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb84-780"><a href="#cb84-780"></a>loss <span class="op">=</span> F.cross_entropy(logits, Ytr)</span>
<span id="cb84-781"><a href="#cb84-781"></a>loss.item()</span>
<span id="cb84-782"><a href="#cb84-782"></a><span class="in">```</span></span>
<span id="cb84-783"><a href="#cb84-783"></a></span>
<span id="cb84-784"><a href="#cb84-784"></a>Loss on dev/validation dataset, it's not much different from loss on training as the model is still underfitting, it still generalizes thing:</span>
<span id="cb84-785"><a href="#cb84-785"></a></span>
<span id="cb84-788"><a href="#cb84-788"></a><span class="in">```{python}</span></span>
<span id="cb84-789"><a href="#cb84-789"></a>emb <span class="op">=</span> C[Xdev] <span class="co"># (32, 3, 2)</span></span>
<span id="cb84-790"><a href="#cb84-790"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb84-791"><a href="#cb84-791"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb84-792"><a href="#cb84-792"></a>loss <span class="op">=</span> F.cross_entropy(logits, Ydev)</span>
<span id="cb84-793"><a href="#cb84-793"></a>loss.item()</span>
<span id="cb84-794"><a href="#cb84-794"></a><span class="in">```</span></span>
<span id="cb84-795"><a href="#cb84-795"></a></span>
<span id="cb84-796"><a href="#cb84-796"></a>Visualizing loss, we can see the loss shaking significantly as the batch size still small - 32.</span>
<span id="cb84-797"><a href="#cb84-797"></a></span>
<span id="cb84-800"><a href="#cb84-800"></a><span class="in">```{python}</span></span>
<span id="cb84-801"><a href="#cb84-801"></a>plt.plot(stepi, lossi)</span>
<span id="cb84-802"><a href="#cb84-802"></a><span class="in">```</span></span>
<span id="cb84-803"><a href="#cb84-803"></a></span>
<span id="cb84-804"><a href="#cb84-804"></a>Visualizing the character embeddings, we can see the model can cluster for eg. vowels a, e, i, o, u.</span>
<span id="cb84-805"><a href="#cb84-805"></a></span>
<span id="cb84-808"><a href="#cb84-808"></a><span class="in">```{python}</span></span>
<span id="cb84-809"><a href="#cb84-809"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb84-810"><a href="#cb84-810"></a>plt.scatter(C[:,<span class="dv">0</span>].data, C[:, <span class="dv">1</span>].data, s<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb84-811"><a href="#cb84-811"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(C.shape[<span class="dv">0</span>]):</span>
<span id="cb84-812"><a href="#cb84-812"></a>    plt.text(C[i,<span class="dv">0</span>].item(),C[i,<span class="dv">1</span>].item(), itos[i], ha<span class="op">=</span><span class="st">"center"</span>, va<span class="op">=</span><span class="st">"center"</span>, color<span class="op">=</span><span class="st">"white"</span>)</span>
<span id="cb84-813"><a href="#cb84-813"></a>plt.grid(<span class="st">'minor'</span>)</span>
<span id="cb84-814"><a href="#cb84-814"></a><span class="in">```</span></span>
<span id="cb84-815"><a href="#cb84-815"></a></span>
<span id="cb84-816"><a href="#cb84-816"></a><span class="fu">## experiment: larger hidden layer, larger embedding size</span></span>
<span id="cb84-817"><a href="#cb84-817"></a></span>
<span id="cb84-818"><a href="#cb84-818"></a>Now we can experiment a larger hidden layer (300), and larger embedding_size (10). Below is the whole code:</span>
<span id="cb84-819"><a href="#cb84-819"></a></span>
<span id="cb84-822"><a href="#cb84-822"></a><span class="in">```{python}</span></span>
<span id="cb84-823"><a href="#cb84-823"></a><span class="co">#| code-fold: true</span></span>
<span id="cb84-824"><a href="#cb84-824"></a></span>
<span id="cb84-825"><a href="#cb84-825"></a><span class="co"># hyper-parameters</span></span>
<span id="cb84-826"><a href="#cb84-826"></a>block_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># number of chracters / inputs to predict the nextone</span></span>
<span id="cb84-827"><a href="#cb84-827"></a>no_chars <span class="op">=</span> <span class="dv">27</span> <span class="co"># number of possible chracters, include '.'</span></span>
<span id="cb84-828"><a href="#cb84-828"></a>emb_size <span class="op">=</span> <span class="dv">10</span> <span class="co"># no of dimensions of the embedding space.</span></span>
<span id="cb84-829"><a href="#cb84-829"></a>hidden_size <span class="op">=</span> <span class="dv">300</span> <span class="co"># size of the hidden - tanh layer</span></span>
<span id="cb84-830"><a href="#cb84-830"></a>batch_size <span class="op">=</span> <span class="dv">32</span> <span class="co"># minibatch size for training, 2, 4, 8, 16, 32, 64, etc</span></span>
<span id="cb84-831"><a href="#cb84-831"></a></span>
<span id="cb84-832"><a href="#cb84-832"></a><span class="co"># build the dataset</span></span>
<span id="cb84-833"><a href="#cb84-833"></a><span class="kw">def</span> buid_dataset(words):</span>
<span id="cb84-834"><a href="#cb84-834"></a></span>
<span id="cb84-835"><a href="#cb84-835"></a>    X, Y <span class="op">=</span> [], []</span>
<span id="cb84-836"><a href="#cb84-836"></a></span>
<span id="cb84-837"><a href="#cb84-837"></a>    <span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb84-838"><a href="#cb84-838"></a>        context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size</span>
<span id="cb84-839"><a href="#cb84-839"></a>        <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb84-840"><a href="#cb84-840"></a>            ix <span class="op">=</span> stoi[ch]</span>
<span id="cb84-841"><a href="#cb84-841"></a>            X.append(context)</span>
<span id="cb84-842"><a href="#cb84-842"></a>            Y.append(ix)</span>
<span id="cb84-843"><a href="#cb84-843"></a>            context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix]</span>
<span id="cb84-844"><a href="#cb84-844"></a></span>
<span id="cb84-845"><a href="#cb84-845"></a>    X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb84-846"><a href="#cb84-846"></a>    Y <span class="op">=</span> torch.tensor(Y)</span>
<span id="cb84-847"><a href="#cb84-847"></a>    <span class="bu">print</span>(X.shape, Y.shape)</span>
<span id="cb84-848"><a href="#cb84-848"></a>    <span class="cf">return</span> X, Y</span>
<span id="cb84-849"><a href="#cb84-849"></a></span>
<span id="cb84-850"><a href="#cb84-850"></a><span class="im">import</span> random</span>
<span id="cb84-851"><a href="#cb84-851"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb84-852"><a href="#cb84-852"></a>random.shuffle(words)</span>
<span id="cb84-853"><a href="#cb84-853"></a>n1 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb84-854"><a href="#cb84-854"></a>n2 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb84-855"><a href="#cb84-855"></a></span>
<span id="cb84-856"><a href="#cb84-856"></a><span class="co"># 80 - 10 - 10 splits</span></span>
<span id="cb84-857"><a href="#cb84-857"></a>Xtr, Ytr <span class="op">=</span> buid_dataset(words[:n1])</span>
<span id="cb84-858"><a href="#cb84-858"></a>Xdev, Ydev <span class="op">=</span> buid_dataset(words[n1:n2])</span>
<span id="cb84-859"><a href="#cb84-859"></a>Xte, Yte <span class="op">=</span> buid_dataset(words[n2:])</span>
<span id="cb84-860"><a href="#cb84-860"></a></span>
<span id="cb84-861"><a href="#cb84-861"></a><span class="co"># Lookup table - 10 dimensional space</span></span>
<span id="cb84-862"><a href="#cb84-862"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>) <span class="co"># for reproductivity</span></span>
<span id="cb84-863"><a href="#cb84-863"></a>C <span class="op">=</span> torch.randn((no_chars, emb_size), generator<span class="op">=</span>g)</span>
<span id="cb84-864"><a href="#cb84-864"></a></span>
<span id="cb84-865"><a href="#cb84-865"></a><span class="co"># Layer 1 - tanh - 300 neurons</span></span>
<span id="cb84-866"><a href="#cb84-866"></a>W1 <span class="op">=</span> torch.randn((block_size <span class="op">*</span> emb_size, hidden_size), generator<span class="op">=</span>g)</span>
<span id="cb84-867"><a href="#cb84-867"></a>b1 <span class="op">=</span> torch.randn(hidden_size, generator<span class="op">=</span>g)</span>
<span id="cb84-868"><a href="#cb84-868"></a></span>
<span id="cb84-869"><a href="#cb84-869"></a><span class="co"># Layer 2 - softmax</span></span>
<span id="cb84-870"><a href="#cb84-870"></a>W2 <span class="op">=</span> torch.randn((hidden_size, no_chars), generator<span class="op">=</span>g)</span>
<span id="cb84-871"><a href="#cb84-871"></a>b2 <span class="op">=</span> torch.randn(no_chars, generator<span class="op">=</span>g)</span>
<span id="cb84-872"><a href="#cb84-872"></a></span>
<span id="cb84-873"><a href="#cb84-873"></a><span class="co"># All params</span></span>
<span id="cb84-874"><a href="#cb84-874"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb84-875"><a href="#cb84-875"></a><span class="bu">print</span>(<span class="st">"No of params: "</span>, <span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span>
<span id="cb84-876"><a href="#cb84-876"></a></span>
<span id="cb84-877"><a href="#cb84-877"></a><span class="co"># Pre-training</span></span>
<span id="cb84-878"><a href="#cb84-878"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb84-879"><a href="#cb84-879"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb84-880"><a href="#cb84-880"></a></span>
<span id="cb84-881"><a href="#cb84-881"></a><span class="co"># Stats holders</span></span>
<span id="cb84-882"><a href="#cb84-882"></a>lossi <span class="op">=</span> []</span>
<span id="cb84-883"><a href="#cb84-883"></a>stepi <span class="op">=</span> []</span>
<span id="cb84-884"><a href="#cb84-884"></a></span>
<span id="cb84-885"><a href="#cb84-885"></a><span class="co"># Training on Xtr, Ytr</span></span>
<span id="cb84-886"><a href="#cb84-886"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5_000</span>): <span class="co">#200_000</span></span>
<span id="cb84-887"><a href="#cb84-887"></a>    <span class="co"># minibatch construct</span></span>
<span id="cb84-888"><a href="#cb84-888"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,))</span>
<span id="cb84-889"><a href="#cb84-889"></a>    <span class="co"># forward pass:</span></span>
<span id="cb84-890"><a href="#cb84-890"></a>    emb <span class="op">=</span> C[Xtr[ix]]</span>
<span id="cb84-891"><a href="#cb84-891"></a>    h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, block_size <span class="op">*</span> emb_size) <span class="op">@</span> W1 <span class="op">+</span> b1)</span>
<span id="cb84-892"><a href="#cb84-892"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb84-893"><a href="#cb84-893"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Ytr[ix])</span>
<span id="cb84-894"><a href="#cb84-894"></a>    <span class="co"># backward pass:</span></span>
<span id="cb84-895"><a href="#cb84-895"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb84-896"><a href="#cb84-896"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb84-897"><a href="#cb84-897"></a>    loss.backward()</span>
<span id="cb84-898"><a href="#cb84-898"></a>    <span class="co"># update</span></span>
<span id="cb84-899"><a href="#cb84-899"></a>    lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;=</span> <span class="dv">100_000</span> <span class="cf">else</span> <span class="fl">0.01</span></span>
<span id="cb84-900"><a href="#cb84-900"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb84-901"><a href="#cb84-901"></a>        p.data <span class="op">+=</span> <span class="op">-</span> lr <span class="op">*</span> p.grad</span>
<span id="cb84-902"><a href="#cb84-902"></a></span>
<span id="cb84-903"><a href="#cb84-903"></a>    <span class="co"># track stats</span></span>
<span id="cb84-904"><a href="#cb84-904"></a>    lossi.append(loss.item())</span>
<span id="cb84-905"><a href="#cb84-905"></a>    stepi.append(i)</span>
<span id="cb84-906"><a href="#cb84-906"></a></span>
<span id="cb84-907"><a href="#cb84-907"></a><span class="bu">print</span>(<span class="st">"Loss on minibatch: "</span>, loss.item())</span>
<span id="cb84-908"><a href="#cb84-908"></a><span class="in">```</span></span>
<span id="cb84-909"><a href="#cb84-909"></a></span>
<span id="cb84-912"><a href="#cb84-912"></a><span class="in">```{python}</span></span>
<span id="cb84-913"><a href="#cb84-913"></a>emb <span class="op">=</span> C[Xtr]</span>
<span id="cb84-914"><a href="#cb84-914"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, block_size <span class="op">*</span> emb_size) <span class="op">@</span> W1 <span class="op">+</span> b1)</span>
<span id="cb84-915"><a href="#cb84-915"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb84-916"><a href="#cb84-916"></a>loss <span class="op">=</span> F.cross_entropy(logits, Ytr)</span>
<span id="cb84-917"><a href="#cb84-917"></a><span class="bu">print</span>(<span class="st">"Loss on whole training set: "</span>, loss.item())</span>
<span id="cb84-918"><a href="#cb84-918"></a></span>
<span id="cb84-919"><a href="#cb84-919"></a>emb <span class="op">=</span> C[Xdev]</span>
<span id="cb84-920"><a href="#cb84-920"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, block_size <span class="op">*</span> emb_size) <span class="op">@</span> W1 <span class="op">+</span> b1)</span>
<span id="cb84-921"><a href="#cb84-921"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb84-922"><a href="#cb84-922"></a>loss <span class="op">=</span> F.cross_entropy(logits, Ydev)</span>
<span id="cb84-923"><a href="#cb84-923"></a><span class="bu">print</span>(<span class="st">"Loss on dev/validation set: "</span>, loss.item())</span>
<span id="cb84-924"><a href="#cb84-924"></a></span>
<span id="cb84-925"><a href="#cb84-925"></a>emb <span class="op">=</span> C[Xte]</span>
<span id="cb84-926"><a href="#cb84-926"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, block_size <span class="op">*</span> emb_size) <span class="op">@</span> W1 <span class="op">+</span> b1)</span>
<span id="cb84-927"><a href="#cb84-927"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb84-928"><a href="#cb84-928"></a>loss <span class="op">=</span> F.cross_entropy(logits, Yte)</span>
<span id="cb84-929"><a href="#cb84-929"></a><span class="bu">print</span>(<span class="st">"Loss on test set: "</span>, loss.item())</span>
<span id="cb84-930"><a href="#cb84-930"></a><span class="in">```</span></span>
<span id="cb84-931"><a href="#cb84-931"></a></span>
<span id="cb84-932"><a href="#cb84-932"></a><span class="fu">## summary of our final code, conclusion</span></span>
<span id="cb84-933"><a href="#cb84-933"></a></span>
<span id="cb84-936"><a href="#cb84-936"></a><span class="in">```{python}</span></span>
<span id="cb84-937"><a href="#cb84-937"></a>plt.plot(stepi, lossi)</span>
<span id="cb84-938"><a href="#cb84-938"></a><span class="in">```</span></span>
<span id="cb84-939"><a href="#cb84-939"></a></span>
<span id="cb84-942"><a href="#cb84-942"></a><span class="in">```{python}</span></span>
<span id="cb84-943"><a href="#cb84-943"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb84-944"><a href="#cb84-944"></a>plt.scatter(C[:,<span class="dv">0</span>].data, C[:, <span class="dv">1</span>].data, s<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb84-945"><a href="#cb84-945"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(C.shape[<span class="dv">0</span>]):</span>
<span id="cb84-946"><a href="#cb84-946"></a>    plt.text(C[i,<span class="dv">0</span>].item(),C[i,<span class="dv">1</span>].item(), itos[i], ha<span class="op">=</span><span class="st">"center"</span>, va<span class="op">=</span><span class="st">"center"</span>, color<span class="op">=</span><span class="st">"white"</span>)</span>
<span id="cb84-947"><a href="#cb84-947"></a>plt.grid(<span class="st">'minor'</span>)</span>
<span id="cb84-948"><a href="#cb84-948"></a><span class="in">```</span></span>
<span id="cb84-949"><a href="#cb84-949"></a></span>
<span id="cb84-950"><a href="#cb84-950"></a>We can see the loss on validation set and test set are quite similar as we are not try different scenarios to calibrate/tune hyperparamters much. So they both have the same suprise to the model training by <span class="in">`Xtr`</span>.</span>
<span id="cb84-951"><a href="#cb84-951"></a></span>
<span id="cb84-952"><a href="#cb84-952"></a>We still have rooms for improvement!</span>
<span id="cb84-953"><a href="#cb84-953"></a></span>
<span id="cb84-954"><a href="#cb84-954"></a><span class="fu">## sampling from the model</span></span>
<span id="cb84-955"><a href="#cb84-955"></a></span>
<span id="cb84-956"><a href="#cb84-956"></a>But our networks now can generate more name-like name!</span>
<span id="cb84-957"><a href="#cb84-957"></a></span>
<span id="cb84-960"><a href="#cb84-960"></a><span class="in">```{python}</span></span>
<span id="cb84-961"><a href="#cb84-961"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span> <span class="op">+</span> <span class="dv">10</span>)</span>
<span id="cb84-962"><a href="#cb84-962"></a></span>
<span id="cb84-963"><a href="#cb84-963"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb84-964"><a href="#cb84-964"></a></span>
<span id="cb84-965"><a href="#cb84-965"></a>    out <span class="op">=</span> []</span>
<span id="cb84-966"><a href="#cb84-966"></a>    context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size <span class="co"># initialize with all ...</span></span>
<span id="cb84-967"><a href="#cb84-967"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb84-968"><a href="#cb84-968"></a>      emb <span class="op">=</span> C[torch.tensor([context])] <span class="co"># (1,block_size,d)</span></span>
<span id="cb84-969"><a href="#cb84-969"></a>      h <span class="op">=</span> torch.tanh(emb.view(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">@</span> W1 <span class="op">+</span> b1)</span>
<span id="cb84-970"><a href="#cb84-970"></a>      logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb84-971"><a href="#cb84-971"></a>      probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb84-972"><a href="#cb84-972"></a>      ix <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>, generator<span class="op">=</span>g).item()</span>
<span id="cb84-973"><a href="#cb84-973"></a>      context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix]</span>
<span id="cb84-974"><a href="#cb84-974"></a>      out.append(ix)</span>
<span id="cb84-975"><a href="#cb84-975"></a>      <span class="cf">if</span> ix <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb84-976"><a href="#cb84-976"></a>        <span class="cf">break</span></span>
<span id="cb84-977"><a href="#cb84-977"></a></span>
<span id="cb84-978"><a href="#cb84-978"></a>    <span class="bu">print</span>(<span class="st">''</span>.join(itos[i] <span class="cf">for</span> i <span class="kw">in</span> out))</span>
<span id="cb84-979"><a href="#cb84-979"></a><span class="in">```</span></span>
<span id="cb84-980"><a href="#cb84-980"></a></span>
<span id="cb84-981"><a href="#cb84-981"></a><span class="fu">## google collab (new!!) notebook advertisement</span></span>
<span id="cb84-982"><a href="#cb84-982"></a></span>
<span id="cb84-983"><a href="#cb84-983"></a>Colab link: <span class="ot">&lt;https://colab.research.google.com/drive/1YIfmkftLrz6MPTOO9Vwqrop2Q5llHIGK?usp=sharing&gt;</span></span>
<span id="cb84-984"><a href="#cb84-984"></a></span>
<span id="cb84-985"><a href="#cb84-985"></a>Thanks Andrej!</span>
<span id="cb84-986"><a href="#cb84-986"></a></span>
<span id="cb84-987"><a href="#cb84-987"></a><span class="fu"># resources</span></span>
<span id="cb84-988"><a href="#cb84-988"></a></span>
<span id="cb84-989"><a href="#cb84-989"></a><span class="ss">1.  </span><span class="co">[</span><span class="ot">**A Neural Probabilistic Language Model**, Bengio et al. (2003)</span><span class="co">](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)</span></span>
<span id="cb84-990"><a href="#cb84-990"></a><span class="ss">2.  </span><span class="co">[</span><span class="ot">Video lecturer</span><span class="co">](https://www.youtube.com/watch?v=TCH_1BHY58I)</span></span>
<span id="cb84-991"><a href="#cb84-991"></a><span class="ss">3.  </span><span class="co">[</span><span class="ot">Notebook</span><span class="co">](https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part2_mlp.ipynb)</span></span>
<span id="cb84-992"><a href="#cb84-992"></a><span class="ss">4.  </span><span class="co">[</span><span class="ot">`makemore` on Github</span><span class="co">](https://github.com/karpathy/makemore)</span></span>
<span id="cb84-993"><a href="#cb84-993"></a><span class="ss">5.  </span><span class="co">[</span><span class="ot">`torch.Tensor()` documentation</span><span class="co">](https://pytorch.org/docs/main/tensors.html)</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block"><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"><i class="fa-brands fa-creative-commons" aria-label="creative-commons"></i></a> 2023-2025 Le Khac Tuan</span></p>
</div>   
    <div class="nav-footer-center">
<p><span class="faux-block"> Designed with <i class="fa-solid fa-heart" aria-label="heart"></i>, <span id="commit-info">Loading last commit…</span> </span></p>
</div>
    <div class="nav-footer-right">
<p><span class="faux-block">Made with <a href="https://quarto.org/">Quarto</a></span></p>
</div>
  </div>
</footer>
<script type="application/javascript" src="commit_info.js"></script>




</body></html>