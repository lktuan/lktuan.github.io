<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tuan Le Khac">
<meta name="dcterms.date" content="2024-11-20">
<meta name="description" content="I am a Risk Modeler üöÄ">

<title>Le Khac Tuan - NN-Z2H Lesson 3: Building makemore part 2 - MLP</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../img/rocket_1613268.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="html/styles.scss">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Le Khac Tuan</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../curriculum/index.html"> 
<span class="menu-text">Curriculum</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Â≠¶Ê±âËØ≠ÁöÑÊó•ËÆ∞.html"> 
<span class="menu-text">Â≠¶Ê±âËØ≠ÁöÑÊó•ËÆ∞</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../jiu_jitsu_journal/index.html"> 
<span class="menu-text">Jiu Jitsu Journal</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lktuan"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/tuanlekhac/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.facebook.com/toilatuan.lk/"> <i class="bi bi-facebook" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:tuan.lekhac0905@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">NN-Z2H Lesson 3: Building makemore part 2 - MLP</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                  <div>
        <div class="description">
          implement a multilayer perceptron (MLP) character-level language model, introduce model training, learning rate tuning, hyperparameters, evaluation, train/dev/test splits, under/overfitting, etc.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">til</div>
                <div class="quarto-category">python</div>
                <div class="quarto-category">andrej karpathy</div>
                <div class="quarto-category">nn-z2h</div>
                <div class="quarto-category">bigram</div>
                <div class="quarto-category">neural networks</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://lktuan.github.io/">Tuan Le Khac</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 20, 2024</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">November 20, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#part-1-intro-to-mlp" id="toc-part-1-intro-to-mlp" class="nav-link active" data-scroll-target="#part-1-intro-to-mlp">PART 1: intro to MLP</a>
  <ul class="collapse">
  <li><a href="#bengio-et-al.-2003-mlp-language-model-paper-walkthrough" id="toc-bengio-et-al.-2003-mlp-language-model-paper-walkthrough" class="nav-link" data-scroll-target="#bengio-et-al.-2003-mlp-language-model-paper-walkthrough">Bengio et al.&nbsp;2003 (MLP language model) paper walkthrough</a>
  <ul class="collapse">
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology">Methodology:</a></li>
  <li><a href="#neural-architecture" id="toc-neural-architecture" class="nav-link" data-scroll-target="#neural-architecture">Neural architecture:</a></li>
  </ul></li>
  <li><a href="#re-building-our-training-dataset" id="toc-re-building-our-training-dataset" class="nav-link" data-scroll-target="#re-building-our-training-dataset">(re-)building our training dataset</a></li>
  <li><a href="#implementing-the-embedding-lookup-table" id="toc-implementing-the-embedding-lookup-table" class="nav-link" data-scroll-target="#implementing-the-embedding-lookup-table">implementing the embedding lookup table</a></li>
  <li><a href="#implementing-the-hidden-layer-internals-of-torch.tensor-storage-views" id="toc-implementing-the-hidden-layer-internals-of-torch.tensor-storage-views" class="nav-link" data-scroll-target="#implementing-the-hidden-layer-internals-of-torch.tensor-storage-views">implementing the hidden layer + internals of <code>torch.Tensor</code>: <code>storage</code>, <code>views</code></a></li>
  <li><a href="#implementing-the-output-layer" id="toc-implementing-the-output-layer" class="nav-link" data-scroll-target="#implementing-the-output-layer">implementing the output layer</a></li>
  <li><a href="#implementing-the-negative-log-likelihood-loss" id="toc-implementing-the-negative-log-likelihood-loss" class="nav-link" data-scroll-target="#implementing-the-negative-log-likelihood-loss">implementing the negative log likelihood loss</a></li>
  <li><a href="#summary-of-the-full-network" id="toc-summary-of-the-full-network" class="nav-link" data-scroll-target="#summary-of-the-full-network">summary of the full network</a></li>
  </ul></li>
  <li><a href="#part-2-intro-to-many-basics-of-machine-learning" id="toc-part-2-intro-to-many-basics-of-machine-learning" class="nav-link" data-scroll-target="#part-2-intro-to-many-basics-of-machine-learning">PART 2: intro to many basics of machine learning</a>
  <ul class="collapse">
  <li><a href="#introducing-f.cross_entropy-and-why" id="toc-introducing-f.cross_entropy-and-why" class="nav-link" data-scroll-target="#introducing-f.cross_entropy-and-why">introducing <code>F.cross_entropy</code> and why</a></li>
  <li><a href="#implementing-the-training-loop-overfitting-one-batch" id="toc-implementing-the-training-loop-overfitting-one-batch" class="nav-link" data-scroll-target="#implementing-the-training-loop-overfitting-one-batch">implementing the training loop, overfitting one batch</a></li>
  <li><a href="#training-on-the-full-dataset-minibatches" id="toc-training-on-the-full-dataset-minibatches" class="nav-link" data-scroll-target="#training-on-the-full-dataset-minibatches">training on the full dataset, minibatches</a></li>
  <li><a href="#finding-a-good-initial-learning-rate" id="toc-finding-a-good-initial-learning-rate" class="nav-link" data-scroll-target="#finding-a-good-initial-learning-rate">finding a good initial learning rate</a></li>
  <li><a href="#splitting-up-the-dataset-into-trainvaltest-splits-and-why" id="toc-splitting-up-the-dataset-into-trainvaltest-splits-and-why" class="nav-link" data-scroll-target="#splitting-up-the-dataset-into-trainvaltest-splits-and-why">splitting up the dataset into train/val/test splits and why</a></li>
  <li><a href="#experiment-larger-hidden-layer" id="toc-experiment-larger-hidden-layer" class="nav-link" data-scroll-target="#experiment-larger-hidden-layer">experiment: larger hidden layer</a></li>
  <li><a href="#visualizing-the-character-embeddings" id="toc-visualizing-the-character-embeddings" class="nav-link" data-scroll-target="#visualizing-the-character-embeddings">visualizing the character embeddings</a></li>
  <li><a href="#experiment-larger-embedding-size" id="toc-experiment-larger-embedding-size" class="nav-link" data-scroll-target="#experiment-larger-embedding-size">experiment: larger embedding size</a></li>
  <li><a href="#summary-of-our-final-code-conclusion" id="toc-summary-of-our-final-code-conclusion" class="nav-link" data-scroll-target="#summary-of-our-final-code-conclusion">summary of our final code, conclusion</a></li>
  <li><a href="#sampling-from-the-model" id="toc-sampling-from-the-model" class="nav-link" data-scroll-target="#sampling-from-the-model">sampling from the model</a></li>
  <li><a href="#google-collab-new-notebook-advertisement" id="toc-google-collab-new-notebook-advertisement" class="nav-link" data-scroll-target="#google-collab-new-notebook-advertisement">google collab (new!!) notebook advertisement</a></li>
  </ul></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">resources</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>In the previous lecture, we built a simple <code>bigram</code> character-level language model, using 2 different approaches that are (1) count, and (2) 1 layer neural network. They produced the same (and both poor - since the context is 1 character only) result but the neural network option offers more flexibility so that we can complexify our model to get better performance.</p>
<p>In this lecture we are going to implement 20-years ago neural probabilistic language model by <em>Bengio et al.&nbsp;(2003)</em>.</p>
<section id="part-1-intro-to-mlp" class="level1">
<h1>PART 1: intro to MLP</h1>
<section id="bengio-et-al.-2003-mlp-language-model-paper-walkthrough" class="level2">
<h2 class="anchored" data-anchor-id="bengio-et-al.-2003-mlp-language-model-paper-walkthrough">Bengio et al.&nbsp;2003 (MLP language model) paper walkthrough</h2>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<p><strong>Problem Statement</strong>:</p>
<ul>
<li>Traditional n-gram language models suffer from the <em>curse of dimensionality</em>: they can‚Äôt effectively generalize to word sequences not seen in training data;</li>
<li>The core issue is treating words as atomic units with no <em>inherent similarity</em> to each other;</li>
<li>For example, if we‚Äôve seen ‚Äúdog is eating‚Äù in training but never ‚Äúcat is eating‚Äù, n-gram models can‚Äôt leverage the similarity between ‚Äúdog‚Äù and ‚Äúcat‚Äù;</li>
<li>This leads to poor probability estimates for rare or unseen word sequences.</li>
</ul>
<p><strong>Solution</strong>:</p>
<ul>
<li>Learn a <em>distributed representation</em> (embedding) for each word in a continuous vector space where similar words are close to each other;</li>
<li>Use a neural network architecture with:
<ul>
<li>Input layer: concatenated embeddings of n-1 previous words;</li>
<li>Hidden layer: dense neural network with <code>tanh</code> activation;</li>
<li>Output layer: softmax over entire vocabulary to predict next word probability.</li>
</ul></li>
</ul>
<p><strong>The model simultaneously learns</strong>:</p>
<ul>
<li>Word feature vectors (embeddings) that capture <em>semantic/syntactic word similarities</em>;</li>
<li>Neural network parameters that combine these features to estimate probability distributions.</li>
</ul>
<p><strong>Key advantages</strong>:</p>
<ul>
<li>Words with similar meanings get similar feature vectors, enabling better <em>generalization</em>;</li>
<li>The probability function is smooth with respect to word embeddings, so similar words yield <em>similar predictions</em>;</li>
<li>Can generalize to <em>unseen sequences</em> by leveraging learned word similarities.</li>
</ul>
</section>
<section id="methodology" class="level3">
<h3 class="anchored" data-anchor-id="methodology">Methodology:</h3>
<ul>
<li><p>Traditional Problem:</p>
<ul>
<li>In n-gram models, each word sequence of length n is a separate parameter;</li>
<li>For vocabulary size <span class="math inline">\(|V|\)</span>, need <span class="math inline">\(|V|^n\)</span> parameters;</li>
<li>Most sequences never appear in training, leading to poor generalization;</li>
</ul></li>
<li><p>Solution via <strong>Distributed Representation</strong>:</p>
<ul>
<li>Each word mapped to a dense vector in <span class="math inline">\(R^m\)</span> (typically m=50-100);</li>
<li>Similar words get similar vectors through training;</li>
<li>Probability function is smooth w.r.t these vectors;</li>
<li>Key benefit: If ‚Äúdog‚Äù and ‚Äúcat‚Äù have similar vectors, model can generalize from ‚Äúdog is eating‚Äù to ‚Äúcat is eating‚Äù;</li>
<li>Number of parameters reduces to <span class="math inline">\(O(|V|√óm + m√óh + h√ó|V|)\)</span>, where <span class="math inline">\(h\)</span> is hidden layer size;</li>
<li>This is much smaller than <span class="math inline">\(|V|^n\)</span> and allows better generalization;</li>
</ul></li>
</ul>
</section>
<section id="neural-architecture" class="level3">
<h3 class="anchored" data-anchor-id="neural-architecture">Neural architecture:</h3>
<p><strong>Input Layer</strong>:</p>
<ul>
<li>Takes <span class="math inline">\(n-1\)</span> previous words (context window);</li>
<li>Each word i mapped to vector <span class="math inline">\(C(i) ‚àà R^m\)</span> via lookup table;</li>
<li>Concatenates these vectors: <span class="math inline">\(x = [C(w‚Çú‚Çã‚Çô‚Çä‚ÇÅ), ..., C(w‚Çú‚Çã‚ÇÅ)]\)</span>;</li>
<li><span class="math inline">\(x\)</span> dimension is <span class="math inline">\((n-1)√óm\)</span>;</li>
</ul>
<p><strong>Hidden Layer</strong>:</p>
<ul>
<li>Dense layer with tanh activation;</li>
<li>Computation: <span class="math inline">\(h = tanh(d + Hx)\)</span>;</li>
<li><span class="math inline">\(H\)</span> is weight matrix, <span class="math inline">\(d\)</span> is bias vector;</li>
<li>Maps concatenated context to hidden representation;</li>
</ul>
<p><strong>Output Layer</strong>:</p>
<ul>
<li>Computes probability distribution over all words;</li>
<li><span class="math inline">\(y = b + Wx + Uh\)</span>;</li>
<li>Softmax activation: <span class="math inline">\(P(w‚Çú|context) = exp(y·µ¢)/Œ£‚±ºexp(y‚±º)\)</span>;</li>
<li><span class="math inline">\(W\)</span> provides ‚Äúshortcut‚Äù connections from input to output;</li>
<li>Direct connection helps learn simpler patterns;</li>
</ul>
<p><strong>Training</strong>:</p>
<ul>
<li>Maximizes log-likelihood of training data;</li>
<li>Uses stochastic gradient descent;</li>
<li>Learns both word vectors <span class="math inline">\(C(i)\)</span> and neural network parameters <span class="math inline">\((H, d, W, U, b)\)</span>;</li>
<li>Word vectors capture similarities as they help predict similar contexts;</li>
<li>Can initialize word vectors randomly or with pretrained vectors.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="NLM_Bengio_etal.png" class="img-fluid figure-img"></p>
<figcaption>Neural Language Model proposed by (Bengio et al., 2003). C(i) is the i th word embedding.</figcaption>
</figure>
</div>
</section>
</section>
<section id="re-building-our-training-dataset" class="level2">
<h2 class="anchored" data-anchor-id="re-building-our-training-dataset">(re-)building our training dataset</h2>
<p>Loading library, reading data, building dictionary:</p>
<div id="3bc0c14d" class="cell" data-execution_count="1">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="96b3162d" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2"></a></span>
<span id="cb2-3"><a href="#cb2-3"></a>url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt"</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>words <span class="op">=</span> pd.read_csv(url, header<span class="op">=</span><span class="va">None</span>).iloc[:, <span class="dv">0</span>].tolist()</span>
<span id="cb2-5"><a href="#cb2-5"></a>words[:<span class="dv">8</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']</code></pre>
</div>
</div>
<div id="cf6ada3e" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="bu">len</span>(words)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>32033</code></pre>
</div>
</div>
<div id="5cb85282" class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># build the vocabulary of characters and mapping to/from integer</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(<span class="st">''</span>.join(words))))</span>
<span id="cb6-3"><a href="#cb6-3"></a>stoi <span class="op">=</span> {s:i<span class="op">+</span><span class="dv">1</span> <span class="cf">for</span> i, s <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb6-4"><a href="#cb6-4"></a>stoi[<span class="st">'.'</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-5"><a href="#cb6-5"></a></span>
<span id="cb6-6"><a href="#cb6-6"></a>itos <span class="op">=</span> {i: s <span class="cf">for</span> s, i <span class="kw">in</span> stoi.items()}</span>
<span id="cb6-7"><a href="#cb6-7"></a>itos</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>{1: 'a',
 2: 'b',
 3: 'c',
 4: 'd',
 5: 'e',
 6: 'f',
 7: 'g',
 8: 'h',
 9: 'i',
 10: 'j',
 11: 'k',
 12: 'l',
 13: 'm',
 14: 'n',
 15: 'o',
 16: 'p',
 17: 'q',
 18: 'r',
 19: 's',
 20: 't',
 21: 'u',
 22: 'v',
 23: 'w',
 24: 'x',
 25: 'y',
 26: 'z',
 0: '.'}</code></pre>
</div>
</div>
<p>Building the dataset:</p>
<div id="8212c8cc" class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>block_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># the context length: how many characters do we take to predict the next one?</span></span>
<span id="cb8-2"><a href="#cb8-2"></a>X, Y <span class="op">=</span> [], []</span>
<span id="cb8-3"><a href="#cb8-3"></a></span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="cf">for</span> w <span class="kw">in</span> words[:<span class="dv">5</span>]:</span>
<span id="cb8-5"><a href="#cb8-5"></a>    <span class="bu">print</span>(w)</span>
<span id="cb8-6"><a href="#cb8-6"></a>    context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size <span class="co"># 0 so context will be padded by '.'</span></span>
<span id="cb8-7"><a href="#cb8-7"></a>    <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb8-8"><a href="#cb8-8"></a>        ix <span class="op">=</span> stoi[ch]</span>
<span id="cb8-9"><a href="#cb8-9"></a>        X.append(context)</span>
<span id="cb8-10"><a href="#cb8-10"></a>        Y.append(ix)</span>
<span id="cb8-11"><a href="#cb8-11"></a>        <span class="bu">print</span>(<span class="st">''</span>.join(itos[i] <span class="cf">for</span> i <span class="kw">in</span> context), <span class="st">'-----&gt;'</span>, itos[ix] )</span>
<span id="cb8-12"><a href="#cb8-12"></a>        context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix] <span class="co"># rolling to the next one</span></span>
<span id="cb8-13"><a href="#cb8-13"></a></span>
<span id="cb8-14"><a href="#cb8-14"></a>X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb8-15"><a href="#cb8-15"></a>Y <span class="op">=</span> torch.tensor(Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>emma
... -----&gt; e
..e -----&gt; m
.em -----&gt; m
emm -----&gt; a
mma -----&gt; .
olivia
... -----&gt; o
..o -----&gt; l
.ol -----&gt; i
oli -----&gt; v
liv -----&gt; i
ivi -----&gt; a
via -----&gt; .
ava
... -----&gt; a
..a -----&gt; v
.av -----&gt; a
ava -----&gt; .
isabella
... -----&gt; i
..i -----&gt; s
.is -----&gt; a
isa -----&gt; b
sab -----&gt; e
abe -----&gt; l
bel -----&gt; l
ell -----&gt; a
lla -----&gt; .
sophia
... -----&gt; s
..s -----&gt; o
.so -----&gt; p
sop -----&gt; h
oph -----&gt; i
phi -----&gt; a
hia -----&gt; .</code></pre>
</div>
</div>
<div id="05df5b19" class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>X.shape, X.dtype, Y.shape, Y.dtype</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)</code></pre>
</div>
</div>
</section>
<section id="implementing-the-embedding-lookup-table" class="level2">
<h2 class="anchored" data-anchor-id="implementing-the-embedding-lookup-table">implementing the embedding lookup table</h2>
<p>In the paper they cram 17k word into as-low-as-possible 30 dimensions space, for our data, we just cram words into 2D space.</p>
<div id="a3b20eab" class="cell" data-execution_count="7">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>C <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We can access the element of <code>torch.tensor</code> by:</p>
<div id="b5d48a60" class="cell" data-execution_count="8">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a>C[<span class="dv">5</span>] <span class="co"># can be integer, list [5, 6, 7], or torch.tensor([5,6,7])</span></span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="co"># &gt; tensor([1.0825, 0.2010])</span></span>
<span id="cb13-3"><a href="#cb13-3"></a></span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="co"># or</span></span>
<span id="cb13-5"><a href="#cb13-5"></a></span>
<span id="cb13-6"><a href="#cb13-6"></a>F.one_hot(torch.tensor(<span class="dv">5</span>), num_classes<span class="op">=</span><span class="dv">27</span>).<span class="bu">float</span>() <span class="op">@</span> C</span>
<span id="cb13-7"><a href="#cb13-7"></a><span class="co"># produce identical result, remember torch.tensor() infer long dtype int64, so we need to cast to float</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>tensor([-0.3284,  0.1093])</code></pre>
</div>
</div>
<p>‚Ä¶but in this lecture accessing by <code>C[5]</code> would be sufficient. We can even access using a more than 1 dimension tensor:</p>
<div id="ae9e8d11" class="cell" data-execution_count="9">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="bu">print</span>(C[X].shape)</span>
<span id="cb15-2"><a href="#cb15-2"></a><span class="bu">print</span>(X[<span class="dv">13</span>, <span class="dv">2</span>]) <span class="co"># integer 1 for 13rd index of 2nd dimension</span></span>
<span id="cb15-3"><a href="#cb15-3"></a><span class="bu">print</span>(C[X][<span class="dv">13</span>,<span class="dv">2</span>]) <span class="co"># will be the embedding of that element</span></span>
<span id="cb15-4"><a href="#cb15-4"></a><span class="bu">print</span>(C[<span class="dv">1</span>]) <span class="co"># so C[X][13,2] = C[1]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([32, 3, 2])
tensor(1)
tensor([-0.0513, -0.0358])
tensor([-0.0513, -0.0358])</code></pre>
</div>
</div>
<p>PyTorch is great for embedding words:</p>
<div id="ef143a99" class="cell" data-execution_count="10">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>emb <span class="op">=</span> C[X]</span>
<span id="cb17-2"><a href="#cb17-2"></a>emb.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>torch.Size([32, 3, 2])</code></pre>
</div>
</div>
<p>We‚Äôve compeleted the first layer with <code>context</code> and lookup table!</p>
</section>
<section id="implementing-the-hidden-layer-internals-of-torch.tensor-storage-views" class="level2">
<h2 class="anchored" data-anchor-id="implementing-the-hidden-layer-internals-of-torch.tensor-storage-views">implementing the hidden layer + internals of <code>torch.Tensor</code>: <code>storage</code>, <code>views</code></h2>
<div id="f35fce16" class="cell" data-execution_count="11">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="co"># input of tanh layer will be 6 (3 words in context x 2 dimensions)</span></span>
<span id="cb19-2"><a href="#cb19-2"></a><span class="co"># and the number or neurons is up to us - let's set it 100</span></span>
<span id="cb19-3"><a href="#cb19-3"></a>W1 <span class="op">=</span> torch.randn((<span class="dv">6</span>, <span class="dv">100</span>))</span>
<span id="cb19-4"><a href="#cb19-4"></a>b1 <span class="op">=</span> torch.randn(<span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now we need to do something like <code>emb @ W1 + b1</code>, but <code>emb.shape</code> is <code>[32, 3, 2]</code> and <code>W1.shape</code> is <code>[6, 100]</code>. We need to somehow concatnate/transform:</p>
<div id="48442ded" class="cell" data-execution_count="12">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a><span class="co"># emb[:, 0, :] is tensor for each input in the 3-words context, shape is [32, 2]</span></span>
<span id="cb20-2"><a href="#cb20-2"></a><span class="co"># cat 3 of them using the 2nd dimension (index 1) -&gt; so we set dim = 1</span></span>
<span id="cb20-3"><a href="#cb20-3"></a>torch.cat([emb[:, <span class="dv">0</span>, :], emb[:, <span class="dv">1</span>, :], emb[:, <span class="dv">2</span>, :]], dim<span class="op">=</span><span class="dv">1</span>).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>torch.Size([32, 6])</code></pre>
</div>
</div>
<p>However this code does not change dynamically when we change the block size. We will be using <code>torch.unbind()</code></p>
<div id="19191167" class="cell" data-execution_count="13">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="co"># this is good!</span></span>
<span id="cb22-2"><a href="#cb22-2"></a>torch.cat(torch.unbind(emb, <span class="dv">1</span>), <span class="dv">1</span>).shape</span>
<span id="cb22-3"><a href="#cb22-3"></a><span class="co"># new memory for storage is created, so it is not efficient</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>torch.Size([32, 6])</code></pre>
</div>
</div>
<p>This works, but we have a better and more efficient way to do this. Since:</p>
<ul>
<li>every <code>torch.Tensor</code> have <code>.storage()</code> which is one-dimensional vector tensor;</li>
<li>when we call <code>.view()</code>, we instruct how this vector tensor is interpreted;</li>
<li>no memory is being changed/copied/moved/or created. the storage is identical.</li>
</ul>
<p>Readmore: <a href="http://blog.ezyang.com/2019/05/pytorch-internals/" class="uri">http://blog.ezyang.com/2019/05/pytorch-internals/</a></p>
<p>So this hidden layer can be declared:</p>
<div id="0c7fcaff" class="cell" data-execution_count="14">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="co"># instead or 32 we can write emb.shape[1], or -1 (whatever fitted)</span></span>
<span id="cb24-2"><a href="#cb24-2"></a>h <span class="op">=</span> emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1</span>
<span id="cb24-3"><a href="#cb24-3"></a>h.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>torch.Size([32, 100])</code></pre>
</div>
</div>
<p>Notice that in the final operation, <code>b1</code> will be broadcasted.</p>
</section>
<section id="implementing-the-output-layer" class="level2">
<h2 class="anchored" data-anchor-id="implementing-the-output-layer">implementing the output layer</h2>
<div id="67f5f616" class="cell" data-execution_count="15">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a>W2 <span class="op">=</span> torch.randn((<span class="dv">100</span>, <span class="dv">27</span>))</span>
<span id="cb26-2"><a href="#cb26-2"></a>b2 <span class="op">=</span> torch.randn(<span class="dv">27</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In Deep Learning, people use <code>logits</code> for what raw output that range from negative inf to positive inf.</p>
<div id="18001789" class="cell" data-execution_count="16">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="8f5120ed" class="cell" data-execution_count="17">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a>logits.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>torch.Size([32, 27])</code></pre>
</div>
</div>
<p>Now we need to exponentiate it and get the probability.</p>
<div id="0085c1ff" class="cell" data-execution_count="18">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a>counts <span class="op">=</span> logits.exp()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="59f48cd4" class="cell" data-execution_count="19">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a>probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="afeb64b4" class="cell" data-execution_count="20">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a>probs.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>torch.Size([32, 27])</code></pre>
</div>
</div>
<p>Every row of <code>probs</code> has sum of 1.</p>
<div id="947d8586" class="cell" data-execution_count="21">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a>probs[<span class="dv">0</span>].<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>tensor(1.)</code></pre>
</div>
</div>
<p>And this is the <code>probs</code> of each ground true <code>Y</code> in current output of the neural nets:</p>
<div id="c7d10fe8" class="cell" data-execution_count="22">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a>probs[torch.arange(<span class="dv">32</span>), Y]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>tensor([4.0100e-01, 7.6919e-13, 2.8720e-17, 2.5311e-13, 5.2105e-15, 3.8929e-22,
        5.9739e-21, 1.2991e-12, 1.8388e-09, 1.1690e-01, 1.2747e-14, 9.2677e-15,
        3.9299e-18, 3.2560e-01, 6.4046e-16, 9.9983e-01, 4.8943e-01, 1.4460e-16,
        1.0446e-15, 1.8470e-19, 7.3648e-12, 2.2317e-10, 4.7360e-09, 8.1282e-25,
        9.6208e-12, 1.0319e-21, 2.7964e-07, 3.5155e-01, 3.8356e-29, 8.4523e-16,
        6.1166e-10, 4.6813e-04])</code></pre>
</div>
</div>
<p>Result is not good as we‚Äôve not trained the network yet!</p>
</section>
<section id="implementing-the-negative-log-likelihood-loss" class="level2">
<h2 class="anchored" data-anchor-id="implementing-the-negative-log-likelihood-loss">implementing the negative log likelihood loss</h2>
<p>We define the negative log likelihood as:</p>
<div id="2e0a0b15" class="cell" data-execution_count="23">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a>loss <span class="op">=</span> <span class="op">-</span> probs[torch.arange(<span class="dv">32</span>), Y].log().mean()</span>
<span id="cb38-2"><a href="#cb38-2"></a>loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>tensor(27.2166)</code></pre>
</div>
</div>
</section>
<section id="summary-of-the-full-network" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-the-full-network">summary of the full network</h2>
<p>Dataset:</p>
<div id="baf88b0f" class="cell" data-execution_count="24">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a>X.shape, Y.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>(torch.Size([32, 3]), torch.Size([32]))</code></pre>
</div>
</div>
<p>Neural network layers:</p>
<div id="08fde533" class="cell" data-execution_count="25">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb42-2"><a href="#cb42-2"></a>C <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">2</span>), generator<span class="op">=</span>g)</span>
<span id="cb42-3"><a href="#cb42-3"></a>W1 <span class="op">=</span> torch.randn((<span class="dv">6</span>, <span class="dv">100</span>), generator<span class="op">=</span>g)</span>
<span id="cb42-4"><a href="#cb42-4"></a>b1 <span class="op">=</span> torch.randn(<span class="dv">100</span>, generator<span class="op">=</span>g)</span>
<span id="cb42-5"><a href="#cb42-5"></a>W2 <span class="op">=</span> torch.randn((<span class="dv">100</span>, <span class="dv">27</span>), generator<span class="op">=</span>g)</span>
<span id="cb42-6"><a href="#cb42-6"></a>b2 <span class="op">=</span> torch.randn(<span class="dv">27</span>, generator<span class="op">=</span>g)</span>
<span id="cb42-7"><a href="#cb42-7"></a></span>
<span id="cb42-8"><a href="#cb42-8"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Size of the network:</p>
<div id="22918508" class="cell" data-execution_count="26">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a><span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>3481</code></pre>
</div>
</div>
<p>Constructing forward pass:</p>
<div id="2b9ae564" class="cell" data-execution_count="27">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1"></a>emb <span class="op">=</span> C[X] <span class="co"># (32, 3, 2)</span></span>
<span id="cb45-2"><a href="#cb45-2"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb45-3"><a href="#cb45-3"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb45-4"><a href="#cb45-4"></a>counts <span class="op">=</span> logits.exp()</span>
<span id="cb45-5"><a href="#cb45-5"></a>probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb45-6"><a href="#cb45-6"></a>loss <span class="op">=</span> <span class="op">-</span> probs[torch.arange(<span class="dv">32</span>), Y].log().mean()</span>
<span id="cb45-7"><a href="#cb45-7"></a>loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>tensor(17.7697)</code></pre>
</div>
</div>
</section>
</section>
<section id="part-2-intro-to-many-basics-of-machine-learning" class="level1">
<h1>PART 2: intro to many basics of machine learning</h1>
<section id="introducing-f.cross_entropy-and-why" class="level2">
<h2 class="anchored" data-anchor-id="introducing-f.cross_entropy-and-why">introducing <code>F.cross_entropy</code> and why</h2>
<p>We re-define loss:</p>
<div id="5cc515b5" class="cell" data-execution_count="28">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1"></a>loss <span class="op">=</span> F.cross_entropy(logits, Y)</span>
<span id="cb47-2"><a href="#cb47-2"></a>loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>tensor(17.7697)</code></pre>
</div>
</div>
<p>Why?</p>
<ul>
<li>Pytorch will create more intermediate tensor for every assignment: <code>counts</code>, <code>probs</code> -&gt; more memory;</li>
<li>Backward pass will be more optimized, because the expressions are much analytically and mathematically interpreted;</li>
<li>Cross entropy can be significantly &amp; numerically well behaved (for eg when we exponentiate a large positive number we got inf, PyTorch cross entropy will calculate the max of set and subtract it - which will not impact the exp result)</li>
</ul>
</section>
<section id="implementing-the-training-loop-overfitting-one-batch" class="level2">
<h2 class="anchored" data-anchor-id="implementing-the-training-loop-overfitting-one-batch">implementing the training loop, overfitting one batch</h2>
<p>So the forward pass, backward pass, and update loop will be implemented as below:</p>
<div id="3a424ca5" class="cell" data-execution_count="29">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb49-2"><a href="#cb49-2"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="320fca45" class="cell" data-execution_count="30">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb50-2"><a href="#cb50-2"></a>    <span class="co"># forward pass:</span></span>
<span id="cb50-3"><a href="#cb50-3"></a>    emb <span class="op">=</span> C[X] <span class="co"># (32, 3, 2)</span></span>
<span id="cb50-4"><a href="#cb50-4"></a>    h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb50-5"><a href="#cb50-5"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb50-6"><a href="#cb50-6"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Y)</span>
<span id="cb50-7"><a href="#cb50-7"></a>    <span class="bu">print</span>(loss.item())</span>
<span id="cb50-8"><a href="#cb50-8"></a>    <span class="co"># backward pass:</span></span>
<span id="cb50-9"><a href="#cb50-9"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb50-10"><a href="#cb50-10"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb50-11"><a href="#cb50-11"></a>    loss.backward()</span>
<span id="cb50-12"><a href="#cb50-12"></a>    <span class="co"># update</span></span>
<span id="cb50-13"><a href="#cb50-13"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb50-14"><a href="#cb50-14"></a>        p.data <span class="op">+=</span> <span class="op">-</span><span class="fl">0.1</span> <span class="op">*</span> p.grad</span>
<span id="cb50-15"><a href="#cb50-15"></a></span>
<span id="cb50-16"><a href="#cb50-16"></a><span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>17.76971435546875
13.656400680541992
11.298768997192383
9.452457427978516
7.984262466430664
6.891321182250977
6.100014686584473
5.452036380767822
4.898152828216553
4.4146647453308105
3.985848903656006
3.6028313636779785
3.262141466140747
2.961380958557129
2.6982972621917725
2.469712495803833
2.271660566329956
2.1012837886810303
1.9571772813796997
1.8374857902526855
1.7380963563919067
1.6535115242004395
1.579089879989624
1.5117661952972412
1.4496047496795654
1.3913118839263916
1.335992455482483
1.283052921295166
1.2321910858154297
1.18338143825531
1.1367989778518677
1.092664361000061
1.051092505455017
1.0120267868041992
0.9752704501152039
0.9405565857887268
0.9076124429702759
0.8761918544769287
0.8460890054702759
0.8171356916427612
0.7891990542411804
0.7621747255325317
0.735981285572052
0.7105579376220703
0.6858609914779663
0.6618649959564209
0.6385657787322998
0.6159818768501282
0.594165563583374
0.5732105374336243
0.553256094455719
0.5344882011413574
0.5171166658401489
0.5013314485549927
0.4872424602508545
0.47484052181243896
0.46399760246276855
0.45451420545578003
0.446170836687088
0.43876633048057556
0.4321330785751343
0.42613884806632996
0.42067962884902954
0.4156753420829773
0.4110613763332367
0.4067871570587158
0.40281057357788086
0.3990972638130188
0.395617812871933
0.3923477530479431
0.3892652690410614
0.38635194301605225
0.3835916221141815
0.3809700012207031
0.37847405672073364
0.37609291076660156
0.3738163411617279
0.3716348707675934
0.36954087018966675
0.36752671003341675
0.3655855655670166
0.36371129751205444
0.36189836263656616
0.3601415753364563
0.3584362268447876
0.35677796602249146
0.3551628589630127
0.35358697175979614
0.35204699635505676
0.35053980350494385
0.34906232357025146
0.3476122319698334
0.3461865782737732
0.3447835445404053
0.34340089559555054
0.34203681349754333
0.3406899571418762
0.3393586277961731
0.33804190158843994
0.33673879504203796
0.33673879504203796</code></pre>
</div>
</div>
<p>We are fitting 32 examples to a neural nets of 3481 params, so it‚Äôs super easy to be overfitting. We got a low final loss, but it would never be 0, because the output can varry for the same input, for eg, <code>...</code>.</p>
<div id="01a71cbd" class="cell" data-execution_count="31">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1"></a>logits.<span class="bu">max</span>(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>torch.return_types.max(
values=tensor([11.3379, 13.0894, 18.9434, 17.5786, 12.7205, 11.3379, 13.0234, 11.5993,
        13.5442, 15.5540, 12.5548, 17.6663, 11.3379, 12.6919, 13.9530, 17.0410,
        11.3379, 13.8052, 11.4075, 13.1371, 15.8441, 12.1391,  7.9167,  7.8798,
        13.8772, 11.3379, 13.0983, 13.6820, 11.2885, 14.2338, 15.6165, 12.0455],
       grad_fn=&lt;MaxBackward0&gt;),
indices=tensor([ 1, 13, 13,  1,  0,  1, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  1, 19,
         1,  2,  5, 12, 12,  1,  0,  1, 15, 16,  8,  9,  1,  0]))</code></pre>
</div>
</div>
</section>
<section id="training-on-the-full-dataset-minibatches" class="level2">
<h2 class="anchored" data-anchor-id="training-on-the-full-dataset-minibatches">training on the full dataset, minibatches</h2>
</section>
<section id="finding-a-good-initial-learning-rate" class="level2">
<h2 class="anchored" data-anchor-id="finding-a-good-initial-learning-rate">finding a good initial learning rate</h2>
</section>
<section id="splitting-up-the-dataset-into-trainvaltest-splits-and-why" class="level2">
<h2 class="anchored" data-anchor-id="splitting-up-the-dataset-into-trainvaltest-splits-and-why">splitting up the dataset into train/val/test splits and why</h2>
</section>
<section id="experiment-larger-hidden-layer" class="level2">
<h2 class="anchored" data-anchor-id="experiment-larger-hidden-layer">experiment: larger hidden layer</h2>
</section>
<section id="visualizing-the-character-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-the-character-embeddings">visualizing the character embeddings</h2>
</section>
<section id="experiment-larger-embedding-size" class="level2">
<h2 class="anchored" data-anchor-id="experiment-larger-embedding-size">experiment: larger embedding size</h2>
</section>
<section id="summary-of-our-final-code-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-our-final-code-conclusion">summary of our final code, conclusion</h2>
</section>
<section id="sampling-from-the-model" class="level2">
<h2 class="anchored" data-anchor-id="sampling-from-the-model">sampling from the model</h2>
</section>
<section id="google-collab-new-notebook-advertisement" class="level2">
<h2 class="anchored" data-anchor-id="google-collab-new-notebook-advertisement">google collab (new!!) notebook advertisement</h2>
</section>
</section>
<section id="resources" class="level1">
<h1>resources</h1>
<ol type="1">
<li><a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"><strong>A Neural Probabilistic Language Model</strong>, Bengio et al.&nbsp;(2003)</a></li>
<li><a href="https://www.youtube.com/watch?v=TCH_1BHY58I">Video lecturer</a></li>
<li><a href="https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part2_mlp.ipynb">Notebook</a></li>
<li><a href="https://github.com/karpathy/makemore"><code>makemore</code> on Github</a></li>
<li><a href="https://pytorch.org/docs/main/tensors.html"><code>torch.Tensor()</code> documentation</a></li>
</ol>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/lktuan\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb54" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb54-1"><a href="#cb54-1"></a><span class="co">---</span></span>
<span id="cb54-2"><a href="#cb54-2"></a><span class="an">title:</span><span class="co"> "NN-Z2H Lesson 3: Building makemore part 2 - MLP"</span></span>
<span id="cb54-3"><a href="#cb54-3"></a><span class="an">description:</span><span class="co"> "implement a multilayer perceptron (MLP) character-level language model, introduce model training, learning rate tuning, hyperparameters, evaluation, train/dev/test splits, under/overfitting, etc."</span></span>
<span id="cb54-4"><a href="#cb54-4"></a><span class="an">author:</span></span>
<span id="cb54-5"><a href="#cb54-5"></a><span class="co">  - name: "Tuan Le Khac"</span></span>
<span id="cb54-6"><a href="#cb54-6"></a><span class="co">    url: https://lktuan.github.io/</span></span>
<span id="cb54-7"><a href="#cb54-7"></a><span class="an">categories:</span><span class="co"> [til, python, andrej karpathy, nn-z2h, bigram, neural networks] </span></span>
<span id="cb54-8"><a href="#cb54-8"></a><span class="an">date:</span><span class="co"> 11-20-2024</span></span>
<span id="cb54-9"><a href="#cb54-9"></a><span class="an">date-modified:</span><span class="co"> 11-20-2024</span></span>
<span id="cb54-10"><a href="#cb54-10"></a><span class="an">image:</span><span class="co"> NLM_Bengio_etal.png</span></span>
<span id="cb54-11"><a href="#cb54-11"></a><span class="an">draft:</span><span class="co"> false</span></span>
<span id="cb54-12"><a href="#cb54-12"></a><span class="an">css:</span><span class="co"> html/styles.scss</span></span>
<span id="cb54-13"><a href="#cb54-13"></a><span class="an">fig-cap-location:</span><span class="co"> bottom</span></span>
<span id="cb54-14"><a href="#cb54-14"></a><span class="an">editor:</span><span class="co"> visual</span></span>
<span id="cb54-15"><a href="#cb54-15"></a><span class="an">format:</span></span>
<span id="cb54-16"><a href="#cb54-16"></a><span class="co">  html:</span></span>
<span id="cb54-17"><a href="#cb54-17"></a><span class="co">    code-overflow: wrap</span></span>
<span id="cb54-18"><a href="#cb54-18"></a><span class="co">    code-tools: true</span></span>
<span id="cb54-19"><a href="#cb54-19"></a><span class="co">    code-fold: show</span></span>
<span id="cb54-20"><a href="#cb54-20"></a><span class="co">    code-annotations: hover</span></span>
<span id="cb54-21"><a href="#cb54-21"></a><span class="co">---</span></span>
<span id="cb54-22"><a href="#cb54-22"></a></span>
<span id="cb54-23"><a href="#cb54-23"></a>In the previous lecture, we built a simple <span class="in">`bigram`</span> character-level language model, using 2 different approaches that are (1) count, and (2) 1 layer neural network. They produced the same (and both poor - since the context is 1 character only) result but the neural network option offers more flexibility so that we can complexify our model to get better performance.</span>
<span id="cb54-24"><a href="#cb54-24"></a></span>
<span id="cb54-25"><a href="#cb54-25"></a>In this lecture we are going to implement 20-years ago neural probabilistic language model by *Bengio et al. (2003)*.</span>
<span id="cb54-26"><a href="#cb54-26"></a></span>
<span id="cb54-27"><a href="#cb54-27"></a><span class="fu"># PART 1: intro to MLP</span></span>
<span id="cb54-28"><a href="#cb54-28"></a></span>
<span id="cb54-29"><a href="#cb54-29"></a><span class="fu">## Bengio et al. 2003 (MLP language model) paper walkthrough</span></span>
<span id="cb54-30"><a href="#cb54-30"></a></span>
<span id="cb54-31"><a href="#cb54-31"></a><span class="fu">### Summary</span></span>
<span id="cb54-32"><a href="#cb54-32"></a></span>
<span id="cb54-33"><a href="#cb54-33"></a>**Problem Statement**:</span>
<span id="cb54-34"><a href="#cb54-34"></a></span>
<span id="cb54-35"><a href="#cb54-35"></a><span class="ss">-   </span>Traditional n-gram language models suffer from the *curse of dimensionality*: they can't effectively generalize to word sequences not seen in training data;</span>
<span id="cb54-36"><a href="#cb54-36"></a><span class="ss">-   </span>The core issue is treating words as atomic units with no *inherent similarity* to each other;</span>
<span id="cb54-37"><a href="#cb54-37"></a><span class="ss">-   </span>For example, if we've seen "dog is eating" in training but never "cat is eating", n-gram models can't leverage the similarity between "dog" and "cat";</span>
<span id="cb54-38"><a href="#cb54-38"></a><span class="ss">-   </span>This leads to poor probability estimates for rare or unseen word sequences.</span>
<span id="cb54-39"><a href="#cb54-39"></a></span>
<span id="cb54-40"><a href="#cb54-40"></a>**Solution**:</span>
<span id="cb54-41"><a href="#cb54-41"></a></span>
<span id="cb54-42"><a href="#cb54-42"></a><span class="ss">-   </span>Learn a *distributed representation* (embedding) for each word in a continuous vector space where similar words are close to each other;</span>
<span id="cb54-43"><a href="#cb54-43"></a><span class="ss">-   </span>Use a neural network architecture with:</span>
<span id="cb54-44"><a href="#cb54-44"></a><span class="ss">    -   </span>Input layer: concatenated embeddings of n-1 previous words;</span>
<span id="cb54-45"><a href="#cb54-45"></a><span class="ss">    -   </span>Hidden layer: dense neural network with <span class="in">`tanh`</span> activation;</span>
<span id="cb54-46"><a href="#cb54-46"></a><span class="ss">    -   </span>Output layer: softmax over entire vocabulary to predict next word probability.</span>
<span id="cb54-47"><a href="#cb54-47"></a></span>
<span id="cb54-48"><a href="#cb54-48"></a>**The model simultaneously learns**:</span>
<span id="cb54-49"><a href="#cb54-49"></a></span>
<span id="cb54-50"><a href="#cb54-50"></a><span class="ss">-   </span>Word feature vectors (embeddings) that capture *semantic/syntactic word similarities*;</span>
<span id="cb54-51"><a href="#cb54-51"></a><span class="ss">-   </span>Neural network parameters that combine these features to estimate probability distributions.</span>
<span id="cb54-52"><a href="#cb54-52"></a></span>
<span id="cb54-53"><a href="#cb54-53"></a>**Key advantages**:</span>
<span id="cb54-54"><a href="#cb54-54"></a></span>
<span id="cb54-55"><a href="#cb54-55"></a><span class="ss">-   </span>Words with similar meanings get similar feature vectors, enabling better *generalization*;</span>
<span id="cb54-56"><a href="#cb54-56"></a><span class="ss">-   </span>The probability function is smooth with respect to word embeddings, so similar words yield *similar predictions*;</span>
<span id="cb54-57"><a href="#cb54-57"></a><span class="ss">-   </span>Can generalize to *unseen sequences* by leveraging learned word similarities.</span>
<span id="cb54-58"><a href="#cb54-58"></a></span>
<span id="cb54-59"><a href="#cb54-59"></a><span class="fu">### Methodology:</span></span>
<span id="cb54-60"><a href="#cb54-60"></a></span>
<span id="cb54-61"><a href="#cb54-61"></a><span class="ss">-   </span>Traditional Problem:</span>
<span id="cb54-62"><a href="#cb54-62"></a></span>
<span id="cb54-63"><a href="#cb54-63"></a><span class="ss">    -   </span>In n-gram models, each word sequence of length n is a separate parameter;</span>
<span id="cb54-64"><a href="#cb54-64"></a><span class="ss">    -   </span>For vocabulary size $|V|$, need $|V|^n$ parameters;</span>
<span id="cb54-65"><a href="#cb54-65"></a><span class="ss">    -   </span>Most sequences never appear in training, leading to poor generalization;</span>
<span id="cb54-66"><a href="#cb54-66"></a></span>
<span id="cb54-67"><a href="#cb54-67"></a><span class="ss">-   </span>Solution via **Distributed Representation**:</span>
<span id="cb54-68"><a href="#cb54-68"></a></span>
<span id="cb54-69"><a href="#cb54-69"></a><span class="ss">    -   </span>Each word mapped to a dense vector in $R^m$ (typically m=50-100);</span>
<span id="cb54-70"><a href="#cb54-70"></a><span class="ss">    -   </span>Similar words get similar vectors through training;</span>
<span id="cb54-71"><a href="#cb54-71"></a><span class="ss">    -   </span>Probability function is smooth w.r.t these vectors;</span>
<span id="cb54-72"><a href="#cb54-72"></a><span class="ss">    -   </span>Key benefit: If "dog" and "cat" have similar vectors, model can generalize from "dog is eating" to "cat is eating";</span>
<span id="cb54-73"><a href="#cb54-73"></a><span class="ss">    -   </span>Number of parameters reduces to $O(|V|√óm + m√óh + h√ó|V|)$, where $h$ is hidden layer size;</span>
<span id="cb54-74"><a href="#cb54-74"></a><span class="ss">    -   </span>This is much smaller than $|V|^n$ and allows better generalization;</span>
<span id="cb54-75"><a href="#cb54-75"></a></span>
<span id="cb54-76"><a href="#cb54-76"></a><span class="fu">### Neural architecture:</span></span>
<span id="cb54-77"><a href="#cb54-77"></a></span>
<span id="cb54-78"><a href="#cb54-78"></a>**Input Layer**:</span>
<span id="cb54-79"><a href="#cb54-79"></a></span>
<span id="cb54-80"><a href="#cb54-80"></a><span class="ss">-   </span>Takes $n-1$ previous words (context window);</span>
<span id="cb54-81"><a href="#cb54-81"></a><span class="ss">-   </span>Each word i mapped to vector $C(i) ‚àà R^m$ via lookup table;</span>
<span id="cb54-82"><a href="#cb54-82"></a><span class="ss">-   </span>Concatenates these vectors: $x = <span class="co">[</span><span class="ot">C(w‚Çú‚Çã‚Çô‚Çä‚ÇÅ), ..., C(w‚Çú‚Çã‚ÇÅ)</span><span class="co">]</span>$;</span>
<span id="cb54-83"><a href="#cb54-83"></a><span class="ss">-   </span>$x$ dimension is $(n-1)√óm$;</span>
<span id="cb54-84"><a href="#cb54-84"></a></span>
<span id="cb54-85"><a href="#cb54-85"></a>**Hidden Layer**:</span>
<span id="cb54-86"><a href="#cb54-86"></a></span>
<span id="cb54-87"><a href="#cb54-87"></a><span class="ss">-   </span>Dense layer with tanh activation;</span>
<span id="cb54-88"><a href="#cb54-88"></a><span class="ss">-   </span>Computation: $h = tanh(d + Hx)$;</span>
<span id="cb54-89"><a href="#cb54-89"></a><span class="ss">-   </span>$H$ is weight matrix, $d$ is bias vector;</span>
<span id="cb54-90"><a href="#cb54-90"></a><span class="ss">-   </span>Maps concatenated context to hidden representation;</span>
<span id="cb54-91"><a href="#cb54-91"></a></span>
<span id="cb54-92"><a href="#cb54-92"></a>**Output Layer**:</span>
<span id="cb54-93"><a href="#cb54-93"></a></span>
<span id="cb54-94"><a href="#cb54-94"></a><span class="ss">-   </span>Computes probability distribution over all words;</span>
<span id="cb54-95"><a href="#cb54-95"></a><span class="ss">-   </span>$y = b + Wx + Uh$;</span>
<span id="cb54-96"><a href="#cb54-96"></a><span class="ss">-   </span>Softmax activation: $P(w‚Çú|context) = exp(y·µ¢)/Œ£‚±ºexp(y‚±º)$;</span>
<span id="cb54-97"><a href="#cb54-97"></a><span class="ss">-   </span>$W$ provides "shortcut" connections from input to output;</span>
<span id="cb54-98"><a href="#cb54-98"></a><span class="ss">-   </span>Direct connection helps learn simpler patterns;</span>
<span id="cb54-99"><a href="#cb54-99"></a></span>
<span id="cb54-100"><a href="#cb54-100"></a>**Training**:</span>
<span id="cb54-101"><a href="#cb54-101"></a></span>
<span id="cb54-102"><a href="#cb54-102"></a><span class="ss">-   </span>Maximizes log-likelihood of training data;</span>
<span id="cb54-103"><a href="#cb54-103"></a><span class="ss">-   </span>Uses stochastic gradient descent;</span>
<span id="cb54-104"><a href="#cb54-104"></a><span class="ss">-   </span>Learns both word vectors $C(i)$ and neural network parameters $(H, d, W, U, b)$;</span>
<span id="cb54-105"><a href="#cb54-105"></a><span class="ss">-   </span>Word vectors capture similarities as they help predict similar contexts;</span>
<span id="cb54-106"><a href="#cb54-106"></a><span class="ss">-   </span>Can initialize word vectors randomly or with pretrained vectors.</span>
<span id="cb54-107"><a href="#cb54-107"></a></span>
<span id="cb54-108"><a href="#cb54-108"></a><span class="al">![Neural Language Model proposed by (Bengio et al., 2003). C(i) is the i th word embedding.](NLM_Bengio_etal.png)</span></span>
<span id="cb54-109"><a href="#cb54-109"></a></span>
<span id="cb54-110"><a href="#cb54-110"></a><span class="fu">## (re-)building our training dataset</span></span>
<span id="cb54-111"><a href="#cb54-111"></a></span>
<span id="cb54-112"><a href="#cb54-112"></a>Loading library, reading data, building dictionary:</span>
<span id="cb54-113"><a href="#cb54-113"></a></span>
<span id="cb54-116"><a href="#cb54-116"></a><span class="in">```{python}</span></span>
<span id="cb54-117"><a href="#cb54-117"></a><span class="im">import</span> torch</span>
<span id="cb54-118"><a href="#cb54-118"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb54-119"><a href="#cb54-119"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb54-120"><a href="#cb54-120"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb54-121"><a href="#cb54-121"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb54-122"><a href="#cb54-122"></a><span class="in">```</span></span>
<span id="cb54-123"><a href="#cb54-123"></a></span>
<span id="cb54-126"><a href="#cb54-126"></a><span class="in">```{python}</span></span>
<span id="cb54-127"><a href="#cb54-127"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb54-128"><a href="#cb54-128"></a></span>
<span id="cb54-129"><a href="#cb54-129"></a>url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt"</span></span>
<span id="cb54-130"><a href="#cb54-130"></a>words <span class="op">=</span> pd.read_csv(url, header<span class="op">=</span><span class="va">None</span>).iloc[:, <span class="dv">0</span>].tolist()</span>
<span id="cb54-131"><a href="#cb54-131"></a>words[:<span class="dv">8</span>]</span>
<span id="cb54-132"><a href="#cb54-132"></a><span class="in">```</span></span>
<span id="cb54-133"><a href="#cb54-133"></a></span>
<span id="cb54-136"><a href="#cb54-136"></a><span class="in">```{python}</span></span>
<span id="cb54-137"><a href="#cb54-137"></a><span class="bu">len</span>(words)</span>
<span id="cb54-138"><a href="#cb54-138"></a><span class="in">```</span></span>
<span id="cb54-139"><a href="#cb54-139"></a></span>
<span id="cb54-142"><a href="#cb54-142"></a><span class="in">```{python}</span></span>
<span id="cb54-143"><a href="#cb54-143"></a><span class="co"># build the vocabulary of characters and mapping to/from integer</span></span>
<span id="cb54-144"><a href="#cb54-144"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(<span class="st">''</span>.join(words))))</span>
<span id="cb54-145"><a href="#cb54-145"></a>stoi <span class="op">=</span> {s:i<span class="op">+</span><span class="dv">1</span> <span class="cf">for</span> i, s <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb54-146"><a href="#cb54-146"></a>stoi[<span class="st">'.'</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb54-147"><a href="#cb54-147"></a></span>
<span id="cb54-148"><a href="#cb54-148"></a>itos <span class="op">=</span> {i: s <span class="cf">for</span> s, i <span class="kw">in</span> stoi.items()}</span>
<span id="cb54-149"><a href="#cb54-149"></a>itos</span>
<span id="cb54-150"><a href="#cb54-150"></a><span class="in">```</span></span>
<span id="cb54-151"><a href="#cb54-151"></a></span>
<span id="cb54-152"><a href="#cb54-152"></a>Building the dataset:</span>
<span id="cb54-153"><a href="#cb54-153"></a></span>
<span id="cb54-156"><a href="#cb54-156"></a><span class="in">```{python}</span></span>
<span id="cb54-157"><a href="#cb54-157"></a>block_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># the context length: how many characters do we take to predict the next one?</span></span>
<span id="cb54-158"><a href="#cb54-158"></a>X, Y <span class="op">=</span> [], []</span>
<span id="cb54-159"><a href="#cb54-159"></a></span>
<span id="cb54-160"><a href="#cb54-160"></a><span class="cf">for</span> w <span class="kw">in</span> words[:<span class="dv">5</span>]:</span>
<span id="cb54-161"><a href="#cb54-161"></a>    <span class="bu">print</span>(w)</span>
<span id="cb54-162"><a href="#cb54-162"></a>    context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size <span class="co"># 0 so context will be padded by '.'</span></span>
<span id="cb54-163"><a href="#cb54-163"></a>    <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb54-164"><a href="#cb54-164"></a>        ix <span class="op">=</span> stoi[ch]</span>
<span id="cb54-165"><a href="#cb54-165"></a>        X.append(context)</span>
<span id="cb54-166"><a href="#cb54-166"></a>        Y.append(ix)</span>
<span id="cb54-167"><a href="#cb54-167"></a>        <span class="bu">print</span>(<span class="st">''</span>.join(itos[i] <span class="cf">for</span> i <span class="kw">in</span> context), <span class="st">'-----&gt;'</span>, itos[ix] )</span>
<span id="cb54-168"><a href="#cb54-168"></a>        context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix] <span class="co"># rolling to the next one</span></span>
<span id="cb54-169"><a href="#cb54-169"></a></span>
<span id="cb54-170"><a href="#cb54-170"></a>X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb54-171"><a href="#cb54-171"></a>Y <span class="op">=</span> torch.tensor(Y)</span>
<span id="cb54-172"><a href="#cb54-172"></a><span class="in">```</span></span>
<span id="cb54-173"><a href="#cb54-173"></a></span>
<span id="cb54-176"><a href="#cb54-176"></a><span class="in">```{python}</span></span>
<span id="cb54-177"><a href="#cb54-177"></a>X.shape, X.dtype, Y.shape, Y.dtype</span>
<span id="cb54-178"><a href="#cb54-178"></a><span class="in">```</span></span>
<span id="cb54-179"><a href="#cb54-179"></a></span>
<span id="cb54-180"><a href="#cb54-180"></a><span class="fu">## implementing the embedding lookup table</span></span>
<span id="cb54-181"><a href="#cb54-181"></a></span>
<span id="cb54-182"><a href="#cb54-182"></a>In the paper they cram 17k word into as-low-as-possible 30 dimensions space, for our data, we just cram words into 2D space.</span>
<span id="cb54-183"><a href="#cb54-183"></a></span>
<span id="cb54-186"><a href="#cb54-186"></a><span class="in">```{python}</span></span>
<span id="cb54-187"><a href="#cb54-187"></a>C <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">2</span>))</span>
<span id="cb54-188"><a href="#cb54-188"></a><span class="in">```</span></span>
<span id="cb54-189"><a href="#cb54-189"></a></span>
<span id="cb54-190"><a href="#cb54-190"></a>We can access the element of <span class="in">`torch.tensor`</span> by:</span>
<span id="cb54-191"><a href="#cb54-191"></a></span>
<span id="cb54-194"><a href="#cb54-194"></a><span class="in">```{python}</span></span>
<span id="cb54-195"><a href="#cb54-195"></a>C[<span class="dv">5</span>] <span class="co"># can be integer, list [5, 6, 7], or torch.tensor([5,6,7])</span></span>
<span id="cb54-196"><a href="#cb54-196"></a><span class="co"># &gt; tensor([1.0825, 0.2010])</span></span>
<span id="cb54-197"><a href="#cb54-197"></a></span>
<span id="cb54-198"><a href="#cb54-198"></a><span class="co"># or</span></span>
<span id="cb54-199"><a href="#cb54-199"></a></span>
<span id="cb54-200"><a href="#cb54-200"></a>F.one_hot(torch.tensor(<span class="dv">5</span>), num_classes<span class="op">=</span><span class="dv">27</span>).<span class="bu">float</span>() <span class="op">@</span> C</span>
<span id="cb54-201"><a href="#cb54-201"></a><span class="co"># produce identical result, remember torch.tensor() infer long dtype int64, so we need to cast to float</span></span>
<span id="cb54-202"><a href="#cb54-202"></a><span class="in">```</span></span>
<span id="cb54-203"><a href="#cb54-203"></a></span>
<span id="cb54-204"><a href="#cb54-204"></a>...but in this lecture accessing by <span class="in">`C[5]`</span> would be sufficient. We can even access using a more than 1 dimension tensor:</span>
<span id="cb54-205"><a href="#cb54-205"></a></span>
<span id="cb54-208"><a href="#cb54-208"></a><span class="in">```{python}</span></span>
<span id="cb54-209"><a href="#cb54-209"></a><span class="bu">print</span>(C[X].shape)</span>
<span id="cb54-210"><a href="#cb54-210"></a><span class="bu">print</span>(X[<span class="dv">13</span>, <span class="dv">2</span>]) <span class="co"># integer 1 for 13rd index of 2nd dimension</span></span>
<span id="cb54-211"><a href="#cb54-211"></a><span class="bu">print</span>(C[X][<span class="dv">13</span>,<span class="dv">2</span>]) <span class="co"># will be the embedding of that element</span></span>
<span id="cb54-212"><a href="#cb54-212"></a><span class="bu">print</span>(C[<span class="dv">1</span>]) <span class="co"># so C[X][13,2] = C[1]</span></span>
<span id="cb54-213"><a href="#cb54-213"></a><span class="in">```</span></span>
<span id="cb54-214"><a href="#cb54-214"></a></span>
<span id="cb54-215"><a href="#cb54-215"></a>PyTorch is great for embedding words:</span>
<span id="cb54-216"><a href="#cb54-216"></a></span>
<span id="cb54-219"><a href="#cb54-219"></a><span class="in">```{python}</span></span>
<span id="cb54-220"><a href="#cb54-220"></a>emb <span class="op">=</span> C[X]</span>
<span id="cb54-221"><a href="#cb54-221"></a>emb.shape</span>
<span id="cb54-222"><a href="#cb54-222"></a><span class="in">```</span></span>
<span id="cb54-223"><a href="#cb54-223"></a></span>
<span id="cb54-224"><a href="#cb54-224"></a>We've compeleted the first layer with <span class="in">`context`</span> and lookup table!</span>
<span id="cb54-225"><a href="#cb54-225"></a></span>
<span id="cb54-226"><a href="#cb54-226"></a><span class="fu">## implementing the hidden layer + internals of `torch.Tensor`: `storage`, `views`</span></span>
<span id="cb54-227"><a href="#cb54-227"></a></span>
<span id="cb54-230"><a href="#cb54-230"></a><span class="in">```{python}</span></span>
<span id="cb54-231"><a href="#cb54-231"></a><span class="co"># input of tanh layer will be 6 (3 words in context x 2 dimensions)</span></span>
<span id="cb54-232"><a href="#cb54-232"></a><span class="co"># and the number or neurons is up to us - let's set it 100</span></span>
<span id="cb54-233"><a href="#cb54-233"></a>W1 <span class="op">=</span> torch.randn((<span class="dv">6</span>, <span class="dv">100</span>))</span>
<span id="cb54-234"><a href="#cb54-234"></a>b1 <span class="op">=</span> torch.randn(<span class="dv">100</span>)</span>
<span id="cb54-235"><a href="#cb54-235"></a><span class="in">```</span></span>
<span id="cb54-236"><a href="#cb54-236"></a></span>
<span id="cb54-237"><a href="#cb54-237"></a>Now we need to do something like <span class="in">`emb @ W1 + b1`</span>, but <span class="in">`emb.shape`</span> is <span class="in">`[32, 3, 2]`</span> and <span class="in">`W1.shape`</span> is <span class="in">`[6, 100]`</span>. We need to somehow concatnate/transform:</span>
<span id="cb54-238"><a href="#cb54-238"></a></span>
<span id="cb54-241"><a href="#cb54-241"></a><span class="in">```{python}</span></span>
<span id="cb54-242"><a href="#cb54-242"></a><span class="co"># emb[:, 0, :] is tensor for each input in the 3-words context, shape is [32, 2]</span></span>
<span id="cb54-243"><a href="#cb54-243"></a><span class="co"># cat 3 of them using the 2nd dimension (index 1) -&gt; so we set dim = 1</span></span>
<span id="cb54-244"><a href="#cb54-244"></a>torch.cat([emb[:, <span class="dv">0</span>, :], emb[:, <span class="dv">1</span>, :], emb[:, <span class="dv">2</span>, :]], dim<span class="op">=</span><span class="dv">1</span>).shape</span>
<span id="cb54-245"><a href="#cb54-245"></a><span class="in">```</span></span>
<span id="cb54-246"><a href="#cb54-246"></a></span>
<span id="cb54-247"><a href="#cb54-247"></a>However this code does not change dynamically when we change the block size. We will be using <span class="in">`torch.unbind()`</span></span>
<span id="cb54-248"><a href="#cb54-248"></a></span>
<span id="cb54-251"><a href="#cb54-251"></a><span class="in">```{python}</span></span>
<span id="cb54-252"><a href="#cb54-252"></a><span class="co"># this is good!</span></span>
<span id="cb54-253"><a href="#cb54-253"></a>torch.cat(torch.unbind(emb, <span class="dv">1</span>), <span class="dv">1</span>).shape</span>
<span id="cb54-254"><a href="#cb54-254"></a><span class="co"># new memory for storage is created, so it is not efficient</span></span>
<span id="cb54-255"><a href="#cb54-255"></a><span class="in">```</span></span>
<span id="cb54-256"><a href="#cb54-256"></a></span>
<span id="cb54-257"><a href="#cb54-257"></a>This works, but we have a better and more efficient way to do this. Since:</span>
<span id="cb54-258"><a href="#cb54-258"></a></span>
<span id="cb54-259"><a href="#cb54-259"></a><span class="ss">- </span>every <span class="in">`torch.Tensor`</span> have <span class="in">`.storage()`</span> which is one-dimensional vector tensor;</span>
<span id="cb54-260"><a href="#cb54-260"></a><span class="ss">- </span>when we call <span class="in">`.view()`</span>, we instruct how this vector tensor is interpreted;</span>
<span id="cb54-261"><a href="#cb54-261"></a><span class="ss">- </span>no memory is being changed/copied/moved/or created. the storage is identical.</span>
<span id="cb54-262"><a href="#cb54-262"></a></span>
<span id="cb54-263"><a href="#cb54-263"></a>Readmore: <span class="ot">&lt;http://blog.ezyang.com/2019/05/pytorch-internals/&gt;</span></span>
<span id="cb54-264"><a href="#cb54-264"></a></span>
<span id="cb54-265"><a href="#cb54-265"></a>So this hidden layer can be declared:</span>
<span id="cb54-266"><a href="#cb54-266"></a></span>
<span id="cb54-269"><a href="#cb54-269"></a><span class="in">```{python}</span></span>
<span id="cb54-270"><a href="#cb54-270"></a><span class="co"># instead or 32 we can write emb.shape[1], or -1 (whatever fitted)</span></span>
<span id="cb54-271"><a href="#cb54-271"></a>h <span class="op">=</span> emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1</span>
<span id="cb54-272"><a href="#cb54-272"></a>h.shape</span>
<span id="cb54-273"><a href="#cb54-273"></a><span class="in">```</span></span>
<span id="cb54-274"><a href="#cb54-274"></a></span>
<span id="cb54-275"><a href="#cb54-275"></a>Notice that in the final operation, <span class="in">`b1`</span> will be broadcasted.</span>
<span id="cb54-276"><a href="#cb54-276"></a></span>
<span id="cb54-277"><a href="#cb54-277"></a><span class="fu">## implementing the output layer</span></span>
<span id="cb54-278"><a href="#cb54-278"></a></span>
<span id="cb54-281"><a href="#cb54-281"></a><span class="in">```{python}</span></span>
<span id="cb54-282"><a href="#cb54-282"></a>W2 <span class="op">=</span> torch.randn((<span class="dv">100</span>, <span class="dv">27</span>))</span>
<span id="cb54-283"><a href="#cb54-283"></a>b2 <span class="op">=</span> torch.randn(<span class="dv">27</span>)</span>
<span id="cb54-284"><a href="#cb54-284"></a><span class="in">```</span></span>
<span id="cb54-285"><a href="#cb54-285"></a></span>
<span id="cb54-286"><a href="#cb54-286"></a>In Deep Learning, people use <span class="in">`logits`</span> for what raw output that range from negative inf to positive inf.</span>
<span id="cb54-287"><a href="#cb54-287"></a></span>
<span id="cb54-290"><a href="#cb54-290"></a><span class="in">```{python}</span></span>
<span id="cb54-291"><a href="#cb54-291"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb54-292"><a href="#cb54-292"></a><span class="in">```</span></span>
<span id="cb54-293"><a href="#cb54-293"></a></span>
<span id="cb54-296"><a href="#cb54-296"></a><span class="in">```{python}</span></span>
<span id="cb54-297"><a href="#cb54-297"></a>logits.shape</span>
<span id="cb54-298"><a href="#cb54-298"></a><span class="in">```</span></span>
<span id="cb54-299"><a href="#cb54-299"></a></span>
<span id="cb54-300"><a href="#cb54-300"></a>Now we need to exponentiate it and get the probability.</span>
<span id="cb54-301"><a href="#cb54-301"></a></span>
<span id="cb54-304"><a href="#cb54-304"></a><span class="in">```{python}</span></span>
<span id="cb54-305"><a href="#cb54-305"></a>counts <span class="op">=</span> logits.exp()</span>
<span id="cb54-306"><a href="#cb54-306"></a><span class="in">```</span></span>
<span id="cb54-307"><a href="#cb54-307"></a></span>
<span id="cb54-310"><a href="#cb54-310"></a><span class="in">```{python}</span></span>
<span id="cb54-311"><a href="#cb54-311"></a>probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb54-312"><a href="#cb54-312"></a><span class="in">```</span></span>
<span id="cb54-313"><a href="#cb54-313"></a></span>
<span id="cb54-316"><a href="#cb54-316"></a><span class="in">```{python}</span></span>
<span id="cb54-317"><a href="#cb54-317"></a>probs.shape</span>
<span id="cb54-318"><a href="#cb54-318"></a><span class="in">```</span></span>
<span id="cb54-319"><a href="#cb54-319"></a></span>
<span id="cb54-320"><a href="#cb54-320"></a>Every row of <span class="in">`probs`</span> has sum of 1.</span>
<span id="cb54-321"><a href="#cb54-321"></a></span>
<span id="cb54-324"><a href="#cb54-324"></a><span class="in">```{python}</span></span>
<span id="cb54-325"><a href="#cb54-325"></a>probs[<span class="dv">0</span>].<span class="bu">sum</span>()</span>
<span id="cb54-326"><a href="#cb54-326"></a><span class="in">```</span></span>
<span id="cb54-327"><a href="#cb54-327"></a></span>
<span id="cb54-328"><a href="#cb54-328"></a>And this is the <span class="in">`probs`</span> of each ground true <span class="in">`Y`</span> in current output of the neural nets:</span>
<span id="cb54-329"><a href="#cb54-329"></a></span>
<span id="cb54-332"><a href="#cb54-332"></a><span class="in">```{python}</span></span>
<span id="cb54-333"><a href="#cb54-333"></a>probs[torch.arange(<span class="dv">32</span>), Y]</span>
<span id="cb54-334"><a href="#cb54-334"></a><span class="in">```</span></span>
<span id="cb54-335"><a href="#cb54-335"></a></span>
<span id="cb54-336"><a href="#cb54-336"></a>Result is not good as we've not trained the network yet!</span>
<span id="cb54-337"><a href="#cb54-337"></a></span>
<span id="cb54-338"><a href="#cb54-338"></a><span class="fu">## implementing the negative log likelihood loss</span></span>
<span id="cb54-339"><a href="#cb54-339"></a></span>
<span id="cb54-340"><a href="#cb54-340"></a>We define the negative log likelihood as:</span>
<span id="cb54-341"><a href="#cb54-341"></a></span>
<span id="cb54-344"><a href="#cb54-344"></a><span class="in">```{python}</span></span>
<span id="cb54-345"><a href="#cb54-345"></a>loss <span class="op">=</span> <span class="op">-</span> probs[torch.arange(<span class="dv">32</span>), Y].log().mean()</span>
<span id="cb54-346"><a href="#cb54-346"></a>loss</span>
<span id="cb54-347"><a href="#cb54-347"></a><span class="in">```</span></span>
<span id="cb54-348"><a href="#cb54-348"></a></span>
<span id="cb54-349"><a href="#cb54-349"></a><span class="fu">## summary of the full network</span></span>
<span id="cb54-350"><a href="#cb54-350"></a></span>
<span id="cb54-351"><a href="#cb54-351"></a>Dataset:</span>
<span id="cb54-352"><a href="#cb54-352"></a></span>
<span id="cb54-355"><a href="#cb54-355"></a><span class="in">```{python}</span></span>
<span id="cb54-356"><a href="#cb54-356"></a>X.shape, Y.shape</span>
<span id="cb54-357"><a href="#cb54-357"></a><span class="in">```</span></span>
<span id="cb54-358"><a href="#cb54-358"></a></span>
<span id="cb54-359"><a href="#cb54-359"></a>Neural network layers:</span>
<span id="cb54-360"><a href="#cb54-360"></a></span>
<span id="cb54-363"><a href="#cb54-363"></a><span class="in">```{python}</span></span>
<span id="cb54-364"><a href="#cb54-364"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb54-365"><a href="#cb54-365"></a>C <span class="op">=</span> torch.randn((<span class="dv">27</span>, <span class="dv">2</span>), generator<span class="op">=</span>g)</span>
<span id="cb54-366"><a href="#cb54-366"></a>W1 <span class="op">=</span> torch.randn((<span class="dv">6</span>, <span class="dv">100</span>), generator<span class="op">=</span>g)</span>
<span id="cb54-367"><a href="#cb54-367"></a>b1 <span class="op">=</span> torch.randn(<span class="dv">100</span>, generator<span class="op">=</span>g)</span>
<span id="cb54-368"><a href="#cb54-368"></a>W2 <span class="op">=</span> torch.randn((<span class="dv">100</span>, <span class="dv">27</span>), generator<span class="op">=</span>g)</span>
<span id="cb54-369"><a href="#cb54-369"></a>b2 <span class="op">=</span> torch.randn(<span class="dv">27</span>, generator<span class="op">=</span>g)</span>
<span id="cb54-370"><a href="#cb54-370"></a></span>
<span id="cb54-371"><a href="#cb54-371"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb54-372"><a href="#cb54-372"></a><span class="in">```</span></span>
<span id="cb54-373"><a href="#cb54-373"></a></span>
<span id="cb54-374"><a href="#cb54-374"></a>Size of the network:</span>
<span id="cb54-375"><a href="#cb54-375"></a></span>
<span id="cb54-378"><a href="#cb54-378"></a><span class="in">```{python}</span></span>
<span id="cb54-379"><a href="#cb54-379"></a><span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters)</span>
<span id="cb54-380"><a href="#cb54-380"></a><span class="in">```</span></span>
<span id="cb54-381"><a href="#cb54-381"></a></span>
<span id="cb54-382"><a href="#cb54-382"></a>Constructing forward pass:</span>
<span id="cb54-383"><a href="#cb54-383"></a></span>
<span id="cb54-386"><a href="#cb54-386"></a><span class="in">```{python}</span></span>
<span id="cb54-387"><a href="#cb54-387"></a>emb <span class="op">=</span> C[X] <span class="co"># (32, 3, 2)</span></span>
<span id="cb54-388"><a href="#cb54-388"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb54-389"><a href="#cb54-389"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb54-390"><a href="#cb54-390"></a>counts <span class="op">=</span> logits.exp()</span>
<span id="cb54-391"><a href="#cb54-391"></a>probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb54-392"><a href="#cb54-392"></a>loss <span class="op">=</span> <span class="op">-</span> probs[torch.arange(<span class="dv">32</span>), Y].log().mean()</span>
<span id="cb54-393"><a href="#cb54-393"></a>loss</span>
<span id="cb54-394"><a href="#cb54-394"></a><span class="in">```</span></span>
<span id="cb54-395"><a href="#cb54-395"></a></span>
<span id="cb54-396"><a href="#cb54-396"></a><span class="fu"># PART 2: intro to many basics of machine learning</span></span>
<span id="cb54-397"><a href="#cb54-397"></a></span>
<span id="cb54-398"><a href="#cb54-398"></a><span class="fu">## introducing `F.cross_entropy` and why</span></span>
<span id="cb54-399"><a href="#cb54-399"></a></span>
<span id="cb54-400"><a href="#cb54-400"></a>We re-define loss:</span>
<span id="cb54-401"><a href="#cb54-401"></a></span>
<span id="cb54-404"><a href="#cb54-404"></a><span class="in">```{python}</span></span>
<span id="cb54-405"><a href="#cb54-405"></a>loss <span class="op">=</span> F.cross_entropy(logits, Y)</span>
<span id="cb54-406"><a href="#cb54-406"></a>loss</span>
<span id="cb54-407"><a href="#cb54-407"></a><span class="in">```</span></span>
<span id="cb54-408"><a href="#cb54-408"></a></span>
<span id="cb54-409"><a href="#cb54-409"></a>Why?</span>
<span id="cb54-410"><a href="#cb54-410"></a></span>
<span id="cb54-411"><a href="#cb54-411"></a><span class="ss">- </span>Pytorch will create more intermediate tensor for every assignment: <span class="in">`counts`</span>, <span class="in">`probs`</span> -&gt; more memory;</span>
<span id="cb54-412"><a href="#cb54-412"></a><span class="ss">- </span>Backward pass will be more optimized, because the expressions are much analytically and mathematically interpreted;</span>
<span id="cb54-413"><a href="#cb54-413"></a><span class="ss">- </span>Cross entropy can be significantly &amp; numerically well behaved (for eg when we exponentiate a large positive number we got inf, PyTorch cross entropy will calculate the max of set and subtract it - which will not impact the exp result)</span>
<span id="cb54-414"><a href="#cb54-414"></a></span>
<span id="cb54-415"><a href="#cb54-415"></a><span class="fu">## implementing the training loop, overfitting one batch</span></span>
<span id="cb54-416"><a href="#cb54-416"></a></span>
<span id="cb54-417"><a href="#cb54-417"></a>So the forward pass, backward pass, and update loop will be implemented as below:</span>
<span id="cb54-418"><a href="#cb54-418"></a></span>
<span id="cb54-421"><a href="#cb54-421"></a><span class="in">```{python}</span></span>
<span id="cb54-422"><a href="#cb54-422"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb54-423"><a href="#cb54-423"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb54-424"><a href="#cb54-424"></a><span class="in">```</span></span>
<span id="cb54-425"><a href="#cb54-425"></a></span>
<span id="cb54-428"><a href="#cb54-428"></a><span class="in">```{python}</span></span>
<span id="cb54-429"><a href="#cb54-429"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb54-430"><a href="#cb54-430"></a>    <span class="co"># forward pass:</span></span>
<span id="cb54-431"><a href="#cb54-431"></a>    emb <span class="op">=</span> C[X] <span class="co"># (32, 3, 2)</span></span>
<span id="cb54-432"><a href="#cb54-432"></a>    h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>) <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (32, 100)</span></span>
<span id="cb54-433"><a href="#cb54-433"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (32, 27)</span></span>
<span id="cb54-434"><a href="#cb54-434"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Y)</span>
<span id="cb54-435"><a href="#cb54-435"></a>    <span class="bu">print</span>(loss.item())</span>
<span id="cb54-436"><a href="#cb54-436"></a>    <span class="co"># backward pass:</span></span>
<span id="cb54-437"><a href="#cb54-437"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb54-438"><a href="#cb54-438"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb54-439"><a href="#cb54-439"></a>    loss.backward()</span>
<span id="cb54-440"><a href="#cb54-440"></a>    <span class="co"># update</span></span>
<span id="cb54-441"><a href="#cb54-441"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb54-442"><a href="#cb54-442"></a>        p.data <span class="op">+=</span> <span class="op">-</span><span class="fl">0.1</span> <span class="op">*</span> p.grad</span>
<span id="cb54-443"><a href="#cb54-443"></a></span>
<span id="cb54-444"><a href="#cb54-444"></a><span class="bu">print</span>(loss.item())</span>
<span id="cb54-445"><a href="#cb54-445"></a><span class="in">```</span></span>
<span id="cb54-446"><a href="#cb54-446"></a></span>
<span id="cb54-447"><a href="#cb54-447"></a>We are fitting 32 examples to a neural nets of 3481 params, so it's super easy to be overfitting. We got a low final loss, but it would never be 0, because the output can varry for the same input, for eg, <span class="in">`...`</span>.</span>
<span id="cb54-448"><a href="#cb54-448"></a></span>
<span id="cb54-451"><a href="#cb54-451"></a><span class="in">```{python}</span></span>
<span id="cb54-452"><a href="#cb54-452"></a>logits.<span class="bu">max</span>(<span class="dv">1</span>)</span>
<span id="cb54-453"><a href="#cb54-453"></a><span class="in">```</span></span>
<span id="cb54-454"><a href="#cb54-454"></a></span>
<span id="cb54-455"><a href="#cb54-455"></a><span class="fu">## training on the full dataset, minibatches</span></span>
<span id="cb54-456"><a href="#cb54-456"></a></span>
<span id="cb54-457"><a href="#cb54-457"></a><span class="fu">## finding a good initial learning rate</span></span>
<span id="cb54-458"><a href="#cb54-458"></a></span>
<span id="cb54-459"><a href="#cb54-459"></a><span class="fu">## splitting up the dataset into train/val/test splits and why</span></span>
<span id="cb54-460"><a href="#cb54-460"></a></span>
<span id="cb54-461"><a href="#cb54-461"></a><span class="fu">## experiment: larger hidden layer</span></span>
<span id="cb54-462"><a href="#cb54-462"></a></span>
<span id="cb54-463"><a href="#cb54-463"></a><span class="fu">## visualizing the character embeddings</span></span>
<span id="cb54-464"><a href="#cb54-464"></a></span>
<span id="cb54-465"><a href="#cb54-465"></a><span class="fu">## experiment: larger embedding size</span></span>
<span id="cb54-466"><a href="#cb54-466"></a></span>
<span id="cb54-467"><a href="#cb54-467"></a><span class="fu">## summary of our final code, conclusion</span></span>
<span id="cb54-468"><a href="#cb54-468"></a></span>
<span id="cb54-469"><a href="#cb54-469"></a><span class="fu">## sampling from the model</span></span>
<span id="cb54-470"><a href="#cb54-470"></a></span>
<span id="cb54-471"><a href="#cb54-471"></a><span class="fu">## google collab (new!!) notebook advertisement</span></span>
<span id="cb54-472"><a href="#cb54-472"></a></span>
<span id="cb54-473"><a href="#cb54-473"></a><span class="fu"># resources</span></span>
<span id="cb54-474"><a href="#cb54-474"></a></span>
<span id="cb54-475"><a href="#cb54-475"></a><span class="ss">1.  </span><span class="co">[</span><span class="ot">**A Neural Probabilistic Language Model**, Bengio et al. (2003)</span><span class="co">](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)</span></span>
<span id="cb54-476"><a href="#cb54-476"></a><span class="ss">2.  </span><span class="co">[</span><span class="ot">Video lecturer</span><span class="co">](https://www.youtube.com/watch?v=TCH_1BHY58I)</span></span>
<span id="cb54-477"><a href="#cb54-477"></a><span class="ss">3.  </span><span class="co">[</span><span class="ot">Notebook</span><span class="co">](https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part2_mlp.ipynb)</span></span>
<span id="cb54-478"><a href="#cb54-478"></a><span class="ss">4.  </span><span class="co">[</span><span class="ot">`makemore` on Github</span><span class="co">](https://github.com/karpathy/makemore)</span></span>
<span id="cb54-479"><a href="#cb54-479"></a><span class="ss">5.  </span><span class="co">[</span><span class="ot">`torch.Tensor()` documentation</span><span class="co">](https://pytorch.org/docs/main/tensors.html)</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block"><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"><i class="fa-brands fa-creative-commons" aria-label="creative-commons"></i></a> 2023-2024 Le Khac Tuan</span></p>
</div>   
    <div class="nav-footer-center">
<p><span class="faux-block">Designed with <i class="fa-solid fa-heart" aria-label="heart"></i></span></p>
</div>
    <div class="nav-footer-right">
<p><span class="faux-block">Made with <a href="https://quarto.org/">Quarto</a></span> <span class="faux-block"><a href="https://github.com/lktuan/lktuan.github.io">View the source at <i class="fa-brands fa-github" aria-label="github"></i> GitHub</a></span></p>
</div>
  </div>
</footer>




</body></html>