<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tuan Le Khac">
<meta name="dcterms.date" content="2024-11-26">
<meta name="description" content="This is Tuan’s blog">

<title>Le Khac Tuan - NN-Z2H Lesson 4: Building makemore part 3 - Activations &amp; Gradients, BatchNorm</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../img/rocket_1613268.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Le Khac Tuan</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../curriculum/index.html"> 
<span class="menu-text">Curriculum</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../学汉语的日记.html"> 
<span class="menu-text">学汉语的日记</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../jiu_jitsu_journal/index.html"> 
<span class="menu-text">Jiu Jitsu Journal</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lktuan"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/tuanlekhac/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.facebook.com/toilatuan.lk/"> <i class="bi bi-facebook" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/Halle4231"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:tuan.lekhac0905@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">NN-Z2H Lesson 4: Building makemore part 3 - Activations &amp; Gradients, BatchNorm</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                  <div>
        <div class="description">
          dive into the internals of MLPs, scrutinize the statistics of the forward pass activations, backward pass gradients, understand the health of your deep network, introduce batch normalization
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">til</div>
                <div class="quarto-category">python</div>
                <div class="quarto-category">andrej karpathy</div>
                <div class="quarto-category">nn-z2h</div>
                <div class="quarto-category">neural networks</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://lktuan.github.io/">Tuan Le Khac</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 26, 2024</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">November 29, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#part-1-intro" id="toc-part-1-intro" class="nav-link active" data-scroll-target="#part-1-intro">Part 1: intro</a>
  <ul class="collapse">
  <li><a href="#starter-code" id="toc-starter-code" class="nav-link" data-scroll-target="#starter-code">starter code</a></li>
  <li><a href="#fixing-the-initial-loss" id="toc-fixing-the-initial-loss" class="nav-link" data-scroll-target="#fixing-the-initial-loss">fixing the initial loss</a></li>
  <li><a href="#fixing-the-saturated-tanh" id="toc-fixing-the-saturated-tanh" class="nav-link" data-scroll-target="#fixing-the-saturated-tanh">fixing the saturated <code>tanh</code></a></li>
  <li><a href="#calculating-the-init-scale-kaiming-init" id="toc-calculating-the-init-scale-kaiming-init" class="nav-link" data-scroll-target="#calculating-the-init-scale-kaiming-init">calculating the init scale: “Kaiming init”</a></li>
  <li><a href="#batch-normalization" id="toc-batch-normalization" class="nav-link" data-scroll-target="#batch-normalization">batch normalization</a></li>
  <li><a href="#real-example-resnet50-walkthrough" id="toc-real-example-resnet50-walkthrough" class="nav-link" data-scroll-target="#real-example-resnet50-walkthrough">real example: <code>resnet50</code> walkthrough</a></li>
  <li><a href="#summary-of-the-lecture" id="toc-summary-of-the-lecture" class="nav-link" data-scroll-target="#summary-of-the-lecture">summary of the lecture</a>
  <ul class="collapse">
  <li><a href="#loss-logs" id="toc-loss-logs" class="nav-link" data-scroll-target="#loss-logs">loss logs</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#part-2-pytorch-ifying-the-code-and-train-a-deeper-network" id="toc-part-2-pytorch-ifying-the-code-and-train-a-deeper-network" class="nav-link" data-scroll-target="#part-2-pytorch-ifying-the-code-and-train-a-deeper-network">Part 2: PyTorch-ifying the code, and train a deeper network</a>
  <ul class="collapse">
  <li><a href="#viz-1-forward-pass-activations-statistics" id="toc-viz-1-forward-pass-activations-statistics" class="nav-link" data-scroll-target="#viz-1-forward-pass-activations-statistics">viz #1: forward pass activations statistics</a></li>
  <li><a href="#viz-2-backward-pass-gradient-statistics" id="toc-viz-2-backward-pass-gradient-statistics" class="nav-link" data-scroll-target="#viz-2-backward-pass-gradient-statistics">viz #2: backward pass gradient statistics</a></li>
  <li><a href="#the-fully-linear-case-of-no-non-linearity" id="toc-the-fully-linear-case-of-no-non-linearity" class="nav-link" data-scroll-target="#the-fully-linear-case-of-no-non-linearity">the fully linear case of no non-linearity</a></li>
  <li><a href="#viz-3-parameter-activation-and-gradient-statistics" id="toc-viz-3-parameter-activation-and-gradient-statistics" class="nav-link" data-scroll-target="#viz-3-parameter-activation-and-gradient-statistics">viz #3: parameter activation and gradient statistics</a></li>
  <li><a href="#viz-4-update-data-ratio-over-time" id="toc-viz-4-update-data-ratio-over-time" class="nav-link" data-scroll-target="#viz-4-update-data-ratio-over-time">viz #4: update data ratio over time</a></li>
  <li><a href="#bringing-back-batchnorm-looking-at-the-visualizations" id="toc-bringing-back-batchnorm-looking-at-the-visualizations" class="nav-link" data-scroll-target="#bringing-back-batchnorm-looking-at-the-visualizations">bringing back batchnorm, looking at the visualizations</a></li>
  <li><a href="#summary-of-the-lecture-for-real-this-time" id="toc-summary-of-the-lecture-for-real-this-time" class="nav-link" data-scroll-target="#summary-of-the-lecture-for-real-this-time">summary of the lecture for real this time</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises:</a></li>
  </ul></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">resources:</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="callout callout-style-default callout-important callout-titled" title="This is not orginal content!">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This is not orginal content!
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is my study notes / codes along with Andrej Karpathy’s “<a href="https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Neural Networks: Zero to Hero</a>” series.</p>
</div>
</div>
<p>We want to stay a bit longer with the MLPs, to have more concrete intuitive of the <strong>activations</strong> in the neural nets and <strong>gradients</strong> that flowing backwards. It’s good to learn about the development history of these architectures. Since Recurrent Neural Network (RNN), they are although very <em>expressive</em> but not easily <em>optimizable</em> with current gradient techniques we have so far. Let’s get started!</p>
<section id="part-1-intro" class="level1">
<h1>Part 1: intro</h1>
<section id="starter-code" class="level2">
<h2 class="anchored" data-anchor-id="starter-code">starter code</h2>
<div id="56ff6a7c" class="cell" data-execution_count="1">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="2f02400f" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2"></a></span>
<span id="cb2-3"><a href="#cb2-3"></a>url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt"</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>words <span class="op">=</span> pd.read_csv(url, header<span class="op">=</span><span class="va">None</span>).iloc[:, <span class="dv">0</span>].tolist()</span>
<span id="cb2-5"><a href="#cb2-5"></a>words[:<span class="dv">8</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']</code></pre>
</div>
</div>
<div id="f548b189" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="bu">len</span>(words)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>32033</code></pre>
</div>
</div>
<div id="f3c3b0fa" class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># build the vocabulary of characters and mapping to/from integer</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(<span class="st">''</span>.join(words))))</span>
<span id="cb6-3"><a href="#cb6-3"></a>stoi <span class="op">=</span> {s:i<span class="op">+</span><span class="dv">1</span> <span class="cf">for</span> i, s <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb6-4"><a href="#cb6-4"></a>stoi[<span class="st">'.'</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-5"><a href="#cb6-5"></a>itos <span class="op">=</span> {i: s <span class="cf">for</span> s, i <span class="kw">in</span> stoi.items()}</span>
<span id="cb6-6"><a href="#cb6-6"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(itos)</span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="bu">print</span>(itos)</span>
<span id="cb6-8"><a href="#cb6-8"></a><span class="bu">print</span>(vocab_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}
27</code></pre>
</div>
</div>
<div id="9e1be96b" class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>block_size <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="co"># build the dataset</span></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="kw">def</span> buid_dataset(words):</span>
<span id="cb8-4"><a href="#cb8-4"></a>    X, Y <span class="op">=</span> [], []</span>
<span id="cb8-5"><a href="#cb8-5"></a></span>
<span id="cb8-6"><a href="#cb8-6"></a>    <span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb8-7"><a href="#cb8-7"></a>        context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size</span>
<span id="cb8-8"><a href="#cb8-8"></a>        <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb8-9"><a href="#cb8-9"></a>            ix <span class="op">=</span> stoi[ch]</span>
<span id="cb8-10"><a href="#cb8-10"></a>            X.append(context)</span>
<span id="cb8-11"><a href="#cb8-11"></a>            Y.append(ix)</span>
<span id="cb8-12"><a href="#cb8-12"></a>            context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix]</span>
<span id="cb8-13"><a href="#cb8-13"></a></span>
<span id="cb8-14"><a href="#cb8-14"></a>    X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb8-15"><a href="#cb8-15"></a>    Y <span class="op">=</span> torch.tensor(Y)</span>
<span id="cb8-16"><a href="#cb8-16"></a>    <span class="bu">print</span>(X.shape, Y.shape)</span>
<span id="cb8-17"><a href="#cb8-17"></a>    <span class="cf">return</span> X, Y</span>
<span id="cb8-18"><a href="#cb8-18"></a></span>
<span id="cb8-19"><a href="#cb8-19"></a><span class="im">import</span> random</span>
<span id="cb8-20"><a href="#cb8-20"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb8-21"><a href="#cb8-21"></a>random.shuffle(words)</span>
<span id="cb8-22"><a href="#cb8-22"></a>n1 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb8-23"><a href="#cb8-23"></a>n2 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb8-24"><a href="#cb8-24"></a></span>
<span id="cb8-25"><a href="#cb8-25"></a>Xtr, Ytr <span class="op">=</span> buid_dataset(words[:n1])        <span class="co"># 80#</span></span>
<span id="cb8-26"><a href="#cb8-26"></a>Xdev, Ydev <span class="op">=</span> buid_dataset(words[n1:n2])    <span class="co"># 10%</span></span>
<span id="cb8-27"><a href="#cb8-27"></a>Xte, Yte <span class="op">=</span> buid_dataset(words[n2:])        <span class="co"># 10%</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([182625, 3]) torch.Size([182625])
torch.Size([22655, 3]) torch.Size([22655])
torch.Size([22866, 3]) torch.Size([22866])</code></pre>
</div>
</div>
<div id="06cb447b" class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># MLP revisited</span></span>
<span id="cb10-2"><a href="#cb10-2"></a>n_emb <span class="op">=</span> <span class="dv">10</span> <span class="co"># no of dimensions of the embedding space.</span></span>
<span id="cb10-3"><a href="#cb10-3"></a>n_hidden <span class="op">=</span> <span class="dv">200</span> <span class="co"># size of the hidden - tanh layer</span></span>
<span id="cb10-4"><a href="#cb10-4"></a></span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="co"># Lookup table - 10 dimensional space</span></span>
<span id="cb10-6"><a href="#cb10-6"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>) <span class="co"># for reproductivity</span></span>
<span id="cb10-7"><a href="#cb10-7"></a>C <span class="op">=</span> torch.randn((vocab_size, n_emb),                  generator<span class="op">=</span>g)</span>
<span id="cb10-8"><a href="#cb10-8"></a></span>
<span id="cb10-9"><a href="#cb10-9"></a><span class="co"># Layer 1 - tanh - 300 neurons</span></span>
<span id="cb10-10"><a href="#cb10-10"></a>W1 <span class="op">=</span> torch.randn((block_size <span class="op">*</span> n_emb, n_hidden),      generator<span class="op">=</span>g)</span>
<span id="cb10-11"><a href="#cb10-11"></a>b1 <span class="op">=</span> torch.randn(n_hidden,                            generator<span class="op">=</span>g)</span>
<span id="cb10-12"><a href="#cb10-12"></a></span>
<span id="cb10-13"><a href="#cb10-13"></a><span class="co"># Layer 2 - softmax</span></span>
<span id="cb10-14"><a href="#cb10-14"></a>W2 <span class="op">=</span> torch.randn((n_hidden, vocab_size),              generator<span class="op">=</span>g)</span>
<span id="cb10-15"><a href="#cb10-15"></a>b2 <span class="op">=</span> torch.randn(vocab_size,                          generator<span class="op">=</span>g)</span>
<span id="cb10-16"><a href="#cb10-16"></a></span>
<span id="cb10-17"><a href="#cb10-17"></a><span class="co"># All params</span></span>
<span id="cb10-18"><a href="#cb10-18"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb10-19"><a href="#cb10-19"></a><span class="bu">print</span>(<span class="st">"No of params: "</span>, <span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span>
<span id="cb10-20"><a href="#cb10-20"></a></span>
<span id="cb10-21"><a href="#cb10-21"></a><span class="co"># Pre-training</span></span>
<span id="cb10-22"><a href="#cb10-22"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb10-23"><a href="#cb10-23"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>No of params:  11897</code></pre>
</div>
</div>
<div id="e65c93ea" class="cell" data-execution_count="7">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="co"># Optimization</span></span>
<span id="cb12-2"><a href="#cb12-2"></a>max_steps <span class="op">=</span> <span class="dv">50_000</span> <span class="co">#200_000</span></span>
<span id="cb12-3"><a href="#cb12-3"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb12-4"><a href="#cb12-4"></a></span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="co"># Stats holders</span></span>
<span id="cb12-6"><a href="#cb12-6"></a>lossi <span class="op">=</span> []</span>
<span id="cb12-7"><a href="#cb12-7"></a></span>
<span id="cb12-8"><a href="#cb12-8"></a><span class="co"># Training on Xtr, Ytr</span></span>
<span id="cb12-9"><a href="#cb12-9"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb12-10"><a href="#cb12-10"></a></span>
<span id="cb12-11"><a href="#cb12-11"></a>    <span class="co"># minibatch construct      </span></span>
<span id="cb12-12"><a href="#cb12-12"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,)) </span>
<span id="cb12-13"><a href="#cb12-13"></a>    Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix] <span class="co"># batch X, Y</span></span>
<span id="cb12-14"><a href="#cb12-14"></a></span>
<span id="cb12-15"><a href="#cb12-15"></a>    <span class="co"># forward pass:</span></span>
<span id="cb12-16"><a href="#cb12-16"></a>    emb <span class="op">=</span> C[Xb] <span class="co"># embed the characters into vectors   </span></span>
<span id="cb12-17"><a href="#cb12-17"></a>    emb_cat <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate the vectors</span></span>
<span id="cb12-18"><a href="#cb12-18"></a>    h_pre_act <span class="op">=</span> emb_cat <span class="op">@</span> W1 <span class="op">+</span> b1 <span class="co"># hidden layer pre-activation</span></span>
<span id="cb12-19"><a href="#cb12-19"></a>    h <span class="op">=</span> torch.tanh(h_pre_act) <span class="co"># hidden layer</span></span>
<span id="cb12-20"><a href="#cb12-20"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># output layer</span></span>
<span id="cb12-21"><a href="#cb12-21"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Yb) <span class="co"># loss function</span></span>
<span id="cb12-22"><a href="#cb12-22"></a></span>
<span id="cb12-23"><a href="#cb12-23"></a>    <span class="co"># backward pass:</span></span>
<span id="cb12-24"><a href="#cb12-24"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb12-25"><a href="#cb12-25"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb12-26"><a href="#cb12-26"></a>    loss.backward()</span>
<span id="cb12-27"><a href="#cb12-27"></a></span>
<span id="cb12-28"><a href="#cb12-28"></a>    <span class="co"># update</span></span>
<span id="cb12-29"><a href="#cb12-29"></a>    lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;=</span> max_steps <span class="op">/</span> <span class="dv">2</span> <span class="cf">else</span> <span class="fl">0.01</span> <span class="co"># step learning rate decay</span></span>
<span id="cb12-30"><a href="#cb12-30"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb12-31"><a href="#cb12-31"></a>        p.data <span class="op">+=</span> <span class="op">-</span> lr <span class="op">*</span> p.grad</span>
<span id="cb12-32"><a href="#cb12-32"></a></span>
<span id="cb12-33"><a href="#cb12-33"></a>    <span class="co"># track stats</span></span>
<span id="cb12-34"><a href="#cb12-34"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="co"># print once every while</span></span>
<span id="cb12-35"><a href="#cb12-35"></a>      <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">:7d}</span><span class="ss">/</span><span class="sc">{</span>max_steps<span class="sc">:7d}</span><span class="ss">: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb12-36"><a href="#cb12-36"></a>    lossi.append(loss.log10().item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>      0/  50000: 22.5552
  10000/  50000: 2.3148
  20000/  50000: 2.1559
  30000/  50000: 2.3941
  40000/  50000: 2.2245</code></pre>
</div>
</div>
<div id="ac4de265" class="cell" data-execution_count="8">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>plt.plot(lossi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-9-output-1.png" width="571" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="b12a3f1f" class="cell" data-execution_count="9">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="at">@torch.no_grad</span>() <span class="co"># disables gradient tracking</span></span>
<span id="cb15-2"><a href="#cb15-2"></a><span class="kw">def</span> split_loss(split: <span class="bu">str</span>):</span>
<span id="cb15-3"><a href="#cb15-3"></a>  x, y <span class="op">=</span> {</span>
<span id="cb15-4"><a href="#cb15-4"></a>    <span class="st">'train'</span>: (Xtr, Ytr),</span>
<span id="cb15-5"><a href="#cb15-5"></a>    <span class="st">'val'</span>: (Xdev, Ydev),</span>
<span id="cb15-6"><a href="#cb15-6"></a>    <span class="st">'test'</span>: (Xte, Yte)</span>
<span id="cb15-7"><a href="#cb15-7"></a>  }[split]</span>
<span id="cb15-8"><a href="#cb15-8"></a>  emb <span class="op">=</span> C[x] <span class="co"># (N, block_size, n_emb)</span></span>
<span id="cb15-9"><a href="#cb15-9"></a>  emb_cat <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate into (N, block_size * n_emb)</span></span>
<span id="cb15-10"><a href="#cb15-10"></a>  h <span class="op">=</span> torch.tanh(emb_cat <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (N, n_hidden)</span></span>
<span id="cb15-11"><a href="#cb15-11"></a>  logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (N, vocab_size)</span></span>
<span id="cb15-12"><a href="#cb15-12"></a>  loss <span class="op">=</span> F.cross_entropy(logits, y) <span class="co"># loss function</span></span>
<span id="cb15-13"><a href="#cb15-13"></a>  <span class="bu">print</span>(split, loss.item())</span>
<span id="cb15-14"><a href="#cb15-14"></a></span>
<span id="cb15-15"><a href="#cb15-15"></a>split_loss(<span class="st">'train'</span>)</span>
<span id="cb15-16"><a href="#cb15-16"></a>split_loss(<span class="st">'val'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>train 2.22798752784729
val 2.250197410583496</code></pre>
</div>
</div>
<div id="72b69df6" class="cell" data-execution_count="10">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="co"># sample from the model</span></span>
<span id="cb17-2"><a href="#cb17-2"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span> <span class="op">+</span> <span class="dv">10</span>)</span>
<span id="cb17-3"><a href="#cb17-3"></a></span>
<span id="cb17-4"><a href="#cb17-4"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb17-5"><a href="#cb17-5"></a>    </span>
<span id="cb17-6"><a href="#cb17-6"></a>    out <span class="op">=</span> []</span>
<span id="cb17-7"><a href="#cb17-7"></a>    context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size <span class="co"># initialize with all ...</span></span>
<span id="cb17-8"><a href="#cb17-8"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb17-9"><a href="#cb17-9"></a>      <span class="co"># forward pass the neural net</span></span>
<span id="cb17-10"><a href="#cb17-10"></a>      emb <span class="op">=</span> C[torch.tensor([context])] <span class="co"># (1,block_size,n_embd)</span></span>
<span id="cb17-11"><a href="#cb17-11"></a>      h <span class="op">=</span> torch.tanh(emb.view(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">@</span> W1 <span class="op">+</span> b1)</span>
<span id="cb17-12"><a href="#cb17-12"></a>      logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb17-13"><a href="#cb17-13"></a>      probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb17-14"><a href="#cb17-14"></a>      <span class="co"># sample from the distribution</span></span>
<span id="cb17-15"><a href="#cb17-15"></a>      ix <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>, generator<span class="op">=</span>g).item()</span>
<span id="cb17-16"><a href="#cb17-16"></a>      <span class="co"># shift the context window and track the samples</span></span>
<span id="cb17-17"><a href="#cb17-17"></a>      context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix]</span>
<span id="cb17-18"><a href="#cb17-18"></a>      out.append(ix)</span>
<span id="cb17-19"><a href="#cb17-19"></a>      <span class="co"># if we sample the special '.' token, break</span></span>
<span id="cb17-20"><a href="#cb17-20"></a>      <span class="cf">if</span> ix <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb17-21"><a href="#cb17-21"></a>        <span class="cf">break</span></span>
<span id="cb17-22"><a href="#cb17-22"></a>    </span>
<span id="cb17-23"><a href="#cb17-23"></a>    <span class="bu">print</span>(<span class="st">''</span>.join(itos[i] <span class="cf">for</span> i <span class="kw">in</span> out)) <span class="co"># decode and print the generated word</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>moraagmyaz.
seel.
npyn.
alarethastendrarg.
aderedieliighlynnelle.
elieananaraelyn.
malara.
noshabergihimies.
kindreelle.
jeberorius.
kynd.
riyah.
faeha.
kaysh.
samyah.
hil.
salynnsti.
zakel.
juren.
cresti.</code></pre>
</div>
</div>
<p>Okay so now our network has multiple things wrong at the initialization, let’s list down below. The final code will be presented in the end of part 1, with <code># 👈</code> for lines that had been added / modified. The right code cell below re-initializes states at the beginning of network’s parameter (in my notebook, it’s rendered <strong>linearly</strong>!).</p>
<div id="a8e6bffa" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>n_emb <span class="op">=</span> <span class="dv">10</span> <span class="co"># no of dimensions of the embedding space.</span></span>
<span id="cb19-2"><a href="#cb19-2"></a>n_hidden <span class="op">=</span> <span class="dv">200</span> <span class="co"># size of the hidden - tanh layer</span></span>
<span id="cb19-3"><a href="#cb19-3"></a><span class="co"># Lookup table - 10 dimensional space</span></span>
<span id="cb19-4"><a href="#cb19-4"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>) <span class="co"># for reproductivity</span></span>
<span id="cb19-5"><a href="#cb19-5"></a>C <span class="op">=</span> torch.randn((vocab_size, n_emb),                  generator<span class="op">=</span>g)</span>
<span id="cb19-6"><a href="#cb19-6"></a><span class="co"># Layer 1 - tanh - 300 neurons</span></span>
<span id="cb19-7"><a href="#cb19-7"></a>W1 <span class="op">=</span> torch.randn((block_size <span class="op">*</span> n_emb, n_hidden),      generator<span class="op">=</span>g)</span>
<span id="cb19-8"><a href="#cb19-8"></a>b1 <span class="op">=</span> torch.randn(n_hidden,                            generator<span class="op">=</span>g)</span>
<span id="cb19-9"><a href="#cb19-9"></a><span class="co"># Layer 2 - softmax</span></span>
<span id="cb19-10"><a href="#cb19-10"></a>W2 <span class="op">=</span> torch.randn((n_hidden, vocab_size),              generator<span class="op">=</span>g)</span>
<span id="cb19-11"><a href="#cb19-11"></a>b2 <span class="op">=</span> torch.randn(vocab_size,                          generator<span class="op">=</span>g)</span>
<span id="cb19-12"><a href="#cb19-12"></a><span class="co"># All params</span></span>
<span id="cb19-13"><a href="#cb19-13"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb19-14"><a href="#cb19-14"></a><span class="co"># Pre-training</span></span>
<span id="cb19-15"><a href="#cb19-15"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb19-16"><a href="#cb19-16"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb19-17"><a href="#cb19-17"></a><span class="co"># Optimization</span></span>
<span id="cb19-18"><a href="#cb19-18"></a>max_steps <span class="op">=</span> <span class="dv">50_000</span> <span class="co">#200_000</span></span>
<span id="cb19-19"><a href="#cb19-19"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb19-20"><a href="#cb19-20"></a><span class="co"># Training on Xtr, Ytr</span></span>
<span id="cb19-21"><a href="#cb19-21"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb19-22"><a href="#cb19-22"></a>    <span class="co"># minibatch construct      </span></span>
<span id="cb19-23"><a href="#cb19-23"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,)) </span>
<span id="cb19-24"><a href="#cb19-24"></a>    Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix] <span class="co"># batch X, Y</span></span>
<span id="cb19-25"><a href="#cb19-25"></a>    <span class="co"># forward pass:</span></span>
<span id="cb19-26"><a href="#cb19-26"></a>    emb <span class="op">=</span> C[Xb] <span class="co"># embed the characters into vectors   </span></span>
<span id="cb19-27"><a href="#cb19-27"></a>    emb_cat <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate the vectors</span></span>
<span id="cb19-28"><a href="#cb19-28"></a>    h_pre_act <span class="op">=</span> emb_cat <span class="op">@</span> W1 <span class="op">+</span> b1 <span class="co"># hidden layer pre-activation</span></span>
<span id="cb19-29"><a href="#cb19-29"></a>    h <span class="op">=</span> torch.tanh(h_pre_act) <span class="co"># hidden layer</span></span>
<span id="cb19-30"><a href="#cb19-30"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># output layer</span></span>
<span id="cb19-31"><a href="#cb19-31"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Yb) <span class="co"># loss function</span></span>
<span id="cb19-32"><a href="#cb19-32"></a></span>
<span id="cb19-33"><a href="#cb19-33"></a>    <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="fixing-the-initial-loss" class="level2">
<h2 class="anchored" data-anchor-id="fixing-the-initial-loss">fixing the initial loss</h2>
<p>We can see at the <code>step = 0</code>, the loss was <code>27</code> and after some <code>k</code>s training loops it decreased to <code>1</code> or <code>2</code>. It extremely high at the begining. In practice, we should give the network somehow the expectation we want when generating a character after some characters (<code>3</code>).</p>
<div id="e977daf8" class="cell" data-execution_count="12">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>loss.item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>24.27707862854004</code></pre>
</div>
</div>
<p>In this case, without training yet, we expect all <code>27</code> characters’ possibilities to be equal (<code>1 / 27.0</code>) ~ <strong>uniform distribution</strong>, so the loss ~ negative log likelihood would be:</p>
<div id="f8423883" class="cell" data-execution_count="13">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="op">-</span> torch.tensor(<span class="dv">1</span> <span class="op">/</span> <span class="fl">27.0</span>).log()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>tensor(3.2958)</code></pre>
</div>
</div>
<p>It’s far lower than <code>27</code>, we say that the network is <strong>confidently wrong</strong>. Andrej demonstrated by another simple 5 elements tensor and showed that the loss is lowest when all elements are equal.</p>
<p>We want the <code>logits</code> to be low entropy as possible (but not equal to <code>0</code>, which will be showed later), we added multipliers <code>0.01</code> to <code>W2</code>, and <code>0</code> to <code>b2</code>. We got the loss to be <code>3.xx</code> at the beginning.</p>
<div id="0ce2432b" class="cell" data-execution_count="14">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="co"># Layer 2 - softmax</span></span>
<span id="cb24-2"><a href="#cb24-2"></a>W2 <span class="op">=</span> torch.randn((n_hidden, vocab_size),              generator<span class="op">=</span>g) <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb24-3"><a href="#cb24-3"></a>b2 <span class="op">=</span> torch.randn(vocab_size,                          generator<span class="op">=</span>g) <span class="op">*</span> <span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now re-train the model and we will notice the the <code>lossi</code> will not look like the <em>hookey stick</em> anymore! Morever the final loss on train set and dev set is better!</p>
</section>
<section id="fixing-the-saturated-tanh" class="level2">
<h2 class="anchored" data-anchor-id="fixing-the-saturated-tanh">fixing the saturated <code>tanh</code></h2>
<p>The <code>logits</code> are now okay, the next problem is about the <code>h</code> - the activations of the hidden states! It’s hard to see but in the output of code cell below, there are too many values of <code>1</code> and <code>-1</code> in this tensor.</p>
<div id="2fb2a237" class="cell" data-execution_count="15">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a>h</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>tensor([[-0.9999, -0.7462, -0.9995,  ...,  0.6402, -1.0000, -0.9974],
        [-0.9983, -1.0000, -1.0000,  ...,  1.0000, -1.0000, -1.0000],
        [-0.9793, -0.9999, -1.0000,  ...,  0.7836, -0.7058,  0.2913],
        ...,
        [-1.0000, -0.9995, -0.9891,  ...,  0.9995, -0.8793,  0.9375],
        [-0.9997, -0.9674, -1.0000,  ..., -0.8636, -0.0804,  0.7250],
        [ 0.9988,  1.0000,  0.9998,  ..., -1.0000,  0.9901,  0.9985]],
       grad_fn=&lt;TanhBackward0&gt;)</code></pre>
</div>
</div>
<p>Recall that <code>tanh</code> is activation function that squashing arbitrary numbers to the range <code>[-1:1]</code>. Let’s visualize the distribution of <code>h</code>.</p>
<div id="6f7b0e2e" class="cell" data-execution_count="16">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a>plt.hist(h.view(<span class="op">-</span><span class="dv">1</span>).tolist(), <span class="dv">50</span>)<span class="op">;</span> <span class="co"># the ";" removes the presenting of data in code-block's output</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-17-output-1.png" width="583" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Most of them were distributed to the extreme values <code>-1</code> and <code>1</code>. Now come to the <code>h_pre_act</code>, we can see a <strong>flat-tails distribution</strong> from <code>-15</code> to <code>15</code>.</p>
<div id="8d15022a" class="cell" data-execution_count="17">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a>plt.hist(h_pre_act.view(<span class="op">-</span><span class="dv">1</span>).tolist(), <span class="dv">50</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-18-output-1.png" width="575" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Looking back to how we implemented <code>tanh</code> in <code>micrograd</code> (which is mathematically the same with <code>PyTorch</code>), we’re multiplying the forward node’s gradient with <code>(1 - t**2)</code>, which <code>t</code> is local <code>tanh</code>. When <code>tanh</code> is near <code>-1</code> or <code>1</code>, this is close to <code>0</code>, we are <strong>killing the gradients</strong>. We are stopping the backpropagation through this <code>tanh</code> unit.</p>
<div id="4f627b75" class="cell" data-execution_count="18">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a>...</span>
<span id="cb29-2"><a href="#cb29-2"></a>    <span class="kw">def</span> tanh(<span class="va">self</span>):</span>
<span id="cb29-3"><a href="#cb29-3"></a>        x <span class="op">=</span> <span class="va">self</span>.data</span>
<span id="cb29-4"><a href="#cb29-4"></a>        t <span class="op">=</span> (math.exp(<span class="dv">2</span><span class="op">*</span>x) <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> (math.exp(<span class="dv">2</span><span class="op">*</span>x) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb29-5"><a href="#cb29-5"></a>        out <span class="op">=</span> Value(t, (<span class="va">self</span>, ), <span class="st">'tanh'</span>)</span>
<span id="cb29-6"><a href="#cb29-6"></a></span>
<span id="cb29-7"><a href="#cb29-7"></a>        <span class="kw">def</span> _backward():</span>
<span id="cb29-8"><a href="#cb29-8"></a>            <span class="va">self</span>.grad <span class="op">+=</span> (<span class="dv">1</span> <span class="op">-</span> t<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> out.grad</span>
<span id="cb29-9"><a href="#cb29-9"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="cb29-10"><a href="#cb29-10"></a>        <span class="cf">return</span> out</span>
<span id="cb29-11"><a href="#cb29-11"></a>...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>When the gradients become zero, the previous nodes’ gradients will be <strong>vanishing</strong>. We call this <strong>saturated <code>tanh</code></strong>, this leads to <strong>dead neurons</strong> ~ always off and because the gradient is zero then they will never be turned on, and happens for other activations as well: <code>sigmoid</code>, <code>ReLU</code>, etc (but less significant on <code>Leaky ReLU</code> or <code>ELU</code>). The network is not learning!</p>
<p>The same with <code>logits</code>, now we want <code>h</code> to be more near zero, we add multipliers to the <code>W1</code> and <code>b1</code>:</p>
<div id="561c7af5" class="cell" data-execution_count="19">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="co"># Layer 1 - tanh - 300 neurons</span></span>
<span id="cb30-2"><a href="#cb30-2"></a>W1 <span class="op">=</span> torch.randn((block_size <span class="op">*</span> n_emb, n_hidden),      generator<span class="op">=</span>g)  <span class="op">*</span> <span class="fl">0.2</span></span>
<span id="cb30-3"><a href="#cb30-3"></a>b1 <span class="op">=</span> torch.randn(n_hidden,                            generator<span class="op">=</span>g) <span class="op">*</span> <span class="fl">0.01</span> <span class="co"># keep a little bit entropy, </span></span>
<span id="cb30-4"><a href="#cb30-4"></a><span class="co"># It's okay to initialize the b1 to zero but AK found emperically this will enhance the optimiaztion</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We can see now less peak distribution of <code>h</code>:</p>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tanh_0.2mult.png" class="img-fluid figure-img"></p>
<figcaption><code>tanh</code></figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="pre_act_tanh_0.2mult.png" class="img-fluid figure-img"></p>
<figcaption>pre-activation <code>tanh</code></figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="calculating-the-init-scale-kaiming-init" class="level2">
<h2 class="anchored" data-anchor-id="calculating-the-init-scale-kaiming-init">calculating the init scale: “Kaiming init”</h2>
<p>Now let’s look to the number <code>0.02</code>, in practice no one will set it manually. Let’s look into the example below to see how parameters of Gaussian Distribution of <code>y</code> differ from <code>x</code> when multiplying by <code>W</code>.</p>
<p>The question is how we set the <code>W</code> to preserve the Gaussian Distribution of X. Emperical researches found out that the multiplier to <code>W</code> should be square root of the “fan in”, in this case is <code>10^0.5</code>.</p>
<div id="04c0a932" class="cell" data-execution_count="20">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">1000</span>, <span class="dv">10</span>)</span>
<span id="cb31-2"><a href="#cb31-2"></a>W <span class="op">=</span> torch.randn(<span class="dv">10</span>, <span class="dv">200</span>)</span>
<span id="cb31-3"><a href="#cb31-3"></a>y <span class="op">=</span> x <span class="op">@</span> W</span>
<span id="cb31-4"><a href="#cb31-4"></a></span>
<span id="cb31-5"><a href="#cb31-5"></a>W1 <span class="op">=</span> torch.randn(<span class="dv">10</span>, <span class="dv">200</span>) <span class="op">/</span> <span class="dv">10</span><span class="op">**</span><span class="fl">0.5</span></span>
<span id="cb31-6"><a href="#cb31-6"></a>y1 <span class="op">=</span> x <span class="op">@</span> W1</span>
<span id="cb31-7"><a href="#cb31-7"></a><span class="bu">print</span>(x.mean(), x.std())</span>
<span id="cb31-8"><a href="#cb31-8"></a><span class="bu">print</span>(y.mean(), y.std())</span>
<span id="cb31-9"><a href="#cb31-9"></a><span class="bu">print</span>(y1.mean(), y1.std())</span>
<span id="cb31-10"><a href="#cb31-10"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">5</span>))</span>
<span id="cb31-11"><a href="#cb31-11"></a>plt.subplot(<span class="dv">131</span>).set_title(<span class="st">"Input X"</span>)</span>
<span id="cb31-12"><a href="#cb31-12"></a>plt.hist(x.view(<span class="op">-</span><span class="dv">1</span>).tolist(), <span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>)<span class="op">;</span></span>
<span id="cb31-13"><a href="#cb31-13"></a>plt.subplot(<span class="dv">132</span>).set_title(<span class="st">"Initial output y, expanded by W"</span>)</span>
<span id="cb31-14"><a href="#cb31-14"></a>plt.hist(y.view(<span class="op">-</span><span class="dv">1</span>).tolist(), <span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>)<span class="op">;</span></span>
<span id="cb31-15"><a href="#cb31-15"></a>plt.subplot(<span class="dv">133</span>).set_title(<span class="st">"y1, preserve the X's Gaussian Dist"</span>)</span>
<span id="cb31-16"><a href="#cb31-16"></a>plt.hist(y1.view(<span class="op">-</span><span class="dv">1</span>).tolist(), <span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(-0.0039) tensor(1.0023)
tensor(-0.0044) tensor(3.1220)
tensor(-0.0002) tensor(1.0091)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-21-output-2.png" width="1546" height="431" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Please investigate more here:</p>
<ol type="1">
<li>Kaiming et al.&nbsp;paper: <a href="https://arxiv.org/abs/1502.01852" class="uri">https://arxiv.org/abs/1502.01852</a></li>
<li>Implementation in <code>Pytorch</code>: <a href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_" class="uri">https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_</a></li>
</ol>
<p>It’s recommended in Kaiming paper to use a <strong>gain</strong> multiplier base on nonlinearity/activation function (<a href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.calculate_gain">here</a>), for <code>tanh</code> it’s <code>5/3</code>. We endup modified the initialization of <code>W1</code> with:</p>
<div id="af695900" class="cell" data-execution_count="21">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a>W1 <span class="op">=</span> torch.randn((block_size <span class="op">*</span> n_emb, n_hidden),      generator<span class="op">=</span>g)  <span class="op">*</span> (<span class="dv">5</span><span class="op">/</span><span class="dv">3</span>) <span class="op">/</span> ((block_size <span class="op">*</span> n_emb)<span class="op">**</span><span class="fl">0.5</span>) <span class="co"># * 0.2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In this case is roughly <code>0.3</code>, re-train and although the loss only improved so insignificant (because previously we set it to be <code>0.2</code> - very close), but we’ve parameterized this hyper-constant.</p>
</section>
<section id="batch-normalization" class="level2">
<h2 class="anchored" data-anchor-id="batch-normalization">batch normalization</h2>
<p>As discussed before, we dont want the <code>h_pre_act</code> to be way too small (~is not doing anything) or too large (~saturated), we want it to just roughly follow the standardized Gaussian Distribution (ie. mean equal to 0, std equal to 1).</p>
<p>We’ve done it at the initialization, <em>why don’t we just normalize the hidden states to be unit Gaussian</em>? in batch normalization, this can be achieved by 4 steps, demonstrated with our case:</p>
<div id="6c69275c" class="cell" data-execution_count="22">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a><span class="co"># 1. mini-batch mean</span></span>
<span id="cb34-2"><a href="#cb34-2"></a>hpa_mean <span class="op">=</span> h_pre_act.mean(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-3"><a href="#cb34-3"></a><span class="co"># 2. mini-batch variance / standard deviation</span></span>
<span id="cb34-4"><a href="#cb34-4"></a>hpa_std <span class="op">=</span> h_pre_act.std(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-5"><a href="#cb34-5"></a><span class="co"># 3. normalize</span></span>
<span id="cb34-6"><a href="#cb34-6"></a>h_pre_act <span class="op">=</span> (h_pre_act <span class="op">-</span> hpa_mean) <span class="op">/</span> hpa_std</span>
<span id="cb34-7"><a href="#cb34-7"></a><span class="co"># 4. scale and shift</span></span>
<span id="cb34-8"><a href="#cb34-8"></a><span class="co"># multiply by a "gain" then "shift" it with a bias</span></span>
<span id="cb34-9"><a href="#cb34-9"></a>bngain <span class="op">=</span> torch.ones((<span class="dv">1</span>, n_hidden))</span>
<span id="cb34-10"><a href="#cb34-10"></a>bnbias <span class="op">=</span> torch.zeros((<span class="dv">1</span>, n_hidden))</span>
<span id="cb34-11"><a href="#cb34-11"></a>h_pre_act <span class="op">=</span> bngain <span class="op">*</span> h_pre_act <span class="op">+</span> bnbias</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We modified our code accordingly and re-run the code, actually this time the model did not improve much. Because actually this is very <strong>simple and shallow</strong> neural network. We also notice that the training loop now is slower than before, because the calculation volumn is bigger. Batch Normalization also unexpectedly comes up with a side effect, the forward and backward pass of any input now also depend on the mini-batch, not just itself (because of <code>mean()</code>/<code>std()</code>). This effect is suprisingly a good thing and acts as a <strong>regularizer</strong>.</p>
<p>There are also non-coupling regularizers such as: Linear Normalization, Layer Normalization, Group Normalization.</p>
<p>One othering to consider is in the deployment/testing phase, we dont want to use the batch norm calculated by a mini-batch. Instead we want to use the mean and standard deviation from the whole training data set:</p>
<div id="dcbe96c8" class="cell" data-execution_count="23">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a><span class="co"># calibrate the batch norm after training</span></span>
<span id="cb35-2"><a href="#cb35-2"></a></span>
<span id="cb35-3"><a href="#cb35-3"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb35-4"><a href="#cb35-4"></a>    <span class="co"># pass the training set through</span></span>
<span id="cb35-5"><a href="#cb35-5"></a>    emb <span class="op">=</span> C[x_train]</span>
<span id="cb35-6"><a href="#cb35-6"></a>    embcat <span class="op">=</span> emb.view(<span class="op">-</span><span class="dv">1</span>, emb.shape[<span class="dv">1</span>] <span class="op">*</span> emb.shape[<span class="dv">2</span>])</span>
<span id="cb35-7"><a href="#cb35-7"></a>    hpreact <span class="op">=</span> embcat <span class="op">@</span> W1 <span class="op">+</span> b1</span>
<span id="cb35-8"><a href="#cb35-8"></a>    <span class="co"># measure the mean/std over the entire training set</span></span>
<span id="cb35-9"><a href="#cb35-9"></a>    bnmean <span class="op">=</span> hpreact.mean(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-10"><a href="#cb35-10"></a>    bnstd <span class="op">=</span> hpreact.std(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Rather, we can also use the running mean and standard deviation as implemented below which will give close estimates. Remaining 2 notes on the BN are:</p>
<ol type="1">
<li>Dividing zeros: we add a <span class="math inline">\(\epsilon\)</span> value to the <strong>variance</strong> to avoid. We do not include this here as it likely not to happen with out example;</li>
<li>The bias <code>b1</code> will be subtracting in BN calculation, we will notice the <code>b1.grad</code> will be zeros as it does not impact any other calculation. Thus when using the BN, for layer before like weight, we should remove the bias. The <code>bnbias</code> now will be incharge for biasing the distributions.</li>
</ol>
</section>
<section id="real-example-resnet50-walkthrough" class="level2">
<h2 class="anchored" data-anchor-id="real-example-resnet50-walkthrough">real example: <code>resnet50</code> walkthrough</h2>
<p>The code AK presented here: <a href="https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L108" class="uri">https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L108</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="resnet50.png" class="img-fluid figure-img"></p>
<figcaption>The architecture of ResNet-50 model.</figcaption>
</figure>
</div>
</section>
<section id="summary-of-the-lecture" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-the-lecture">summary of the lecture</h2>
<p>Understand the activations (non-linearity) and gradients is crucial when training deep / large neural networks, in part 1 we have observed some issue and come up with many solutions:</p>
<ol type="1">
<li>Confidently wrong of network at init leads to hookey stick for loss in training loop: adding multipliers to <code>logits</code>’s weights and biases;</li>
<li>Flat-tails distribution or saturated <code>tanh</code>: Kaiming init;</li>
<li>Normalization of the hidden states: introduction to BN.</li>
</ol>
<p>Our final code in part 1 (un-fold to see), <code># 👈</code> indicates a change:</p>
<div id="ac7389b9" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a>block_size <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb36-2"><a href="#cb36-2"></a></span>
<span id="cb36-3"><a href="#cb36-3"></a><span class="co"># MLP revisited</span></span>
<span id="cb36-4"><a href="#cb36-4"></a>n_emb <span class="op">=</span> <span class="dv">10</span> <span class="co"># no of dimensions of the embedding space.</span></span>
<span id="cb36-5"><a href="#cb36-5"></a>n_hidden <span class="op">=</span> <span class="dv">200</span> <span class="co"># size of the hidden - tanh layer</span></span>
<span id="cb36-6"><a href="#cb36-6"></a></span>
<span id="cb36-7"><a href="#cb36-7"></a><span class="co"># Lookup table - 10 dimensional space</span></span>
<span id="cb36-8"><a href="#cb36-8"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>) <span class="co"># for reproductivity</span></span>
<span id="cb36-9"><a href="#cb36-9"></a>C <span class="op">=</span> torch.randn((vocab_size, n_emb),                  generator<span class="op">=</span>g)</span>
<span id="cb36-10"><a href="#cb36-10"></a></span>
<span id="cb36-11"><a href="#cb36-11"></a><span class="co"># Layer 1 - tanh - 300 neurons</span></span>
<span id="cb36-12"><a href="#cb36-12"></a>W1 <span class="op">=</span> torch.randn((block_size <span class="op">*</span> n_emb, n_hidden),      generator<span class="op">=</span>g) <span class="op">*</span> (<span class="dv">5</span><span class="op">/</span><span class="dv">3</span>) <span class="op">/</span> ((block_size <span class="op">*</span> n_emb)<span class="op">**</span><span class="fl">0.5</span>) <span class="co"># * 0.2       # 👈</span></span>
<span id="cb36-13"><a href="#cb36-13"></a><span class="co"># b1 = torch.randn(n_hidden,                            generator=g) * 0.01       # 👈</span></span>
<span id="cb36-14"><a href="#cb36-14"></a></span>
<span id="cb36-15"><a href="#cb36-15"></a><span class="co"># Layer 2 - softmax</span></span>
<span id="cb36-16"><a href="#cb36-16"></a>W2 <span class="op">=</span> torch.randn((n_hidden, vocab_size),              generator<span class="op">=</span>g) <span class="op">*</span> <span class="fl">0.01</span>       <span class="co"># 👈</span></span>
<span id="cb36-17"><a href="#cb36-17"></a>b2 <span class="op">=</span> torch.randn(vocab_size,                          generator<span class="op">=</span>g) <span class="op">*</span> <span class="dv">0</span>          <span class="co"># 👈</span></span>
<span id="cb36-18"><a href="#cb36-18"></a></span>
<span id="cb36-19"><a href="#cb36-19"></a><span class="co"># Batch Normalization gain and bias</span></span>
<span id="cb36-20"><a href="#cb36-20"></a>bngain <span class="op">=</span> torch.ones((<span class="dv">1</span>, n_hidden))                                              <span class="co"># 👈</span></span>
<span id="cb36-21"><a href="#cb36-21"></a>bnbias <span class="op">=</span> torch.zeros((<span class="dv">1</span>, n_hidden))                                             <span class="co"># 👈</span></span>
<span id="cb36-22"><a href="#cb36-22"></a></span>
<span id="cb36-23"><a href="#cb36-23"></a><span class="co"># Add running mean/std</span></span>
<span id="cb36-24"><a href="#cb36-24"></a>bnmean_running <span class="op">=</span> torch.zeros((<span class="dv">1</span>, n_hidden))                             <span class="co"># 👈</span></span>
<span id="cb36-25"><a href="#cb36-25"></a>bnstd_running <span class="op">=</span> torch.ones((<span class="dv">1</span>, n_hidden))                               <span class="co"># 👈</span></span>
<span id="cb36-26"><a href="#cb36-26"></a></span>
<span id="cb36-27"><a href="#cb36-27"></a><span class="co"># All params (deleted b1)</span></span>
<span id="cb36-28"><a href="#cb36-28"></a>parameters <span class="op">=</span> [C, W1, W2, b2, bngain, bnbias]                                <span class="co"># 👈</span></span>
<span id="cb36-29"><a href="#cb36-29"></a><span class="bu">print</span>(<span class="st">"No of params: "</span>, <span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span>
<span id="cb36-30"><a href="#cb36-30"></a></span>
<span id="cb36-31"><a href="#cb36-31"></a><span class="co"># Pre-training</span></span>
<span id="cb36-32"><a href="#cb36-32"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb36-33"><a href="#cb36-33"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb36-34"><a href="#cb36-34"></a></span>
<span id="cb36-35"><a href="#cb36-35"></a><span class="co"># Optimization</span></span>
<span id="cb36-36"><a href="#cb36-36"></a>max_steps <span class="op">=</span> <span class="dv">50_000</span> <span class="co">#200_000</span></span>
<span id="cb36-37"><a href="#cb36-37"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb36-38"><a href="#cb36-38"></a></span>
<span id="cb36-39"><a href="#cb36-39"></a><span class="co"># Stats holders</span></span>
<span id="cb36-40"><a href="#cb36-40"></a>lossi <span class="op">=</span> []</span>
<span id="cb36-41"><a href="#cb36-41"></a></span>
<span id="cb36-42"><a href="#cb36-42"></a><span class="co"># Training on Xtr, Ytr</span></span>
<span id="cb36-43"><a href="#cb36-43"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb36-44"><a href="#cb36-44"></a></span>
<span id="cb36-45"><a href="#cb36-45"></a>    <span class="co"># minibatch construct      </span></span>
<span id="cb36-46"><a href="#cb36-46"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,)) </span>
<span id="cb36-47"><a href="#cb36-47"></a>    Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix] <span class="co"># batch X, Y</span></span>
<span id="cb36-48"><a href="#cb36-48"></a></span>
<span id="cb36-49"><a href="#cb36-49"></a>    <span class="co"># forward pass:</span></span>
<span id="cb36-50"><a href="#cb36-50"></a>    emb <span class="op">=</span> C[Xb] <span class="co"># embed the characters into vectors   </span></span>
<span id="cb36-51"><a href="#cb36-51"></a>    emb_cat <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate the vectors</span></span>
<span id="cb36-52"><a href="#cb36-52"></a>    <span class="co"># Linear layer</span></span>
<span id="cb36-53"><a href="#cb36-53"></a>    h_pre_act <span class="op">=</span> emb_cat <span class="op">@</span> W1 <span class="co"># + b1 # hidden layer pre-activation                               # 👈</span></span>
<span id="cb36-54"><a href="#cb36-54"></a>    <span class="co"># BatchNorm layer</span></span>
<span id="cb36-55"><a href="#cb36-55"></a>    bnmeani <span class="op">=</span> h_pre_act.mean(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)                                                   <span class="co"># 👈</span></span>
<span id="cb36-56"><a href="#cb36-56"></a>    bnstdi <span class="op">=</span> h_pre_act.std(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)                                                     <span class="co"># 👈</span></span>
<span id="cb36-57"><a href="#cb36-57"></a>    h_pre_act <span class="op">=</span> bngain <span class="op">*</span> ((h_pre_act <span class="op">-</span> bnmeani) <span class="op">/</span> bnstdi) <span class="op">+</span> bnbias                              <span class="co"># 👈</span></span>
<span id="cb36-58"><a href="#cb36-58"></a>    <span class="co"># Updating running mean and std (this runs outside the training loop)</span></span>
<span id="cb36-59"><a href="#cb36-59"></a>    <span class="cf">with</span> torch.no_grad():                                                                       <span class="co"># 👈</span></span>
<span id="cb36-60"><a href="#cb36-60"></a>        bnmean_running <span class="op">=</span> <span class="fl">0.999</span> <span class="op">*</span> bnmean_running <span class="op">+</span> <span class="fl">0.001</span> <span class="op">*</span> bnmeani                               <span class="co"># 👈</span></span>
<span id="cb36-61"><a href="#cb36-61"></a>        bnstd_running <span class="op">=</span> <span class="fl">0.999</span> <span class="op">*</span> bnstd_running <span class="op">+</span> <span class="fl">0.001</span> <span class="op">*</span> bnstdi                                  <span class="co"># 👈</span></span>
<span id="cb36-62"><a href="#cb36-62"></a>    <span class="co"># Non-linearity</span></span>
<span id="cb36-63"><a href="#cb36-63"></a>    h <span class="op">=</span> torch.tanh(h_pre_act) <span class="co"># hidden layer</span></span>
<span id="cb36-64"><a href="#cb36-64"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># output layer</span></span>
<span id="cb36-65"><a href="#cb36-65"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Yb) <span class="co"># loss function</span></span>
<span id="cb36-66"><a href="#cb36-66"></a></span>
<span id="cb36-67"><a href="#cb36-67"></a>    <span class="co"># backward pass:</span></span>
<span id="cb36-68"><a href="#cb36-68"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb36-69"><a href="#cb36-69"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb36-70"><a href="#cb36-70"></a>    loss.backward()</span>
<span id="cb36-71"><a href="#cb36-71"></a></span>
<span id="cb36-72"><a href="#cb36-72"></a>    <span class="co"># update</span></span>
<span id="cb36-73"><a href="#cb36-73"></a>    lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;=</span> max_steps <span class="op">/</span> <span class="dv">2</span> <span class="cf">else</span> <span class="fl">0.01</span> <span class="co"># step learning rate decay</span></span>
<span id="cb36-74"><a href="#cb36-74"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb36-75"><a href="#cb36-75"></a>        p.data <span class="op">+=</span> <span class="op">-</span> lr <span class="op">*</span> p.grad</span>
<span id="cb36-76"><a href="#cb36-76"></a></span>
<span id="cb36-77"><a href="#cb36-77"></a>    <span class="co"># track stats</span></span>
<span id="cb36-78"><a href="#cb36-78"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="co"># print once every while</span></span>
<span id="cb36-79"><a href="#cb36-79"></a>      <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">:7d}</span><span class="ss">/</span><span class="sc">{</span>max_steps<span class="sc">:7d}</span><span class="ss">: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb36-80"><a href="#cb36-80"></a>    lossi.append(loss.log10().item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>No of params:  12097
      0/  50000: 3.3045
  10000/  50000: 2.2005
  20000/  50000: 2.1628
  30000/  50000: 2.0014
  40000/  50000: 2.1175</code></pre>
</div>
</div>
<div id="b5102b6f" class="cell" data-execution_count="25">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a>plt.plot(lossi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-26-output-1.png" width="579" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="22ad83d7" class="cell" data-execution_count="26">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a><span class="at">@torch.no_grad</span>() <span class="co"># disables gradient tracking</span></span>
<span id="cb39-2"><a href="#cb39-2"></a><span class="kw">def</span> split_loss(split: <span class="bu">str</span>):</span>
<span id="cb39-3"><a href="#cb39-3"></a>  x, y <span class="op">=</span> {</span>
<span id="cb39-4"><a href="#cb39-4"></a>    <span class="st">'train'</span>: (Xtr, Ytr),</span>
<span id="cb39-5"><a href="#cb39-5"></a>    <span class="st">'val'</span>: (Xdev, Ydev),</span>
<span id="cb39-6"><a href="#cb39-6"></a>    <span class="st">'test'</span>: (Xte, Yte)</span>
<span id="cb39-7"><a href="#cb39-7"></a>  }[split]</span>
<span id="cb39-8"><a href="#cb39-8"></a>  emb <span class="op">=</span> C[x]</span>
<span id="cb39-9"><a href="#cb39-9"></a>  emb_cat <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) </span>
<span id="cb39-10"><a href="#cb39-10"></a>  h_pre_act <span class="op">=</span> emb_cat <span class="op">@</span> W1 <span class="op">+</span> b1                                                                                         <span class="co"># 👈 </span></span>
<span id="cb39-11"><a href="#cb39-11"></a>  <span class="co"># h_pre_act = bngain * ((h_pre_act - h_pre_act.mean(0, keepdim=True)) / h_pre_act.std(0, keepdim=True)) + bnbias      # 👈</span></span>
<span id="cb39-12"><a href="#cb39-12"></a>  <span class="co"># h_pre_act = bngain * ((h_pre_act - bnmean) / bnstd) + bnbias                                                        # 👈</span></span>
<span id="cb39-13"><a href="#cb39-13"></a>  h_pre_act <span class="op">=</span> bngain <span class="op">*</span> ((h_pre_act <span class="op">-</span> bnmean_running) <span class="op">/</span> bnstd_running) <span class="op">+</span> bnbias                                          <span class="co"># 👈</span></span>
<span id="cb39-14"><a href="#cb39-14"></a>  h <span class="op">=</span> torch.tanh(h_pre_act) </span>
<span id="cb39-15"><a href="#cb39-15"></a>  logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb39-16"><a href="#cb39-16"></a>  loss <span class="op">=</span> F.cross_entropy(logits, y)</span>
<span id="cb39-17"><a href="#cb39-17"></a>  <span class="bu">print</span>(split, loss.item())</span>
<span id="cb39-18"><a href="#cb39-18"></a></span>
<span id="cb39-19"><a href="#cb39-19"></a>split_loss(<span class="st">'train'</span>)</span>
<span id="cb39-20"><a href="#cb39-20"></a>split_loss(<span class="st">'val'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>train 2.4766831398010254
val 2.490326404571533</code></pre>
</div>
</div>
<section id="loss-logs" class="level3">
<h3 class="anchored" data-anchor-id="loss-logs">loss logs</h3>
<p>The numbers somehow are approximate, I don’t know why my Thinkpad-E14 gave different results when running codes multiple times 😂.</p>
<table class="table-striped table-hover table">
<caption>Loss logs</caption>
<colgroup>
<col style="width: 10%">
<col style="width: 40%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>What we did</th>
<th>Loss we got (accum)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>original</td>
<td><p>train 2.1169614791870117</p>
<p>val 2.1623435020446777</p></td>
</tr>
<tr class="even">
<td>2</td>
<td>fixed softmax confidently wrong</td>
<td><p>train 2.0666463375091553</p>
<p>val 2.1468191146850586</p></td>
</tr>
<tr class="odd">
<td>3</td>
<td>fixed <code>tanh</code> layer too saturated at init</td>
<td><p>train 2.033477544784546</p>
<p>val 2.115907907485962</p></td>
</tr>
<tr class="even">
<td>4</td>
<td>used semi principle “kaiming init” instead of hacking init</td>
<td><p>train 2.038902997970581</p>
<p>val 2.1138899326324463</p></td>
</tr>
<tr class="odd">
<td>5</td>
<td>added batch norm layer</td>
<td><p>train 2.0662825107574463</p>
<p>val 2.1201331615448</p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="part-2-pytorch-ifying-the-code-and-train-a-deeper-network" class="level1">
<h1>Part 2: PyTorch-ifying the code, and train a deeper network</h1>
<p>Below is PyTorch-ified code by Andrej, some comments inputted by me:</p>
<div id="0d606bcb" class="cell" data-execution_count="27">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1"></a><span class="co"># Let's train a deeper network</span></span>
<span id="cb41-2"><a href="#cb41-2"></a><span class="co"># The classes we create here are the same API as nn.Module in PyTorch</span></span>
<span id="cb41-3"><a href="#cb41-3"></a></span>
<span id="cb41-4"><a href="#cb41-4"></a><span class="kw">class</span> Linear:</span>
<span id="cb41-5"><a href="#cb41-5"></a>    <span class="co">"""</span></span>
<span id="cb41-6"><a href="#cb41-6"></a><span class="co">    Simplifying Pytorch Linear Layer: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear</span></span>
<span id="cb41-7"><a href="#cb41-7"></a><span class="co">    """</span></span>
<span id="cb41-8"><a href="#cb41-8"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, fan_in, fan_out, bias<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb41-9"><a href="#cb41-9"></a>        <span class="va">self</span>.weight <span class="op">=</span> torch.randn((fan_in, fan_out), generator<span class="op">=</span>g) <span class="op">/</span> fan_in<span class="op">**</span><span class="fl">0.5</span></span>
<span id="cb41-10"><a href="#cb41-10"></a>        <span class="va">self</span>.bias <span class="op">=</span> torch.zeros(fan_out) <span class="cf">if</span> bias <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb41-11"><a href="#cb41-11"></a></span>
<span id="cb41-12"><a href="#cb41-12"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb41-13"><a href="#cb41-13"></a>        <span class="va">self</span>.out <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.weight</span>
<span id="cb41-14"><a href="#cb41-14"></a>        <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb41-15"><a href="#cb41-15"></a>            <span class="va">self</span>.out <span class="op">+=</span> <span class="va">self</span>.bias</span>
<span id="cb41-16"><a href="#cb41-16"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb41-17"><a href="#cb41-17"></a></span>
<span id="cb41-18"><a href="#cb41-18"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb41-19"><a href="#cb41-19"></a>        <span class="cf">return</span> [<span class="va">self</span>.weight] <span class="op">+</span> ([] <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> [<span class="va">self</span>.bias])</span>
<span id="cb41-20"><a href="#cb41-20"></a></span>
<span id="cb41-21"><a href="#cb41-21"></a></span>
<span id="cb41-22"><a href="#cb41-22"></a><span class="kw">class</span> BatchNorm1d:</span>
<span id="cb41-23"><a href="#cb41-23"></a>    <span class="co">"""</span></span>
<span id="cb41-24"><a href="#cb41-24"></a><span class="co">    Simplifying Pytorch BatchNorm1D: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html</span></span>
<span id="cb41-25"><a href="#cb41-25"></a><span class="co">    """</span> </span>
<span id="cb41-26"><a href="#cb41-26"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, eps<span class="op">=</span><span class="fl">1e-5</span>, momentum<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb41-27"><a href="#cb41-27"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb41-28"><a href="#cb41-28"></a>        <span class="va">self</span>.momentum <span class="op">=</span> momentum</span>
<span id="cb41-29"><a href="#cb41-29"></a>        <span class="va">self</span>.training <span class="op">=</span> <span class="va">True</span> <span class="co"># to differentiate usage of class in training or evaluation (using running mean/std)</span></span>
<span id="cb41-30"><a href="#cb41-30"></a>        <span class="co"># parameters (trained with backprop)</span></span>
<span id="cb41-31"><a href="#cb41-31"></a>        <span class="va">self</span>.gamma <span class="op">=</span> torch.ones(dim) <span class="co"># gain</span></span>
<span id="cb41-32"><a href="#cb41-32"></a>        <span class="va">self</span>.beta <span class="op">=</span> torch.zeros(dim) <span class="co"># bias</span></span>
<span id="cb41-33"><a href="#cb41-33"></a>        <span class="co"># buffers (trained with a running 'momentum update')</span></span>
<span id="cb41-34"><a href="#cb41-34"></a>        <span class="va">self</span>.running_mean <span class="op">=</span> torch.zeros(dim)</span>
<span id="cb41-35"><a href="#cb41-35"></a>        <span class="va">self</span>.running_var <span class="op">=</span> torch.ones(dim)</span>
<span id="cb41-36"><a href="#cb41-36"></a></span>
<span id="cb41-37"><a href="#cb41-37"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb41-38"><a href="#cb41-38"></a>        <span class="co"># calculate the forward pass</span></span>
<span id="cb41-39"><a href="#cb41-39"></a>        <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb41-40"><a href="#cb41-40"></a>            xmean <span class="op">=</span> x.mean(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch mean</span></span>
<span id="cb41-41"><a href="#cb41-41"></a>            xvar <span class="op">=</span> x.var(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch variance, follow the paper exactly</span></span>
<span id="cb41-42"><a href="#cb41-42"></a>        <span class="cf">else</span>:</span>
<span id="cb41-43"><a href="#cb41-43"></a>            xmean <span class="op">=</span> <span class="va">self</span>.running_mean</span>
<span id="cb41-44"><a href="#cb41-44"></a>            xvar <span class="op">=</span> <span class="va">self</span>.running_var</span>
<span id="cb41-45"><a href="#cb41-45"></a>        xhat <span class="op">=</span> (x <span class="op">-</span> xmean) <span class="op">/</span> torch.sqrt(xvar <span class="op">+</span> <span class="va">self</span>.eps) <span class="co"># normalize to unit variance</span></span>
<span id="cb41-46"><a href="#cb41-46"></a>        <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.gamma <span class="op">*</span> xhat <span class="op">+</span> <span class="va">self</span>.beta <span class="co"># to tracking and visualizing data later on, PyTorch does not have this</span></span>
<span id="cb41-47"><a href="#cb41-47"></a>        <span class="co"># update the buffers</span></span>
<span id="cb41-48"><a href="#cb41-48"></a>        <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb41-49"><a href="#cb41-49"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb41-50"><a href="#cb41-50"></a>                <span class="va">self</span>.running_mean <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> <span class="va">self</span>.running_mean <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> xmean</span>
<span id="cb41-51"><a href="#cb41-51"></a>                <span class="va">self</span>.running_var <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> <span class="va">self</span>.running_var <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> xvar</span>
<span id="cb41-52"><a href="#cb41-52"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb41-53"><a href="#cb41-53"></a></span>
<span id="cb41-54"><a href="#cb41-54"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb41-55"><a href="#cb41-55"></a>        <span class="cf">return</span> [<span class="va">self</span>.gamma, <span class="va">self</span>.beta]</span>
<span id="cb41-56"><a href="#cb41-56"></a></span>
<span id="cb41-57"><a href="#cb41-57"></a><span class="kw">class</span> Tanh:</span>
<span id="cb41-58"><a href="#cb41-58"></a>    <span class="co">"""</span></span>
<span id="cb41-59"><a href="#cb41-59"></a><span class="co">    Just calculate the Tanh, just PyTorch: https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html</span></span>
<span id="cb41-60"><a href="#cb41-60"></a><span class="co">    """</span></span>
<span id="cb41-61"><a href="#cb41-61"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb41-62"><a href="#cb41-62"></a>        <span class="va">self</span>.out <span class="op">=</span> torch.tanh(x)</span>
<span id="cb41-63"><a href="#cb41-63"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb41-64"><a href="#cb41-64"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb41-65"><a href="#cb41-65"></a>        <span class="cf">return</span> []</span>
<span id="cb41-66"><a href="#cb41-66"></a></span>
<span id="cb41-67"><a href="#cb41-67"></a>n_embd <span class="op">=</span> <span class="dv">10</span> <span class="co"># the dimensionality of the character embedding vectors</span></span>
<span id="cb41-68"><a href="#cb41-68"></a>n_hidden <span class="op">=</span> <span class="dv">100</span> <span class="co"># the number of neurons in the hidden layer of the MLP</span></span>
<span id="cb41-69"><a href="#cb41-69"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>) <span class="co"># for reproducibility</span></span>
<span id="cb41-70"><a href="#cb41-70"></a></span>
<span id="cb41-71"><a href="#cb41-71"></a>C <span class="op">=</span> torch.randn((vocab_size, n_embd),            generator<span class="op">=</span>g)</span>
<span id="cb41-72"><a href="#cb41-72"></a></span>
<span id="cb41-73"><a href="#cb41-73"></a>layers <span class="op">=</span> [</span>
<span id="cb41-74"><a href="#cb41-74"></a>    Linear(n_embd <span class="op">*</span> block_size, n_hidden), Tanh(),</span>
<span id="cb41-75"><a href="#cb41-75"></a>    Linear(           n_hidden, n_hidden), Tanh(),</span>
<span id="cb41-76"><a href="#cb41-76"></a>    Linear(           n_hidden, n_hidden), Tanh(),</span>
<span id="cb41-77"><a href="#cb41-77"></a>    Linear(           n_hidden, n_hidden), Tanh(),</span>
<span id="cb41-78"><a href="#cb41-78"></a>    Linear(           n_hidden, n_hidden), Tanh(),</span>
<span id="cb41-79"><a href="#cb41-79"></a>    Linear(           n_hidden, vocab_size),</span>
<span id="cb41-80"><a href="#cb41-80"></a>]</span>
<span id="cb41-81"><a href="#cb41-81"></a></span>
<span id="cb41-82"><a href="#cb41-82"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb41-83"><a href="#cb41-83"></a>    <span class="co"># last layer: make less confident</span></span>
<span id="cb41-84"><a href="#cb41-84"></a>    layers[<span class="op">-</span><span class="dv">1</span>].weight <span class="op">*=</span> <span class="fl">0.1</span></span>
<span id="cb41-85"><a href="#cb41-85"></a>    <span class="co"># all other layers: apply gain</span></span>
<span id="cb41-86"><a href="#cb41-86"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layers[:<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb41-87"><a href="#cb41-87"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(layer, Linear):</span>
<span id="cb41-88"><a href="#cb41-88"></a>            layer.weight <span class="op">*=</span> <span class="dv">5</span><span class="op">/</span><span class="dv">3</span></span>
<span id="cb41-89"><a href="#cb41-89"></a></span>
<span id="cb41-90"><a href="#cb41-90"></a>parameters <span class="op">=</span> [C] <span class="op">+</span> [p <span class="cf">for</span> layer <span class="kw">in</span> layers <span class="cf">for</span> p <span class="kw">in</span> layer.parameters()]</span>
<span id="cb41-91"><a href="#cb41-91"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters)) <span class="co"># number of parameters in total</span></span>
<span id="cb41-92"><a href="#cb41-92"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb41-93"><a href="#cb41-93"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>46497</code></pre>
</div>
</div>
<div id="c0dda97c" class="cell" data-execution_count="28">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a><span class="co"># same optimization as last time</span></span>
<span id="cb43-2"><a href="#cb43-2"></a>max_steps <span class="op">=</span> <span class="dv">200000</span></span>
<span id="cb43-3"><a href="#cb43-3"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb43-4"><a href="#cb43-4"></a>lossi <span class="op">=</span> []</span>
<span id="cb43-5"><a href="#cb43-5"></a>ud <span class="op">=</span> []</span>
<span id="cb43-6"><a href="#cb43-6"></a></span>
<span id="cb43-7"><a href="#cb43-7"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb43-8"><a href="#cb43-8"></a>  </span>
<span id="cb43-9"><a href="#cb43-9"></a>    <span class="co"># minibatch construct</span></span>
<span id="cb43-10"><a href="#cb43-10"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,), generator<span class="op">=</span>g)</span>
<span id="cb43-11"><a href="#cb43-11"></a>    Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix] <span class="co"># batch X,Y</span></span>
<span id="cb43-12"><a href="#cb43-12"></a></span>
<span id="cb43-13"><a href="#cb43-13"></a>    <span class="co"># forward pass</span></span>
<span id="cb43-14"><a href="#cb43-14"></a>    emb <span class="op">=</span> C[Xb] <span class="co"># embed the characters into vectors</span></span>
<span id="cb43-15"><a href="#cb43-15"></a>    x <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate the vectors</span></span>
<span id="cb43-16"><a href="#cb43-16"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb43-17"><a href="#cb43-17"></a>        x <span class="op">=</span> layer(x)</span>
<span id="cb43-18"><a href="#cb43-18"></a>    loss <span class="op">=</span> F.cross_entropy(x, Yb) <span class="co"># loss function</span></span>
<span id="cb43-19"><a href="#cb43-19"></a></span>
<span id="cb43-20"><a href="#cb43-20"></a>    <span class="co"># backward pass</span></span>
<span id="cb43-21"><a href="#cb43-21"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb43-22"><a href="#cb43-22"></a>        layer.out.retain_grad() <span class="co"># AFTER_DEBUG: would take out retain_graph</span></span>
<span id="cb43-23"><a href="#cb43-23"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb43-24"><a href="#cb43-24"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb43-25"><a href="#cb43-25"></a>    loss.backward()</span>
<span id="cb43-26"><a href="#cb43-26"></a></span>
<span id="cb43-27"><a href="#cb43-27"></a>    <span class="co"># update</span></span>
<span id="cb43-28"><a href="#cb43-28"></a>    lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">150000</span> <span class="cf">else</span> <span class="fl">0.01</span> <span class="co"># step learning rate decay</span></span>
<span id="cb43-29"><a href="#cb43-29"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb43-30"><a href="#cb43-30"></a>        p.data <span class="op">+=</span> <span class="op">-</span>lr <span class="op">*</span> p.grad</span>
<span id="cb43-31"><a href="#cb43-31"></a></span>
<span id="cb43-32"><a href="#cb43-32"></a>    <span class="co"># track stats</span></span>
<span id="cb43-33"><a href="#cb43-33"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="co"># print every once in a while</span></span>
<span id="cb43-34"><a href="#cb43-34"></a>        <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">:7d}</span><span class="ss">/</span><span class="sc">{</span>max_steps<span class="sc">:7d}</span><span class="ss">: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb43-35"><a href="#cb43-35"></a>    lossi.append(loss.log10().item())</span>
<span id="cb43-36"><a href="#cb43-36"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb43-37"><a href="#cb43-37"></a>        ud.append([((lr<span class="op">*</span>p.grad).std() <span class="op">/</span> p.data.std()).log10().item() <span class="cf">for</span> p <span class="kw">in</span> parameters])</span>
<span id="cb43-38"><a href="#cb43-38"></a></span>
<span id="cb43-39"><a href="#cb43-39"></a>    <span class="cf">break</span></span>
<span id="cb43-40"><a href="#cb43-40"></a>    <span class="co"># if i &gt;= 1000:</span></span>
<span id="cb43-41"><a href="#cb43-41"></a>    <span class="co">#     break # AFTER_DEBUG: would take out obviously to run full optimization</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>      0/ 200000: 3.2962</code></pre>
</div>
</div>
<section id="viz-1-forward-pass-activations-statistics" class="level2">
<h2 class="anchored" data-anchor-id="viz-1-forward-pass-activations-statistics">viz #1: forward pass activations statistics</h2>
<div id="7d864881" class="cell" data-execution_count="29">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1"></a><span class="co"># visualize histograms</span></span>
<span id="cb45-2"><a href="#cb45-2"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">11</span>, <span class="dv">3</span>)) <span class="co"># width and height of the plot</span></span>
<span id="cb45-3"><a href="#cb45-3"></a>legends <span class="op">=</span> []</span>
<span id="cb45-4"><a href="#cb45-4"></a><span class="cf">for</span> i, layer <span class="kw">in</span> <span class="bu">enumerate</span>(layers[:<span class="op">-</span><span class="dv">1</span>]): <span class="co"># note: exclude the output layer</span></span>
<span id="cb45-5"><a href="#cb45-5"></a>  <span class="cf">if</span> <span class="bu">isinstance</span>(layer, Tanh):</span>
<span id="cb45-6"><a href="#cb45-6"></a>    t <span class="op">=</span> layer.out</span>
<span id="cb45-7"><a href="#cb45-7"></a>    <span class="bu">print</span>(<span class="st">'layer </span><span class="sc">%d</span><span class="st"> (</span><span class="sc">%10s</span><span class="st">): mean </span><span class="sc">%+.2f</span><span class="st">, std </span><span class="sc">%.2f</span><span class="st">, saturated: </span><span class="sc">%.2f%%</span><span class="st">'</span> <span class="op">%</span> (i, layer.__class__.<span class="va">__name__</span>, t.mean(), t.std(), (t.<span class="bu">abs</span>() <span class="op">&gt;</span> <span class="fl">0.97</span>).<span class="bu">float</span>().mean()<span class="op">*</span><span class="dv">100</span>))</span>
<span id="cb45-8"><a href="#cb45-8"></a>    hy, hx <span class="op">=</span> torch.histogram(t, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb45-9"><a href="#cb45-9"></a>    plt.plot(hx[:<span class="op">-</span><span class="dv">1</span>].detach(), hy.detach())</span>
<span id="cb45-10"><a href="#cb45-10"></a>    legends.append(<span class="ss">f'layer </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>layer<span class="sc">.</span>__class__<span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb45-11"><a href="#cb45-11"></a>plt.legend(legends)<span class="op">;</span></span>
<span id="cb45-12"><a href="#cb45-12"></a>plt.title(<span class="st">'activation distribution'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>layer 1 (      Tanh): mean -0.02, std 0.75, saturated: 20.25%
layer 3 (      Tanh): mean -0.00, std 0.69, saturated: 8.38%
layer 5 (      Tanh): mean +0.00, std 0.67, saturated: 6.62%
layer 7 (      Tanh): mean -0.01, std 0.66, saturated: 5.47%
layer 9 (      Tanh): mean -0.02, std 0.66, saturated: 6.12%</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>Text(0.5, 1.0, 'activation distribution')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-30-output-3.png" width="855" height="283" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>If we set the gain to <code>1</code>, the std is shrinking, and the saturation is coming to zeros, due to the first layer is pretty decent, but the next ones are shrinking to zero because of the <code>tanh()</code> - a squashing function.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode numberSource md number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb48-1"><a href="#cb48-1"></a>layer 1 (      Tanh): mean -0.02, std 0.62, saturated: 3.50%</span>
<span id="cb48-2"><a href="#cb48-2"></a>layer 3 (      Tanh): mean -0.00, std 0.48, saturated: 0.03%</span>
<span id="cb48-3"><a href="#cb48-3"></a>layer 5 (      Tanh): mean +0.00, std 0.41, saturated: 0.06%</span>
<span id="cb48-4"><a href="#cb48-4"></a>layer 7 (      Tanh): mean +0.00, std 0.35, saturated: 0.00%</span>
<span id="cb48-5"><a href="#cb48-5"></a>layer 9 (      Tanh): mean -0.02, std 0.32, saturated: 0.00%</span>
<span id="cb48-6"><a href="#cb48-6"></a>Text(0.5, 1.0, 'activation distribution')</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="viz1_gain1.png" class="img-fluid figure-img"></p>
<figcaption>If the gain is 1</figcaption>
</figure>
</div>
<p>But if we set the gain is far too high, let’s say <code>3</code>, we can see the saturation is too high.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode numberSource md number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb49-1"><a href="#cb49-1"></a>layer 1 (      Tanh): mean -0.03, std 0.85, saturated: 47.66%</span>
<span id="cb49-2"><a href="#cb49-2"></a>layer 3 (      Tanh): mean +0.00, std 0.84, saturated: 40.47%</span>
<span id="cb49-3"><a href="#cb49-3"></a>layer 5 (      Tanh): mean -0.01, std 0.84, saturated: 42.38%</span>
<span id="cb49-4"><a href="#cb49-4"></a>layer 7 (      Tanh): mean -0.01, std 0.84, saturated: 42.00%</span>
<span id="cb49-5"><a href="#cb49-5"></a>layer 9 (      Tanh): mean -0.03, std 0.84, saturated: 42.41%</span>
<span id="cb49-6"><a href="#cb49-6"></a>Text(0.5, 1.0, 'activation distribution')</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="viz1_gain3.png" class="img-fluid figure-img"></p>
<figcaption>If the gain is 3</figcaption>
</figure>
</div>
<p>So <code>5/3</code> is a nice one, balancing the std and saturation.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Why 5/3?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why 5/3?
</div>
</div>
<div class="callout-body-container callout-body">
<p>A comment in his video explains why <code>5/3</code> is recommended, it comes from the avg of <span class="math inline">\([\tanh(x)]^2\)</span> where <span class="math inline">\(x\)</span> is distributed as a Gaussian:</p>
<p><span class="math inline">\(\int_{-\infty}^{\infty} \frac{[\tanh(x)]^2 \exp(-\frac{x^2}{2})}{\sqrt{2\pi}} \, dx \approx 0.39\)</span></p>
<blockquote class="blockquote">
<p>The square root of this value is how much the <code>tanh</code> squeezes the variance of the incoming variable: 0.39 ** .5 ~= 0.63 ~= 3/5 (hence 5/3 is just an approximation of the exact gain).</p>
</blockquote>
</div>
</div>
</section>
<section id="viz-2-backward-pass-gradient-statistics" class="level2">
<h2 class="anchored" data-anchor-id="viz-2-backward-pass-gradient-statistics">viz #2: backward pass gradient statistics</h2>
<p>Similarly, we can do the same thing with gradients. With the setting of gain as <code>5/3</code>, the distribution of gradients through layers quite the same. Layer by layer, the value of gradients will be shrank close to zero, the distributions would be more and more peak, so the gain here will help expanding those distributions.</p>
<div id="dfe866ce" class="cell" data-execution_count="30">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1"></a><span class="co"># visualize histograms</span></span>
<span id="cb50-2"><a href="#cb50-2"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">11</span>, <span class="dv">3</span>)) <span class="co"># width and height of the plot</span></span>
<span id="cb50-3"><a href="#cb50-3"></a>legends <span class="op">=</span> []</span>
<span id="cb50-4"><a href="#cb50-4"></a><span class="cf">for</span> i, layer <span class="kw">in</span> <span class="bu">enumerate</span>(layers[:<span class="op">-</span><span class="dv">1</span>]): <span class="co"># note: exclude the output layer</span></span>
<span id="cb50-5"><a href="#cb50-5"></a>  <span class="cf">if</span> <span class="bu">isinstance</span>(layer, Tanh):</span>
<span id="cb50-6"><a href="#cb50-6"></a>    t <span class="op">=</span> layer.out.grad</span>
<span id="cb50-7"><a href="#cb50-7"></a>    <span class="bu">print</span>(<span class="st">'layer </span><span class="sc">%d</span><span class="st"> (</span><span class="sc">%10s</span><span class="st">): mean </span><span class="sc">%+f</span><span class="st">, std </span><span class="sc">%e</span><span class="st">'</span> <span class="op">%</span> (i, layer.__class__.<span class="va">__name__</span>, t.mean(), t.std()))</span>
<span id="cb50-8"><a href="#cb50-8"></a>    hy, hx <span class="op">=</span> torch.histogram(t, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb50-9"><a href="#cb50-9"></a>    plt.plot(hx[:<span class="op">-</span><span class="dv">1</span>].detach(), hy.detach())</span>
<span id="cb50-10"><a href="#cb50-10"></a>    legends.append(<span class="ss">f'layer </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>layer<span class="sc">.</span>__class__<span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb50-11"><a href="#cb50-11"></a>plt.legend(legends)<span class="op">;</span></span>
<span id="cb50-12"><a href="#cb50-12"></a>plt.title(<span class="st">'gradient distribution'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>layer 1 (      Tanh): mean +0.000010, std 4.205588e-04
layer 3 (      Tanh): mean -0.000003, std 3.991179e-04
layer 5 (      Tanh): mean +0.000003, std 3.743020e-04
layer 7 (      Tanh): mean +0.000015, std 3.290473e-04
layer 9 (      Tanh): mean -0.000014, std 3.054035e-04</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>Text(0.5, 1.0, 'gradient distribution')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-31-output-3.png" width="888" height="283" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="the-fully-linear-case-of-no-non-linearity" class="level2">
<h2 class="anchored" data-anchor-id="the-fully-linear-case-of-no-non-linearity">the fully linear case of no non-linearity</h2>
<p>Now imagine if we remove the <code>tanh</code> from all layers, the recommend gain now for Linear is <code>1</code>.</p>
<div id="9a455b65" class="cell" data-execution_count="31">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1"></a>layers <span class="op">=</span> [</span>
<span id="cb53-2"><a href="#cb53-2"></a>  Linear(n_embd <span class="op">*</span> block_size, n_hidden), <span class="co">#Tanh(),</span></span>
<span id="cb53-3"><a href="#cb53-3"></a>  Linear(           n_hidden, n_hidden), <span class="co">#Tanh(),</span></span>
<span id="cb53-4"><a href="#cb53-4"></a>  Linear(           n_hidden, n_hidden), <span class="co">#Tanh(),</span></span>
<span id="cb53-5"><a href="#cb53-5"></a>  Linear(           n_hidden, n_hidden), <span class="co">#Tanh(),</span></span>
<span id="cb53-6"><a href="#cb53-6"></a>  Linear(           n_hidden, n_hidden), <span class="co">#Tanh(),</span></span>
<span id="cb53-7"><a href="#cb53-7"></a>  Linear(           n_hidden, vocab_size),</span>
<span id="cb53-8"><a href="#cb53-8"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>But you’ll end up getting a pure linear network. No matter of how many Linear Layers you stacked up, it just the combination of all layers to a massive linear function <span class="math inline">\(y = xA^T + b\)</span>, which will greatly limit the capacity of the neural nets.</p>
</section>
<section id="viz-3-parameter-activation-and-gradient-statistics" class="level2">
<h2 class="anchored" data-anchor-id="viz-3-parameter-activation-and-gradient-statistics">viz #3: parameter activation and gradient statistics</h2>
<p>We can also visualize the distribution of paramaters, here below only weight for simplicity (ignoring gamma, beta, etc…). We observed mean, std, and the grad to data ratio (to see how much the data will be updated).</p>
<p>Problem for the last layer is shown in code output below, the weights on last layer are 10 times bigger than previous ones, and the grad to data ratio is too high.</p>
<p>We can try run 1st 1000 training loops and this can be slight reduced, but since we are using a simple optimizer SGD rather than modern one like Adam, it is still problematic.</p>
<div id="01311979" class="cell" data-execution_count="32">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1"></a><span class="co"># visualize histograms</span></span>
<span id="cb54-2"><a href="#cb54-2"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">11</span>, <span class="dv">3</span>)) <span class="co"># width and height of the plot</span></span>
<span id="cb54-3"><a href="#cb54-3"></a>legends <span class="op">=</span> []</span>
<span id="cb54-4"><a href="#cb54-4"></a><span class="cf">for</span> i,p <span class="kw">in</span> <span class="bu">enumerate</span>(parameters):</span>
<span id="cb54-5"><a href="#cb54-5"></a>  t <span class="op">=</span> p.grad</span>
<span id="cb54-6"><a href="#cb54-6"></a>  <span class="cf">if</span> p.ndim <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb54-7"><a href="#cb54-7"></a>    <span class="bu">print</span>(<span class="st">'weight </span><span class="sc">%10s</span><span class="st"> | mean </span><span class="sc">%+f</span><span class="st"> | std </span><span class="sc">%e</span><span class="st"> | grad:data ratio </span><span class="sc">%e</span><span class="st">'</span> <span class="op">%</span> (<span class="bu">tuple</span>(p.shape), t.mean(), t.std(), t.std() <span class="op">/</span> p.std()))</span>
<span id="cb54-8"><a href="#cb54-8"></a>    hy, hx <span class="op">=</span> torch.histogram(t, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb54-9"><a href="#cb54-9"></a>    plt.plot(hx[:<span class="op">-</span><span class="dv">1</span>].detach(), hy.detach())</span>
<span id="cb54-10"><a href="#cb54-10"></a>    legends.append(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span><span class="bu">tuple</span>(p.shape)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb54-11"><a href="#cb54-11"></a>plt.legend(legends)</span>
<span id="cb54-12"><a href="#cb54-12"></a>plt.title(<span class="st">'weights gradient distribution'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>weight   (27, 10) | mean -0.000031 | std 1.365078e-03 | grad:data ratio 1.364090e-03
weight  (30, 100) | mean -0.000049 | std 1.207430e-03 | grad:data ratio 3.871660e-03
weight (100, 100) | mean +0.000016 | std 1.096730e-03 | grad:data ratio 6.601988e-03
weight (100, 100) | mean -0.000010 | std 9.893572e-04 | grad:data ratio 5.893091e-03
weight (100, 100) | mean -0.000011 | std 8.623432e-04 | grad:data ratio 5.158123e-03
weight (100, 100) | mean -0.000004 | std 7.388576e-04 | grad:data ratio 4.415211e-03
weight  (100, 27) | mean -0.000000 | std 2.364824e-02 | grad:data ratio 2.328203e+00</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-33-output-2.png" width="881" height="283" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="viz-4-update-data-ratio-over-time" class="level2">
<h2 class="anchored" data-anchor-id="viz-4-update-data-ratio-over-time">viz #4: update data ratio over time</h2>
<p>The grad to data above ratio is at the end not really informative (only at one point in time), what matter is actual amount which we change the data in these tensors (over time). AK introduce a tracking list <code>ud</code> (update to data). This calculates the ratio between (std) of the grad to the data of parameters (and <code>log10()</code> for a nicer viz) <strong>without context of gradient</strong>.</p>
<div id="ef31baee" class="cell" data-execution_count="33">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">11</span>, <span class="dv">3</span>))</span>
<span id="cb56-2"><a href="#cb56-2"></a>legends <span class="op">=</span> []</span>
<span id="cb56-3"><a href="#cb56-3"></a><span class="cf">for</span> i,p <span class="kw">in</span> <span class="bu">enumerate</span>(parameters):</span>
<span id="cb56-4"><a href="#cb56-4"></a>  <span class="cf">if</span> p.ndim <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb56-5"><a href="#cb56-5"></a>    plt.plot([ud[j][i] <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(ud))])</span>
<span id="cb56-6"><a href="#cb56-6"></a>    legends.append(<span class="st">'param </span><span class="sc">%d</span><span class="st">'</span> <span class="op">%</span> i)</span>
<span id="cb56-7"><a href="#cb56-7"></a>plt.plot([<span class="dv">0</span>, <span class="bu">len</span>(ud)], [<span class="op">-</span><span class="dv">3</span>, <span class="op">-</span><span class="dv">3</span>], <span class="st">'k'</span>) <span class="co"># these ratios should be ~1e-3, indicate on plot</span></span>
<span id="cb56-8"><a href="#cb56-8"></a>plt.legend(legends)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Below is the visualization from data collected after 1000 training loops:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="viz4_1000times.png" class="img-fluid figure-img"></p>
<figcaption>Viz 4 1000</figcaption>
</figure>
</div>
<p>Recall what we did to the last layer, avoiding over confidence, so the pink line looks different among others. In general, the learning process are good, if we change the learning rate to <code>0.0001</code>, the chart looks much worse.</p>
<p>Below are viz 1 after 1000 training loops:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="viz1_1000times.png" class="img-fluid figure-img"></p>
<figcaption>Viz 1 1000</figcaption>
</figure>
</div>
<p>and viz 2:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="viz2_1000times.png" class="img-fluid figure-img"></p>
<figcaption>Viz 2 1000</figcaption>
</figure>
</div>
<p>and viz 3:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="viz3_1000times.png" class="img-fluid figure-img"></p>
<figcaption>Viz 3 1000</figcaption>
</figure>
</div>
<p>Pretty decent till now. Let’s bring the BatchNorm back.</p>
</section>
<section id="bringing-back-batchnorm-looking-at-the-visualizations" class="level2">
<h2 class="anchored" data-anchor-id="bringing-back-batchnorm-looking-at-the-visualizations">bringing back batchnorm, looking at the visualizations</h2>
<p>We re-define the layers, and change <code>gamma</code> in last layer under no gradient instead of <code>weight</code>. We also dont want the “manual normalization” fan-in, and the gain <code>5/3</code> as well:</p>
<div id="4d9f0d00" class="cell" data-execution_count="34">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1"></a>layers <span class="op">=</span> [</span>
<span id="cb57-2"><a href="#cb57-2"></a>  Linear(n_embd <span class="op">*</span> block_size, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb57-3"><a href="#cb57-3"></a>  Linear(           n_hidden, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb57-4"><a href="#cb57-4"></a>  Linear(           n_hidden, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb57-5"><a href="#cb57-5"></a>  Linear(           n_hidden, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb57-6"><a href="#cb57-6"></a>  Linear(           n_hidden, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb57-7"><a href="#cb57-7"></a>  Linear(           n_hidden, vocab_size, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(vocab_size),</span>
<span id="cb57-8"><a href="#cb57-8"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="summary-of-the-lecture-for-real-this-time" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-the-lecture-for-real-this-time">summary of the lecture for real this time</h2>
<ol type="1">
<li>Intruduction of Batch Normalization - the 1st one of modern innovation to stablize Deep NN training;</li>
<li>PyTorch-ifying code;</li>
<li>Introduction to some diagnostic tools that we can use to verify the network is in good state dynamically.</li>
</ol>
<p>What he did not try to improve here is the loss of the network. It’s now somehow bottleneck not by the Optimization, but by the Context Length he suspect.</p>
<blockquote class="blockquote">
<p>Training Neural Network is like balancing a pencil on a finger.</p>
</blockquote>
<p>Final network architecture and training:</p>
<div id="1c3ea2f8" class="cell" data-execution_count="35">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1"></a><span class="co"># BatchNorm1D and Tanh are the same</span></span>
<span id="cb58-2"><a href="#cb58-2"></a><span class="kw">class</span> Linear:</span>
<span id="cb58-3"><a href="#cb58-3"></a>    <span class="co">"""</span></span>
<span id="cb58-4"><a href="#cb58-4"></a><span class="co">    Simplifying Pytorch Linear Layer: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear</span></span>
<span id="cb58-5"><a href="#cb58-5"></a><span class="co">    """</span></span>
<span id="cb58-6"><a href="#cb58-6"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, fan_in, fan_out, bias<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb58-7"><a href="#cb58-7"></a>        <span class="va">self</span>.weight <span class="op">=</span> torch.randn((fan_in, fan_out), generator<span class="op">=</span>g) <span class="co"># / fan_in**0.5</span></span>
<span id="cb58-8"><a href="#cb58-8"></a>        <span class="va">self</span>.bias <span class="op">=</span> torch.zeros(fan_out) <span class="cf">if</span> bias <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb58-9"><a href="#cb58-9"></a></span>
<span id="cb58-10"><a href="#cb58-10"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb58-11"><a href="#cb58-11"></a>        <span class="va">self</span>.out <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.weight</span>
<span id="cb58-12"><a href="#cb58-12"></a>        <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb58-13"><a href="#cb58-13"></a>            <span class="va">self</span>.out <span class="op">+=</span> <span class="va">self</span>.bias</span>
<span id="cb58-14"><a href="#cb58-14"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb58-15"><a href="#cb58-15"></a></span>
<span id="cb58-16"><a href="#cb58-16"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb58-17"><a href="#cb58-17"></a>        <span class="cf">return</span> [<span class="va">self</span>.weight] <span class="op">+</span> ([] <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> [<span class="va">self</span>.bias])</span>
<span id="cb58-18"><a href="#cb58-18"></a></span>
<span id="cb58-19"><a href="#cb58-19"></a>n_embd <span class="op">=</span> <span class="dv">10</span> <span class="co"># the dimensionality of the character embedding vectors</span></span>
<span id="cb58-20"><a href="#cb58-20"></a>n_hidden <span class="op">=</span> <span class="dv">100</span> <span class="co"># the number of neurons in the hidden layer of the MLP</span></span>
<span id="cb58-21"><a href="#cb58-21"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>) <span class="co"># for reproducibility</span></span>
<span id="cb58-22"><a href="#cb58-22"></a>C <span class="op">=</span> torch.randn((vocab_size, n_embd),            generator<span class="op">=</span>g)</span>
<span id="cb58-23"><a href="#cb58-23"></a>layers <span class="op">=</span> [</span>
<span id="cb58-24"><a href="#cb58-24"></a>  Linear(n_embd <span class="op">*</span> block_size, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb58-25"><a href="#cb58-25"></a>  Linear(           n_hidden, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb58-26"><a href="#cb58-26"></a>  Linear(           n_hidden, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb58-27"><a href="#cb58-27"></a>  Linear(           n_hidden, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb58-28"><a href="#cb58-28"></a>  Linear(           n_hidden, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb58-29"><a href="#cb58-29"></a>  Linear(           n_hidden, vocab_size, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(vocab_size),</span>
<span id="cb58-30"><a href="#cb58-30"></a>]</span>
<span id="cb58-31"><a href="#cb58-31"></a></span>
<span id="cb58-32"><a href="#cb58-32"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb58-33"><a href="#cb58-33"></a>    <span class="co"># last layer: make less confident</span></span>
<span id="cb58-34"><a href="#cb58-34"></a>    layers[<span class="op">-</span><span class="dv">1</span>].gamma <span class="op">*=</span> <span class="fl">0.1</span></span>
<span id="cb58-35"><a href="#cb58-35"></a>    <span class="co"># all other layers: apply gain</span></span>
<span id="cb58-36"><a href="#cb58-36"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layers[:<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb58-37"><a href="#cb58-37"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(layer, Linear):</span>
<span id="cb58-38"><a href="#cb58-38"></a>            layer.weight <span class="op">*=</span> <span class="fl">1.0</span> <span class="co">#5/3</span></span>
<span id="cb58-39"><a href="#cb58-39"></a></span>
<span id="cb58-40"><a href="#cb58-40"></a>parameters <span class="op">=</span> [C] <span class="op">+</span> [p <span class="cf">for</span> layer <span class="kw">in</span> layers <span class="cf">for</span> p <span class="kw">in</span> layer.parameters()]</span>
<span id="cb58-41"><a href="#cb58-41"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters)) <span class="co"># number of parameters in total</span></span>
<span id="cb58-42"><a href="#cb58-42"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb58-43"><a href="#cb58-43"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb58-44"><a href="#cb58-44"></a></span>
<span id="cb58-45"><a href="#cb58-45"></a><span class="co"># same optimization as last time</span></span>
<span id="cb58-46"><a href="#cb58-46"></a>max_steps <span class="op">=</span> <span class="dv">200000</span></span>
<span id="cb58-47"><a href="#cb58-47"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb58-48"><a href="#cb58-48"></a>lossi <span class="op">=</span> []</span>
<span id="cb58-49"><a href="#cb58-49"></a>ud <span class="op">=</span> []</span>
<span id="cb58-50"><a href="#cb58-50"></a></span>
<span id="cb58-51"><a href="#cb58-51"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb58-52"><a href="#cb58-52"></a>  </span>
<span id="cb58-53"><a href="#cb58-53"></a>    <span class="co"># minibatch construct</span></span>
<span id="cb58-54"><a href="#cb58-54"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,), generator<span class="op">=</span>g)</span>
<span id="cb58-55"><a href="#cb58-55"></a>    Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix] <span class="co"># batch X,Y</span></span>
<span id="cb58-56"><a href="#cb58-56"></a></span>
<span id="cb58-57"><a href="#cb58-57"></a>    <span class="co"># forward pass</span></span>
<span id="cb58-58"><a href="#cb58-58"></a>    emb <span class="op">=</span> C[Xb] <span class="co"># embed the characters into vectors</span></span>
<span id="cb58-59"><a href="#cb58-59"></a>    x <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate the vectors</span></span>
<span id="cb58-60"><a href="#cb58-60"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb58-61"><a href="#cb58-61"></a>        x <span class="op">=</span> layer(x)</span>
<span id="cb58-62"><a href="#cb58-62"></a>    loss <span class="op">=</span> F.cross_entropy(x, Yb) <span class="co"># loss function</span></span>
<span id="cb58-63"><a href="#cb58-63"></a></span>
<span id="cb58-64"><a href="#cb58-64"></a>    <span class="co"># backward pass</span></span>
<span id="cb58-65"><a href="#cb58-65"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb58-66"><a href="#cb58-66"></a>        layer.out.retain_grad() <span class="co"># AFTER_DEBUG: would take out retain_graph</span></span>
<span id="cb58-67"><a href="#cb58-67"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb58-68"><a href="#cb58-68"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb58-69"><a href="#cb58-69"></a>    loss.backward()</span>
<span id="cb58-70"><a href="#cb58-70"></a></span>
<span id="cb58-71"><a href="#cb58-71"></a>    <span class="co"># update</span></span>
<span id="cb58-72"><a href="#cb58-72"></a>    lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">150000</span> <span class="cf">else</span> <span class="fl">0.01</span> <span class="co"># step learning rate decay</span></span>
<span id="cb58-73"><a href="#cb58-73"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb58-74"><a href="#cb58-74"></a>        p.data <span class="op">+=</span> <span class="op">-</span>lr <span class="op">*</span> p.grad</span>
<span id="cb58-75"><a href="#cb58-75"></a></span>
<span id="cb58-76"><a href="#cb58-76"></a>    <span class="co"># track stats</span></span>
<span id="cb58-77"><a href="#cb58-77"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="co"># print every once in a while</span></span>
<span id="cb58-78"><a href="#cb58-78"></a>        <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">:7d}</span><span class="ss">/</span><span class="sc">{</span>max_steps<span class="sc">:7d}</span><span class="ss">: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb58-79"><a href="#cb58-79"></a>    lossi.append(loss.log10().item())</span>
<span id="cb58-80"><a href="#cb58-80"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb58-81"><a href="#cb58-81"></a>        ud.append([((lr<span class="op">*</span>p.grad).std() <span class="op">/</span> p.data.std()).log10().item() <span class="cf">for</span> p <span class="kw">in</span> parameters])</span>
<span id="cb58-82"><a href="#cb58-82"></a></span>
<span id="cb58-83"><a href="#cb58-83"></a>    <span class="co"># break</span></span>
<span id="cb58-84"><a href="#cb58-84"></a>    <span class="co"># if i &gt;= 1000:</span></span>
<span id="cb58-85"><a href="#cb58-85"></a>    <span class="co">#     break # AFTER_DEBUG: would take out obviously to run full optimization</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>47024
      0/ 200000: 3.2870
  10000/ 200000: 2.4521
  20000/ 200000: 2.0847
  30000/ 200000: 2.1838
  40000/ 200000: 2.1515
  50000/ 200000: 2.2246
  60000/ 200000: 1.9450
  70000/ 200000: 2.2514
  80000/ 200000: 2.4420
  90000/ 200000: 2.0624
 100000/ 200000: 2.5850
 110000/ 200000: 2.3225
 120000/ 200000: 2.2004
 130000/ 200000: 2.0352
 140000/ 200000: 1.8516
 150000/ 200000: 2.0424
 160000/ 200000: 2.2229
 170000/ 200000: 2.0384
 180000/ 200000: 2.2274
 190000/ 200000: 2.0901</code></pre>
</div>
</div>
<p>Final visualization:</p>
<p>Viz 1:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="viz1_final.png" class="img-fluid figure-img"></p>
<figcaption>Viz 1 final</figcaption>
</figure>
</div>
<p>Viz 2:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="viz2_final.png" class="img-fluid figure-img"></p>
<figcaption>Viz 2 final</figcaption>
</figure>
</div>
<p>Viz 3:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="viz3_final.png" class="img-fluid figure-img"></p>
<figcaption>Viz 3 final</figcaption>
</figure>
</div>
<p>Viz 4:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="viz4_final.png" class="img-fluid figure-img"></p>
<figcaption>Viz 4 final</figcaption>
</figure>
</div>
<p>The final loss on train/val:</p>
<div id="90609a85" class="cell" data-execution_count="36">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1"></a><span class="at">@torch.no_grad</span>() <span class="co"># this decorator disables gradient tracking</span></span>
<span id="cb60-2"><a href="#cb60-2"></a><span class="kw">def</span> split_loss(split):</span>
<span id="cb60-3"><a href="#cb60-3"></a>  x,y <span class="op">=</span> {</span>
<span id="cb60-4"><a href="#cb60-4"></a>    <span class="st">'train'</span>: (Xtr, Ytr),</span>
<span id="cb60-5"><a href="#cb60-5"></a>    <span class="st">'val'</span>: (Xdev, Ydev),</span>
<span id="cb60-6"><a href="#cb60-6"></a>    <span class="st">'test'</span>: (Xte, Yte),</span>
<span id="cb60-7"><a href="#cb60-7"></a>  }[split]</span>
<span id="cb60-8"><a href="#cb60-8"></a>  emb <span class="op">=</span> C[x] <span class="co"># (N, block_size, n_embd)</span></span>
<span id="cb60-9"><a href="#cb60-9"></a>  x <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concat into (N, block_size * n_embd)</span></span>
<span id="cb60-10"><a href="#cb60-10"></a>  <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb60-11"><a href="#cb60-11"></a>    x <span class="op">=</span> layer(x)</span>
<span id="cb60-12"><a href="#cb60-12"></a>  loss <span class="op">=</span> F.cross_entropy(x, y)</span>
<span id="cb60-13"><a href="#cb60-13"></a>  <span class="bu">print</span>(split, loss.item())</span>
<span id="cb60-14"><a href="#cb60-14"></a></span>
<span id="cb60-15"><a href="#cb60-15"></a><span class="co"># put layers into eval mode</span></span>
<span id="cb60-16"><a href="#cb60-16"></a><span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb60-17"><a href="#cb60-17"></a>  layer.training <span class="op">=</span> <span class="va">False</span></span>
<span id="cb60-18"><a href="#cb60-18"></a>split_loss(<span class="st">'train'</span>)</span>
<span id="cb60-19"><a href="#cb60-19"></a>split_loss(<span class="st">'val'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>train 2.103635549545288
val 2.1365904808044434</code></pre>
</div>
</div>
<p>Sample from the model:</p>
<div id="711ba3e9" class="cell" data-execution_count="37">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1"></a><span class="co"># sample from the model</span></span>
<span id="cb62-2"><a href="#cb62-2"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span> <span class="op">+</span> <span class="dv">10</span>)</span>
<span id="cb62-3"><a href="#cb62-3"></a></span>
<span id="cb62-4"><a href="#cb62-4"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb62-5"><a href="#cb62-5"></a>    </span>
<span id="cb62-6"><a href="#cb62-6"></a>    out <span class="op">=</span> []</span>
<span id="cb62-7"><a href="#cb62-7"></a>    context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size <span class="co"># initialize with all ...</span></span>
<span id="cb62-8"><a href="#cb62-8"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb62-9"><a href="#cb62-9"></a>      <span class="co"># forward pass the neural net</span></span>
<span id="cb62-10"><a href="#cb62-10"></a>      emb <span class="op">=</span> C[torch.tensor([context])] <span class="co"># (1,block_size,n_embd)</span></span>
<span id="cb62-11"><a href="#cb62-11"></a>      x <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate the vectors</span></span>
<span id="cb62-12"><a href="#cb62-12"></a>      <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb62-13"><a href="#cb62-13"></a>        x <span class="op">=</span> layer(x)</span>
<span id="cb62-14"><a href="#cb62-14"></a>      logits <span class="op">=</span> x</span>
<span id="cb62-15"><a href="#cb62-15"></a>      probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb62-16"><a href="#cb62-16"></a>      <span class="co"># sample from the distribution</span></span>
<span id="cb62-17"><a href="#cb62-17"></a>      ix <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>, generator<span class="op">=</span>g).item()</span>
<span id="cb62-18"><a href="#cb62-18"></a>      <span class="co"># shift the context window and track the samples</span></span>
<span id="cb62-19"><a href="#cb62-19"></a>      context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix]</span>
<span id="cb62-20"><a href="#cb62-20"></a>      out.append(ix)</span>
<span id="cb62-21"><a href="#cb62-21"></a>      <span class="co"># if we sample the special '.' token, break</span></span>
<span id="cb62-22"><a href="#cb62-22"></a>      <span class="cf">if</span> ix <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb62-23"><a href="#cb62-23"></a>        <span class="cf">break</span></span>
<span id="cb62-24"><a href="#cb62-24"></a>    </span>
<span id="cb62-25"><a href="#cb62-25"></a>    <span class="bu">print</span>(<span class="st">''</span>.join(itos[i] <span class="cf">for</span> i <span class="kw">in</span> out)) <span class="co"># decode and print the generated word</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>mona.
mayannielle.
dhryah.
rethan.
ejdraeg.
adelynnelin.
shi.
jen.
edelisson.
arleigh.
malaia.
nosadbergiaghiel.
kinde.
jennex.
terofius.
kaven.
jamyleyeh.
yuma.
myston.
azhil.</code></pre>
</div>
</div>
<p>Happy learning!</p>
</section>
<section id="exercises" class="level2">
<h2 class="anchored" data-anchor-id="exercises">Exercises:</h2>
<ul>
<li>E01: I did not get around to seeing what happens when you initialize all weights and biases to zero. Try this and train the neural net. You might think either that 1) the network trains just fine or 2) the network doesn’t train at all, but actually it is 3) the network trains but only partially, and achieves a pretty bad final performance. Inspect the gradients and activations to figure out what is happening and why the network is only partially training, and what part is being trained exactly.</li>
<li>E02: BatchNorm, unlike other normalization layers like LayerNorm/GroupNorm etc. has the big advantage that after training, the batchnorm gamma/beta can be “folded into” the weights of the preceeding Linear layers, effectively erasing the need to forward it at test time. Set up a small 3-layer MLP with batchnorms, train the network, then “fold” the batchnorm gamma/beta into the preceeding Linear layer’s W,b by creating a new W2, b2 and erasing the batch norm. Verify that this gives the same forward pass during inference. i.e.&nbsp;we see that the batchnorm is there just for stabilizing the training, and can be thrown out after training is done! pretty cool.</li>
</ul>
</section>
</section>
<section id="resources" class="level1">
<h1>resources:</h1>
<ol type="1">
<li>other people learn from AK like me: <a href="https://bedirtapkan.com/posts/blog_posts/karpathy_3_makemore_activations/" class="uri">https://bedirtapkan.com/posts/blog_posts/karpathy_3_makemore_activations/</a>; <a href="https://skeptric.com/index.html#category=makemore" class="uri">https://skeptric.com/index.html#category=makemore</a> - a replicate (?) with more OOPs on another dataset;</li>
<li>some good papers recommended by Andrej:
<ul>
<li>“Kaiming init” paper: <a href="https://arxiv.org/abs/1502.01852" class="uri">https://arxiv.org/abs/1502.01852</a>;</li>
<li>BatchNorm paper: <a href="https://arxiv.org/abs/1502.03167" class="uri">https://arxiv.org/abs/1502.03167</a>;</li>
<li>Bengio et al.&nbsp;2003 MLP language model paper (pdf): <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" class="uri">https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf</a>;</li>
<li>Good paper illustrating some of the problems with batchnorm in practice: <a href="https://arxiv.org/abs/2105.07576" class="uri">https://arxiv.org/abs/2105.07576</a>.</li>
</ul></li>
<li>Notebook: <a href="https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part3_bn.ipynb" class="uri">https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part3_bn.ipynb</a></li>
</ol>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/lktuan\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb64" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb64-1"><a href="#cb64-1"></a><span class="co">---</span></span>
<span id="cb64-2"><a href="#cb64-2"></a><span class="an">title:</span><span class="co"> "NN-Z2H Lesson 4: Building makemore part 3 - Activations &amp; Gradients, BatchNorm"</span></span>
<span id="cb64-3"><a href="#cb64-3"></a><span class="an">description:</span><span class="co"> "dive into the internals of MLPs, scrutinize the statistics of the forward pass activations, backward pass gradients, understand the health of your deep network, introduce batch normalization"</span></span>
<span id="cb64-4"><a href="#cb64-4"></a><span class="an">author:</span></span>
<span id="cb64-5"><a href="#cb64-5"></a><span class="co">  - name: "Tuan Le Khac"</span></span>
<span id="cb64-6"><a href="#cb64-6"></a><span class="co">    url: https://lktuan.github.io/</span></span>
<span id="cb64-7"><a href="#cb64-7"></a><span class="an">categories:</span><span class="co"> [til, python, andrej karpathy, nn-z2h, neural networks] </span></span>
<span id="cb64-8"><a href="#cb64-8"></a><span class="an">date:</span><span class="co"> 11-26-2024</span></span>
<span id="cb64-9"><a href="#cb64-9"></a><span class="an">date-modified:</span><span class="co"> 11-29-2024</span></span>
<span id="cb64-10"><a href="#cb64-10"></a><span class="an">image:</span><span class="co"> resnet50.png</span></span>
<span id="cb64-11"><a href="#cb64-11"></a><span class="an">draft:</span><span class="co"> false</span></span>
<span id="cb64-12"><a href="#cb64-12"></a><span class="an">fig-cap-location:</span><span class="co"> bottom</span></span>
<span id="cb64-13"><a href="#cb64-13"></a><span class="an">editor:</span><span class="co"> visual</span></span>
<span id="cb64-14"><a href="#cb64-14"></a><span class="an">format:</span></span>
<span id="cb64-15"><a href="#cb64-15"></a><span class="co">  html:</span></span>
<span id="cb64-16"><a href="#cb64-16"></a><span class="co">    code-overflow: wrap</span></span>
<span id="cb64-17"><a href="#cb64-17"></a><span class="co">    code-tools: true</span></span>
<span id="cb64-18"><a href="#cb64-18"></a><span class="co">    code-fold: show</span></span>
<span id="cb64-19"><a href="#cb64-19"></a><span class="co">    code-annotations: hover</span></span>
<span id="cb64-20"><a href="#cb64-20"></a><span class="co">---</span></span>
<span id="cb64-21"><a href="#cb64-21"></a></span>
<span id="cb64-22"><a href="#cb64-22"></a>::: {.callout-important title="This is not orginal content!"}</span>
<span id="cb64-23"><a href="#cb64-23"></a>This is my study notes / codes along with Andrej Karpathy's "<span class="co">[</span><span class="ot">Neural Networks: Zero to Hero</span><span class="co">](https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)</span>" series.</span>
<span id="cb64-24"><a href="#cb64-24"></a>:::</span>
<span id="cb64-25"><a href="#cb64-25"></a></span>
<span id="cb64-26"><a href="#cb64-26"></a>We want to stay a bit longer with the MLPs, to have more concrete intuitive of the **activations** in the neural nets and **gradients** that flowing backwards. It's good to learn about the development history of these architectures. Since Recurrent Neural Network (RNN), they are although very *expressive* but not easily *optimizable* with current gradient techniques we have so far. Let's get started!</span>
<span id="cb64-27"><a href="#cb64-27"></a></span>
<span id="cb64-28"><a href="#cb64-28"></a><span class="fu"># Part 1: intro</span></span>
<span id="cb64-29"><a href="#cb64-29"></a></span>
<span id="cb64-30"><a href="#cb64-30"></a><span class="fu">## starter code</span></span>
<span id="cb64-31"><a href="#cb64-31"></a></span>
<span id="cb64-34"><a href="#cb64-34"></a><span class="in">```{python}</span></span>
<span id="cb64-35"><a href="#cb64-35"></a><span class="im">import</span> torch</span>
<span id="cb64-36"><a href="#cb64-36"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb64-37"><a href="#cb64-37"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb64-38"><a href="#cb64-38"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb64-39"><a href="#cb64-39"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb64-40"><a href="#cb64-40"></a><span class="in">```</span></span>
<span id="cb64-41"><a href="#cb64-41"></a></span>
<span id="cb64-44"><a href="#cb64-44"></a><span class="in">```{python}</span></span>
<span id="cb64-45"><a href="#cb64-45"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb64-46"><a href="#cb64-46"></a></span>
<span id="cb64-47"><a href="#cb64-47"></a>url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt"</span></span>
<span id="cb64-48"><a href="#cb64-48"></a>words <span class="op">=</span> pd.read_csv(url, header<span class="op">=</span><span class="va">None</span>).iloc[:, <span class="dv">0</span>].tolist()</span>
<span id="cb64-49"><a href="#cb64-49"></a>words[:<span class="dv">8</span>]</span>
<span id="cb64-50"><a href="#cb64-50"></a><span class="in">```</span></span>
<span id="cb64-51"><a href="#cb64-51"></a></span>
<span id="cb64-54"><a href="#cb64-54"></a><span class="in">```{python}</span></span>
<span id="cb64-55"><a href="#cb64-55"></a><span class="bu">len</span>(words)</span>
<span id="cb64-56"><a href="#cb64-56"></a><span class="in">```</span></span>
<span id="cb64-57"><a href="#cb64-57"></a></span>
<span id="cb64-60"><a href="#cb64-60"></a><span class="in">```{python}</span></span>
<span id="cb64-61"><a href="#cb64-61"></a><span class="co"># build the vocabulary of characters and mapping to/from integer</span></span>
<span id="cb64-62"><a href="#cb64-62"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(<span class="st">''</span>.join(words))))</span>
<span id="cb64-63"><a href="#cb64-63"></a>stoi <span class="op">=</span> {s:i<span class="op">+</span><span class="dv">1</span> <span class="cf">for</span> i, s <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb64-64"><a href="#cb64-64"></a>stoi[<span class="st">'.'</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb64-65"><a href="#cb64-65"></a>itos <span class="op">=</span> {i: s <span class="cf">for</span> s, i <span class="kw">in</span> stoi.items()}</span>
<span id="cb64-66"><a href="#cb64-66"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(itos)</span>
<span id="cb64-67"><a href="#cb64-67"></a><span class="bu">print</span>(itos)</span>
<span id="cb64-68"><a href="#cb64-68"></a><span class="bu">print</span>(vocab_size)</span>
<span id="cb64-69"><a href="#cb64-69"></a><span class="in">```</span></span>
<span id="cb64-70"><a href="#cb64-70"></a></span>
<span id="cb64-73"><a href="#cb64-73"></a><span class="in">```{python}</span></span>
<span id="cb64-74"><a href="#cb64-74"></a>block_size <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb64-75"><a href="#cb64-75"></a><span class="co"># build the dataset</span></span>
<span id="cb64-76"><a href="#cb64-76"></a><span class="kw">def</span> buid_dataset(words):</span>
<span id="cb64-77"><a href="#cb64-77"></a>    X, Y <span class="op">=</span> [], []</span>
<span id="cb64-78"><a href="#cb64-78"></a></span>
<span id="cb64-79"><a href="#cb64-79"></a>    <span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb64-80"><a href="#cb64-80"></a>        context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size</span>
<span id="cb64-81"><a href="#cb64-81"></a>        <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb64-82"><a href="#cb64-82"></a>            ix <span class="op">=</span> stoi[ch]</span>
<span id="cb64-83"><a href="#cb64-83"></a>            X.append(context)</span>
<span id="cb64-84"><a href="#cb64-84"></a>            Y.append(ix)</span>
<span id="cb64-85"><a href="#cb64-85"></a>            context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix]</span>
<span id="cb64-86"><a href="#cb64-86"></a></span>
<span id="cb64-87"><a href="#cb64-87"></a>    X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb64-88"><a href="#cb64-88"></a>    Y <span class="op">=</span> torch.tensor(Y)</span>
<span id="cb64-89"><a href="#cb64-89"></a>    <span class="bu">print</span>(X.shape, Y.shape)</span>
<span id="cb64-90"><a href="#cb64-90"></a>    <span class="cf">return</span> X, Y</span>
<span id="cb64-91"><a href="#cb64-91"></a></span>
<span id="cb64-92"><a href="#cb64-92"></a><span class="im">import</span> random</span>
<span id="cb64-93"><a href="#cb64-93"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb64-94"><a href="#cb64-94"></a>random.shuffle(words)</span>
<span id="cb64-95"><a href="#cb64-95"></a>n1 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb64-96"><a href="#cb64-96"></a>n2 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb64-97"><a href="#cb64-97"></a></span>
<span id="cb64-98"><a href="#cb64-98"></a>Xtr, Ytr <span class="op">=</span> buid_dataset(words[:n1])        <span class="co"># 80#</span></span>
<span id="cb64-99"><a href="#cb64-99"></a>Xdev, Ydev <span class="op">=</span> buid_dataset(words[n1:n2])    <span class="co"># 10%</span></span>
<span id="cb64-100"><a href="#cb64-100"></a>Xte, Yte <span class="op">=</span> buid_dataset(words[n2:])        <span class="co"># 10%</span></span>
<span id="cb64-101"><a href="#cb64-101"></a><span class="in">```</span></span>
<span id="cb64-102"><a href="#cb64-102"></a></span>
<span id="cb64-105"><a href="#cb64-105"></a><span class="in">```{python}</span></span>
<span id="cb64-106"><a href="#cb64-106"></a><span class="co"># MLP revisited</span></span>
<span id="cb64-107"><a href="#cb64-107"></a>n_emb <span class="op">=</span> <span class="dv">10</span> <span class="co"># no of dimensions of the embedding space.</span></span>
<span id="cb64-108"><a href="#cb64-108"></a>n_hidden <span class="op">=</span> <span class="dv">200</span> <span class="co"># size of the hidden - tanh layer</span></span>
<span id="cb64-109"><a href="#cb64-109"></a></span>
<span id="cb64-110"><a href="#cb64-110"></a><span class="co"># Lookup table - 10 dimensional space</span></span>
<span id="cb64-111"><a href="#cb64-111"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>) <span class="co"># for reproductivity</span></span>
<span id="cb64-112"><a href="#cb64-112"></a>C <span class="op">=</span> torch.randn((vocab_size, n_emb),                  generator<span class="op">=</span>g)</span>
<span id="cb64-113"><a href="#cb64-113"></a></span>
<span id="cb64-114"><a href="#cb64-114"></a><span class="co"># Layer 1 - tanh - 300 neurons</span></span>
<span id="cb64-115"><a href="#cb64-115"></a>W1 <span class="op">=</span> torch.randn((block_size <span class="op">*</span> n_emb, n_hidden),      generator<span class="op">=</span>g)</span>
<span id="cb64-116"><a href="#cb64-116"></a>b1 <span class="op">=</span> torch.randn(n_hidden,                            generator<span class="op">=</span>g)</span>
<span id="cb64-117"><a href="#cb64-117"></a></span>
<span id="cb64-118"><a href="#cb64-118"></a><span class="co"># Layer 2 - softmax</span></span>
<span id="cb64-119"><a href="#cb64-119"></a>W2 <span class="op">=</span> torch.randn((n_hidden, vocab_size),              generator<span class="op">=</span>g)</span>
<span id="cb64-120"><a href="#cb64-120"></a>b2 <span class="op">=</span> torch.randn(vocab_size,                          generator<span class="op">=</span>g)</span>
<span id="cb64-121"><a href="#cb64-121"></a></span>
<span id="cb64-122"><a href="#cb64-122"></a><span class="co"># All params</span></span>
<span id="cb64-123"><a href="#cb64-123"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb64-124"><a href="#cb64-124"></a><span class="bu">print</span>(<span class="st">"No of params: "</span>, <span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span>
<span id="cb64-125"><a href="#cb64-125"></a></span>
<span id="cb64-126"><a href="#cb64-126"></a><span class="co"># Pre-training</span></span>
<span id="cb64-127"><a href="#cb64-127"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb64-128"><a href="#cb64-128"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb64-129"><a href="#cb64-129"></a><span class="in">```</span></span>
<span id="cb64-130"><a href="#cb64-130"></a></span>
<span id="cb64-133"><a href="#cb64-133"></a><span class="in">```{python}</span></span>
<span id="cb64-134"><a href="#cb64-134"></a><span class="co"># Optimization</span></span>
<span id="cb64-135"><a href="#cb64-135"></a>max_steps <span class="op">=</span> <span class="dv">50_000</span> <span class="co">#200_000</span></span>
<span id="cb64-136"><a href="#cb64-136"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb64-137"><a href="#cb64-137"></a></span>
<span id="cb64-138"><a href="#cb64-138"></a><span class="co"># Stats holders</span></span>
<span id="cb64-139"><a href="#cb64-139"></a>lossi <span class="op">=</span> []</span>
<span id="cb64-140"><a href="#cb64-140"></a></span>
<span id="cb64-141"><a href="#cb64-141"></a><span class="co"># Training on Xtr, Ytr</span></span>
<span id="cb64-142"><a href="#cb64-142"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb64-143"><a href="#cb64-143"></a></span>
<span id="cb64-144"><a href="#cb64-144"></a>    <span class="co"># minibatch construct      </span></span>
<span id="cb64-145"><a href="#cb64-145"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,)) </span>
<span id="cb64-146"><a href="#cb64-146"></a>    Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix] <span class="co"># batch X, Y</span></span>
<span id="cb64-147"><a href="#cb64-147"></a></span>
<span id="cb64-148"><a href="#cb64-148"></a>    <span class="co"># forward pass:</span></span>
<span id="cb64-149"><a href="#cb64-149"></a>    emb <span class="op">=</span> C[Xb] <span class="co"># embed the characters into vectors   </span></span>
<span id="cb64-150"><a href="#cb64-150"></a>    emb_cat <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate the vectors</span></span>
<span id="cb64-151"><a href="#cb64-151"></a>    h_pre_act <span class="op">=</span> emb_cat <span class="op">@</span> W1 <span class="op">+</span> b1 <span class="co"># hidden layer pre-activation</span></span>
<span id="cb64-152"><a href="#cb64-152"></a>    h <span class="op">=</span> torch.tanh(h_pre_act) <span class="co"># hidden layer</span></span>
<span id="cb64-153"><a href="#cb64-153"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># output layer</span></span>
<span id="cb64-154"><a href="#cb64-154"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Yb) <span class="co"># loss function</span></span>
<span id="cb64-155"><a href="#cb64-155"></a></span>
<span id="cb64-156"><a href="#cb64-156"></a>    <span class="co"># backward pass:</span></span>
<span id="cb64-157"><a href="#cb64-157"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb64-158"><a href="#cb64-158"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb64-159"><a href="#cb64-159"></a>    loss.backward()</span>
<span id="cb64-160"><a href="#cb64-160"></a></span>
<span id="cb64-161"><a href="#cb64-161"></a>    <span class="co"># update</span></span>
<span id="cb64-162"><a href="#cb64-162"></a>    lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;=</span> max_steps <span class="op">/</span> <span class="dv">2</span> <span class="cf">else</span> <span class="fl">0.01</span> <span class="co"># step learning rate decay</span></span>
<span id="cb64-163"><a href="#cb64-163"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb64-164"><a href="#cb64-164"></a>        p.data <span class="op">+=</span> <span class="op">-</span> lr <span class="op">*</span> p.grad</span>
<span id="cb64-165"><a href="#cb64-165"></a></span>
<span id="cb64-166"><a href="#cb64-166"></a>    <span class="co"># track stats</span></span>
<span id="cb64-167"><a href="#cb64-167"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="co"># print once every while</span></span>
<span id="cb64-168"><a href="#cb64-168"></a>      <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">:7d}</span><span class="ss">/</span><span class="sc">{</span>max_steps<span class="sc">:7d}</span><span class="ss">: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb64-169"><a href="#cb64-169"></a>    lossi.append(loss.log10().item())</span>
<span id="cb64-170"><a href="#cb64-170"></a><span class="in">```</span></span>
<span id="cb64-171"><a href="#cb64-171"></a></span>
<span id="cb64-174"><a href="#cb64-174"></a><span class="in">```{python}</span></span>
<span id="cb64-175"><a href="#cb64-175"></a>plt.plot(lossi)</span>
<span id="cb64-176"><a href="#cb64-176"></a><span class="in">```</span></span>
<span id="cb64-177"><a href="#cb64-177"></a></span>
<span id="cb64-180"><a href="#cb64-180"></a><span class="in">```{python}</span></span>
<span id="cb64-181"><a href="#cb64-181"></a><span class="at">@torch.no_grad</span>() <span class="co"># disables gradient tracking</span></span>
<span id="cb64-182"><a href="#cb64-182"></a><span class="kw">def</span> split_loss(split: <span class="bu">str</span>):</span>
<span id="cb64-183"><a href="#cb64-183"></a>  x, y <span class="op">=</span> {</span>
<span id="cb64-184"><a href="#cb64-184"></a>    <span class="st">'train'</span>: (Xtr, Ytr),</span>
<span id="cb64-185"><a href="#cb64-185"></a>    <span class="st">'val'</span>: (Xdev, Ydev),</span>
<span id="cb64-186"><a href="#cb64-186"></a>    <span class="st">'test'</span>: (Xte, Yte)</span>
<span id="cb64-187"><a href="#cb64-187"></a>  }[split]</span>
<span id="cb64-188"><a href="#cb64-188"></a>  emb <span class="op">=</span> C[x] <span class="co"># (N, block_size, n_emb)</span></span>
<span id="cb64-189"><a href="#cb64-189"></a>  emb_cat <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate into (N, block_size * n_emb)</span></span>
<span id="cb64-190"><a href="#cb64-190"></a>  h <span class="op">=</span> torch.tanh(emb_cat <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># (N, n_hidden)</span></span>
<span id="cb64-191"><a href="#cb64-191"></a>  logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (N, vocab_size)</span></span>
<span id="cb64-192"><a href="#cb64-192"></a>  loss <span class="op">=</span> F.cross_entropy(logits, y) <span class="co"># loss function</span></span>
<span id="cb64-193"><a href="#cb64-193"></a>  <span class="bu">print</span>(split, loss.item())</span>
<span id="cb64-194"><a href="#cb64-194"></a></span>
<span id="cb64-195"><a href="#cb64-195"></a>split_loss(<span class="st">'train'</span>)</span>
<span id="cb64-196"><a href="#cb64-196"></a>split_loss(<span class="st">'val'</span>)</span>
<span id="cb64-197"><a href="#cb64-197"></a><span class="in">```</span></span>
<span id="cb64-198"><a href="#cb64-198"></a></span>
<span id="cb64-201"><a href="#cb64-201"></a><span class="in">```{python}</span></span>
<span id="cb64-202"><a href="#cb64-202"></a><span class="co"># sample from the model</span></span>
<span id="cb64-203"><a href="#cb64-203"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span> <span class="op">+</span> <span class="dv">10</span>)</span>
<span id="cb64-204"><a href="#cb64-204"></a></span>
<span id="cb64-205"><a href="#cb64-205"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb64-206"><a href="#cb64-206"></a>    </span>
<span id="cb64-207"><a href="#cb64-207"></a>    out <span class="op">=</span> []</span>
<span id="cb64-208"><a href="#cb64-208"></a>    context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size <span class="co"># initialize with all ...</span></span>
<span id="cb64-209"><a href="#cb64-209"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb64-210"><a href="#cb64-210"></a>      <span class="co"># forward pass the neural net</span></span>
<span id="cb64-211"><a href="#cb64-211"></a>      emb <span class="op">=</span> C[torch.tensor([context])] <span class="co"># (1,block_size,n_embd)</span></span>
<span id="cb64-212"><a href="#cb64-212"></a>      h <span class="op">=</span> torch.tanh(emb.view(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">@</span> W1 <span class="op">+</span> b1)</span>
<span id="cb64-213"><a href="#cb64-213"></a>      logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb64-214"><a href="#cb64-214"></a>      probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb64-215"><a href="#cb64-215"></a>      <span class="co"># sample from the distribution</span></span>
<span id="cb64-216"><a href="#cb64-216"></a>      ix <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>, generator<span class="op">=</span>g).item()</span>
<span id="cb64-217"><a href="#cb64-217"></a>      <span class="co"># shift the context window and track the samples</span></span>
<span id="cb64-218"><a href="#cb64-218"></a>      context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix]</span>
<span id="cb64-219"><a href="#cb64-219"></a>      out.append(ix)</span>
<span id="cb64-220"><a href="#cb64-220"></a>      <span class="co"># if we sample the special '.' token, break</span></span>
<span id="cb64-221"><a href="#cb64-221"></a>      <span class="cf">if</span> ix <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb64-222"><a href="#cb64-222"></a>        <span class="cf">break</span></span>
<span id="cb64-223"><a href="#cb64-223"></a>    </span>
<span id="cb64-224"><a href="#cb64-224"></a>    <span class="bu">print</span>(<span class="st">''</span>.join(itos[i] <span class="cf">for</span> i <span class="kw">in</span> out)) <span class="co"># decode and print the generated word</span></span>
<span id="cb64-225"><a href="#cb64-225"></a><span class="in">```</span></span>
<span id="cb64-226"><a href="#cb64-226"></a></span>
<span id="cb64-227"><a href="#cb64-227"></a>Okay so now our network has multiple things wrong at the initialization, let's list down below. The final code will be presented in the end of part 1, with <span class="in">`# 👈`</span> for lines that had been added / modified. The right code cell below re-initializes states at the beginning of network's parameter (in my notebook, it's rendered **linearly**!).</span>
<span id="cb64-228"><a href="#cb64-228"></a></span>
<span id="cb64-231"><a href="#cb64-231"></a><span class="in">```{python}</span></span>
<span id="cb64-232"><a href="#cb64-232"></a><span class="co">#| code-fold: true</span></span>
<span id="cb64-233"><a href="#cb64-233"></a>n_emb <span class="op">=</span> <span class="dv">10</span> <span class="co"># no of dimensions of the embedding space.</span></span>
<span id="cb64-234"><a href="#cb64-234"></a>n_hidden <span class="op">=</span> <span class="dv">200</span> <span class="co"># size of the hidden - tanh layer</span></span>
<span id="cb64-235"><a href="#cb64-235"></a><span class="co"># Lookup table - 10 dimensional space</span></span>
<span id="cb64-236"><a href="#cb64-236"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>) <span class="co"># for reproductivity</span></span>
<span id="cb64-237"><a href="#cb64-237"></a>C <span class="op">=</span> torch.randn((vocab_size, n_emb),                  generator<span class="op">=</span>g)</span>
<span id="cb64-238"><a href="#cb64-238"></a><span class="co"># Layer 1 - tanh - 300 neurons</span></span>
<span id="cb64-239"><a href="#cb64-239"></a>W1 <span class="op">=</span> torch.randn((block_size <span class="op">*</span> n_emb, n_hidden),      generator<span class="op">=</span>g)</span>
<span id="cb64-240"><a href="#cb64-240"></a>b1 <span class="op">=</span> torch.randn(n_hidden,                            generator<span class="op">=</span>g)</span>
<span id="cb64-241"><a href="#cb64-241"></a><span class="co"># Layer 2 - softmax</span></span>
<span id="cb64-242"><a href="#cb64-242"></a>W2 <span class="op">=</span> torch.randn((n_hidden, vocab_size),              generator<span class="op">=</span>g)</span>
<span id="cb64-243"><a href="#cb64-243"></a>b2 <span class="op">=</span> torch.randn(vocab_size,                          generator<span class="op">=</span>g)</span>
<span id="cb64-244"><a href="#cb64-244"></a><span class="co"># All params</span></span>
<span id="cb64-245"><a href="#cb64-245"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb64-246"><a href="#cb64-246"></a><span class="co"># Pre-training</span></span>
<span id="cb64-247"><a href="#cb64-247"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb64-248"><a href="#cb64-248"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb64-249"><a href="#cb64-249"></a><span class="co"># Optimization</span></span>
<span id="cb64-250"><a href="#cb64-250"></a>max_steps <span class="op">=</span> <span class="dv">50_000</span> <span class="co">#200_000</span></span>
<span id="cb64-251"><a href="#cb64-251"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb64-252"><a href="#cb64-252"></a><span class="co"># Training on Xtr, Ytr</span></span>
<span id="cb64-253"><a href="#cb64-253"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb64-254"><a href="#cb64-254"></a>    <span class="co"># minibatch construct      </span></span>
<span id="cb64-255"><a href="#cb64-255"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,)) </span>
<span id="cb64-256"><a href="#cb64-256"></a>    Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix] <span class="co"># batch X, Y</span></span>
<span id="cb64-257"><a href="#cb64-257"></a>    <span class="co"># forward pass:</span></span>
<span id="cb64-258"><a href="#cb64-258"></a>    emb <span class="op">=</span> C[Xb] <span class="co"># embed the characters into vectors   </span></span>
<span id="cb64-259"><a href="#cb64-259"></a>    emb_cat <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate the vectors</span></span>
<span id="cb64-260"><a href="#cb64-260"></a>    h_pre_act <span class="op">=</span> emb_cat <span class="op">@</span> W1 <span class="op">+</span> b1 <span class="co"># hidden layer pre-activation</span></span>
<span id="cb64-261"><a href="#cb64-261"></a>    h <span class="op">=</span> torch.tanh(h_pre_act) <span class="co"># hidden layer</span></span>
<span id="cb64-262"><a href="#cb64-262"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># output layer</span></span>
<span id="cb64-263"><a href="#cb64-263"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Yb) <span class="co"># loss function</span></span>
<span id="cb64-264"><a href="#cb64-264"></a></span>
<span id="cb64-265"><a href="#cb64-265"></a>    <span class="cf">break</span></span>
<span id="cb64-266"><a href="#cb64-266"></a><span class="in">```</span></span>
<span id="cb64-267"><a href="#cb64-267"></a></span>
<span id="cb64-268"><a href="#cb64-268"></a><span class="fu">## fixing the initial loss</span></span>
<span id="cb64-269"><a href="#cb64-269"></a></span>
<span id="cb64-270"><a href="#cb64-270"></a>We can see at the <span class="in">`step = 0`</span>, the loss was <span class="in">`27`</span> and after some <span class="in">`k`</span>s training loops it decreased to <span class="in">`1`</span> or <span class="in">`2`</span>. It extremely high at the begining. In practice, we should give the network somehow the expectation we want when generating a character after some characters (<span class="in">`3`</span>).</span>
<span id="cb64-271"><a href="#cb64-271"></a></span>
<span id="cb64-274"><a href="#cb64-274"></a><span class="in">```{python}</span></span>
<span id="cb64-275"><a href="#cb64-275"></a>loss.item()</span>
<span id="cb64-276"><a href="#cb64-276"></a><span class="in">```</span></span>
<span id="cb64-277"><a href="#cb64-277"></a></span>
<span id="cb64-278"><a href="#cb64-278"></a>In this case, without training yet, we expect all <span class="in">`27`</span> characters' possibilities to be equal (<span class="in">`1 / 27.0`</span>) \~ **uniform distribution**, so the loss \~ negative log likelihood would be:</span>
<span id="cb64-279"><a href="#cb64-279"></a></span>
<span id="cb64-282"><a href="#cb64-282"></a><span class="in">```{python}</span></span>
<span id="cb64-283"><a href="#cb64-283"></a><span class="op">-</span> torch.tensor(<span class="dv">1</span> <span class="op">/</span> <span class="fl">27.0</span>).log()</span>
<span id="cb64-284"><a href="#cb64-284"></a><span class="in">```</span></span>
<span id="cb64-285"><a href="#cb64-285"></a></span>
<span id="cb64-286"><a href="#cb64-286"></a>It's far lower than <span class="in">`27`</span>, we say that the network is **confidently wrong**. Andrej demonstrated by another simple 5 elements tensor and showed that the loss is lowest when all elements are equal.</span>
<span id="cb64-287"><a href="#cb64-287"></a></span>
<span id="cb64-288"><a href="#cb64-288"></a>We want the <span class="in">`logits`</span> to be low entropy as possible (but not equal to <span class="in">`0`</span>, which will be showed later), we added multipliers <span class="in">`0.01`</span> to <span class="in">`W2`</span>, and <span class="in">`0`</span> to <span class="in">`b2`</span>. We got the loss to be <span class="in">`3.xx`</span> at the beginning.</span>
<span id="cb64-289"><a href="#cb64-289"></a></span>
<span id="cb64-292"><a href="#cb64-292"></a><span class="in">```{python}</span></span>
<span id="cb64-293"><a href="#cb64-293"></a><span class="co">#| eval: false</span></span>
<span id="cb64-294"><a href="#cb64-294"></a><span class="co"># Layer 2 - softmax</span></span>
<span id="cb64-295"><a href="#cb64-295"></a>W2 <span class="op">=</span> torch.randn((n_hidden, vocab_size),              generator<span class="op">=</span>g) <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb64-296"><a href="#cb64-296"></a>b2 <span class="op">=</span> torch.randn(vocab_size,                          generator<span class="op">=</span>g) <span class="op">*</span> <span class="dv">0</span></span>
<span id="cb64-297"><a href="#cb64-297"></a><span class="in">```</span></span>
<span id="cb64-298"><a href="#cb64-298"></a></span>
<span id="cb64-299"><a href="#cb64-299"></a>Now re-train the model and we will notice the the <span class="in">`lossi`</span> will not look like the *hookey stick* anymore! Morever the final loss on train set and dev set is better!</span>
<span id="cb64-300"><a href="#cb64-300"></a></span>
<span id="cb64-301"><a href="#cb64-301"></a><span class="fu">## fixing the saturated `tanh`</span></span>
<span id="cb64-302"><a href="#cb64-302"></a></span>
<span id="cb64-303"><a href="#cb64-303"></a>The <span class="in">`logits`</span> are now okay, the next problem is about the <span class="in">`h`</span> - the activations of the hidden states! It's hard to see but in the output of code cell below, there are too many values of <span class="in">`1`</span> and <span class="in">`-1`</span> in this tensor.</span>
<span id="cb64-304"><a href="#cb64-304"></a></span>
<span id="cb64-307"><a href="#cb64-307"></a><span class="in">```{python}</span></span>
<span id="cb64-308"><a href="#cb64-308"></a>h</span>
<span id="cb64-309"><a href="#cb64-309"></a><span class="in">```</span></span>
<span id="cb64-310"><a href="#cb64-310"></a></span>
<span id="cb64-311"><a href="#cb64-311"></a>Recall that <span class="in">`tanh`</span> is activation function that squashing arbitrary numbers to the range <span class="in">`[-1:1]`</span>. Let's visualize the distribution of <span class="in">`h`</span>.</span>
<span id="cb64-312"><a href="#cb64-312"></a></span>
<span id="cb64-315"><a href="#cb64-315"></a><span class="in">```{python}</span></span>
<span id="cb64-316"><a href="#cb64-316"></a>plt.hist(h.view(<span class="op">-</span><span class="dv">1</span>).tolist(), <span class="dv">50</span>)<span class="op">;</span> <span class="co"># the ";" removes the presenting of data in code-block's output</span></span>
<span id="cb64-317"><a href="#cb64-317"></a><span class="in">```</span></span>
<span id="cb64-318"><a href="#cb64-318"></a></span>
<span id="cb64-319"><a href="#cb64-319"></a>Most of them were distributed to the extreme values <span class="in">`-1`</span> and <span class="in">`1`</span>. Now come to the <span class="in">`h_pre_act`</span>, we can see a **flat-tails distribution** from <span class="in">`-15`</span> to <span class="in">`15`</span>.</span>
<span id="cb64-320"><a href="#cb64-320"></a></span>
<span id="cb64-323"><a href="#cb64-323"></a><span class="in">```{python}</span></span>
<span id="cb64-324"><a href="#cb64-324"></a>plt.hist(h_pre_act.view(<span class="op">-</span><span class="dv">1</span>).tolist(), <span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb64-325"><a href="#cb64-325"></a><span class="in">```</span></span>
<span id="cb64-326"><a href="#cb64-326"></a></span>
<span id="cb64-327"><a href="#cb64-327"></a>Looking back to how we implemented <span class="in">`tanh`</span> in <span class="in">`micrograd`</span> (which is mathematically the same with <span class="in">`PyTorch`</span>), we're multiplying the forward node's gradient with <span class="in">`(1 - t**2)`</span>, which <span class="in">`t`</span> is local <span class="in">`tanh`</span>. When <span class="in">`tanh`</span> is near <span class="in">`-1`</span> or <span class="in">`1`</span>, this is close to <span class="in">`0`</span>, we are **killing the gradients**. We are stopping the backpropagation through this <span class="in">`tanh`</span> unit.</span>
<span id="cb64-328"><a href="#cb64-328"></a></span>
<span id="cb64-331"><a href="#cb64-331"></a><span class="in">```{python}</span></span>
<span id="cb64-332"><a href="#cb64-332"></a><span class="co">#| eval: false</span></span>
<span id="cb64-333"><a href="#cb64-333"></a>...</span>
<span id="cb64-334"><a href="#cb64-334"></a>    <span class="kw">def</span> tanh(<span class="va">self</span>):</span>
<span id="cb64-335"><a href="#cb64-335"></a>        x <span class="op">=</span> <span class="va">self</span>.data</span>
<span id="cb64-336"><a href="#cb64-336"></a>        t <span class="op">=</span> (math.exp(<span class="dv">2</span><span class="op">*</span>x) <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> (math.exp(<span class="dv">2</span><span class="op">*</span>x) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb64-337"><a href="#cb64-337"></a>        out <span class="op">=</span> Value(t, (<span class="va">self</span>, ), <span class="st">'tanh'</span>)</span>
<span id="cb64-338"><a href="#cb64-338"></a></span>
<span id="cb64-339"><a href="#cb64-339"></a>        <span class="kw">def</span> _backward():</span>
<span id="cb64-340"><a href="#cb64-340"></a>            <span class="va">self</span>.grad <span class="op">+=</span> (<span class="dv">1</span> <span class="op">-</span> t<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> out.grad</span>
<span id="cb64-341"><a href="#cb64-341"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="cb64-342"><a href="#cb64-342"></a>        <span class="cf">return</span> out</span>
<span id="cb64-343"><a href="#cb64-343"></a>...</span>
<span id="cb64-344"><a href="#cb64-344"></a><span class="in">```</span></span>
<span id="cb64-345"><a href="#cb64-345"></a></span>
<span id="cb64-346"><a href="#cb64-346"></a>When the gradients become zero, the previous nodes' gradients will be **vanishing**. We call this **saturated `tanh`**, this leads to **dead neurons** \~ always off and because the gradient is zero then they will never be turned on, and happens for other activations as well: <span class="in">`sigmoid`</span>, <span class="in">`ReLU`</span>, etc (but less significant on <span class="in">`Leaky ReLU`</span> or <span class="in">`ELU`</span>). The network is not learning!</span>
<span id="cb64-347"><a href="#cb64-347"></a></span>
<span id="cb64-348"><a href="#cb64-348"></a>The same with <span class="in">`logits`</span>, now we want <span class="in">`h`</span> to be more near zero, we add multipliers to the <span class="in">`W1`</span> and <span class="in">`b1`</span>:</span>
<span id="cb64-349"><a href="#cb64-349"></a></span>
<span id="cb64-352"><a href="#cb64-352"></a><span class="in">```{python}</span></span>
<span id="cb64-353"><a href="#cb64-353"></a><span class="co">#| eval: false</span></span>
<span id="cb64-354"><a href="#cb64-354"></a><span class="co"># Layer 1 - tanh - 300 neurons</span></span>
<span id="cb64-355"><a href="#cb64-355"></a>W1 <span class="op">=</span> torch.randn((block_size <span class="op">*</span> n_emb, n_hidden),      generator<span class="op">=</span>g)  <span class="op">*</span> <span class="fl">0.2</span></span>
<span id="cb64-356"><a href="#cb64-356"></a>b1 <span class="op">=</span> torch.randn(n_hidden,                            generator<span class="op">=</span>g) <span class="op">*</span> <span class="fl">0.01</span> <span class="co"># keep a little bit entropy, </span></span>
<span id="cb64-357"><a href="#cb64-357"></a><span class="co"># It's okay to initialize the b1 to zero but AK found emperically this will enhance the optimiaztion</span></span>
<span id="cb64-358"><a href="#cb64-358"></a><span class="in">```</span></span>
<span id="cb64-359"><a href="#cb64-359"></a></span>
<span id="cb64-360"><a href="#cb64-360"></a>We can see now less peak distribution of <span class="in">`h`</span>:</span>
<span id="cb64-361"><a href="#cb64-361"></a></span>
<span id="cb64-362"><a href="#cb64-362"></a>::: columns</span>
<span id="cb64-363"><a href="#cb64-363"></a>::: {.column width="50%"}</span>
<span id="cb64-364"><a href="#cb64-364"></a><span class="al">![`tanh`](tanh_0.2mult.png)</span></span>
<span id="cb64-365"><a href="#cb64-365"></a>:::</span>
<span id="cb64-366"><a href="#cb64-366"></a></span>
<span id="cb64-367"><a href="#cb64-367"></a>::: {.column width="50%"}</span>
<span id="cb64-368"><a href="#cb64-368"></a><span class="al">![pre-activation `tanh`](pre_act_tanh_0.2mult.png)</span></span>
<span id="cb64-369"><a href="#cb64-369"></a>:::</span>
<span id="cb64-370"><a href="#cb64-370"></a>:::</span>
<span id="cb64-371"><a href="#cb64-371"></a></span>
<span id="cb64-372"><a href="#cb64-372"></a><span class="fu">## calculating the init scale: “Kaiming init”</span></span>
<span id="cb64-373"><a href="#cb64-373"></a></span>
<span id="cb64-374"><a href="#cb64-374"></a>Now let's look to the number <span class="in">`0.02`</span>, in practice no one will set it manually. Let's look into the example below to see how parameters of Gaussian Distribution of <span class="in">`y`</span> differ from <span class="in">`x`</span> when multiplying by <span class="in">`W`</span>.</span>
<span id="cb64-375"><a href="#cb64-375"></a></span>
<span id="cb64-376"><a href="#cb64-376"></a>The question is how we set the <span class="in">`W`</span> to preserve the Gaussian Distribution of X. Emperical researches found out that the multiplier to <span class="in">`W`</span> should be square root of the "fan in", in this case is <span class="in">`10^0.5`</span>.</span>
<span id="cb64-377"><a href="#cb64-377"></a></span>
<span id="cb64-380"><a href="#cb64-380"></a><span class="in">```{python}</span></span>
<span id="cb64-381"><a href="#cb64-381"></a>x <span class="op">=</span> torch.randn(<span class="dv">1000</span>, <span class="dv">10</span>)</span>
<span id="cb64-382"><a href="#cb64-382"></a>W <span class="op">=</span> torch.randn(<span class="dv">10</span>, <span class="dv">200</span>)</span>
<span id="cb64-383"><a href="#cb64-383"></a>y <span class="op">=</span> x <span class="op">@</span> W</span>
<span id="cb64-384"><a href="#cb64-384"></a></span>
<span id="cb64-385"><a href="#cb64-385"></a>W1 <span class="op">=</span> torch.randn(<span class="dv">10</span>, <span class="dv">200</span>) <span class="op">/</span> <span class="dv">10</span><span class="op">**</span><span class="fl">0.5</span></span>
<span id="cb64-386"><a href="#cb64-386"></a>y1 <span class="op">=</span> x <span class="op">@</span> W1</span>
<span id="cb64-387"><a href="#cb64-387"></a><span class="bu">print</span>(x.mean(), x.std())</span>
<span id="cb64-388"><a href="#cb64-388"></a><span class="bu">print</span>(y.mean(), y.std())</span>
<span id="cb64-389"><a href="#cb64-389"></a><span class="bu">print</span>(y1.mean(), y1.std())</span>
<span id="cb64-390"><a href="#cb64-390"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">5</span>))</span>
<span id="cb64-391"><a href="#cb64-391"></a>plt.subplot(<span class="dv">131</span>).set_title(<span class="st">"Input X"</span>)</span>
<span id="cb64-392"><a href="#cb64-392"></a>plt.hist(x.view(<span class="op">-</span><span class="dv">1</span>).tolist(), <span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>)<span class="op">;</span></span>
<span id="cb64-393"><a href="#cb64-393"></a>plt.subplot(<span class="dv">132</span>).set_title(<span class="st">"Initial output y, expanded by W"</span>)</span>
<span id="cb64-394"><a href="#cb64-394"></a>plt.hist(y.view(<span class="op">-</span><span class="dv">1</span>).tolist(), <span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>)<span class="op">;</span></span>
<span id="cb64-395"><a href="#cb64-395"></a>plt.subplot(<span class="dv">133</span>).set_title(<span class="st">"y1, preserve the X's Gaussian Dist"</span>)</span>
<span id="cb64-396"><a href="#cb64-396"></a>plt.hist(y1.view(<span class="op">-</span><span class="dv">1</span>).tolist(), <span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>)<span class="op">;</span></span>
<span id="cb64-397"><a href="#cb64-397"></a><span class="in">```</span></span>
<span id="cb64-398"><a href="#cb64-398"></a></span>
<span id="cb64-399"><a href="#cb64-399"></a>Please investigate more here:</span>
<span id="cb64-400"><a href="#cb64-400"></a></span>
<span id="cb64-401"><a href="#cb64-401"></a><span class="ss">1.  </span>Kaiming et al. paper: <span class="ot">&lt;https://arxiv.org/abs/1502.01852&gt;</span></span>
<span id="cb64-402"><a href="#cb64-402"></a><span class="ss">2.  </span>Implementation in <span class="in">`Pytorch`</span>: <span class="ot">&lt;https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_&gt;</span></span>
<span id="cb64-403"><a href="#cb64-403"></a></span>
<span id="cb64-404"><a href="#cb64-404"></a>It's recommended in Kaiming paper to use a **gain** multiplier base on nonlinearity/activation function (<span class="co">[</span><span class="ot">here</span><span class="co">](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.calculate_gain)</span>), for <span class="in">`tanh`</span> it's <span class="in">`5/3`</span>. We endup modified the initialization of <span class="in">`W1`</span> with:</span>
<span id="cb64-405"><a href="#cb64-405"></a></span>
<span id="cb64-408"><a href="#cb64-408"></a><span class="in">```{python}</span></span>
<span id="cb64-409"><a href="#cb64-409"></a><span class="co">#| eval: false</span></span>
<span id="cb64-410"><a href="#cb64-410"></a>W1 <span class="op">=</span> torch.randn((block_size <span class="op">*</span> n_emb, n_hidden),      generator<span class="op">=</span>g)  <span class="op">*</span> (<span class="dv">5</span><span class="op">/</span><span class="dv">3</span>) <span class="op">/</span> ((block_size <span class="op">*</span> n_emb)<span class="op">**</span><span class="fl">0.5</span>) <span class="co"># * 0.2</span></span>
<span id="cb64-411"><a href="#cb64-411"></a><span class="in">```</span></span>
<span id="cb64-412"><a href="#cb64-412"></a></span>
<span id="cb64-413"><a href="#cb64-413"></a>In this case is roughly <span class="in">`0.3`</span>, re-train and although the loss only improved so insignificant (because previously we set it to be <span class="in">`0.2`</span> - very close), but we've parameterized this hyper-constant.</span>
<span id="cb64-414"><a href="#cb64-414"></a></span>
<span id="cb64-415"><a href="#cb64-415"></a><span class="fu">## batch normalization</span></span>
<span id="cb64-416"><a href="#cb64-416"></a></span>
<span id="cb64-417"><a href="#cb64-417"></a>As discussed before, we dont want the <span class="in">`h_pre_act`</span> to be way too small (\~is not doing anything) or too large (\~saturated), we want it to just roughly follow the standardized Gaussian Distribution (ie. mean equal to 0, std equal to 1).</span>
<span id="cb64-418"><a href="#cb64-418"></a></span>
<span id="cb64-419"><a href="#cb64-419"></a>We've done it at the initialization, *why don't we just normalize the hidden states to be unit Gaussian*? in batch normalization, this can be achieved by 4 steps, demonstrated with our case:</span>
<span id="cb64-420"><a href="#cb64-420"></a></span>
<span id="cb64-423"><a href="#cb64-423"></a><span class="in">```{python}</span></span>
<span id="cb64-424"><a href="#cb64-424"></a><span class="co">#| eval: false</span></span>
<span id="cb64-425"><a href="#cb64-425"></a><span class="co"># 1. mini-batch mean</span></span>
<span id="cb64-426"><a href="#cb64-426"></a>hpa_mean <span class="op">=</span> h_pre_act.mean(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-427"><a href="#cb64-427"></a><span class="co"># 2. mini-batch variance / standard deviation</span></span>
<span id="cb64-428"><a href="#cb64-428"></a>hpa_std <span class="op">=</span> h_pre_act.std(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-429"><a href="#cb64-429"></a><span class="co"># 3. normalize</span></span>
<span id="cb64-430"><a href="#cb64-430"></a>h_pre_act <span class="op">=</span> (h_pre_act <span class="op">-</span> hpa_mean) <span class="op">/</span> hpa_std</span>
<span id="cb64-431"><a href="#cb64-431"></a><span class="co"># 4. scale and shift</span></span>
<span id="cb64-432"><a href="#cb64-432"></a><span class="co"># multiply by a "gain" then "shift" it with a bias</span></span>
<span id="cb64-433"><a href="#cb64-433"></a>bngain <span class="op">=</span> torch.ones((<span class="dv">1</span>, n_hidden))</span>
<span id="cb64-434"><a href="#cb64-434"></a>bnbias <span class="op">=</span> torch.zeros((<span class="dv">1</span>, n_hidden))</span>
<span id="cb64-435"><a href="#cb64-435"></a>h_pre_act <span class="op">=</span> bngain <span class="op">*</span> h_pre_act <span class="op">+</span> bnbias</span>
<span id="cb64-436"><a href="#cb64-436"></a><span class="in">```</span></span>
<span id="cb64-437"><a href="#cb64-437"></a></span>
<span id="cb64-438"><a href="#cb64-438"></a>We modified our code accordingly and re-run the code, actually this time the model did not improve much. Because actually this is very **simple and shallow** neural network. We also notice that the training loop now is slower than before, because the calculation volumn is bigger. Batch Normalization also unexpectedly comes up with a side effect, the forward and backward pass of any input now also depend on the mini-batch, not just itself (because of `mean()`/`std()`). This effect is suprisingly a good thing and acts as a **regularizer**.</span>
<span id="cb64-439"><a href="#cb64-439"></a></span>
<span id="cb64-440"><a href="#cb64-440"></a>There are also non-coupling regularizers such as: Linear Normalization, Layer Normalization, Group Normalization.</span>
<span id="cb64-441"><a href="#cb64-441"></a></span>
<span id="cb64-442"><a href="#cb64-442"></a>One othering to consider is in the deployment/testing phase, we dont want to use the batch norm calculated by a mini-batch. Instead we want to use the mean and standard deviation from the whole training data set:</span>
<span id="cb64-443"><a href="#cb64-443"></a></span>
<span id="cb64-446"><a href="#cb64-446"></a><span class="in">```{python}</span></span>
<span id="cb64-447"><a href="#cb64-447"></a><span class="co">#| eval: false</span></span>
<span id="cb64-448"><a href="#cb64-448"></a><span class="co"># calibrate the batch norm after training</span></span>
<span id="cb64-449"><a href="#cb64-449"></a></span>
<span id="cb64-450"><a href="#cb64-450"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb64-451"><a href="#cb64-451"></a>    <span class="co"># pass the training set through</span></span>
<span id="cb64-452"><a href="#cb64-452"></a>    emb <span class="op">=</span> C[x_train]</span>
<span id="cb64-453"><a href="#cb64-453"></a>    embcat <span class="op">=</span> emb.view(<span class="op">-</span><span class="dv">1</span>, emb.shape[<span class="dv">1</span>] <span class="op">*</span> emb.shape[<span class="dv">2</span>])</span>
<span id="cb64-454"><a href="#cb64-454"></a>    hpreact <span class="op">=</span> embcat <span class="op">@</span> W1 <span class="op">+</span> b1</span>
<span id="cb64-455"><a href="#cb64-455"></a>    <span class="co"># measure the mean/std over the entire training set</span></span>
<span id="cb64-456"><a href="#cb64-456"></a>    bnmean <span class="op">=</span> hpreact.mean(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-457"><a href="#cb64-457"></a>    bnstd <span class="op">=</span> hpreact.std(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-458"><a href="#cb64-458"></a><span class="in">```</span></span>
<span id="cb64-459"><a href="#cb64-459"></a></span>
<span id="cb64-460"><a href="#cb64-460"></a>Rather, we can also use the running mean and standard deviation as implemented below which will give close estimates. Remaining 2 notes on the BN are:</span>
<span id="cb64-461"><a href="#cb64-461"></a></span>
<span id="cb64-462"><a href="#cb64-462"></a><span class="ss">1.  </span>Dividing zeros: we add a $\epsilon$ value to the **variance** to avoid. We do not include this here as it likely not to happen with out example;</span>
<span id="cb64-463"><a href="#cb64-463"></a><span class="ss">2.  </span>The bias <span class="in">`b1`</span> will be subtracting in BN calculation, we will notice the <span class="in">`b1.grad`</span> will be zeros as it does not impact any other calculation. Thus when using the BN, for layer before like weight, we should remove the bias. The <span class="in">`bnbias`</span> now will be incharge for biasing the distributions.</span>
<span id="cb64-464"><a href="#cb64-464"></a></span>
<span id="cb64-465"><a href="#cb64-465"></a><span class="fu">## real example: `resnet50` walkthrough</span></span>
<span id="cb64-466"><a href="#cb64-466"></a></span>
<span id="cb64-467"><a href="#cb64-467"></a>The code AK presented here: <span class="ot">&lt;https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L108&gt;</span></span>
<span id="cb64-468"><a href="#cb64-468"></a></span>
<span id="cb64-469"><a href="#cb64-469"></a><span class="al">![The architecture of ResNet-50 model.](resnet50.png)</span></span>
<span id="cb64-470"><a href="#cb64-470"></a></span>
<span id="cb64-471"><a href="#cb64-471"></a><span class="fu">## summary of the lecture</span></span>
<span id="cb64-472"><a href="#cb64-472"></a></span>
<span id="cb64-473"><a href="#cb64-473"></a>Understand the activations (non-linearity) and gradients is crucial when training deep / large neural networks, in part 1 we have observed some issue and come up with many solutions:</span>
<span id="cb64-474"><a href="#cb64-474"></a></span>
<span id="cb64-475"><a href="#cb64-475"></a><span class="ss">1.  </span>Confidently wrong of network at init leads to hookey stick for loss in training loop: adding multipliers to <span class="in">`logits`</span>'s weights and biases;</span>
<span id="cb64-476"><a href="#cb64-476"></a><span class="ss">2.  </span>Flat-tails distribution or saturated <span class="in">`tanh`</span>: Kaiming init;</span>
<span id="cb64-477"><a href="#cb64-477"></a><span class="ss">3.  </span>Normalization of the hidden states: introduction to BN.</span>
<span id="cb64-478"><a href="#cb64-478"></a></span>
<span id="cb64-479"><a href="#cb64-479"></a>Our final code in part 1 (un-fold to see), <span class="in">`# 👈`</span> indicates a change:</span>
<span id="cb64-480"><a href="#cb64-480"></a></span>
<span id="cb64-483"><a href="#cb64-483"></a><span class="in">```{python}</span></span>
<span id="cb64-484"><a href="#cb64-484"></a><span class="co">#| code-fold: true</span></span>
<span id="cb64-485"><a href="#cb64-485"></a>block_size <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb64-486"><a href="#cb64-486"></a></span>
<span id="cb64-487"><a href="#cb64-487"></a><span class="co"># MLP revisited</span></span>
<span id="cb64-488"><a href="#cb64-488"></a>n_emb <span class="op">=</span> <span class="dv">10</span> <span class="co"># no of dimensions of the embedding space.</span></span>
<span id="cb64-489"><a href="#cb64-489"></a>n_hidden <span class="op">=</span> <span class="dv">200</span> <span class="co"># size of the hidden - tanh layer</span></span>
<span id="cb64-490"><a href="#cb64-490"></a></span>
<span id="cb64-491"><a href="#cb64-491"></a><span class="co"># Lookup table - 10 dimensional space</span></span>
<span id="cb64-492"><a href="#cb64-492"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>) <span class="co"># for reproductivity</span></span>
<span id="cb64-493"><a href="#cb64-493"></a>C <span class="op">=</span> torch.randn((vocab_size, n_emb),                  generator<span class="op">=</span>g)</span>
<span id="cb64-494"><a href="#cb64-494"></a></span>
<span id="cb64-495"><a href="#cb64-495"></a><span class="co"># Layer 1 - tanh - 300 neurons</span></span>
<span id="cb64-496"><a href="#cb64-496"></a>W1 <span class="op">=</span> torch.randn((block_size <span class="op">*</span> n_emb, n_hidden),      generator<span class="op">=</span>g) <span class="op">*</span> (<span class="dv">5</span><span class="op">/</span><span class="dv">3</span>) <span class="op">/</span> ((block_size <span class="op">*</span> n_emb)<span class="op">**</span><span class="fl">0.5</span>) <span class="co"># * 0.2       # 👈</span></span>
<span id="cb64-497"><a href="#cb64-497"></a><span class="co"># b1 = torch.randn(n_hidden,                            generator=g) * 0.01       # 👈</span></span>
<span id="cb64-498"><a href="#cb64-498"></a></span>
<span id="cb64-499"><a href="#cb64-499"></a><span class="co"># Layer 2 - softmax</span></span>
<span id="cb64-500"><a href="#cb64-500"></a>W2 <span class="op">=</span> torch.randn((n_hidden, vocab_size),              generator<span class="op">=</span>g) <span class="op">*</span> <span class="fl">0.01</span>       <span class="co"># 👈</span></span>
<span id="cb64-501"><a href="#cb64-501"></a>b2 <span class="op">=</span> torch.randn(vocab_size,                          generator<span class="op">=</span>g) <span class="op">*</span> <span class="dv">0</span>          <span class="co"># 👈</span></span>
<span id="cb64-502"><a href="#cb64-502"></a></span>
<span id="cb64-503"><a href="#cb64-503"></a><span class="co"># Batch Normalization gain and bias</span></span>
<span id="cb64-504"><a href="#cb64-504"></a>bngain <span class="op">=</span> torch.ones((<span class="dv">1</span>, n_hidden))                                              <span class="co"># 👈</span></span>
<span id="cb64-505"><a href="#cb64-505"></a>bnbias <span class="op">=</span> torch.zeros((<span class="dv">1</span>, n_hidden))                                             <span class="co"># 👈</span></span>
<span id="cb64-506"><a href="#cb64-506"></a></span>
<span id="cb64-507"><a href="#cb64-507"></a><span class="co"># Add running mean/std</span></span>
<span id="cb64-508"><a href="#cb64-508"></a>bnmean_running <span class="op">=</span> torch.zeros((<span class="dv">1</span>, n_hidden))                             <span class="co"># 👈</span></span>
<span id="cb64-509"><a href="#cb64-509"></a>bnstd_running <span class="op">=</span> torch.ones((<span class="dv">1</span>, n_hidden))                               <span class="co"># 👈</span></span>
<span id="cb64-510"><a href="#cb64-510"></a></span>
<span id="cb64-511"><a href="#cb64-511"></a><span class="co"># All params (deleted b1)</span></span>
<span id="cb64-512"><a href="#cb64-512"></a>parameters <span class="op">=</span> [C, W1, W2, b2, bngain, bnbias]                                <span class="co"># 👈</span></span>
<span id="cb64-513"><a href="#cb64-513"></a><span class="bu">print</span>(<span class="st">"No of params: "</span>, <span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span>
<span id="cb64-514"><a href="#cb64-514"></a></span>
<span id="cb64-515"><a href="#cb64-515"></a><span class="co"># Pre-training</span></span>
<span id="cb64-516"><a href="#cb64-516"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb64-517"><a href="#cb64-517"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb64-518"><a href="#cb64-518"></a></span>
<span id="cb64-519"><a href="#cb64-519"></a><span class="co"># Optimization</span></span>
<span id="cb64-520"><a href="#cb64-520"></a>max_steps <span class="op">=</span> <span class="dv">50_000</span> <span class="co">#200_000</span></span>
<span id="cb64-521"><a href="#cb64-521"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb64-522"><a href="#cb64-522"></a></span>
<span id="cb64-523"><a href="#cb64-523"></a><span class="co"># Stats holders</span></span>
<span id="cb64-524"><a href="#cb64-524"></a>lossi <span class="op">=</span> []</span>
<span id="cb64-525"><a href="#cb64-525"></a></span>
<span id="cb64-526"><a href="#cb64-526"></a><span class="co"># Training on Xtr, Ytr</span></span>
<span id="cb64-527"><a href="#cb64-527"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb64-528"><a href="#cb64-528"></a></span>
<span id="cb64-529"><a href="#cb64-529"></a>    <span class="co"># minibatch construct      </span></span>
<span id="cb64-530"><a href="#cb64-530"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,)) </span>
<span id="cb64-531"><a href="#cb64-531"></a>    Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix] <span class="co"># batch X, Y</span></span>
<span id="cb64-532"><a href="#cb64-532"></a></span>
<span id="cb64-533"><a href="#cb64-533"></a>    <span class="co"># forward pass:</span></span>
<span id="cb64-534"><a href="#cb64-534"></a>    emb <span class="op">=</span> C[Xb] <span class="co"># embed the characters into vectors   </span></span>
<span id="cb64-535"><a href="#cb64-535"></a>    emb_cat <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate the vectors</span></span>
<span id="cb64-536"><a href="#cb64-536"></a>    <span class="co"># Linear layer</span></span>
<span id="cb64-537"><a href="#cb64-537"></a>    h_pre_act <span class="op">=</span> emb_cat <span class="op">@</span> W1 <span class="co"># + b1 # hidden layer pre-activation                               # 👈</span></span>
<span id="cb64-538"><a href="#cb64-538"></a>    <span class="co"># BatchNorm layer</span></span>
<span id="cb64-539"><a href="#cb64-539"></a>    bnmeani <span class="op">=</span> h_pre_act.mean(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)                                                   <span class="co"># 👈</span></span>
<span id="cb64-540"><a href="#cb64-540"></a>    bnstdi <span class="op">=</span> h_pre_act.std(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)                                                     <span class="co"># 👈</span></span>
<span id="cb64-541"><a href="#cb64-541"></a>    h_pre_act <span class="op">=</span> bngain <span class="op">*</span> ((h_pre_act <span class="op">-</span> bnmeani) <span class="op">/</span> bnstdi) <span class="op">+</span> bnbias                              <span class="co"># 👈</span></span>
<span id="cb64-542"><a href="#cb64-542"></a>    <span class="co"># Updating running mean and std (this runs outside the training loop)</span></span>
<span id="cb64-543"><a href="#cb64-543"></a>    <span class="cf">with</span> torch.no_grad():                                                                       <span class="co"># 👈</span></span>
<span id="cb64-544"><a href="#cb64-544"></a>        bnmean_running <span class="op">=</span> <span class="fl">0.999</span> <span class="op">*</span> bnmean_running <span class="op">+</span> <span class="fl">0.001</span> <span class="op">*</span> bnmeani                               <span class="co"># 👈</span></span>
<span id="cb64-545"><a href="#cb64-545"></a>        bnstd_running <span class="op">=</span> <span class="fl">0.999</span> <span class="op">*</span> bnstd_running <span class="op">+</span> <span class="fl">0.001</span> <span class="op">*</span> bnstdi                                  <span class="co"># 👈</span></span>
<span id="cb64-546"><a href="#cb64-546"></a>    <span class="co"># Non-linearity</span></span>
<span id="cb64-547"><a href="#cb64-547"></a>    h <span class="op">=</span> torch.tanh(h_pre_act) <span class="co"># hidden layer</span></span>
<span id="cb64-548"><a href="#cb64-548"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># output layer</span></span>
<span id="cb64-549"><a href="#cb64-549"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Yb) <span class="co"># loss function</span></span>
<span id="cb64-550"><a href="#cb64-550"></a></span>
<span id="cb64-551"><a href="#cb64-551"></a>    <span class="co"># backward pass:</span></span>
<span id="cb64-552"><a href="#cb64-552"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb64-553"><a href="#cb64-553"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb64-554"><a href="#cb64-554"></a>    loss.backward()</span>
<span id="cb64-555"><a href="#cb64-555"></a></span>
<span id="cb64-556"><a href="#cb64-556"></a>    <span class="co"># update</span></span>
<span id="cb64-557"><a href="#cb64-557"></a>    lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;=</span> max_steps <span class="op">/</span> <span class="dv">2</span> <span class="cf">else</span> <span class="fl">0.01</span> <span class="co"># step learning rate decay</span></span>
<span id="cb64-558"><a href="#cb64-558"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb64-559"><a href="#cb64-559"></a>        p.data <span class="op">+=</span> <span class="op">-</span> lr <span class="op">*</span> p.grad</span>
<span id="cb64-560"><a href="#cb64-560"></a></span>
<span id="cb64-561"><a href="#cb64-561"></a>    <span class="co"># track stats</span></span>
<span id="cb64-562"><a href="#cb64-562"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="co"># print once every while</span></span>
<span id="cb64-563"><a href="#cb64-563"></a>      <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">:7d}</span><span class="ss">/</span><span class="sc">{</span>max_steps<span class="sc">:7d}</span><span class="ss">: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb64-564"><a href="#cb64-564"></a>    lossi.append(loss.log10().item())</span>
<span id="cb64-565"><a href="#cb64-565"></a><span class="in">```</span></span>
<span id="cb64-566"><a href="#cb64-566"></a></span>
<span id="cb64-569"><a href="#cb64-569"></a><span class="in">```{python}</span></span>
<span id="cb64-570"><a href="#cb64-570"></a>plt.plot(lossi)</span>
<span id="cb64-571"><a href="#cb64-571"></a><span class="in">```</span></span>
<span id="cb64-572"><a href="#cb64-572"></a></span>
<span id="cb64-575"><a href="#cb64-575"></a><span class="in">```{python}</span></span>
<span id="cb64-576"><a href="#cb64-576"></a><span class="at">@torch.no_grad</span>() <span class="co"># disables gradient tracking</span></span>
<span id="cb64-577"><a href="#cb64-577"></a><span class="kw">def</span> split_loss(split: <span class="bu">str</span>):</span>
<span id="cb64-578"><a href="#cb64-578"></a>  x, y <span class="op">=</span> {</span>
<span id="cb64-579"><a href="#cb64-579"></a>    <span class="st">'train'</span>: (Xtr, Ytr),</span>
<span id="cb64-580"><a href="#cb64-580"></a>    <span class="st">'val'</span>: (Xdev, Ydev),</span>
<span id="cb64-581"><a href="#cb64-581"></a>    <span class="st">'test'</span>: (Xte, Yte)</span>
<span id="cb64-582"><a href="#cb64-582"></a>  }[split]</span>
<span id="cb64-583"><a href="#cb64-583"></a>  emb <span class="op">=</span> C[x]</span>
<span id="cb64-584"><a href="#cb64-584"></a>  emb_cat <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) </span>
<span id="cb64-585"><a href="#cb64-585"></a>  h_pre_act <span class="op">=</span> emb_cat <span class="op">@</span> W1 <span class="op">+</span> b1                                                                                         <span class="co"># 👈 </span></span>
<span id="cb64-586"><a href="#cb64-586"></a>  <span class="co"># h_pre_act = bngain * ((h_pre_act - h_pre_act.mean(0, keepdim=True)) / h_pre_act.std(0, keepdim=True)) + bnbias      # 👈</span></span>
<span id="cb64-587"><a href="#cb64-587"></a>  <span class="co"># h_pre_act = bngain * ((h_pre_act - bnmean) / bnstd) + bnbias                                                        # 👈</span></span>
<span id="cb64-588"><a href="#cb64-588"></a>  h_pre_act <span class="op">=</span> bngain <span class="op">*</span> ((h_pre_act <span class="op">-</span> bnmean_running) <span class="op">/</span> bnstd_running) <span class="op">+</span> bnbias                                          <span class="co"># 👈</span></span>
<span id="cb64-589"><a href="#cb64-589"></a>  h <span class="op">=</span> torch.tanh(h_pre_act) </span>
<span id="cb64-590"><a href="#cb64-590"></a>  logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb64-591"><a href="#cb64-591"></a>  loss <span class="op">=</span> F.cross_entropy(logits, y)</span>
<span id="cb64-592"><a href="#cb64-592"></a>  <span class="bu">print</span>(split, loss.item())</span>
<span id="cb64-593"><a href="#cb64-593"></a></span>
<span id="cb64-594"><a href="#cb64-594"></a>split_loss(<span class="st">'train'</span>)</span>
<span id="cb64-595"><a href="#cb64-595"></a>split_loss(<span class="st">'val'</span>)</span>
<span id="cb64-596"><a href="#cb64-596"></a><span class="in">```</span></span>
<span id="cb64-597"><a href="#cb64-597"></a></span>
<span id="cb64-598"><a href="#cb64-598"></a><span class="fu">### loss logs</span></span>
<span id="cb64-599"><a href="#cb64-599"></a></span>
<span id="cb64-600"><a href="#cb64-600"></a>The numbers somehow are approximate, I don't know why my Thinkpad-E14 gave different results when running codes multiple times 😂.</span>
<span id="cb64-601"><a href="#cb64-601"></a></span>
<span id="cb64-602"><a href="#cb64-602"></a>+--------------+------------------------------------------------------------+--------------------------+</span>
<span id="cb64-603"><a href="#cb64-603"></a>| Step         | What we did                                                | Loss we got (accum)      |</span>
<span id="cb64-604"><a href="#cb64-604"></a>+==============+============================================================+==========================+</span>
<span id="cb64-605"><a href="#cb64-605"></a>| 1            | original                                                   | train 2.1169614791870117 |</span>
<span id="cb64-606"><a href="#cb64-606"></a>|              |                                                            |                          |</span>
<span id="cb64-607"><a href="#cb64-607"></a>|              |                                                            | val 2.1623435020446777   |</span>
<span id="cb64-608"><a href="#cb64-608"></a>+--------------+------------------------------------------------------------+--------------------------+</span>
<span id="cb64-609"><a href="#cb64-609"></a>| 2            | fixed softmax confidently wrong                            | train 2.0666463375091553 |</span>
<span id="cb64-610"><a href="#cb64-610"></a>|              |                                                            |                          |</span>
<span id="cb64-611"><a href="#cb64-611"></a>|              |                                                            | val 2.1468191146850586   |</span>
<span id="cb64-612"><a href="#cb64-612"></a>+--------------+------------------------------------------------------------+--------------------------+</span>
<span id="cb64-613"><a href="#cb64-613"></a>| 3            | fixed <span class="in">`tanh`</span> layer too saturated at init                   | train 2.033477544784546  |</span>
<span id="cb64-614"><a href="#cb64-614"></a>|              |                                                            |                          |</span>
<span id="cb64-615"><a href="#cb64-615"></a>|              |                                                            | val 2.115907907485962    |</span>
<span id="cb64-616"><a href="#cb64-616"></a>+--------------+------------------------------------------------------------+--------------------------+</span>
<span id="cb64-617"><a href="#cb64-617"></a>| 4            | used semi principle "kaiming init" instead of hacking init | train 2.038902997970581  |</span>
<span id="cb64-618"><a href="#cb64-618"></a>|              |                                                            |                          |</span>
<span id="cb64-619"><a href="#cb64-619"></a>|              |                                                            | val 2.1138899326324463   |</span>
<span id="cb64-620"><a href="#cb64-620"></a>+--------------+------------------------------------------------------------+--------------------------+</span>
<span id="cb64-621"><a href="#cb64-621"></a>| 5            | added batch norm layer                                     | train 2.0662825107574463 |</span>
<span id="cb64-622"><a href="#cb64-622"></a>|              |                                                            |                          |</span>
<span id="cb64-623"><a href="#cb64-623"></a>|              |                                                            | val 2.1201331615448      |</span>
<span id="cb64-624"><a href="#cb64-624"></a>+--------------+------------------------------------------------------------+--------------------------+</span>
<span id="cb64-625"><a href="#cb64-625"></a></span>
<span id="cb64-626"><a href="#cb64-626"></a>: Loss logs {tbl-colwidths="<span class="sc">\[</span>10,40,50<span class="sc">\]</span>" .striped .hover}</span>
<span id="cb64-627"><a href="#cb64-627"></a></span>
<span id="cb64-628"><a href="#cb64-628"></a><span class="fu"># Part 2: PyTorch-ifying the code, and train a deeper network</span></span>
<span id="cb64-629"><a href="#cb64-629"></a></span>
<span id="cb64-630"><a href="#cb64-630"></a>Below is PyTorch-ified code by Andrej, some comments inputted by me:</span>
<span id="cb64-631"><a href="#cb64-631"></a></span>
<span id="cb64-634"><a href="#cb64-634"></a><span class="in">```{python}</span></span>
<span id="cb64-635"><a href="#cb64-635"></a><span class="co"># Let's train a deeper network</span></span>
<span id="cb64-636"><a href="#cb64-636"></a><span class="co"># The classes we create here are the same API as nn.Module in PyTorch</span></span>
<span id="cb64-637"><a href="#cb64-637"></a></span>
<span id="cb64-638"><a href="#cb64-638"></a><span class="kw">class</span> Linear:</span>
<span id="cb64-639"><a href="#cb64-639"></a>    <span class="co">"""</span></span>
<span id="cb64-640"><a href="#cb64-640"></a><span class="co">    Simplifying Pytorch Linear Layer: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear</span></span>
<span id="cb64-641"><a href="#cb64-641"></a><span class="co">    """</span></span>
<span id="cb64-642"><a href="#cb64-642"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, fan_in, fan_out, bias<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb64-643"><a href="#cb64-643"></a>        <span class="va">self</span>.weight <span class="op">=</span> torch.randn((fan_in, fan_out), generator<span class="op">=</span>g) <span class="op">/</span> fan_in<span class="op">**</span><span class="fl">0.5</span></span>
<span id="cb64-644"><a href="#cb64-644"></a>        <span class="va">self</span>.bias <span class="op">=</span> torch.zeros(fan_out) <span class="cf">if</span> bias <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb64-645"><a href="#cb64-645"></a></span>
<span id="cb64-646"><a href="#cb64-646"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb64-647"><a href="#cb64-647"></a>        <span class="va">self</span>.out <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.weight</span>
<span id="cb64-648"><a href="#cb64-648"></a>        <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb64-649"><a href="#cb64-649"></a>            <span class="va">self</span>.out <span class="op">+=</span> <span class="va">self</span>.bias</span>
<span id="cb64-650"><a href="#cb64-650"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb64-651"><a href="#cb64-651"></a></span>
<span id="cb64-652"><a href="#cb64-652"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb64-653"><a href="#cb64-653"></a>        <span class="cf">return</span> [<span class="va">self</span>.weight] <span class="op">+</span> ([] <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> [<span class="va">self</span>.bias])</span>
<span id="cb64-654"><a href="#cb64-654"></a></span>
<span id="cb64-655"><a href="#cb64-655"></a></span>
<span id="cb64-656"><a href="#cb64-656"></a><span class="kw">class</span> BatchNorm1d:</span>
<span id="cb64-657"><a href="#cb64-657"></a>    <span class="co">"""</span></span>
<span id="cb64-658"><a href="#cb64-658"></a><span class="co">    Simplifying Pytorch BatchNorm1D: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html</span></span>
<span id="cb64-659"><a href="#cb64-659"></a><span class="co">    """</span> </span>
<span id="cb64-660"><a href="#cb64-660"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, eps<span class="op">=</span><span class="fl">1e-5</span>, momentum<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb64-661"><a href="#cb64-661"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb64-662"><a href="#cb64-662"></a>        <span class="va">self</span>.momentum <span class="op">=</span> momentum</span>
<span id="cb64-663"><a href="#cb64-663"></a>        <span class="va">self</span>.training <span class="op">=</span> <span class="va">True</span> <span class="co"># to differentiate usage of class in training or evaluation (using running mean/std)</span></span>
<span id="cb64-664"><a href="#cb64-664"></a>        <span class="co"># parameters (trained with backprop)</span></span>
<span id="cb64-665"><a href="#cb64-665"></a>        <span class="va">self</span>.gamma <span class="op">=</span> torch.ones(dim) <span class="co"># gain</span></span>
<span id="cb64-666"><a href="#cb64-666"></a>        <span class="va">self</span>.beta <span class="op">=</span> torch.zeros(dim) <span class="co"># bias</span></span>
<span id="cb64-667"><a href="#cb64-667"></a>        <span class="co"># buffers (trained with a running 'momentum update')</span></span>
<span id="cb64-668"><a href="#cb64-668"></a>        <span class="va">self</span>.running_mean <span class="op">=</span> torch.zeros(dim)</span>
<span id="cb64-669"><a href="#cb64-669"></a>        <span class="va">self</span>.running_var <span class="op">=</span> torch.ones(dim)</span>
<span id="cb64-670"><a href="#cb64-670"></a></span>
<span id="cb64-671"><a href="#cb64-671"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb64-672"><a href="#cb64-672"></a>        <span class="co"># calculate the forward pass</span></span>
<span id="cb64-673"><a href="#cb64-673"></a>        <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb64-674"><a href="#cb64-674"></a>            xmean <span class="op">=</span> x.mean(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch mean</span></span>
<span id="cb64-675"><a href="#cb64-675"></a>            xvar <span class="op">=</span> x.var(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch variance, follow the paper exactly</span></span>
<span id="cb64-676"><a href="#cb64-676"></a>        <span class="cf">else</span>:</span>
<span id="cb64-677"><a href="#cb64-677"></a>            xmean <span class="op">=</span> <span class="va">self</span>.running_mean</span>
<span id="cb64-678"><a href="#cb64-678"></a>            xvar <span class="op">=</span> <span class="va">self</span>.running_var</span>
<span id="cb64-679"><a href="#cb64-679"></a>        xhat <span class="op">=</span> (x <span class="op">-</span> xmean) <span class="op">/</span> torch.sqrt(xvar <span class="op">+</span> <span class="va">self</span>.eps) <span class="co"># normalize to unit variance</span></span>
<span id="cb64-680"><a href="#cb64-680"></a>        <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.gamma <span class="op">*</span> xhat <span class="op">+</span> <span class="va">self</span>.beta <span class="co"># to tracking and visualizing data later on, PyTorch does not have this</span></span>
<span id="cb64-681"><a href="#cb64-681"></a>        <span class="co"># update the buffers</span></span>
<span id="cb64-682"><a href="#cb64-682"></a>        <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb64-683"><a href="#cb64-683"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb64-684"><a href="#cb64-684"></a>                <span class="va">self</span>.running_mean <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> <span class="va">self</span>.running_mean <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> xmean</span>
<span id="cb64-685"><a href="#cb64-685"></a>                <span class="va">self</span>.running_var <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> <span class="va">self</span>.running_var <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> xvar</span>
<span id="cb64-686"><a href="#cb64-686"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb64-687"><a href="#cb64-687"></a></span>
<span id="cb64-688"><a href="#cb64-688"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb64-689"><a href="#cb64-689"></a>        <span class="cf">return</span> [<span class="va">self</span>.gamma, <span class="va">self</span>.beta]</span>
<span id="cb64-690"><a href="#cb64-690"></a></span>
<span id="cb64-691"><a href="#cb64-691"></a><span class="kw">class</span> Tanh:</span>
<span id="cb64-692"><a href="#cb64-692"></a>    <span class="co">"""</span></span>
<span id="cb64-693"><a href="#cb64-693"></a><span class="co">    Just calculate the Tanh, just PyTorch: https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html</span></span>
<span id="cb64-694"><a href="#cb64-694"></a><span class="co">    """</span></span>
<span id="cb64-695"><a href="#cb64-695"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb64-696"><a href="#cb64-696"></a>        <span class="va">self</span>.out <span class="op">=</span> torch.tanh(x)</span>
<span id="cb64-697"><a href="#cb64-697"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb64-698"><a href="#cb64-698"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb64-699"><a href="#cb64-699"></a>        <span class="cf">return</span> []</span>
<span id="cb64-700"><a href="#cb64-700"></a></span>
<span id="cb64-701"><a href="#cb64-701"></a>n_embd <span class="op">=</span> <span class="dv">10</span> <span class="co"># the dimensionality of the character embedding vectors</span></span>
<span id="cb64-702"><a href="#cb64-702"></a>n_hidden <span class="op">=</span> <span class="dv">100</span> <span class="co"># the number of neurons in the hidden layer of the MLP</span></span>
<span id="cb64-703"><a href="#cb64-703"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>) <span class="co"># for reproducibility</span></span>
<span id="cb64-704"><a href="#cb64-704"></a></span>
<span id="cb64-705"><a href="#cb64-705"></a>C <span class="op">=</span> torch.randn((vocab_size, n_embd),            generator<span class="op">=</span>g)</span>
<span id="cb64-706"><a href="#cb64-706"></a></span>
<span id="cb64-707"><a href="#cb64-707"></a>layers <span class="op">=</span> [</span>
<span id="cb64-708"><a href="#cb64-708"></a>    Linear(n_embd <span class="op">*</span> block_size, n_hidden), Tanh(),</span>
<span id="cb64-709"><a href="#cb64-709"></a>    Linear(           n_hidden, n_hidden), Tanh(),</span>
<span id="cb64-710"><a href="#cb64-710"></a>    Linear(           n_hidden, n_hidden), Tanh(),</span>
<span id="cb64-711"><a href="#cb64-711"></a>    Linear(           n_hidden, n_hidden), Tanh(),</span>
<span id="cb64-712"><a href="#cb64-712"></a>    Linear(           n_hidden, n_hidden), Tanh(),</span>
<span id="cb64-713"><a href="#cb64-713"></a>    Linear(           n_hidden, vocab_size),</span>
<span id="cb64-714"><a href="#cb64-714"></a>]</span>
<span id="cb64-715"><a href="#cb64-715"></a></span>
<span id="cb64-716"><a href="#cb64-716"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb64-717"><a href="#cb64-717"></a>    <span class="co"># last layer: make less confident</span></span>
<span id="cb64-718"><a href="#cb64-718"></a>    layers[<span class="op">-</span><span class="dv">1</span>].weight <span class="op">*=</span> <span class="fl">0.1</span></span>
<span id="cb64-719"><a href="#cb64-719"></a>    <span class="co"># all other layers: apply gain</span></span>
<span id="cb64-720"><a href="#cb64-720"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layers[:<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb64-721"><a href="#cb64-721"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(layer, Linear):</span>
<span id="cb64-722"><a href="#cb64-722"></a>            layer.weight <span class="op">*=</span> <span class="dv">5</span><span class="op">/</span><span class="dv">3</span></span>
<span id="cb64-723"><a href="#cb64-723"></a></span>
<span id="cb64-724"><a href="#cb64-724"></a>parameters <span class="op">=</span> [C] <span class="op">+</span> [p <span class="cf">for</span> layer <span class="kw">in</span> layers <span class="cf">for</span> p <span class="kw">in</span> layer.parameters()]</span>
<span id="cb64-725"><a href="#cb64-725"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters)) <span class="co"># number of parameters in total</span></span>
<span id="cb64-726"><a href="#cb64-726"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb64-727"><a href="#cb64-727"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb64-728"><a href="#cb64-728"></a><span class="in">```</span></span>
<span id="cb64-729"><a href="#cb64-729"></a></span>
<span id="cb64-732"><a href="#cb64-732"></a><span class="in">```{python}</span></span>
<span id="cb64-733"><a href="#cb64-733"></a><span class="co"># same optimization as last time</span></span>
<span id="cb64-734"><a href="#cb64-734"></a>max_steps <span class="op">=</span> <span class="dv">200000</span></span>
<span id="cb64-735"><a href="#cb64-735"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb64-736"><a href="#cb64-736"></a>lossi <span class="op">=</span> []</span>
<span id="cb64-737"><a href="#cb64-737"></a>ud <span class="op">=</span> []</span>
<span id="cb64-738"><a href="#cb64-738"></a></span>
<span id="cb64-739"><a href="#cb64-739"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb64-740"><a href="#cb64-740"></a>  </span>
<span id="cb64-741"><a href="#cb64-741"></a>    <span class="co"># minibatch construct</span></span>
<span id="cb64-742"><a href="#cb64-742"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,), generator<span class="op">=</span>g)</span>
<span id="cb64-743"><a href="#cb64-743"></a>    Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix] <span class="co"># batch X,Y</span></span>
<span id="cb64-744"><a href="#cb64-744"></a></span>
<span id="cb64-745"><a href="#cb64-745"></a>    <span class="co"># forward pass</span></span>
<span id="cb64-746"><a href="#cb64-746"></a>    emb <span class="op">=</span> C[Xb] <span class="co"># embed the characters into vectors</span></span>
<span id="cb64-747"><a href="#cb64-747"></a>    x <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate the vectors</span></span>
<span id="cb64-748"><a href="#cb64-748"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb64-749"><a href="#cb64-749"></a>        x <span class="op">=</span> layer(x)</span>
<span id="cb64-750"><a href="#cb64-750"></a>    loss <span class="op">=</span> F.cross_entropy(x, Yb) <span class="co"># loss function</span></span>
<span id="cb64-751"><a href="#cb64-751"></a></span>
<span id="cb64-752"><a href="#cb64-752"></a>    <span class="co"># backward pass</span></span>
<span id="cb64-753"><a href="#cb64-753"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb64-754"><a href="#cb64-754"></a>        layer.out.retain_grad() <span class="co"># AFTER_DEBUG: would take out retain_graph</span></span>
<span id="cb64-755"><a href="#cb64-755"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb64-756"><a href="#cb64-756"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb64-757"><a href="#cb64-757"></a>    loss.backward()</span>
<span id="cb64-758"><a href="#cb64-758"></a></span>
<span id="cb64-759"><a href="#cb64-759"></a>    <span class="co"># update</span></span>
<span id="cb64-760"><a href="#cb64-760"></a>    lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">150000</span> <span class="cf">else</span> <span class="fl">0.01</span> <span class="co"># step learning rate decay</span></span>
<span id="cb64-761"><a href="#cb64-761"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb64-762"><a href="#cb64-762"></a>        p.data <span class="op">+=</span> <span class="op">-</span>lr <span class="op">*</span> p.grad</span>
<span id="cb64-763"><a href="#cb64-763"></a></span>
<span id="cb64-764"><a href="#cb64-764"></a>    <span class="co"># track stats</span></span>
<span id="cb64-765"><a href="#cb64-765"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="co"># print every once in a while</span></span>
<span id="cb64-766"><a href="#cb64-766"></a>        <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">:7d}</span><span class="ss">/</span><span class="sc">{</span>max_steps<span class="sc">:7d}</span><span class="ss">: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb64-767"><a href="#cb64-767"></a>    lossi.append(loss.log10().item())</span>
<span id="cb64-768"><a href="#cb64-768"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb64-769"><a href="#cb64-769"></a>        ud.append([((lr<span class="op">*</span>p.grad).std() <span class="op">/</span> p.data.std()).log10().item() <span class="cf">for</span> p <span class="kw">in</span> parameters])</span>
<span id="cb64-770"><a href="#cb64-770"></a></span>
<span id="cb64-771"><a href="#cb64-771"></a>    <span class="cf">break</span></span>
<span id="cb64-772"><a href="#cb64-772"></a>    <span class="co"># if i &gt;= 1000:</span></span>
<span id="cb64-773"><a href="#cb64-773"></a>    <span class="co">#     break # AFTER_DEBUG: would take out obviously to run full optimization</span></span>
<span id="cb64-774"><a href="#cb64-774"></a><span class="in">```</span></span>
<span id="cb64-775"><a href="#cb64-775"></a></span>
<span id="cb64-776"><a href="#cb64-776"></a><span class="fu">## viz #1: forward pass activations statistics</span></span>
<span id="cb64-777"><a href="#cb64-777"></a></span>
<span id="cb64-780"><a href="#cb64-780"></a><span class="in">```{python}</span></span>
<span id="cb64-781"><a href="#cb64-781"></a><span class="co"># visualize histograms</span></span>
<span id="cb64-782"><a href="#cb64-782"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">11</span>, <span class="dv">3</span>)) <span class="co"># width and height of the plot</span></span>
<span id="cb64-783"><a href="#cb64-783"></a>legends <span class="op">=</span> []</span>
<span id="cb64-784"><a href="#cb64-784"></a><span class="cf">for</span> i, layer <span class="kw">in</span> <span class="bu">enumerate</span>(layers[:<span class="op">-</span><span class="dv">1</span>]): <span class="co"># note: exclude the output layer</span></span>
<span id="cb64-785"><a href="#cb64-785"></a>  <span class="cf">if</span> <span class="bu">isinstance</span>(layer, Tanh):</span>
<span id="cb64-786"><a href="#cb64-786"></a>    t <span class="op">=</span> layer.out</span>
<span id="cb64-787"><a href="#cb64-787"></a>    <span class="bu">print</span>(<span class="st">'layer </span><span class="sc">%d</span><span class="st"> (</span><span class="sc">%10s</span><span class="st">): mean </span><span class="sc">%+.2f</span><span class="st">, std </span><span class="sc">%.2f</span><span class="st">, saturated: </span><span class="sc">%.2f%%</span><span class="st">'</span> <span class="op">%</span> (i, layer.__class__.<span class="va">__name__</span>, t.mean(), t.std(), (t.<span class="bu">abs</span>() <span class="op">&gt;</span> <span class="fl">0.97</span>).<span class="bu">float</span>().mean()<span class="op">*</span><span class="dv">100</span>))</span>
<span id="cb64-788"><a href="#cb64-788"></a>    hy, hx <span class="op">=</span> torch.histogram(t, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-789"><a href="#cb64-789"></a>    plt.plot(hx[:<span class="op">-</span><span class="dv">1</span>].detach(), hy.detach())</span>
<span id="cb64-790"><a href="#cb64-790"></a>    legends.append(<span class="ss">f'layer </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>layer<span class="sc">.</span>__class__<span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb64-791"><a href="#cb64-791"></a>plt.legend(legends)<span class="op">;</span></span>
<span id="cb64-792"><a href="#cb64-792"></a>plt.title(<span class="st">'activation distribution'</span>)</span>
<span id="cb64-793"><a href="#cb64-793"></a><span class="in">```</span></span>
<span id="cb64-794"><a href="#cb64-794"></a></span>
<span id="cb64-795"><a href="#cb64-795"></a>If we set the gain to <span class="in">`1`</span>, the std is shrinking, and the saturation is coming to zeros, due to the first layer is pretty decent, but the next ones are shrinking to zero because of the <span class="in">`tanh()`</span> - a squashing function.</span>
<span id="cb64-796"><a href="#cb64-796"></a></span>
<span id="cb64-797"><a href="#cb64-797"></a><span class="in">``` md</span></span>
<span id="cb64-798"><a href="#cb64-798"></a>layer 1 (      Tanh): mean -0.02, std 0.62, saturated: 3.50%</span>
<span id="cb64-799"><a href="#cb64-799"></a>layer 3 (      Tanh): mean -0.00, std 0.48, saturated: 0.03%</span>
<span id="cb64-800"><a href="#cb64-800"></a>layer 5 (      Tanh): mean +0.00, std 0.41, saturated: 0.06%</span>
<span id="cb64-801"><a href="#cb64-801"></a>layer 7 (      Tanh): mean +0.00, std 0.35, saturated: 0.00%</span>
<span id="cb64-802"><a href="#cb64-802"></a>layer 9 (      Tanh): mean -0.02, std 0.32, saturated: 0.00%</span>
<span id="cb64-803"><a href="#cb64-803"></a>Text(0.5, 1.0, 'activation distribution')</span>
<span id="cb64-804"><a href="#cb64-804"></a><span class="in">```</span></span>
<span id="cb64-805"><a href="#cb64-805"></a></span>
<span id="cb64-806"><a href="#cb64-806"></a><span class="al">![If the gain is 1](viz1_gain1.png)</span></span>
<span id="cb64-807"><a href="#cb64-807"></a></span>
<span id="cb64-808"><a href="#cb64-808"></a>But if we set the gain is far too high, let's say <span class="in">`3`</span>, we can see the saturation is too high.</span>
<span id="cb64-809"><a href="#cb64-809"></a></span>
<span id="cb64-810"><a href="#cb64-810"></a><span class="in">``` md</span></span>
<span id="cb64-811"><a href="#cb64-811"></a>layer 1 (      Tanh): mean -0.03, std 0.85, saturated: 47.66%</span>
<span id="cb64-812"><a href="#cb64-812"></a>layer 3 (      Tanh): mean +0.00, std 0.84, saturated: 40.47%</span>
<span id="cb64-813"><a href="#cb64-813"></a>layer 5 (      Tanh): mean -0.01, std 0.84, saturated: 42.38%</span>
<span id="cb64-814"><a href="#cb64-814"></a>layer 7 (      Tanh): mean -0.01, std 0.84, saturated: 42.00%</span>
<span id="cb64-815"><a href="#cb64-815"></a>layer 9 (      Tanh): mean -0.03, std 0.84, saturated: 42.41%</span>
<span id="cb64-816"><a href="#cb64-816"></a>Text(0.5, 1.0, 'activation distribution')</span>
<span id="cb64-817"><a href="#cb64-817"></a><span class="in">```</span></span>
<span id="cb64-818"><a href="#cb64-818"></a></span>
<span id="cb64-819"><a href="#cb64-819"></a><span class="al">![If the gain is 3](viz1_gain3.png)</span></span>
<span id="cb64-820"><a href="#cb64-820"></a></span>
<span id="cb64-821"><a href="#cb64-821"></a>So <span class="in">`5/3`</span> is a nice one, balancing the std and saturation.</span>
<span id="cb64-822"><a href="#cb64-822"></a></span>
<span id="cb64-823"><a href="#cb64-823"></a>::: {.callout-note title="Why 5/3?"}</span>
<span id="cb64-824"><a href="#cb64-824"></a>A comment in his video explains why <span class="in">`5/3`</span> is recommended, it comes from the avg of $<span class="co">[</span><span class="ot">\tanh(x)</span><span class="co">]</span>^2$ where $x$ is distributed as a Gaussian:</span>
<span id="cb64-825"><a href="#cb64-825"></a></span>
<span id="cb64-826"><a href="#cb64-826"></a>$\int_{-\infty}^{\infty} \frac{<span class="co">[</span><span class="ot">\tanh(x)</span><span class="co">]</span>^2 \exp(-\frac{x^2}{2})}{\sqrt{2\pi}} \, dx \approx 0.39$</span>
<span id="cb64-827"><a href="#cb64-827"></a></span>
<span id="cb64-828"><a href="#cb64-828"></a><span class="at">&gt; The square root of this value is how much the </span><span class="in">`tanh`</span><span class="at"> squeezes the variance of the incoming variable: 0.39 </span><span class="sc">\*\*</span><span class="at"> .5 \~= 0.63 \~= 3/5 (hence 5/3 is just an approximation of the exact gain).</span></span>
<span id="cb64-829"><a href="#cb64-829"></a><span class="at">:::</span></span>
<span id="cb64-830"><a href="#cb64-830"></a></span>
<span id="cb64-831"><a href="#cb64-831"></a><span class="fu">## viz #2: backward pass gradient statistics</span></span>
<span id="cb64-832"><a href="#cb64-832"></a></span>
<span id="cb64-833"><a href="#cb64-833"></a>Similarly, we can do the same thing with gradients. With the setting of gain as <span class="in">`5/3`</span>, the distribution of gradients through layers quite the same. Layer by layer, the value of gradients will be shrank close to zero, the distributions would be more and more peak, so the gain here will help expanding those distributions.</span>
<span id="cb64-834"><a href="#cb64-834"></a></span>
<span id="cb64-837"><a href="#cb64-837"></a><span class="in">```{python}</span></span>
<span id="cb64-838"><a href="#cb64-838"></a><span class="co"># visualize histograms</span></span>
<span id="cb64-839"><a href="#cb64-839"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">11</span>, <span class="dv">3</span>)) <span class="co"># width and height of the plot</span></span>
<span id="cb64-840"><a href="#cb64-840"></a>legends <span class="op">=</span> []</span>
<span id="cb64-841"><a href="#cb64-841"></a><span class="cf">for</span> i, layer <span class="kw">in</span> <span class="bu">enumerate</span>(layers[:<span class="op">-</span><span class="dv">1</span>]): <span class="co"># note: exclude the output layer</span></span>
<span id="cb64-842"><a href="#cb64-842"></a>  <span class="cf">if</span> <span class="bu">isinstance</span>(layer, Tanh):</span>
<span id="cb64-843"><a href="#cb64-843"></a>    t <span class="op">=</span> layer.out.grad</span>
<span id="cb64-844"><a href="#cb64-844"></a>    <span class="bu">print</span>(<span class="st">'layer </span><span class="sc">%d</span><span class="st"> (</span><span class="sc">%10s</span><span class="st">): mean </span><span class="sc">%+f</span><span class="st">, std </span><span class="sc">%e</span><span class="st">'</span> <span class="op">%</span> (i, layer.__class__.<span class="va">__name__</span>, t.mean(), t.std()))</span>
<span id="cb64-845"><a href="#cb64-845"></a>    hy, hx <span class="op">=</span> torch.histogram(t, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-846"><a href="#cb64-846"></a>    plt.plot(hx[:<span class="op">-</span><span class="dv">1</span>].detach(), hy.detach())</span>
<span id="cb64-847"><a href="#cb64-847"></a>    legends.append(<span class="ss">f'layer </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>layer<span class="sc">.</span>__class__<span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb64-848"><a href="#cb64-848"></a>plt.legend(legends)<span class="op">;</span></span>
<span id="cb64-849"><a href="#cb64-849"></a>plt.title(<span class="st">'gradient distribution'</span>)</span>
<span id="cb64-850"><a href="#cb64-850"></a><span class="in">```</span></span>
<span id="cb64-851"><a href="#cb64-851"></a></span>
<span id="cb64-852"><a href="#cb64-852"></a><span class="fu">## the fully linear case of no non-linearity</span></span>
<span id="cb64-853"><a href="#cb64-853"></a></span>
<span id="cb64-854"><a href="#cb64-854"></a>Now imagine if we remove the <span class="in">`tanh`</span> from all layers, the recommend gain now for Linear is <span class="in">`1`</span>.</span>
<span id="cb64-855"><a href="#cb64-855"></a></span>
<span id="cb64-858"><a href="#cb64-858"></a><span class="in">```{python}</span></span>
<span id="cb64-859"><a href="#cb64-859"></a><span class="co">#| eval: false</span></span>
<span id="cb64-860"><a href="#cb64-860"></a>layers <span class="op">=</span> [</span>
<span id="cb64-861"><a href="#cb64-861"></a>  Linear(n_embd <span class="op">*</span> block_size, n_hidden), <span class="co">#Tanh(),</span></span>
<span id="cb64-862"><a href="#cb64-862"></a>  Linear(           n_hidden, n_hidden), <span class="co">#Tanh(),</span></span>
<span id="cb64-863"><a href="#cb64-863"></a>  Linear(           n_hidden, n_hidden), <span class="co">#Tanh(),</span></span>
<span id="cb64-864"><a href="#cb64-864"></a>  Linear(           n_hidden, n_hidden), <span class="co">#Tanh(),</span></span>
<span id="cb64-865"><a href="#cb64-865"></a>  Linear(           n_hidden, n_hidden), <span class="co">#Tanh(),</span></span>
<span id="cb64-866"><a href="#cb64-866"></a>  Linear(           n_hidden, vocab_size),</span>
<span id="cb64-867"><a href="#cb64-867"></a>]</span>
<span id="cb64-868"><a href="#cb64-868"></a><span class="in">```</span></span>
<span id="cb64-869"><a href="#cb64-869"></a></span>
<span id="cb64-870"><a href="#cb64-870"></a>But you'll end up getting a pure linear network. No matter of how many Linear Layers you stacked up, it just the combination of all layers to a massive linear function $y = xA^T + b$, which will greatly limit the capacity of the neural nets.</span>
<span id="cb64-871"><a href="#cb64-871"></a></span>
<span id="cb64-872"><a href="#cb64-872"></a><span class="fu">## viz #3: parameter activation and gradient statistics</span></span>
<span id="cb64-873"><a href="#cb64-873"></a></span>
<span id="cb64-874"><a href="#cb64-874"></a>We can also visualize the distribution of paramaters, here below only weight for simplicity (ignoring gamma, beta, etc...). We observed mean, std, and the grad to data ratio (to see how much the data will be updated).</span>
<span id="cb64-875"><a href="#cb64-875"></a></span>
<span id="cb64-876"><a href="#cb64-876"></a>Problem for the last layer is shown in code output below, the weights on last layer are 10 times bigger than previous ones, and the grad to data ratio is too high.</span>
<span id="cb64-877"><a href="#cb64-877"></a></span>
<span id="cb64-878"><a href="#cb64-878"></a>We can try run 1st 1000 training loops and this can be slight reduced, but since we are using a simple optimizer SGD rather than modern one like Adam, it is still problematic.</span>
<span id="cb64-879"><a href="#cb64-879"></a></span>
<span id="cb64-882"><a href="#cb64-882"></a><span class="in">```{python}</span></span>
<span id="cb64-883"><a href="#cb64-883"></a><span class="co"># visualize histograms</span></span>
<span id="cb64-884"><a href="#cb64-884"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">11</span>, <span class="dv">3</span>)) <span class="co"># width and height of the plot</span></span>
<span id="cb64-885"><a href="#cb64-885"></a>legends <span class="op">=</span> []</span>
<span id="cb64-886"><a href="#cb64-886"></a><span class="cf">for</span> i,p <span class="kw">in</span> <span class="bu">enumerate</span>(parameters):</span>
<span id="cb64-887"><a href="#cb64-887"></a>  t <span class="op">=</span> p.grad</span>
<span id="cb64-888"><a href="#cb64-888"></a>  <span class="cf">if</span> p.ndim <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb64-889"><a href="#cb64-889"></a>    <span class="bu">print</span>(<span class="st">'weight </span><span class="sc">%10s</span><span class="st"> | mean </span><span class="sc">%+f</span><span class="st"> | std </span><span class="sc">%e</span><span class="st"> | grad:data ratio </span><span class="sc">%e</span><span class="st">'</span> <span class="op">%</span> (<span class="bu">tuple</span>(p.shape), t.mean(), t.std(), t.std() <span class="op">/</span> p.std()))</span>
<span id="cb64-890"><a href="#cb64-890"></a>    hy, hx <span class="op">=</span> torch.histogram(t, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-891"><a href="#cb64-891"></a>    plt.plot(hx[:<span class="op">-</span><span class="dv">1</span>].detach(), hy.detach())</span>
<span id="cb64-892"><a href="#cb64-892"></a>    legends.append(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span><span class="bu">tuple</span>(p.shape)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb64-893"><a href="#cb64-893"></a>plt.legend(legends)</span>
<span id="cb64-894"><a href="#cb64-894"></a>plt.title(<span class="st">'weights gradient distribution'</span>)<span class="op">;</span></span>
<span id="cb64-895"><a href="#cb64-895"></a><span class="in">```</span></span>
<span id="cb64-896"><a href="#cb64-896"></a></span>
<span id="cb64-897"><a href="#cb64-897"></a><span class="fu">## viz #4: update data ratio over time</span></span>
<span id="cb64-898"><a href="#cb64-898"></a></span>
<span id="cb64-899"><a href="#cb64-899"></a>The grad to data above ratio is at the end not really informative (only at one point in time), what matter is actual amount which we change the data in these tensors (over time). AK introduce a tracking list <span class="in">`ud`</span> (update to data). This calculates the ratio between (std) of the grad to the data of parameters (and <span class="in">`log10()`</span> for a nicer viz) **without context of gradient**.</span>
<span id="cb64-900"><a href="#cb64-900"></a></span>
<span id="cb64-903"><a href="#cb64-903"></a><span class="in">```{python}</span></span>
<span id="cb64-904"><a href="#cb64-904"></a><span class="co">#| eval: false</span></span>
<span id="cb64-905"><a href="#cb64-905"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">11</span>, <span class="dv">3</span>))</span>
<span id="cb64-906"><a href="#cb64-906"></a>legends <span class="op">=</span> []</span>
<span id="cb64-907"><a href="#cb64-907"></a><span class="cf">for</span> i,p <span class="kw">in</span> <span class="bu">enumerate</span>(parameters):</span>
<span id="cb64-908"><a href="#cb64-908"></a>  <span class="cf">if</span> p.ndim <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb64-909"><a href="#cb64-909"></a>    plt.plot([ud[j][i] <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(ud))])</span>
<span id="cb64-910"><a href="#cb64-910"></a>    legends.append(<span class="st">'param </span><span class="sc">%d</span><span class="st">'</span> <span class="op">%</span> i)</span>
<span id="cb64-911"><a href="#cb64-911"></a>plt.plot([<span class="dv">0</span>, <span class="bu">len</span>(ud)], [<span class="op">-</span><span class="dv">3</span>, <span class="op">-</span><span class="dv">3</span>], <span class="st">'k'</span>) <span class="co"># these ratios should be ~1e-3, indicate on plot</span></span>
<span id="cb64-912"><a href="#cb64-912"></a>plt.legend(legends)<span class="op">;</span></span>
<span id="cb64-913"><a href="#cb64-913"></a><span class="in">```</span></span>
<span id="cb64-914"><a href="#cb64-914"></a></span>
<span id="cb64-915"><a href="#cb64-915"></a>Below is the visualization from data collected after 1000 training loops:</span>
<span id="cb64-916"><a href="#cb64-916"></a></span>
<span id="cb64-917"><a href="#cb64-917"></a><span class="al">![Viz 4 1000](viz4_1000times.png)</span></span>
<span id="cb64-918"><a href="#cb64-918"></a></span>
<span id="cb64-919"><a href="#cb64-919"></a>Recall what we did to the last layer, avoiding over confidence, so the pink line looks different among others. In general, the learning process are good, if we change the learning rate to <span class="in">`0.0001`</span>, the chart looks much worse.</span>
<span id="cb64-920"><a href="#cb64-920"></a></span>
<span id="cb64-921"><a href="#cb64-921"></a>Below are viz 1 after 1000 training loops:</span>
<span id="cb64-922"><a href="#cb64-922"></a></span>
<span id="cb64-923"><a href="#cb64-923"></a><span class="al">![Viz 1 1000](viz1_1000times.png)</span></span>
<span id="cb64-924"><a href="#cb64-924"></a></span>
<span id="cb64-925"><a href="#cb64-925"></a>and viz 2:</span>
<span id="cb64-926"><a href="#cb64-926"></a></span>
<span id="cb64-927"><a href="#cb64-927"></a><span class="al">![Viz 2 1000](viz2_1000times.png)</span></span>
<span id="cb64-928"><a href="#cb64-928"></a></span>
<span id="cb64-929"><a href="#cb64-929"></a>and viz 3:</span>
<span id="cb64-930"><a href="#cb64-930"></a></span>
<span id="cb64-931"><a href="#cb64-931"></a><span class="al">![Viz 3 1000](viz3_1000times.png)</span></span>
<span id="cb64-932"><a href="#cb64-932"></a></span>
<span id="cb64-933"><a href="#cb64-933"></a>Pretty decent till now. Let's bring the BatchNorm back.</span>
<span id="cb64-934"><a href="#cb64-934"></a></span>
<span id="cb64-935"><a href="#cb64-935"></a><span class="fu">## bringing back batchnorm, looking at the visualizations</span></span>
<span id="cb64-936"><a href="#cb64-936"></a></span>
<span id="cb64-937"><a href="#cb64-937"></a>We re-define the layers, and change <span class="in">`gamma`</span> in last layer under no gradient instead of <span class="in">`weight`</span>. We also dont want the "manual normalization" fan-in, and the gain <span class="in">`5/3`</span> as well:</span>
<span id="cb64-938"><a href="#cb64-938"></a></span>
<span id="cb64-941"><a href="#cb64-941"></a><span class="in">```{python}</span></span>
<span id="cb64-942"><a href="#cb64-942"></a><span class="co">#| eval: false</span></span>
<span id="cb64-943"><a href="#cb64-943"></a>layers <span class="op">=</span> [</span>
<span id="cb64-944"><a href="#cb64-944"></a>  Linear(n_embd <span class="op">*</span> block_size, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb64-945"><a href="#cb64-945"></a>  Linear(           n_hidden, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb64-946"><a href="#cb64-946"></a>  Linear(           n_hidden, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb64-947"><a href="#cb64-947"></a>  Linear(           n_hidden, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb64-948"><a href="#cb64-948"></a>  Linear(           n_hidden, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb64-949"><a href="#cb64-949"></a>  Linear(           n_hidden, vocab_size, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(vocab_size),</span>
<span id="cb64-950"><a href="#cb64-950"></a>]</span>
<span id="cb64-951"><a href="#cb64-951"></a><span class="in">```</span></span>
<span id="cb64-952"><a href="#cb64-952"></a></span>
<span id="cb64-953"><a href="#cb64-953"></a><span class="fu">## summary of the lecture for real this time</span></span>
<span id="cb64-954"><a href="#cb64-954"></a></span>
<span id="cb64-955"><a href="#cb64-955"></a><span class="ss">1.  </span>Intruduction of Batch Normalization - the 1st one of modern innovation to stablize Deep NN training;</span>
<span id="cb64-956"><a href="#cb64-956"></a><span class="ss">2.  </span>PyTorch-ifying code;</span>
<span id="cb64-957"><a href="#cb64-957"></a><span class="ss">3.  </span>Introduction to some diagnostic tools that we can use to verify the network is in good state dynamically.</span>
<span id="cb64-958"><a href="#cb64-958"></a></span>
<span id="cb64-959"><a href="#cb64-959"></a>What he did not try to improve here is the loss of the network. It's now somehow bottleneck not by the Optimization, but by the Context Length he suspect.</span>
<span id="cb64-960"><a href="#cb64-960"></a></span>
<span id="cb64-961"><a href="#cb64-961"></a><span class="at">&gt; Training Neural Network is like balancing a pencil on a finger.</span></span>
<span id="cb64-962"><a href="#cb64-962"></a></span>
<span id="cb64-963"><a href="#cb64-963"></a>Final network architecture and training:</span>
<span id="cb64-964"><a href="#cb64-964"></a></span>
<span id="cb64-967"><a href="#cb64-967"></a><span class="in">```{python}</span></span>
<span id="cb64-968"><a href="#cb64-968"></a><span class="co">#| code-fold: true</span></span>
<span id="cb64-969"><a href="#cb64-969"></a><span class="co"># BatchNorm1D and Tanh are the same</span></span>
<span id="cb64-970"><a href="#cb64-970"></a><span class="kw">class</span> Linear:</span>
<span id="cb64-971"><a href="#cb64-971"></a>    <span class="co">"""</span></span>
<span id="cb64-972"><a href="#cb64-972"></a><span class="co">    Simplifying Pytorch Linear Layer: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear</span></span>
<span id="cb64-973"><a href="#cb64-973"></a><span class="co">    """</span></span>
<span id="cb64-974"><a href="#cb64-974"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, fan_in, fan_out, bias<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb64-975"><a href="#cb64-975"></a>        <span class="va">self</span>.weight <span class="op">=</span> torch.randn((fan_in, fan_out), generator<span class="op">=</span>g) <span class="co"># / fan_in**0.5</span></span>
<span id="cb64-976"><a href="#cb64-976"></a>        <span class="va">self</span>.bias <span class="op">=</span> torch.zeros(fan_out) <span class="cf">if</span> bias <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb64-977"><a href="#cb64-977"></a></span>
<span id="cb64-978"><a href="#cb64-978"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb64-979"><a href="#cb64-979"></a>        <span class="va">self</span>.out <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.weight</span>
<span id="cb64-980"><a href="#cb64-980"></a>        <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb64-981"><a href="#cb64-981"></a>            <span class="va">self</span>.out <span class="op">+=</span> <span class="va">self</span>.bias</span>
<span id="cb64-982"><a href="#cb64-982"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb64-983"><a href="#cb64-983"></a></span>
<span id="cb64-984"><a href="#cb64-984"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb64-985"><a href="#cb64-985"></a>        <span class="cf">return</span> [<span class="va">self</span>.weight] <span class="op">+</span> ([] <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> [<span class="va">self</span>.bias])</span>
<span id="cb64-986"><a href="#cb64-986"></a></span>
<span id="cb64-987"><a href="#cb64-987"></a>n_embd <span class="op">=</span> <span class="dv">10</span> <span class="co"># the dimensionality of the character embedding vectors</span></span>
<span id="cb64-988"><a href="#cb64-988"></a>n_hidden <span class="op">=</span> <span class="dv">100</span> <span class="co"># the number of neurons in the hidden layer of the MLP</span></span>
<span id="cb64-989"><a href="#cb64-989"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>) <span class="co"># for reproducibility</span></span>
<span id="cb64-990"><a href="#cb64-990"></a>C <span class="op">=</span> torch.randn((vocab_size, n_embd),            generator<span class="op">=</span>g)</span>
<span id="cb64-991"><a href="#cb64-991"></a>layers <span class="op">=</span> [</span>
<span id="cb64-992"><a href="#cb64-992"></a>  Linear(n_embd <span class="op">*</span> block_size, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb64-993"><a href="#cb64-993"></a>  Linear(           n_hidden, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb64-994"><a href="#cb64-994"></a>  Linear(           n_hidden, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb64-995"><a href="#cb64-995"></a>  Linear(           n_hidden, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb64-996"><a href="#cb64-996"></a>  Linear(           n_hidden, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb64-997"><a href="#cb64-997"></a>  Linear(           n_hidden, vocab_size, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(vocab_size),</span>
<span id="cb64-998"><a href="#cb64-998"></a>]</span>
<span id="cb64-999"><a href="#cb64-999"></a></span>
<span id="cb64-1000"><a href="#cb64-1000"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb64-1001"><a href="#cb64-1001"></a>    <span class="co"># last layer: make less confident</span></span>
<span id="cb64-1002"><a href="#cb64-1002"></a>    layers[<span class="op">-</span><span class="dv">1</span>].gamma <span class="op">*=</span> <span class="fl">0.1</span></span>
<span id="cb64-1003"><a href="#cb64-1003"></a>    <span class="co"># all other layers: apply gain</span></span>
<span id="cb64-1004"><a href="#cb64-1004"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layers[:<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb64-1005"><a href="#cb64-1005"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(layer, Linear):</span>
<span id="cb64-1006"><a href="#cb64-1006"></a>            layer.weight <span class="op">*=</span> <span class="fl">1.0</span> <span class="co">#5/3</span></span>
<span id="cb64-1007"><a href="#cb64-1007"></a></span>
<span id="cb64-1008"><a href="#cb64-1008"></a>parameters <span class="op">=</span> [C] <span class="op">+</span> [p <span class="cf">for</span> layer <span class="kw">in</span> layers <span class="cf">for</span> p <span class="kw">in</span> layer.parameters()]</span>
<span id="cb64-1009"><a href="#cb64-1009"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters)) <span class="co"># number of parameters in total</span></span>
<span id="cb64-1010"><a href="#cb64-1010"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb64-1011"><a href="#cb64-1011"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb64-1012"><a href="#cb64-1012"></a></span>
<span id="cb64-1013"><a href="#cb64-1013"></a><span class="co"># same optimization as last time</span></span>
<span id="cb64-1014"><a href="#cb64-1014"></a>max_steps <span class="op">=</span> <span class="dv">200000</span></span>
<span id="cb64-1015"><a href="#cb64-1015"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb64-1016"><a href="#cb64-1016"></a>lossi <span class="op">=</span> []</span>
<span id="cb64-1017"><a href="#cb64-1017"></a>ud <span class="op">=</span> []</span>
<span id="cb64-1018"><a href="#cb64-1018"></a></span>
<span id="cb64-1019"><a href="#cb64-1019"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb64-1020"><a href="#cb64-1020"></a>  </span>
<span id="cb64-1021"><a href="#cb64-1021"></a>    <span class="co"># minibatch construct</span></span>
<span id="cb64-1022"><a href="#cb64-1022"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,), generator<span class="op">=</span>g)</span>
<span id="cb64-1023"><a href="#cb64-1023"></a>    Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix] <span class="co"># batch X,Y</span></span>
<span id="cb64-1024"><a href="#cb64-1024"></a></span>
<span id="cb64-1025"><a href="#cb64-1025"></a>    <span class="co"># forward pass</span></span>
<span id="cb64-1026"><a href="#cb64-1026"></a>    emb <span class="op">=</span> C[Xb] <span class="co"># embed the characters into vectors</span></span>
<span id="cb64-1027"><a href="#cb64-1027"></a>    x <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate the vectors</span></span>
<span id="cb64-1028"><a href="#cb64-1028"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb64-1029"><a href="#cb64-1029"></a>        x <span class="op">=</span> layer(x)</span>
<span id="cb64-1030"><a href="#cb64-1030"></a>    loss <span class="op">=</span> F.cross_entropy(x, Yb) <span class="co"># loss function</span></span>
<span id="cb64-1031"><a href="#cb64-1031"></a></span>
<span id="cb64-1032"><a href="#cb64-1032"></a>    <span class="co"># backward pass</span></span>
<span id="cb64-1033"><a href="#cb64-1033"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb64-1034"><a href="#cb64-1034"></a>        layer.out.retain_grad() <span class="co"># AFTER_DEBUG: would take out retain_graph</span></span>
<span id="cb64-1035"><a href="#cb64-1035"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb64-1036"><a href="#cb64-1036"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb64-1037"><a href="#cb64-1037"></a>    loss.backward()</span>
<span id="cb64-1038"><a href="#cb64-1038"></a></span>
<span id="cb64-1039"><a href="#cb64-1039"></a>    <span class="co"># update</span></span>
<span id="cb64-1040"><a href="#cb64-1040"></a>    lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">150000</span> <span class="cf">else</span> <span class="fl">0.01</span> <span class="co"># step learning rate decay</span></span>
<span id="cb64-1041"><a href="#cb64-1041"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb64-1042"><a href="#cb64-1042"></a>        p.data <span class="op">+=</span> <span class="op">-</span>lr <span class="op">*</span> p.grad</span>
<span id="cb64-1043"><a href="#cb64-1043"></a></span>
<span id="cb64-1044"><a href="#cb64-1044"></a>    <span class="co"># track stats</span></span>
<span id="cb64-1045"><a href="#cb64-1045"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="co"># print every once in a while</span></span>
<span id="cb64-1046"><a href="#cb64-1046"></a>        <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">:7d}</span><span class="ss">/</span><span class="sc">{</span>max_steps<span class="sc">:7d}</span><span class="ss">: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb64-1047"><a href="#cb64-1047"></a>    lossi.append(loss.log10().item())</span>
<span id="cb64-1048"><a href="#cb64-1048"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb64-1049"><a href="#cb64-1049"></a>        ud.append([((lr<span class="op">*</span>p.grad).std() <span class="op">/</span> p.data.std()).log10().item() <span class="cf">for</span> p <span class="kw">in</span> parameters])</span>
<span id="cb64-1050"><a href="#cb64-1050"></a></span>
<span id="cb64-1051"><a href="#cb64-1051"></a>    <span class="co"># break</span></span>
<span id="cb64-1052"><a href="#cb64-1052"></a>    <span class="co"># if i &gt;= 1000:</span></span>
<span id="cb64-1053"><a href="#cb64-1053"></a>    <span class="co">#     break # AFTER_DEBUG: would take out obviously to run full optimization</span></span>
<span id="cb64-1054"><a href="#cb64-1054"></a><span class="in">```</span></span>
<span id="cb64-1055"><a href="#cb64-1055"></a></span>
<span id="cb64-1056"><a href="#cb64-1056"></a>Final visualization:</span>
<span id="cb64-1057"><a href="#cb64-1057"></a></span>
<span id="cb64-1058"><a href="#cb64-1058"></a>Viz 1:</span>
<span id="cb64-1059"><a href="#cb64-1059"></a></span>
<span id="cb64-1060"><a href="#cb64-1060"></a><span class="al">![Viz 1 final](viz1_final.png)</span></span>
<span id="cb64-1061"><a href="#cb64-1061"></a></span>
<span id="cb64-1062"><a href="#cb64-1062"></a>Viz 2:</span>
<span id="cb64-1063"><a href="#cb64-1063"></a></span>
<span id="cb64-1064"><a href="#cb64-1064"></a><span class="al">![Viz 2 final](viz2_final.png)</span></span>
<span id="cb64-1065"><a href="#cb64-1065"></a></span>
<span id="cb64-1066"><a href="#cb64-1066"></a>Viz 3:</span>
<span id="cb64-1067"><a href="#cb64-1067"></a></span>
<span id="cb64-1068"><a href="#cb64-1068"></a><span class="al">![Viz 3 final](viz3_final.png)</span></span>
<span id="cb64-1069"><a href="#cb64-1069"></a></span>
<span id="cb64-1070"><a href="#cb64-1070"></a>Viz 4:</span>
<span id="cb64-1071"><a href="#cb64-1071"></a></span>
<span id="cb64-1072"><a href="#cb64-1072"></a><span class="al">![Viz 4 final](viz4_final.png)</span></span>
<span id="cb64-1073"><a href="#cb64-1073"></a></span>
<span id="cb64-1074"><a href="#cb64-1074"></a>The final loss on train/val:</span>
<span id="cb64-1075"><a href="#cb64-1075"></a></span>
<span id="cb64-1078"><a href="#cb64-1078"></a><span class="in">```{python}</span></span>
<span id="cb64-1079"><a href="#cb64-1079"></a><span class="co">#| code-fold: true</span></span>
<span id="cb64-1080"><a href="#cb64-1080"></a><span class="at">@torch.no_grad</span>() <span class="co"># this decorator disables gradient tracking</span></span>
<span id="cb64-1081"><a href="#cb64-1081"></a><span class="kw">def</span> split_loss(split):</span>
<span id="cb64-1082"><a href="#cb64-1082"></a>  x,y <span class="op">=</span> {</span>
<span id="cb64-1083"><a href="#cb64-1083"></a>    <span class="st">'train'</span>: (Xtr, Ytr),</span>
<span id="cb64-1084"><a href="#cb64-1084"></a>    <span class="st">'val'</span>: (Xdev, Ydev),</span>
<span id="cb64-1085"><a href="#cb64-1085"></a>    <span class="st">'test'</span>: (Xte, Yte),</span>
<span id="cb64-1086"><a href="#cb64-1086"></a>  }[split]</span>
<span id="cb64-1087"><a href="#cb64-1087"></a>  emb <span class="op">=</span> C[x] <span class="co"># (N, block_size, n_embd)</span></span>
<span id="cb64-1088"><a href="#cb64-1088"></a>  x <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concat into (N, block_size * n_embd)</span></span>
<span id="cb64-1089"><a href="#cb64-1089"></a>  <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb64-1090"><a href="#cb64-1090"></a>    x <span class="op">=</span> layer(x)</span>
<span id="cb64-1091"><a href="#cb64-1091"></a>  loss <span class="op">=</span> F.cross_entropy(x, y)</span>
<span id="cb64-1092"><a href="#cb64-1092"></a>  <span class="bu">print</span>(split, loss.item())</span>
<span id="cb64-1093"><a href="#cb64-1093"></a></span>
<span id="cb64-1094"><a href="#cb64-1094"></a><span class="co"># put layers into eval mode</span></span>
<span id="cb64-1095"><a href="#cb64-1095"></a><span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb64-1096"><a href="#cb64-1096"></a>  layer.training <span class="op">=</span> <span class="va">False</span></span>
<span id="cb64-1097"><a href="#cb64-1097"></a>split_loss(<span class="st">'train'</span>)</span>
<span id="cb64-1098"><a href="#cb64-1098"></a>split_loss(<span class="st">'val'</span>)</span>
<span id="cb64-1099"><a href="#cb64-1099"></a><span class="in">```</span></span>
<span id="cb64-1100"><a href="#cb64-1100"></a></span>
<span id="cb64-1101"><a href="#cb64-1101"></a>Sample from the model:</span>
<span id="cb64-1102"><a href="#cb64-1102"></a></span>
<span id="cb64-1105"><a href="#cb64-1105"></a><span class="in">```{python}</span></span>
<span id="cb64-1106"><a href="#cb64-1106"></a><span class="co">#| code-fold: true</span></span>
<span id="cb64-1107"><a href="#cb64-1107"></a><span class="co"># sample from the model</span></span>
<span id="cb64-1108"><a href="#cb64-1108"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span> <span class="op">+</span> <span class="dv">10</span>)</span>
<span id="cb64-1109"><a href="#cb64-1109"></a></span>
<span id="cb64-1110"><a href="#cb64-1110"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb64-1111"><a href="#cb64-1111"></a>    </span>
<span id="cb64-1112"><a href="#cb64-1112"></a>    out <span class="op">=</span> []</span>
<span id="cb64-1113"><a href="#cb64-1113"></a>    context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size <span class="co"># initialize with all ...</span></span>
<span id="cb64-1114"><a href="#cb64-1114"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb64-1115"><a href="#cb64-1115"></a>      <span class="co"># forward pass the neural net</span></span>
<span id="cb64-1116"><a href="#cb64-1116"></a>      emb <span class="op">=</span> C[torch.tensor([context])] <span class="co"># (1,block_size,n_embd)</span></span>
<span id="cb64-1117"><a href="#cb64-1117"></a>      x <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate the vectors</span></span>
<span id="cb64-1118"><a href="#cb64-1118"></a>      <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb64-1119"><a href="#cb64-1119"></a>        x <span class="op">=</span> layer(x)</span>
<span id="cb64-1120"><a href="#cb64-1120"></a>      logits <span class="op">=</span> x</span>
<span id="cb64-1121"><a href="#cb64-1121"></a>      probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb64-1122"><a href="#cb64-1122"></a>      <span class="co"># sample from the distribution</span></span>
<span id="cb64-1123"><a href="#cb64-1123"></a>      ix <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>, generator<span class="op">=</span>g).item()</span>
<span id="cb64-1124"><a href="#cb64-1124"></a>      <span class="co"># shift the context window and track the samples</span></span>
<span id="cb64-1125"><a href="#cb64-1125"></a>      context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix]</span>
<span id="cb64-1126"><a href="#cb64-1126"></a>      out.append(ix)</span>
<span id="cb64-1127"><a href="#cb64-1127"></a>      <span class="co"># if we sample the special '.' token, break</span></span>
<span id="cb64-1128"><a href="#cb64-1128"></a>      <span class="cf">if</span> ix <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb64-1129"><a href="#cb64-1129"></a>        <span class="cf">break</span></span>
<span id="cb64-1130"><a href="#cb64-1130"></a>    </span>
<span id="cb64-1131"><a href="#cb64-1131"></a>    <span class="bu">print</span>(<span class="st">''</span>.join(itos[i] <span class="cf">for</span> i <span class="kw">in</span> out)) <span class="co"># decode and print the generated word</span></span>
<span id="cb64-1132"><a href="#cb64-1132"></a><span class="in">```</span></span>
<span id="cb64-1133"><a href="#cb64-1133"></a></span>
<span id="cb64-1134"><a href="#cb64-1134"></a>Happy learning!</span>
<span id="cb64-1135"><a href="#cb64-1135"></a></span>
<span id="cb64-1136"><a href="#cb64-1136"></a><span class="fu">## Exercises:</span></span>
<span id="cb64-1137"><a href="#cb64-1137"></a></span>
<span id="cb64-1138"><a href="#cb64-1138"></a><span class="ss">-   </span>E01: I did not get around to seeing what happens when you initialize all weights and biases to zero. Try this and train the neural net. You might think either that 1) the network trains just fine or 2) the network doesn't train at all, but actually it is 3) the network trains but only partially, and achieves a pretty bad final performance. Inspect the gradients and activations to figure out what is happening and why the network is only partially training, and what part is being trained exactly.</span>
<span id="cb64-1139"><a href="#cb64-1139"></a><span class="ss">-   </span>E02: BatchNorm, unlike other normalization layers like LayerNorm/GroupNorm etc. has the big advantage that after training, the batchnorm gamma/beta can be "folded into" the weights of the preceeding Linear layers, effectively erasing the need to forward it at test time. Set up a small 3-layer MLP with batchnorms, train the network, then "fold" the batchnorm gamma/beta into the preceeding Linear layer's W,b by creating a new W2, b2 and erasing the batch norm. Verify that this gives the same forward pass during inference. i.e. we see that the batchnorm is there just for stabilizing the training, and can be thrown out after training is done! pretty cool.</span>
<span id="cb64-1140"><a href="#cb64-1140"></a></span>
<span id="cb64-1141"><a href="#cb64-1141"></a><span class="fu"># resources:</span></span>
<span id="cb64-1142"><a href="#cb64-1142"></a></span>
<span id="cb64-1143"><a href="#cb64-1143"></a><span class="ss">1.  </span>other people learn from AK like me: <span class="ot">&lt;https://bedirtapkan.com/posts/blog_posts/karpathy_3_makemore_activations/&gt;</span>; <span class="ot">&lt;https://skeptric.com/index.html#category=makemore&gt;</span> - a replicate (?) with more OOPs on another dataset;</span>
<span id="cb64-1144"><a href="#cb64-1144"></a><span class="ss">2.  </span>some good papers recommended by Andrej:</span>
<span id="cb64-1145"><a href="#cb64-1145"></a><span class="ss">    -   </span>"Kaiming init" paper: <span class="ot">&lt;https://arxiv.org/abs/1502.01852&gt;</span>;</span>
<span id="cb64-1146"><a href="#cb64-1146"></a><span class="ss">    -   </span>BatchNorm paper: <span class="ot">&lt;https://arxiv.org/abs/1502.03167&gt;</span>;</span>
<span id="cb64-1147"><a href="#cb64-1147"></a><span class="ss">    -   </span>Bengio et al. 2003 MLP language model paper (pdf): <span class="ot">&lt;https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&gt;</span>;</span>
<span id="cb64-1148"><a href="#cb64-1148"></a><span class="ss">    -   </span>Good paper illustrating some of the problems with batchnorm in practice: <span class="ot">&lt;https://arxiv.org/abs/2105.07576&gt;</span>.</span>
<span id="cb64-1149"><a href="#cb64-1149"></a><span class="ss">3.  </span>Notebook: <span class="ot">&lt;https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part3_bn.ipynb&gt;</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block"><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"><i class="fa-brands fa-creative-commons" aria-label="creative-commons"></i></a> 2023-2024 Le Khac Tuan</span></p>
</div>   
    <div class="nav-footer-center">
<p><span class="faux-block">Designed with <i class="fa-solid fa-heart" aria-label="heart"></i></span></p>
</div>
    <div class="nav-footer-right">
<p><span class="faux-block">Made with <a href="https://quarto.org/">Quarto</a></span></p>
</div>
  </div>
</footer>




</body></html>