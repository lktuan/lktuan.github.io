<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tuan Le Khac">
<meta name="dcterms.date" content="2024-12-09">
<meta name="description" content="This is Tuan’s blog">

<title>Le Khac Tuan - NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../img/rocket_1613268.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Le Khac Tuan</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../curriculum/index.html"> 
<span class="menu-text">Curriculum</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../学汉语的日记.html"> 
<span class="menu-text">学汉语的日记</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../jiu_jitsu_journal/index.html"> 
<span class="menu-text">Jiu Jitsu Journal</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lktuan"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/tuanlekhac/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.facebook.com/toilatuan.lk/"> <i class="bi bi-facebook" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/Halle4231"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:tuan.lekhac0905@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">NN-Z2H Lesson 6: Building makemore part 5 - Building a <code>WaveNet</code></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                  <div>
        <div class="description">
          CNN/<code>WaveNet</code> and a lot of reading of documentation, keeping track of multidimensional tensor shapes, moving between jupyter notebooks and repository code, …
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">til</div>
                <div class="quarto-category">python</div>
                <div class="quarto-category">andrej karpathy</div>
                <div class="quarto-category">nn-z2h</div>
                <div class="quarto-category">neural networks</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://lktuan.github.io/">Tuan Le Khac</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 9, 2024</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">December 9, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#intro" id="toc-intro" class="nav-link active" data-scroll-target="#intro">1 intro</a>
  <ul class="collapse">
  <li><a href="#starter-code-walkthrough" id="toc-starter-code-walkthrough" class="nav-link" data-scroll-target="#starter-code-walkthrough">starter code walkthrough</a></li>
  <li><a href="#lets-fix-the-learning-rate-plot" id="toc-lets-fix-the-learning-rate-plot" class="nav-link" data-scroll-target="#lets-fix-the-learning-rate-plot">let’s fix the learning rate plot</a></li>
  <li><a href="#pytorchifying-our-code-layers-containers-torch.nn-fun-bugs" id="toc-pytorchifying-our-code-layers-containers-torch.nn-fun-bugs" class="nav-link" data-scroll-target="#pytorchifying-our-code-layers-containers-torch.nn-fun-bugs">pytorchifying our code: layers, containers, <code>torch.nn</code>, fun bugs</a></li>
  </ul></li>
  <li><a href="#implementing-wavenet" id="toc-implementing-wavenet" class="nav-link" data-scroll-target="#implementing-wavenet">2 implementing <code>WaveNet</code></a>
  <ul class="collapse">
  <li><a href="#overview-wavenet" id="toc-overview-wavenet" class="nav-link" data-scroll-target="#overview-wavenet">overview: <code>WaveNet</code></a></li>
  <li><a href="#dataset-bump-the-context-size-to-8" id="toc-dataset-bump-the-context-size-to-8" class="nav-link" data-scroll-target="#dataset-bump-the-context-size-to-8">dataset bump the context size to 8</a></li>
  <li><a href="#re-running-baseline-code-on-block_size-8" id="toc-re-running-baseline-code-on-block_size-8" class="nav-link" data-scroll-target="#re-running-baseline-code-on-block_size-8">re-running baseline code on block_size 8</a></li>
  <li><a href="#implementing-wavenet-1" id="toc-implementing-wavenet-1" class="nav-link" data-scroll-target="#implementing-wavenet-1">implementing <code>WaveNet</code></a></li>
  <li><a href="#training-the-wavenet-first-pass" id="toc-training-the-wavenet-first-pass" class="nav-link" data-scroll-target="#training-the-wavenet-first-pass">training the <code>WaveNet</code>: first pass</a></li>
  <li><a href="#fixing-batchnorm1d-bug" id="toc-fixing-batchnorm1d-bug" class="nav-link" data-scroll-target="#fixing-batchnorm1d-bug">fixing <code>batchnorm1d</code> bug</a></li>
  <li><a href="#re-training-wavenet-with-bug-fix" id="toc-re-training-wavenet-with-bug-fix" class="nav-link" data-scroll-target="#re-training-wavenet-with-bug-fix">re-training <code>WaveNet</code> with bug fix</a></li>
  <li><a href="#scaling-up-our-wavenet" id="toc-scaling-up-our-wavenet" class="nav-link" data-scroll-target="#scaling-up-our-wavenet">scaling up our <code>WaveNet</code></a></li>
  </ul></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions">3 conclusions</a>
  <ul class="collapse">
  <li><a href="#performance-log" id="toc-performance-log" class="nav-link" data-scroll-target="#performance-log">performance log</a></li>
  <li><a href="#experimental-harness" id="toc-experimental-harness" class="nav-link" data-scroll-target="#experimental-harness">experimental harness</a></li>
  <li><a href="#wavenet-but-with-dilated-causal-convolutions" id="toc-wavenet-but-with-dilated-causal-convolutions" class="nav-link" data-scroll-target="#wavenet-but-with-dilated-causal-convolutions"><code>WaveNet</code> but with “dilated causal convolutions”</a></li>
  <li><a href="#torch.nn" id="toc-torch.nn" class="nav-link" data-scroll-target="#torch.nn"><code>torch.nn</code></a></li>
  <li><a href="#the-development-process-of-building-deep-neural-nets" id="toc-the-development-process-of-building-deep-neural-nets" class="nav-link" data-scroll-target="#the-development-process-of-building-deep-neural-nets">the development process of building deep neural nets</a></li>
  <li><a href="#going-forward" id="toc-going-forward" class="nav-link" data-scroll-target="#going-forward">going forward</a></li>
  <li><a href="#improve-on-my-loss-how-far-can-we-improve-a-wavenet-on-this-data" id="toc-improve-on-my-loss-how-far-can-we-improve-a-wavenet-on-this-data" class="nav-link" data-scroll-target="#improve-on-my-loss-how-far-can-we-improve-a-wavenet-on-this-data">improve on my loss! how far can we improve a <code>WaveNet</code> on this data?</a></li>
  </ul></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">4 resources</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="callout callout-style-default callout-important callout-titled" title="This is not orginal content!">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This is not orginal content!
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is my study notes / codes along with Andrej Karpathy’s “<a href="https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Neural Networks: Zero to Hero</a>” series.</p>
</div>
</div>
<p><em>Codes are executed in Colab, this calculation capacity exceeds my computer’s ability.</em></p>
<section id="intro" class="level1">
<h1>1 intro</h1>
<p>We are going to take the 2-layer MLP in the part 3 of <code>makemore</code> and complexify it by:</p>
<ul>
<li>extending the block size: from 3 to 8 characters;</li>
<li>making it deeper rather than 1 hidden layer.</li>
</ul>
<p>then end of with a Convoluntional Neural Network architecture similar to <code>WaveNet</code> (2016) by <a href="https://deepmind.google/">Google DeepMind</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="wavenet.png" class="img-fluid figure-img"></p>
<figcaption>WaveNet model architecture, <a href="https://www.researchgate.net/figure/WaveNet-Model-Architecture-38_fig2_380566531">source</a></figcaption>
</figure>
</div>
<section id="starter-code-walkthrough" class="level2">
<h2 class="anchored" data-anchor-id="starter-code-walkthrough">starter code walkthrough</h2>
<section id="import-libraries" class="level4">
<h4 class="anchored" data-anchor-id="import-libraries">import libraries</h4>
<div id="9b668953" class="cell" data-execution_count="1">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="reading-data" class="level4">
<h4 class="anchored" data-anchor-id="reading-data">reading data</h4>
<div id="becfddf7" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2"></a></span>
<span id="cb2-3"><a href="#cb2-3"></a>url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt"</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>words <span class="op">=</span> pd.read_csv(url, header<span class="op">=</span><span class="va">None</span>).iloc[:, <span class="dv">0</span>].tolist()</span>
<span id="cb2-5"><a href="#cb2-5"></a>words[:<span class="dv">8</span>]</span>
<span id="cb2-6"><a href="#cb2-6"></a></span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="co"># &gt;&gt;&gt; ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="building-vocab" class="level4">
<h4 class="anchored" data-anchor-id="building-vocab">building vocab</h4>
<div id="c433c83f" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># build the vocabulary of characters and mapping to/from integer</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(<span class="st">''</span>.join(words))))</span>
<span id="cb3-3"><a href="#cb3-3"></a>stoi <span class="op">=</span> {s:i<span class="op">+</span><span class="dv">1</span> <span class="cf">for</span> i, s <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb3-4"><a href="#cb3-4"></a>stoi[<span class="st">'.'</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-5"><a href="#cb3-5"></a>itos <span class="op">=</span> {i: s <span class="cf">for</span> s, i <span class="kw">in</span> stoi.items()}</span>
<span id="cb3-6"><a href="#cb3-6"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(itos)</span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="bu">print</span>(itos)</span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="bu">print</span>(vocab_size)</span>
<span id="cb3-9"><a href="#cb3-9"></a></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="co"># itos: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', </span></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="co"># 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', </span></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="co"># 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}</span></span>
<span id="cb3-13"><a href="#cb3-13"></a><span class="co"># vocab_size: 27</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="initializing-randomnization" class="level4">
<h4 class="anchored" data-anchor-id="initializing-randomnization">initializing randomnization</h4>
<div id="c620596e" class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">import</span> random</span>
<span id="cb4-2"><a href="#cb4-2"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb4-3"><a href="#cb4-3"></a>random.shuffle(words)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="create-traindevtest-splits" class="level4">
<h4 class="anchored" data-anchor-id="create-traindevtest-splits">create train/dev/test splits</h4>
<div id="09d1fed6" class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>block_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># context length: how many characters do we take to predict the next one?</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="co"># build the dataset</span></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="kw">def</span> buid_dataset(words):</span>
<span id="cb5-4"><a href="#cb5-4"></a>    X, Y <span class="op">=</span> [], []</span>
<span id="cb5-5"><a href="#cb5-5"></a></span>
<span id="cb5-6"><a href="#cb5-6"></a>    <span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb5-7"><a href="#cb5-7"></a>        context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size</span>
<span id="cb5-8"><a href="#cb5-8"></a>        <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb5-9"><a href="#cb5-9"></a>            ix <span class="op">=</span> stoi[ch]</span>
<span id="cb5-10"><a href="#cb5-10"></a>            X.append(context)</span>
<span id="cb5-11"><a href="#cb5-11"></a>            Y.append(ix)</span>
<span id="cb5-12"><a href="#cb5-12"></a>            context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix]</span>
<span id="cb5-13"><a href="#cb5-13"></a></span>
<span id="cb5-14"><a href="#cb5-14"></a>    X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb5-15"><a href="#cb5-15"></a>    Y <span class="op">=</span> torch.tensor(Y)</span>
<span id="cb5-16"><a href="#cb5-16"></a>    <span class="bu">print</span>(X.shape, Y.shape)</span>
<span id="cb5-17"><a href="#cb5-17"></a>    <span class="cf">return</span> X, Y</span>
<span id="cb5-18"><a href="#cb5-18"></a></span>
<span id="cb5-19"><a href="#cb5-19"></a>n1 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb5-20"><a href="#cb5-20"></a>n2 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb5-21"><a href="#cb5-21"></a></span>
<span id="cb5-22"><a href="#cb5-22"></a>Xtr, Ytr <span class="op">=</span> buid_dataset(words[:n1])        <span class="co"># 80#</span></span>
<span id="cb5-23"><a href="#cb5-23"></a>Xdev, Ydev <span class="op">=</span> buid_dataset(words[n1:n2])    <span class="co"># 10%</span></span>
<span id="cb5-24"><a href="#cb5-24"></a>Xte, Yte <span class="op">=</span> buid_dataset(words[n2:])        <span class="co"># 10%</span></span>
<span id="cb5-25"><a href="#cb5-25"></a></span>
<span id="cb5-26"><a href="#cb5-26"></a><span class="co"># torch.Size([182625, 3]) torch.Size([182625])</span></span>
<span id="cb5-27"><a href="#cb5-27"></a><span class="co"># torch.Size([22655, 3]) torch.Size([22655])</span></span>
<span id="cb5-28"><a href="#cb5-28"></a><span class="co"># torch.Size([22866, 3]) torch.Size([22866])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="input-and-response-preview" class="level4">
<h4 class="anchored" data-anchor-id="input-and-response-preview">input and response preview</h4>
<div id="b3d555ef" class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(Xtr[:<span class="dv">20</span>], Ytr[:<span class="dv">20</span>]):</span>
<span id="cb6-2"><a href="#cb6-2"></a>  <span class="bu">print</span>(<span class="st">''</span>.join(itos[ix.item()] <span class="cf">for</span> ix <span class="kw">in</span> x), <span class="st">'---&gt;'</span>, itos[y.item()])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource md number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1"></a>... ---&gt; y</span>
<span id="cb7-2"><a href="#cb7-2"></a>..y ---&gt; u</span>
<span id="cb7-3"><a href="#cb7-3"></a>.yu ---&gt; h</span>
<span id="cb7-4"><a href="#cb7-4"></a>yuh ---&gt; e</span>
<span id="cb7-5"><a href="#cb7-5"></a>uhe ---&gt; n</span>
<span id="cb7-6"><a href="#cb7-6"></a>hen ---&gt; g</span>
<span id="cb7-7"><a href="#cb7-7"></a>eng ---&gt; .</span>
<span id="cb7-8"><a href="#cb7-8"></a>... ---&gt; d</span>
<span id="cb7-9"><a href="#cb7-9"></a>..d ---&gt; i</span>
<span id="cb7-10"><a href="#cb7-10"></a>.di ---&gt; o</span>
<span id="cb7-11"><a href="#cb7-11"></a>dio ---&gt; n</span>
<span id="cb7-12"><a href="#cb7-12"></a>ion ---&gt; d</span>
<span id="cb7-13"><a href="#cb7-13"></a>ond ---&gt; r</span>
<span id="cb7-14"><a href="#cb7-14"></a>ndr ---&gt; e</span>
<span id="cb7-15"><a href="#cb7-15"></a>dre ---&gt; .</span>
<span id="cb7-16"><a href="#cb7-16"></a>... ---&gt; x</span>
<span id="cb7-17"><a href="#cb7-17"></a>..x ---&gt; a</span>
<span id="cb7-18"><a href="#cb7-18"></a>.xa ---&gt; v</span>
<span id="cb7-19"><a href="#cb7-19"></a>xav ---&gt; i</span>
<span id="cb7-20"><a href="#cb7-20"></a>avi ---&gt; e</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="initializing-objects-in-networks" class="level4">
<h4 class="anchored" data-anchor-id="initializing-objects-in-networks">initializing objects in networks</h4>
<p>Near copy paste of the layers we have developed in Part 3, I added some docstring to the classes.</p>
<section id="class-linear" class="level5">
<h5 class="anchored" data-anchor-id="class-linear">class <code>Linear</code></h5>
<div id="503d3238" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="kw">class</span> Linear:</span>
<span id="cb8-2"><a href="#cb8-2"></a>  <span class="co">"""    </span></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="co">  Applies an affine linear transformation to the incoming data: y = xA^T + b.</span></span>
<span id="cb8-4"><a href="#cb8-4"></a></span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="co">  This class implements a linear (fully connected) layer, which performs a linear </span></span>
<span id="cb8-6"><a href="#cb8-6"></a><span class="co">  transformation on the input tensor. It is typically used in neural network architectures </span></span>
<span id="cb8-7"><a href="#cb8-7"></a><span class="co">  to transform input features between layers.</span></span>
<span id="cb8-8"><a href="#cb8-8"></a></span>
<span id="cb8-9"><a href="#cb8-9"></a><span class="co">  Args:</span></span>
<span id="cb8-10"><a href="#cb8-10"></a><span class="co">      fan_in (int): Number of input features (input dimension).</span></span>
<span id="cb8-11"><a href="#cb8-11"></a><span class="co">      fan_out (int): Number of output features (output dimension).</span></span>
<span id="cb8-12"><a href="#cb8-12"></a><span class="co">      bias (bool, optional): Whether to include a learnable bias term. </span></span>
<span id="cb8-13"><a href="#cb8-13"></a><span class="co">          Defaults to True.</span></span>
<span id="cb8-14"><a href="#cb8-14"></a></span>
<span id="cb8-15"><a href="#cb8-15"></a><span class="co">  Attributes:</span></span>
<span id="cb8-16"><a href="#cb8-16"></a><span class="co">      weight (torch.Tensor): Weight matrix of shape (fan_in, fan_out), </span></span>
<span id="cb8-17"><a href="#cb8-17"></a><span class="co">          initialized using Kaiming initialization.</span></span>
<span id="cb8-18"><a href="#cb8-18"></a><span class="co">      bias (torch.Tensor or None): Bias vector of shape (fan_out), </span></span>
<span id="cb8-19"><a href="#cb8-19"></a><span class="co">          initialized to zeros if bias is True, otherwise None.</span></span>
<span id="cb8-20"><a href="#cb8-20"></a></span>
<span id="cb8-21"><a href="#cb8-21"></a><span class="co">  Methods:</span></span>
<span id="cb8-22"><a href="#cb8-22"></a><span class="co">      __call__(x): Applies the linear transformation to the input tensor x.</span></span>
<span id="cb8-23"><a href="#cb8-23"></a><span class="co">      parameters(): Returns a list of trainable parameters (weight and bias).</span></span>
<span id="cb8-24"><a href="#cb8-24"></a></span>
<span id="cb8-25"><a href="#cb8-25"></a><span class="co">  Example:</span></span>
<span id="cb8-26"><a href="#cb8-26"></a><span class="co">      &gt;&gt;&gt; layer = Linear(10, 5)  # Creates a linear layer with 10 input features and 5 output features</span></span>
<span id="cb8-27"><a href="#cb8-27"></a><span class="co">      &gt;&gt;&gt; x = torch.randn(3, 10)  # Input tensor with batch size 3 and 10 features</span></span>
<span id="cb8-28"><a href="#cb8-28"></a><span class="co">      &gt;&gt;&gt; output = layer(x)  # Applies linear transformation</span></span>
<span id="cb8-29"><a href="#cb8-29"></a><span class="co">      &gt;&gt;&gt; output.shape</span></span>
<span id="cb8-30"><a href="#cb8-30"></a><span class="co">      torch.Size([3, 5])</span></span>
<span id="cb8-31"><a href="#cb8-31"></a><span class="co">  """</span></span>
<span id="cb8-32"><a href="#cb8-32"></a></span>
<span id="cb8-33"><a href="#cb8-33"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, fan_in, fan_out, bias<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb8-34"><a href="#cb8-34"></a>    <span class="va">self</span>.weight <span class="op">=</span> torch.randn((fan_in, fan_out)) <span class="op">/</span> fan_in<span class="op">**</span><span class="fl">0.5</span> <span class="co"># note: kaiming init</span></span>
<span id="cb8-35"><a href="#cb8-35"></a>    <span class="va">self</span>.bias <span class="op">=</span> torch.zeros(fan_out) <span class="cf">if</span> bias <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb8-36"><a href="#cb8-36"></a>  </span>
<span id="cb8-37"><a href="#cb8-37"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb8-38"><a href="#cb8-38"></a>    <span class="va">self</span>.out <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.weight</span>
<span id="cb8-39"><a href="#cb8-39"></a>    <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb8-40"><a href="#cb8-40"></a>      <span class="va">self</span>.out <span class="op">+=</span> <span class="va">self</span>.bias</span>
<span id="cb8-41"><a href="#cb8-41"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb8-42"><a href="#cb8-42"></a>  </span>
<span id="cb8-43"><a href="#cb8-43"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb8-44"><a href="#cb8-44"></a>    <span class="cf">return</span> [<span class="va">self</span>.weight] <span class="op">+</span> ([] <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> [<span class="va">self</span>.bias])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="class-batchnorm1d" class="level5">
<h5 class="anchored" data-anchor-id="class-batchnorm1d">class <code>BatchNorm1d</code></h5>
<div id="7edb675b" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="kw">class</span> BatchNorm1d:</span>
<span id="cb9-2"><a href="#cb9-2"></a>  <span class="co">"""</span></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="co">  Applies Batch Normalization to the input tensor, a technique to improve </span></span>
<span id="cb9-4"><a href="#cb9-4"></a><span class="co">  training stability and performance in deep neural networks.</span></span>
<span id="cb9-5"><a href="#cb9-5"></a></span>
<span id="cb9-6"><a href="#cb9-6"></a><span class="co">  Batch Normalization normalizes the input across the batch dimension, </span></span>
<span id="cb9-7"><a href="#cb9-7"></a><span class="co">  reducing internal covariate shift and allowing higher learning rates. </span></span>
<span id="cb9-8"><a href="#cb9-8"></a><span class="co">  This implementation supports both training and inference modes.</span></span>
<span id="cb9-9"><a href="#cb9-9"></a></span>
<span id="cb9-10"><a href="#cb9-10"></a><span class="co">  Args:</span></span>
<span id="cb9-11"><a href="#cb9-11"></a><span class="co">      dim (int): Number of features or channels to be normalized.</span></span>
<span id="cb9-12"><a href="#cb9-12"></a><span class="co">      eps (float, optional): A small constant added to the denominator for </span></span>
<span id="cb9-13"><a href="#cb9-13"></a><span class="co">          numerical stability to prevent division by zero. </span></span>
<span id="cb9-14"><a href="#cb9-14"></a><span class="co">          Defaults to 1e-5.</span></span>
<span id="cb9-15"><a href="#cb9-15"></a><span class="co">      momentum (float, optional): Momentum for updating running mean and </span></span>
<span id="cb9-16"><a href="#cb9-16"></a><span class="co">          variance during training. Controls the degree of exponential </span></span>
<span id="cb9-17"><a href="#cb9-17"></a><span class="co">          moving average. Defaults to 0.1.</span></span>
<span id="cb9-18"><a href="#cb9-18"></a></span>
<span id="cb9-19"><a href="#cb9-19"></a><span class="co">  Attributes:</span></span>
<span id="cb9-20"><a href="#cb9-20"></a><span class="co">      eps (float): Epsilon value for numerical stability.</span></span>
<span id="cb9-21"><a href="#cb9-21"></a><span class="co">      momentum (float): Momentum for running statistics update.</span></span>
<span id="cb9-22"><a href="#cb9-22"></a><span class="co">      training (bool): Indicates whether the layer is in training or inference mode.</span></span>
<span id="cb9-23"><a href="#cb9-23"></a><span class="co">      gamma (torch.Tensor): Learnable scale parameter of shape (dim,).</span></span>
<span id="cb9-24"><a href="#cb9-24"></a><span class="co">      beta (torch.Tensor): Learnable shift parameter of shape (dim,).</span></span>
<span id="cb9-25"><a href="#cb9-25"></a><span class="co">      running_mean (torch.Tensor): Exponential moving average of batch means.</span></span>
<span id="cb9-26"><a href="#cb9-26"></a><span class="co">      running_var (torch.Tensor): Exponential moving average of batch variances.</span></span>
<span id="cb9-27"><a href="#cb9-27"></a></span>
<span id="cb9-28"><a href="#cb9-28"></a><span class="co">  Methods:</span></span>
<span id="cb9-29"><a href="#cb9-29"></a><span class="co">      __call__(x): Applies batch normalization to the input tensor.</span></span>
<span id="cb9-30"><a href="#cb9-30"></a><span class="co">      parameters(): Returns learnable parameters (gamma and beta).</span></span>
<span id="cb9-31"><a href="#cb9-31"></a></span>
<span id="cb9-32"><a href="#cb9-32"></a><span class="co">  Key Normalization Steps:</span></span>
<span id="cb9-33"><a href="#cb9-33"></a><span class="co">  1. Compute batch mean and variance (in training mode)</span></span>
<span id="cb9-34"><a href="#cb9-34"></a><span class="co">  2. Normalize input by subtracting mean and dividing by standard deviation</span></span>
<span id="cb9-35"><a href="#cb9-35"></a><span class="co">  3. Apply learnable scale (gamma) and shift (beta) parameters</span></span>
<span id="cb9-36"><a href="#cb9-36"></a><span class="co">  4. Update running statistics during training</span></span>
<span id="cb9-37"><a href="#cb9-37"></a></span>
<span id="cb9-38"><a href="#cb9-38"></a><span class="co">  Example:</span></span>
<span id="cb9-39"><a href="#cb9-39"></a><span class="co">      &gt;&gt;&gt; batch_norm = BatchNorm1d(64)  # For 64-channel input</span></span>
<span id="cb9-40"><a href="#cb9-40"></a><span class="co">      &gt;&gt;&gt; x = torch.randn(32, 64)  # Batch of 32 samples with 64 features</span></span>
<span id="cb9-41"><a href="#cb9-41"></a><span class="co">      &gt;&gt;&gt; normalized_x = batch_norm(x)  # Apply batch normalization</span></span>
<span id="cb9-42"><a href="#cb9-42"></a><span class="co">      &gt;&gt;&gt; normalized_x.shape</span></span>
<span id="cb9-43"><a href="#cb9-43"></a><span class="co">      torch.Size([32, 64])</span></span>
<span id="cb9-44"><a href="#cb9-44"></a></span>
<span id="cb9-45"><a href="#cb9-45"></a><span class="co">  Note:</span></span>
<span id="cb9-46"><a href="#cb9-46"></a><span class="co">      - Supports both 2D (batch, features) and 3D (batch, channels, sequence) input tensors</span></span>
<span id="cb9-47"><a href="#cb9-47"></a><span class="co">      - During inference, uses running statistics instead of batch statistics</span></span>
<span id="cb9-48"><a href="#cb9-48"></a><span class="co">  """</span></span>
<span id="cb9-49"><a href="#cb9-49"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, eps<span class="op">=</span><span class="fl">1e-5</span>, momentum<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb9-50"><a href="#cb9-50"></a>    <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb9-51"><a href="#cb9-51"></a>    <span class="va">self</span>.momentum <span class="op">=</span> momentum</span>
<span id="cb9-52"><a href="#cb9-52"></a>    <span class="va">self</span>.training <span class="op">=</span> <span class="va">True</span></span>
<span id="cb9-53"><a href="#cb9-53"></a>    <span class="co"># parameters (trained with backprop)</span></span>
<span id="cb9-54"><a href="#cb9-54"></a>    <span class="va">self</span>.gamma <span class="op">=</span> torch.ones(dim)</span>
<span id="cb9-55"><a href="#cb9-55"></a>    <span class="va">self</span>.beta <span class="op">=</span> torch.zeros(dim)</span>
<span id="cb9-56"><a href="#cb9-56"></a>    <span class="co"># buffers (trained with a running 'momentum update')</span></span>
<span id="cb9-57"><a href="#cb9-57"></a>    <span class="va">self</span>.running_mean <span class="op">=</span> torch.zeros(dim)</span>
<span id="cb9-58"><a href="#cb9-58"></a>    <span class="va">self</span>.running_var <span class="op">=</span> torch.ones(dim)</span>
<span id="cb9-59"><a href="#cb9-59"></a>  </span>
<span id="cb9-60"><a href="#cb9-60"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb9-61"><a href="#cb9-61"></a>    <span class="co"># calculate the forward pass</span></span>
<span id="cb9-62"><a href="#cb9-62"></a>    <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb9-63"><a href="#cb9-63"></a>      xmean <span class="op">=</span> x.mean(dim, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch mean</span></span>
<span id="cb9-64"><a href="#cb9-64"></a>      xvar <span class="op">=</span> x.var(dim, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch variance</span></span>
<span id="cb9-65"><a href="#cb9-65"></a>    <span class="cf">else</span>:</span>
<span id="cb9-66"><a href="#cb9-66"></a>      xmean <span class="op">=</span> <span class="va">self</span>.running_mean</span>
<span id="cb9-67"><a href="#cb9-67"></a>      xvar <span class="op">=</span> <span class="va">self</span>.running_var</span>
<span id="cb9-68"><a href="#cb9-68"></a>    xhat <span class="op">=</span> (x <span class="op">-</span> xmean) <span class="op">/</span> torch.sqrt(xvar <span class="op">+</span> <span class="va">self</span>.eps) <span class="co"># normalize to unit variance</span></span>
<span id="cb9-69"><a href="#cb9-69"></a>    <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.gamma <span class="op">*</span> xhat <span class="op">+</span> <span class="va">self</span>.beta</span>
<span id="cb9-70"><a href="#cb9-70"></a>    <span class="co"># update the buffers</span></span>
<span id="cb9-71"><a href="#cb9-71"></a>    <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb9-72"><a href="#cb9-72"></a>      <span class="cf">with</span> torch.no_grad():</span>
<span id="cb9-73"><a href="#cb9-73"></a>        <span class="va">self</span>.running_mean <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> <span class="va">self</span>.running_mean <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> xmean</span>
<span id="cb9-74"><a href="#cb9-74"></a>        <span class="va">self</span>.running_var <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> <span class="va">self</span>.running_var <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> xvar</span>
<span id="cb9-75"><a href="#cb9-75"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb9-76"><a href="#cb9-76"></a>  </span>
<span id="cb9-77"><a href="#cb9-77"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb9-78"><a href="#cb9-78"></a>    <span class="cf">return</span> [<span class="va">self</span>.gamma, <span class="va">self</span>.beta]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="class-tanh" class="level4">
<h4 class="anchored" data-anchor-id="class-tanh">class <code>Tanh</code></h4>
<div id="9a54d253" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="kw">class</span> Tanh:</span>
<span id="cb10-2"><a href="#cb10-2"></a>    <span class="co">"""</span></span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="co">    Hyperbolic Tangent (Tanh) Activation Function</span></span>
<span id="cb10-4"><a href="#cb10-4"></a></span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="co">    Applies the hyperbolic tangent activation function element-wise to the input tensor. </span></span>
<span id="cb10-6"><a href="#cb10-6"></a><span class="co">    Tanh maps input values to the range [-1, 1], providing a symmetric and non-linear </span></span>
<span id="cb10-7"><a href="#cb10-7"></a><span class="co">    transformation that helps neural networks learn complex patterns.</span></span>
<span id="cb10-8"><a href="#cb10-8"></a></span>
<span id="cb10-9"><a href="#cb10-9"></a><span class="co">    Mathematical Definition:</span></span>
<span id="cb10-10"><a href="#cb10-10"></a><span class="co">    tanh(x) = (e^x - e^-x) / (e^x + e^-x)</span></span>
<span id="cb10-11"><a href="#cb10-11"></a><span class="co">    </span></span>
<span id="cb10-12"><a href="#cb10-12"></a><span class="co">    Key Characteristics:</span></span>
<span id="cb10-13"><a href="#cb10-13"></a><span class="co">    - Output Range: [-1, 1]</span></span>
<span id="cb10-14"><a href="#cb10-14"></a><span class="co">    - Symmetric around the origin</span></span>
<span id="cb10-15"><a href="#cb10-15"></a><span class="co">    - Gradient is always less than 1, which helps mitigate the vanishing gradient problem</span></span>
<span id="cb10-16"><a href="#cb10-16"></a><span class="co">    - Commonly used in recurrent neural networks and hidden layers</span></span>
<span id="cb10-17"><a href="#cb10-17"></a></span>
<span id="cb10-18"><a href="#cb10-18"></a><span class="co">    Methods:</span></span>
<span id="cb10-19"><a href="#cb10-19"></a><span class="co">        __call__(x): Applies the Tanh activation to the input tensor.</span></span>
<span id="cb10-20"><a href="#cb10-20"></a><span class="co">        parameters(): Returns an empty list, as Tanh has no learnable parameters.</span></span>
<span id="cb10-21"><a href="#cb10-21"></a></span>
<span id="cb10-22"><a href="#cb10-22"></a><span class="co">    Attributes:</span></span>
<span id="cb10-23"><a href="#cb10-23"></a><span class="co">        out (torch.Tensor): Stores the output of the most recent forward pass.</span></span>
<span id="cb10-24"><a href="#cb10-24"></a></span>
<span id="cb10-25"><a href="#cb10-25"></a><span class="co">    Example:</span></span>
<span id="cb10-26"><a href="#cb10-26"></a><span class="co">        &gt;&gt;&gt; activation = Tanh()</span></span>
<span id="cb10-27"><a href="#cb10-27"></a><span class="co">        &gt;&gt;&gt; x = torch.tensor([-2.0, 0.0, 2.0])</span></span>
<span id="cb10-28"><a href="#cb10-28"></a><span class="co">        &gt;&gt;&gt; y = activation(x)</span></span>
<span id="cb10-29"><a href="#cb10-29"></a><span class="co">        &gt;&gt;&gt; y</span></span>
<span id="cb10-30"><a href="#cb10-30"></a><span class="co">        tensor([-0.9640, 0.0000, 0.9640])</span></span>
<span id="cb10-31"><a href="#cb10-31"></a></span>
<span id="cb10-32"><a href="#cb10-32"></a><span class="co">    Note:</span></span>
<span id="cb10-33"><a href="#cb10-33"></a><span class="co">        This implementation is stateless and does not modify the input tensor.</span></span>
<span id="cb10-34"><a href="#cb10-34"></a><span class="co">        The activation is applied element-wise, preserving the input tensor's shape.</span></span>
<span id="cb10-35"><a href="#cb10-35"></a><span class="co">    """</span></span>
<span id="cb10-36"><a href="#cb10-36"></a></span>
<span id="cb10-37"><a href="#cb10-37"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb10-38"><a href="#cb10-38"></a>        <span class="va">self</span>.out <span class="op">=</span> torch.tanh(x)</span>
<span id="cb10-39"><a href="#cb10-39"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb10-40"><a href="#cb10-40"></a></span>
<span id="cb10-41"><a href="#cb10-41"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb10-42"><a href="#cb10-42"></a>        <span class="cf">return</span> []</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="random-number-generator" class="level5">
<h5 class="anchored" data-anchor-id="random-number-generator">random number generator</h5>
<div id="bb361336" class="cell" data-execution_count="10">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>torch.manual_seed(<span class="dv">42</span>)<span class="op">;</span> <span class="co"># seed rng for reproducibility</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="network-architecture" class="level5">
<h5 class="anchored" data-anchor-id="network-architecture">network architecture</h5>
<div id="75523e69" class="cell" data-execution_count="11">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="co"># original network</span></span>
<span id="cb12-2"><a href="#cb12-2"></a>n_embd <span class="op">=</span> <span class="dv">10</span> <span class="co"># the dimensionality of the character embedding vectors</span></span>
<span id="cb12-3"><a href="#cb12-3"></a>n_hidden <span class="op">=</span> <span class="dv">200</span> <span class="co"># the number of neurons in the hidden layer of the MLP</span></span>
<span id="cb12-4"><a href="#cb12-4"></a></span>
<span id="cb12-5"><a href="#cb12-5"></a>C <span class="op">=</span> torch.rand((vocab_size, n_embd))</span>
<span id="cb12-6"><a href="#cb12-6"></a>layers <span class="op">=</span> [</span>
<span id="cb12-7"><a href="#cb12-7"></a>  Linear(n_embd <span class="op">*</span> block_size, n_hidden, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb12-8"><a href="#cb12-8"></a>  BatchNorm1d(n_hidden),</span>
<span id="cb12-9"><a href="#cb12-9"></a>  Tanh(),</span>
<span id="cb12-10"><a href="#cb12-10"></a>  Linear(n_hidden, vocab_size),</span>
<span id="cb12-11"><a href="#cb12-11"></a>]</span>
<span id="cb12-12"><a href="#cb12-12"></a></span>
<span id="cb12-13"><a href="#cb12-13"></a><span class="co"># parameter init</span></span>
<span id="cb12-14"><a href="#cb12-14"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-15"><a href="#cb12-15"></a>  layers[<span class="op">-</span><span class="dv">1</span>].weight <span class="op">*=</span> <span class="fl">0.1</span> <span class="co"># last layer make less confident</span></span>
<span id="cb12-16"><a href="#cb12-16"></a></span>
<span id="cb12-17"><a href="#cb12-17"></a>parameters <span class="op">=</span> [C] <span class="op">+</span> [p <span class="cf">for</span> layer <span class="kw">in</span> layers <span class="cf">for</span> p <span class="kw">in</span> layer.parameters()]</span>
<span id="cb12-18"><a href="#cb12-18"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters)) <span class="co"># number of parameters in total</span></span>
<span id="cb12-19"><a href="#cb12-19"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb12-20"><a href="#cb12-20"></a>  p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb12-21"><a href="#cb12-21"></a></span>
<span id="cb12-22"><a href="#cb12-22"></a><span class="co"># model params: 12097</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="optimization" class="level4">
<h4 class="anchored" data-anchor-id="optimization">optimization</h4>
<div id="a9c40491" class="cell" data-execution_count="12">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># same optimization as last time</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>max_steps <span class="op">=</span> <span class="dv">200000</span></span>
<span id="cb13-3"><a href="#cb13-3"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb13-4"><a href="#cb13-4"></a>lossi <span class="op">=</span> []</span>
<span id="cb13-5"><a href="#cb13-5"></a></span>
<span id="cb13-6"><a href="#cb13-6"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb13-7"><a href="#cb13-7"></a>  </span>
<span id="cb13-8"><a href="#cb13-8"></a>  <span class="co"># minibatch construct</span></span>
<span id="cb13-9"><a href="#cb13-9"></a>  ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,))</span>
<span id="cb13-10"><a href="#cb13-10"></a>  Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix] <span class="co"># batch X,Y</span></span>
<span id="cb13-11"><a href="#cb13-11"></a>  </span>
<span id="cb13-12"><a href="#cb13-12"></a>  <span class="co"># forward pass</span></span>
<span id="cb13-13"><a href="#cb13-13"></a>  emb <span class="op">=</span> C[Xb] <span class="co"># embed the characters into vectors   </span></span>
<span id="cb13-14"><a href="#cb13-14"></a>  x <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate the vectors</span></span>
<span id="cb13-15"><a href="#cb13-15"></a>  <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb13-16"><a href="#cb13-16"></a>    x <span class="op">=</span> layer(x)</span>
<span id="cb13-17"><a href="#cb13-17"></a>  loss <span class="op">=</span> F.cross_entropy(x, Yb) <span class="co"># loss function</span></span>
<span id="cb13-18"><a href="#cb13-18"></a>  </span>
<span id="cb13-19"><a href="#cb13-19"></a>  <span class="co"># backward pass</span></span>
<span id="cb13-20"><a href="#cb13-20"></a>  <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb13-21"><a href="#cb13-21"></a>    p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-22"><a href="#cb13-22"></a>  loss.backward()</span>
<span id="cb13-23"><a href="#cb13-23"></a>  </span>
<span id="cb13-24"><a href="#cb13-24"></a>  <span class="co"># update: simple SGD</span></span>
<span id="cb13-25"><a href="#cb13-25"></a>  lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">150000</span> <span class="cf">else</span> <span class="fl">0.01</span> <span class="co"># step learning rate decay</span></span>
<span id="cb13-26"><a href="#cb13-26"></a>  <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb13-27"><a href="#cb13-27"></a>    p.data <span class="op">+=</span> <span class="op">-</span>lr <span class="op">*</span> p.grad</span>
<span id="cb13-28"><a href="#cb13-28"></a></span>
<span id="cb13-29"><a href="#cb13-29"></a>  <span class="co"># track stats</span></span>
<span id="cb13-30"><a href="#cb13-30"></a>  <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="co"># print every once in a while</span></span>
<span id="cb13-31"><a href="#cb13-31"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">:7d}</span><span class="ss">/</span><span class="sc">{</span>max_steps<span class="sc">:7d}</span><span class="ss">: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb13-32"><a href="#cb13-32"></a>  lossi.append(loss.log10().item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="sourceCode" id="cb14"><pre class="sourceCode numberSource md number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb14-1"><a href="#cb14-1"></a><span class="in">     0/ 200000: 3.2885</span></span>
<span id="cb14-2"><a href="#cb14-2"></a>  10000/ 200000: 2.3938</span>
<span id="cb14-3"><a href="#cb14-3"></a>  20000/ 200000: 2.1235</span>
<span id="cb14-4"><a href="#cb14-4"></a>  30000/ 200000: 1.9222</span>
<span id="cb14-5"><a href="#cb14-5"></a>  40000/ 200000: 2.2440</span>
<span id="cb14-6"><a href="#cb14-6"></a>  50000/ 200000: 2.1108</span>
<span id="cb14-7"><a href="#cb14-7"></a>  60000/ 200000: 2.0624</span>
<span id="cb14-8"><a href="#cb14-8"></a>  70000/ 200000: 2.0893</span>
<span id="cb14-9"><a href="#cb14-9"></a>  80000/ 200000: 2.4173</span>
<span id="cb14-10"><a href="#cb14-10"></a>  90000/ 200000: 1.9744</span>
<span id="cb14-11"><a href="#cb14-11"></a> 100000/ 200000: 2.0883</span>
<span id="cb14-12"><a href="#cb14-12"></a> 110000/ 200000: 2.4538</span>
<span id="cb14-13"><a href="#cb14-13"></a> 120000/ 200000: 1.9535</span>
<span id="cb14-14"><a href="#cb14-14"></a> 130000/ 200000: 1.8980</span>
<span id="cb14-15"><a href="#cb14-15"></a> 140000/ 200000: 2.1196</span>
<span id="cb14-16"><a href="#cb14-16"></a> 150000/ 200000: 2.3550</span>
<span id="cb14-17"><a href="#cb14-17"></a> 160000/ 200000: 2.2957</span>
<span id="cb14-18"><a href="#cb14-18"></a> 170000/ 200000: 2.0286</span>
<span id="cb14-19"><a href="#cb14-19"></a> 180000/ 200000: 2.2379</span>
<span id="cb14-20"><a href="#cb14-20"></a> 190000/ 200000: 2.3866</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="observe-training-processevaluation" class="level4">
<h4 class="anchored" data-anchor-id="observe-training-processevaluation">observe training process/evaluation</h4>
<div id="fe224542" class="cell" data-execution_count="13">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>plt.plot(lossi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="1_pre_lossi.png" class="img-fluid figure-img"></p>
<figcaption><code>lossi</code> plot at the beginning</figcaption>
</figure>
</div>
</section>
<section id="calibrate-the-batchnorm-after-training" class="level4">
<h4 class="anchored" data-anchor-id="calibrate-the-batchnorm-after-training">calibrate the <code>batchnorm</code> after training</h4>
<p>We should be using the running mean/variance of the whole dataset splits rather than the last mini-batch.</p>
<div id="024ebc41" class="cell" data-execution_count="14">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="co"># put layers into eval mode (needed for batchnorm especially)</span></span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="cf">for</span> layer <span class="kw">in</span> model.layers:</span>
<span id="cb16-3"><a href="#cb16-3"></a>  layer.training <span class="op">=</span> <span class="va">False</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="calculate-on-whole-training-and-validation-splits" class="level4">
<h4 class="anchored" data-anchor-id="calculate-on-whole-training-and-validation-splits">calculate on whole training and validation splits</h4>
<div id="03e8d0bc" class="cell" data-execution_count="15">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="co"># evaluate the loss</span></span>
<span id="cb17-2"><a href="#cb17-2"></a><span class="at">@torch.no_grad</span>() <span class="co"># this decorator disables gradient tracking inside pytorch</span></span>
<span id="cb17-3"><a href="#cb17-3"></a><span class="kw">def</span> split_loss(split):</span>
<span id="cb17-4"><a href="#cb17-4"></a>  x,y <span class="op">=</span> {</span>
<span id="cb17-5"><a href="#cb17-5"></a>    <span class="st">'train'</span>: (Xtr, Ytr),</span>
<span id="cb17-6"><a href="#cb17-6"></a>    <span class="st">'val'</span>: (Xdev, Ydev),</span>
<span id="cb17-7"><a href="#cb17-7"></a>    <span class="st">'test'</span>: (Xte, Yte),</span>
<span id="cb17-8"><a href="#cb17-8"></a>  }[split]</span>
<span id="cb17-9"><a href="#cb17-9"></a>  logits <span class="op">=</span> model(x)</span>
<span id="cb17-10"><a href="#cb17-10"></a>  loss <span class="op">=</span> F.cross_entropy(logits, y)</span>
<span id="cb17-11"><a href="#cb17-11"></a>  <span class="bu">print</span>(split, loss.item())</span>
<span id="cb17-12"><a href="#cb17-12"></a></span>
<span id="cb17-13"><a href="#cb17-13"></a>split_loss(<span class="st">'train'</span>)</span>
<span id="cb17-14"><a href="#cb17-14"></a>split_loss(<span class="st">'val'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Pretty loss but there are still room for improve:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode numberSource md number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb18-1"><a href="#cb18-1"></a>train 2.0467958450317383</span>
<span id="cb18-2"><a href="#cb18-2"></a>val 2.0989298820495605</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="sample-from-the-model" class="level4">
<h4 class="anchored" data-anchor-id="sample-from-the-model">sample from the model</h4>
<p>Here are Names generated by the model till now, we have relatively name-like results that do not exist in the training set.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode numberSource md number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb19-1"><a href="#cb19-1"></a>liz.</span>
<span id="cb19-2"><a href="#cb19-2"></a>layah.</span>
<span id="cb19-3"><a href="#cb19-3"></a>dan.</span>
<span id="cb19-4"><a href="#cb19-4"></a>hilon.</span>
<span id="cb19-5"><a href="#cb19-5"></a>avani.</span>
<span id="cb19-6"><a href="#cb19-6"></a>korron.</span>
<span id="cb19-7"><a href="#cb19-7"></a>aua.</span>
<span id="cb19-8"><a href="#cb19-8"></a>noon.</span>
<span id="cb19-9"><a href="#cb19-9"></a>bethalyn.</span>
<span id="cb19-10"><a href="#cb19-10"></a>thia.</span>
<span id="cb19-11"><a href="#cb19-11"></a>bote.</span>
<span id="cb19-12"><a href="#cb19-12"></a>jereanail.</span>
<span id="cb19-13"><a href="#cb19-13"></a>vitorien.</span>
<span id="cb19-14"><a href="#cb19-14"></a>zarashivonna.</span>
<span id="cb19-15"><a href="#cb19-15"></a>yakurrren.</span>
<span id="cb19-16"><a href="#cb19-16"></a>jovon.</span>
<span id="cb19-17"><a href="#cb19-17"></a>malynn.</span>
<span id="cb19-18"><a href="#cb19-18"></a>vanna.</span>
<span id="cb19-19"><a href="#cb19-19"></a>caparmana.</span>
<span id="cb19-20"><a href="#cb19-20"></a>shantymonse.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="lets-fix-the-learning-rate-plot" class="level2">
<h2 class="anchored" data-anchor-id="lets-fix-the-learning-rate-plot">let’s fix the learning rate plot</h2>
<p>The plot for <code>lossi</code> looks very crazy, it’s because the batch size of 32 is way too few so this time we got lucky, and next time we got un-lucky. And the mini-batch loss splashed too much. We should probaly fix it.</p>
<p>We pivot to a row for every 1000 observations of <code>lossi</code> and calculate the mean, we end up have 200 oversevation which is easier to see.</p>
<div id="3615eccb" class="cell" data-execution_count="16">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>plt.plot(torch.tensor(lossi).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1000</span>).mean(<span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We can also observe the learning rate decay at 150k training loops.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2_enhance_lossi.png" class="img-fluid figure-img"></p>
<figcaption><code>lossi</code> plot at the beginning</figcaption>
</figure>
</div>
</section>
<section id="pytorchifying-our-code-layers-containers-torch.nn-fun-bugs" class="level2">
<h2 class="anchored" data-anchor-id="pytorchifying-our-code-layers-containers-torch.nn-fun-bugs">pytorchifying our code: layers, containers, <code>torch.nn</code>, fun bugs</h2>
<p>Now we notice that we still have the embedding operation lying outside the pytorch-ified layers. It basically creating a lookup table <code>C</code>, embedding it with our data <code>Y</code> (or <code>Yb</code>), then stretching out to row with <code>view()</code> which is very cheap in PyTorch as no more memory creation is needed.</p>
<p>We modulize this by constructing 2 classes:</p>
<div id="827e4632" class="cell" data-execution_count="17">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="kw">class</span> Embedding:</span>
<span id="cb21-2"><a href="#cb21-2"></a>  </span>
<span id="cb21-3"><a href="#cb21-3"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_embeddings, embedding_dim):</span>
<span id="cb21-4"><a href="#cb21-4"></a>    <span class="va">self</span>.weight <span class="op">=</span> torch.randn((num_embeddings, embedding_dim))</span>
<span id="cb21-5"><a href="#cb21-5"></a>    </span>
<span id="cb21-6"><a href="#cb21-6"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, IX):</span>
<span id="cb21-7"><a href="#cb21-7"></a>    <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.weight[IX]</span>
<span id="cb21-8"><a href="#cb21-8"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb21-9"><a href="#cb21-9"></a>  </span>
<span id="cb21-10"><a href="#cb21-10"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb21-11"><a href="#cb21-11"></a>    <span class="cf">return</span> [<span class="va">self</span>.weight]</span>
<span id="cb21-12"><a href="#cb21-12"></a></span>
<span id="cb21-13"><a href="#cb21-13"></a><span class="co"># -----------------------------------------------------------------------------------------------</span></span>
<span id="cb21-14"><a href="#cb21-14"></a><span class="kw">class</span> Flatten:</span>
<span id="cb21-15"><a href="#cb21-15"></a></span>
<span id="cb21-16"><a href="#cb21-16"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb21-17"><a href="#cb21-17"></a>    <span class="va">self</span>.out <span class="op">=</span> x.view(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb21-18"><a href="#cb21-18"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb21-19"><a href="#cb21-19"></a>  </span>
<span id="cb21-20"><a href="#cb21-20"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb21-21"><a href="#cb21-21"></a>    <span class="cf">return</span> []</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now we can re-define the <code>layers</code> like this:</p>
<div id="7a328fd0" class="cell" data-execution_count="18">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>layers <span class="op">=</span> [</span>
<span id="cb22-2"><a href="#cb22-2"></a>  Embedding(vocab_size, n_embd),</span>
<span id="cb22-3"><a href="#cb22-3"></a>  Flatten(),</span>
<span id="cb22-4"><a href="#cb22-4"></a>  Linear(n_embd <span class="op">*</span> block_size, n_hidden, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb22-5"><a href="#cb22-5"></a>  BatchNorm1d(n_hidden),</span>
<span id="cb22-6"><a href="#cb22-6"></a>  Tanh(),</span>
<span id="cb22-7"><a href="#cb22-7"></a>  Linear(n_hidden, vocab_size),</span>
<span id="cb22-8"><a href="#cb22-8"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>and also remove the <code>C</code>, <code>emb</code> definition in the forward pass construction. Going futher, we will be not only pytorchifying the elements of <code>layers</code> only, but also the <code>layers</code> itself. In PyTorch, we have term <code>containers</code>, which specifying how we organize the layers in a network. And what are we doing here is constructing layers sequentially, which is equivalent to <code>Sequential</code> in the <code>containers</code>:</p>
<div id="8c3ba7a2" class="cell" data-execution_count="19">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="kw">class</span> Sequential:</span>
<span id="cb23-2"><a href="#cb23-2"></a>  </span>
<span id="cb23-3"><a href="#cb23-3"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, layers):</span>
<span id="cb23-4"><a href="#cb23-4"></a>    <span class="va">self</span>.layers <span class="op">=</span> layers</span>
<span id="cb23-5"><a href="#cb23-5"></a>  </span>
<span id="cb23-6"><a href="#cb23-6"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb23-7"><a href="#cb23-7"></a>    <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb23-8"><a href="#cb23-8"></a>      x <span class="op">=</span> layer(x)</span>
<span id="cb23-9"><a href="#cb23-9"></a>    <span class="va">self</span>.out <span class="op">=</span> x</span>
<span id="cb23-10"><a href="#cb23-10"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb23-11"><a href="#cb23-11"></a>  </span>
<span id="cb23-12"><a href="#cb23-12"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb23-13"><a href="#cb23-13"></a>    <span class="co"># get parameters of all layers and stretch them out into one list</span></span>
<span id="cb23-14"><a href="#cb23-14"></a>    <span class="cf">return</span> [p <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers <span class="cf">for</span> p <span class="kw">in</span> layer.parameters()]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>and wrapp the layers into our <code>model</code>:</p>
<div id="08c34583" class="cell" data-execution_count="20">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a>model <span class="op">=</span> Sequential([</span>
<span id="cb24-2"><a href="#cb24-2"></a>  Embedding(vocab_size, n_embd),</span>
<span id="cb24-3"><a href="#cb24-3"></a>  Flatten(),</span>
<span id="cb24-4"><a href="#cb24-4"></a>  Linear(n_embd <span class="op">*</span> block_size, n_hidden, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb24-5"><a href="#cb24-5"></a>  BatchNorm1d(n_hidden),</span>
<span id="cb24-6"><a href="#cb24-6"></a>  Tanh(),</span>
<span id="cb24-7"><a href="#cb24-7"></a>  Linear(n_hidden, vocab_size),</span>
<span id="cb24-8"><a href="#cb24-8"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="implementing-wavenet" class="level1">
<h1>2 implementing <code>WaveNet</code></h1>
<p>So far with the classical MLP following Bengio et al.&nbsp;(2003), we have a <em>embedding layer</em> followed by a <em>hidden layer</em> and end up with a <em>activation layer</em>. Although we added more layer after the embedding, we could not make a significant progress.</p>
<p>The problem is we dont have a <strong>naive way</strong> of making the model bigger in a productive way. We are still in the case that we crushing all the characters into a single all the way at the begining. And even if we make this a bigger layer and add neurons it’s still like silly to squash all that information so fast into a single step.</p>
<section id="overview-wavenet" class="level2">
<h2 class="anchored" data-anchor-id="overview-wavenet">overview: <code>WaveNet</code></h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig3_wavenet.png" class="img-fluid figure-img"></p>
<figcaption>Visualization of the <code>WaveNet</code> idea - Progressive Fusion</figcaption>
</figure>
</div>
</section>
<section id="dataset-bump-the-context-size-to-8" class="level2">
<h2 class="anchored" data-anchor-id="dataset-bump-the-context-size-to-8">dataset bump the context size to 8</h2>
<p>first we change the <code>block_size</code> into <code>8</code> and now our dataset looks like:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode numberSource md number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb25-1"><a href="#cb25-1"></a>........ ---&gt; y</span>
<span id="cb25-2"><a href="#cb25-2"></a>.......y ---&gt; u</span>
<span id="cb25-3"><a href="#cb25-3"></a>......yu ---&gt; h</span>
<span id="cb25-4"><a href="#cb25-4"></a>.....yuh ---&gt; e</span>
<span id="cb25-5"><a href="#cb25-5"></a>....yuhe ---&gt; n</span>
<span id="cb25-6"><a href="#cb25-6"></a>...yuhen ---&gt; g</span>
<span id="cb25-7"><a href="#cb25-7"></a>..yuheng ---&gt; .</span>
<span id="cb25-8"><a href="#cb25-8"></a>........ ---&gt; d</span>
<span id="cb25-9"><a href="#cb25-9"></a>.......d ---&gt; i</span>
<span id="cb25-10"><a href="#cb25-10"></a>......di ---&gt; o</span>
<span id="cb25-11"><a href="#cb25-11"></a>.....dio ---&gt; n</span>
<span id="cb25-12"><a href="#cb25-12"></a>....dion ---&gt; d</span>
<span id="cb25-13"><a href="#cb25-13"></a>...diond ---&gt; r</span>
<span id="cb25-14"><a href="#cb25-14"></a>..diondr ---&gt; e</span>
<span id="cb25-15"><a href="#cb25-15"></a>.diondre ---&gt; .</span>
<span id="cb25-16"><a href="#cb25-16"></a>........ ---&gt; x</span>
<span id="cb25-17"><a href="#cb25-17"></a>.......x ---&gt; a</span>
<span id="cb25-18"><a href="#cb25-18"></a>......xa ---&gt; v</span>
<span id="cb25-19"><a href="#cb25-19"></a>.....xav ---&gt; i</span>
<span id="cb25-20"><a href="#cb25-20"></a>....xavi ---&gt; e</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The model size now bumps to 22k.</p>
</section>
<section id="re-running-baseline-code-on-block_size-8" class="level2">
<h2 class="anchored" data-anchor-id="re-running-baseline-code-on-block_size-8">re-running baseline code on block_size 8</h2>
</section>
<section id="implementing-wavenet-1" class="level2">
<h2 class="anchored" data-anchor-id="implementing-wavenet-1">implementing <code>WaveNet</code></h2>
</section>
<section id="training-the-wavenet-first-pass" class="level2">
<h2 class="anchored" data-anchor-id="training-the-wavenet-first-pass">training the <code>WaveNet</code>: first pass</h2>
</section>
<section id="fixing-batchnorm1d-bug" class="level2">
<h2 class="anchored" data-anchor-id="fixing-batchnorm1d-bug">fixing <code>batchnorm1d</code> bug</h2>
</section>
<section id="re-training-wavenet-with-bug-fix" class="level2">
<h2 class="anchored" data-anchor-id="re-training-wavenet-with-bug-fix">re-training <code>WaveNet</code> with bug fix</h2>
</section>
<section id="scaling-up-our-wavenet" class="level2">
<h2 class="anchored" data-anchor-id="scaling-up-our-wavenet">scaling up our <code>WaveNet</code></h2>
</section>
</section>
<section id="conclusions" class="level1">
<h1>3 conclusions</h1>
<section id="performance-log" class="level2">
<h2 class="anchored" data-anchor-id="performance-log">performance log</h2>
<table class="table">
<colgroup>
<col style="width: 12%">
<col style="width: 41%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>What we did</th>
<th>Loss we got (accum)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>original (3 character context + 200 hidden neurons, 12K params)</td>
<td>train 2.0467958450317383</td>
</tr>
<tr class="even">
<td>val 2.0989298820495605</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>2</td>
<td>context: 3 -&gt; 8 (22K params)</td>
<td></td>
</tr>
<tr class="even">
<td>3</td>
<td>flat -&gt; hierarchical (22K params)</td>
<td></td>
</tr>
<tr class="odd">
<td>4</td>
<td>fix bug in batchnorm</td>
<td></td>
</tr>
<tr class="even">
<td>5</td>
<td>scale up the network: n_embd 24, n_hidden 128 (76K params)</td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="experimental-harness" class="level2">
<h2 class="anchored" data-anchor-id="experimental-harness">experimental harness</h2>
</section>
<section id="wavenet-but-with-dilated-causal-convolutions" class="level2">
<h2 class="anchored" data-anchor-id="wavenet-but-with-dilated-causal-convolutions"><code>WaveNet</code> but with “dilated causal convolutions”</h2>
</section>
<section id="torch.nn" class="level2">
<h2 class="anchored" data-anchor-id="torch.nn"><code>torch.nn</code></h2>
</section>
<section id="the-development-process-of-building-deep-neural-nets" class="level2">
<h2 class="anchored" data-anchor-id="the-development-process-of-building-deep-neural-nets">the development process of building deep neural nets</h2>
</section>
<section id="going-forward" class="level2">
<h2 class="anchored" data-anchor-id="going-forward">going forward</h2>
</section>
<section id="improve-on-my-loss-how-far-can-we-improve-a-wavenet-on-this-data" class="level2">
<h2 class="anchored" data-anchor-id="improve-on-my-loss-how-far-can-we-improve-a-wavenet-on-this-data">improve on my loss! how far can we improve a <code>WaveNet</code> on this data?</h2>
</section>
</section>
<section id="resources" class="level1">
<h1>4 resources</h1>
<ol type="1">
<li>WaveNet 2016 from DeepMind: <a href="https://arxiv.org/abs/1609.03499" class="uri">https://arxiv.org/abs/1609.03499</a>;</li>
<li>Bengio et al.&nbsp;2003 MLP LM: <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" class="uri">https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf</a>;</li>
<li>Notebook: <a href="https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part5_cnn1.ipynb" class="uri">https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part5_cnn1.ipynb</a>;</li>
<li>DeepMind’s blog post from 2016: <a href="https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/" class="uri">https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/</a></li>
</ol>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/lktuan\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb26" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb26-1"><a href="#cb26-1"></a><span class="co">---</span></span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="an">title:</span><span class="co"> "NN-Z2H Lesson 6: Building makemore part 5 - Building a `WaveNet`"</span></span>
<span id="cb26-3"><a href="#cb26-3"></a><span class="an">description:</span><span class="co"> "CNN/`WaveNet` and a lot of reading of documentation, keeping track of multidimensional tensor shapes, moving between jupyter notebooks and repository code, ..."</span></span>
<span id="cb26-4"><a href="#cb26-4"></a><span class="an">author:</span></span>
<span id="cb26-5"><a href="#cb26-5"></a><span class="co">  - name: "Tuan Le Khac"</span></span>
<span id="cb26-6"><a href="#cb26-6"></a><span class="co">    url: https://lktuan.github.io/</span></span>
<span id="cb26-7"><a href="#cb26-7"></a><span class="an">categories:</span><span class="co"> [til, python, andrej karpathy, nn-z2h, neural networks] </span></span>
<span id="cb26-8"><a href="#cb26-8"></a><span class="an">date:</span><span class="co"> 12-09-2024</span></span>
<span id="cb26-9"><a href="#cb26-9"></a><span class="an">date-modified:</span><span class="co"> 12-09-2024</span></span>
<span id="cb26-10"><a href="#cb26-10"></a><span class="an">image:</span><span class="co"> wavenet.png</span></span>
<span id="cb26-11"><a href="#cb26-11"></a><span class="an">draft:</span><span class="co"> false</span></span>
<span id="cb26-12"><a href="#cb26-12"></a><span class="an">format:</span></span>
<span id="cb26-13"><a href="#cb26-13"></a><span class="co">  html:</span></span>
<span id="cb26-14"><a href="#cb26-14"></a><span class="co">    code-overflow: wrap</span></span>
<span id="cb26-15"><a href="#cb26-15"></a><span class="co">    code-tools: true</span></span>
<span id="cb26-16"><a href="#cb26-16"></a><span class="co">    code-fold: show</span></span>
<span id="cb26-17"><a href="#cb26-17"></a><span class="co">    code-annotations: hover</span></span>
<span id="cb26-18"><a href="#cb26-18"></a><span class="an">execute:</span></span>
<span id="cb26-19"><a href="#cb26-19"></a><span class="co">  eval: false</span></span>
<span id="cb26-20"><a href="#cb26-20"></a><span class="co">---</span></span>
<span id="cb26-21"><a href="#cb26-21"></a></span>
<span id="cb26-22"><a href="#cb26-22"></a>::: {.callout-important title="This is not orginal content!"}</span>
<span id="cb26-23"><a href="#cb26-23"></a>This is my study notes / codes along with Andrej Karpathy's "<span class="co">[</span><span class="ot">Neural Networks: Zero to Hero</span><span class="co">](https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)</span>" series.</span>
<span id="cb26-24"><a href="#cb26-24"></a>:::</span>
<span id="cb26-25"><a href="#cb26-25"></a></span>
<span id="cb26-26"><a href="#cb26-26"></a>*Codes are executed in Colab, this calculation capacity exceeds my computer's ability.*</span>
<span id="cb26-27"><a href="#cb26-27"></a></span>
<span id="cb26-28"><a href="#cb26-28"></a><span class="fu"># 1 intro</span></span>
<span id="cb26-29"><a href="#cb26-29"></a></span>
<span id="cb26-30"><a href="#cb26-30"></a>We are going to take the 2-layer MLP in the part 3 of <span class="in">`makemore`</span> and complexify it by:</span>
<span id="cb26-31"><a href="#cb26-31"></a></span>
<span id="cb26-32"><a href="#cb26-32"></a><span class="ss">-   </span>extending the block size: from 3 to 8 characters;</span>
<span id="cb26-33"><a href="#cb26-33"></a><span class="ss">-   </span>making it deeper rather than 1 hidden layer.</span>
<span id="cb26-34"><a href="#cb26-34"></a></span>
<span id="cb26-35"><a href="#cb26-35"></a>then end of with a Convoluntional Neural Network architecture similar to <span class="in">`WaveNet`</span> (2016) by <span class="co">[</span><span class="ot">Google DeepMind</span><span class="co">](https://deepmind.google/)</span>.</span>
<span id="cb26-36"><a href="#cb26-36"></a></span>
<span id="cb26-37"><a href="#cb26-37"></a>!<span class="co">[</span><span class="ot">WaveNet model architecture, [source](https://www.researchgate.net/figure/WaveNet-Model-Architecture-38_fig2_380566531)</span><span class="co">](wavenet.png)</span></span>
<span id="cb26-38"><a href="#cb26-38"></a></span>
<span id="cb26-39"><a href="#cb26-39"></a><span class="fu">## starter code walkthrough</span></span>
<span id="cb26-40"><a href="#cb26-40"></a></span>
<span id="cb26-41"><a href="#cb26-41"></a><span class="fu">#### import libraries</span></span>
<span id="cb26-42"><a href="#cb26-42"></a></span>
<span id="cb26-45"><a href="#cb26-45"></a><span class="in">```{python}</span></span>
<span id="cb26-46"><a href="#cb26-46"></a><span class="im">import</span> torch</span>
<span id="cb26-47"><a href="#cb26-47"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb26-48"><a href="#cb26-48"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb26-49"><a href="#cb26-49"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb26-50"><a href="#cb26-50"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb26-51"><a href="#cb26-51"></a><span class="in">```</span></span>
<span id="cb26-52"><a href="#cb26-52"></a></span>
<span id="cb26-53"><a href="#cb26-53"></a><span class="fu">#### reading data</span></span>
<span id="cb26-54"><a href="#cb26-54"></a></span>
<span id="cb26-57"><a href="#cb26-57"></a><span class="in">```{python}</span></span>
<span id="cb26-58"><a href="#cb26-58"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb26-59"><a href="#cb26-59"></a></span>
<span id="cb26-60"><a href="#cb26-60"></a>url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt"</span></span>
<span id="cb26-61"><a href="#cb26-61"></a>words <span class="op">=</span> pd.read_csv(url, header<span class="op">=</span><span class="va">None</span>).iloc[:, <span class="dv">0</span>].tolist()</span>
<span id="cb26-62"><a href="#cb26-62"></a>words[:<span class="dv">8</span>]</span>
<span id="cb26-63"><a href="#cb26-63"></a></span>
<span id="cb26-64"><a href="#cb26-64"></a><span class="co"># &gt;&gt;&gt; ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']</span></span>
<span id="cb26-65"><a href="#cb26-65"></a><span class="in">```</span></span>
<span id="cb26-66"><a href="#cb26-66"></a></span>
<span id="cb26-67"><a href="#cb26-67"></a><span class="fu">#### building vocab</span></span>
<span id="cb26-68"><a href="#cb26-68"></a></span>
<span id="cb26-71"><a href="#cb26-71"></a><span class="in">```{python}</span></span>
<span id="cb26-72"><a href="#cb26-72"></a><span class="co"># build the vocabulary of characters and mapping to/from integer</span></span>
<span id="cb26-73"><a href="#cb26-73"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(<span class="st">''</span>.join(words))))</span>
<span id="cb26-74"><a href="#cb26-74"></a>stoi <span class="op">=</span> {s:i<span class="op">+</span><span class="dv">1</span> <span class="cf">for</span> i, s <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb26-75"><a href="#cb26-75"></a>stoi[<span class="st">'.'</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb26-76"><a href="#cb26-76"></a>itos <span class="op">=</span> {i: s <span class="cf">for</span> s, i <span class="kw">in</span> stoi.items()}</span>
<span id="cb26-77"><a href="#cb26-77"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(itos)</span>
<span id="cb26-78"><a href="#cb26-78"></a><span class="bu">print</span>(itos)</span>
<span id="cb26-79"><a href="#cb26-79"></a><span class="bu">print</span>(vocab_size)</span>
<span id="cb26-80"><a href="#cb26-80"></a></span>
<span id="cb26-81"><a href="#cb26-81"></a><span class="co"># itos: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', </span></span>
<span id="cb26-82"><a href="#cb26-82"></a><span class="co"># 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', </span></span>
<span id="cb26-83"><a href="#cb26-83"></a><span class="co"># 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}</span></span>
<span id="cb26-84"><a href="#cb26-84"></a><span class="co"># vocab_size: 27</span></span>
<span id="cb26-85"><a href="#cb26-85"></a><span class="in">```</span></span>
<span id="cb26-86"><a href="#cb26-86"></a></span>
<span id="cb26-87"><a href="#cb26-87"></a><span class="fu">#### initializing randomnization</span></span>
<span id="cb26-88"><a href="#cb26-88"></a></span>
<span id="cb26-91"><a href="#cb26-91"></a><span class="in">```{python}</span></span>
<span id="cb26-92"><a href="#cb26-92"></a><span class="im">import</span> random</span>
<span id="cb26-93"><a href="#cb26-93"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb26-94"><a href="#cb26-94"></a>random.shuffle(words)</span>
<span id="cb26-95"><a href="#cb26-95"></a><span class="in">```</span></span>
<span id="cb26-96"><a href="#cb26-96"></a></span>
<span id="cb26-97"><a href="#cb26-97"></a><span class="fu">#### create train/dev/test splits</span></span>
<span id="cb26-98"><a href="#cb26-98"></a></span>
<span id="cb26-101"><a href="#cb26-101"></a><span class="in">```{python}</span></span>
<span id="cb26-102"><a href="#cb26-102"></a>block_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># context length: how many characters do we take to predict the next one?</span></span>
<span id="cb26-103"><a href="#cb26-103"></a><span class="co"># build the dataset</span></span>
<span id="cb26-104"><a href="#cb26-104"></a><span class="kw">def</span> buid_dataset(words):</span>
<span id="cb26-105"><a href="#cb26-105"></a>    X, Y <span class="op">=</span> [], []</span>
<span id="cb26-106"><a href="#cb26-106"></a></span>
<span id="cb26-107"><a href="#cb26-107"></a>    <span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb26-108"><a href="#cb26-108"></a>        context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size</span>
<span id="cb26-109"><a href="#cb26-109"></a>        <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb26-110"><a href="#cb26-110"></a>            ix <span class="op">=</span> stoi[ch]</span>
<span id="cb26-111"><a href="#cb26-111"></a>            X.append(context)</span>
<span id="cb26-112"><a href="#cb26-112"></a>            Y.append(ix)</span>
<span id="cb26-113"><a href="#cb26-113"></a>            context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix]</span>
<span id="cb26-114"><a href="#cb26-114"></a></span>
<span id="cb26-115"><a href="#cb26-115"></a>    X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb26-116"><a href="#cb26-116"></a>    Y <span class="op">=</span> torch.tensor(Y)</span>
<span id="cb26-117"><a href="#cb26-117"></a>    <span class="bu">print</span>(X.shape, Y.shape)</span>
<span id="cb26-118"><a href="#cb26-118"></a>    <span class="cf">return</span> X, Y</span>
<span id="cb26-119"><a href="#cb26-119"></a></span>
<span id="cb26-120"><a href="#cb26-120"></a>n1 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb26-121"><a href="#cb26-121"></a>n2 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb26-122"><a href="#cb26-122"></a></span>
<span id="cb26-123"><a href="#cb26-123"></a>Xtr, Ytr <span class="op">=</span> buid_dataset(words[:n1])        <span class="co"># 80#</span></span>
<span id="cb26-124"><a href="#cb26-124"></a>Xdev, Ydev <span class="op">=</span> buid_dataset(words[n1:n2])    <span class="co"># 10%</span></span>
<span id="cb26-125"><a href="#cb26-125"></a>Xte, Yte <span class="op">=</span> buid_dataset(words[n2:])        <span class="co"># 10%</span></span>
<span id="cb26-126"><a href="#cb26-126"></a></span>
<span id="cb26-127"><a href="#cb26-127"></a><span class="co"># torch.Size([182625, 3]) torch.Size([182625])</span></span>
<span id="cb26-128"><a href="#cb26-128"></a><span class="co"># torch.Size([22655, 3]) torch.Size([22655])</span></span>
<span id="cb26-129"><a href="#cb26-129"></a><span class="co"># torch.Size([22866, 3]) torch.Size([22866])</span></span>
<span id="cb26-130"><a href="#cb26-130"></a><span class="in">```</span></span>
<span id="cb26-131"><a href="#cb26-131"></a></span>
<span id="cb26-132"><a href="#cb26-132"></a><span class="fu">#### input and response preview</span></span>
<span id="cb26-133"><a href="#cb26-133"></a></span>
<span id="cb26-136"><a href="#cb26-136"></a><span class="in">```{python}</span></span>
<span id="cb26-137"><a href="#cb26-137"></a><span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(Xtr[:<span class="dv">20</span>], Ytr[:<span class="dv">20</span>]):</span>
<span id="cb26-138"><a href="#cb26-138"></a>  <span class="bu">print</span>(<span class="st">''</span>.join(itos[ix.item()] <span class="cf">for</span> ix <span class="kw">in</span> x), <span class="st">'---&gt;'</span>, itos[y.item()])</span>
<span id="cb26-139"><a href="#cb26-139"></a><span class="in">```</span></span>
<span id="cb26-140"><a href="#cb26-140"></a></span>
<span id="cb26-141"><a href="#cb26-141"></a><span class="in">``` md</span></span>
<span id="cb26-142"><a href="#cb26-142"></a>... ---&gt; y</span>
<span id="cb26-143"><a href="#cb26-143"></a>..y ---&gt; u</span>
<span id="cb26-144"><a href="#cb26-144"></a>.yu ---&gt; h</span>
<span id="cb26-145"><a href="#cb26-145"></a>yuh ---&gt; e</span>
<span id="cb26-146"><a href="#cb26-146"></a>uhe ---&gt; n</span>
<span id="cb26-147"><a href="#cb26-147"></a>hen ---&gt; g</span>
<span id="cb26-148"><a href="#cb26-148"></a>eng ---&gt; .</span>
<span id="cb26-149"><a href="#cb26-149"></a>... ---&gt; d</span>
<span id="cb26-150"><a href="#cb26-150"></a>..d ---&gt; i</span>
<span id="cb26-151"><a href="#cb26-151"></a>.di ---&gt; o</span>
<span id="cb26-152"><a href="#cb26-152"></a>dio ---&gt; n</span>
<span id="cb26-153"><a href="#cb26-153"></a>ion ---&gt; d</span>
<span id="cb26-154"><a href="#cb26-154"></a>ond ---&gt; r</span>
<span id="cb26-155"><a href="#cb26-155"></a>ndr ---&gt; e</span>
<span id="cb26-156"><a href="#cb26-156"></a>dre ---&gt; .</span>
<span id="cb26-157"><a href="#cb26-157"></a>... ---&gt; x</span>
<span id="cb26-158"><a href="#cb26-158"></a>..x ---&gt; a</span>
<span id="cb26-159"><a href="#cb26-159"></a>.xa ---&gt; v</span>
<span id="cb26-160"><a href="#cb26-160"></a>xav ---&gt; i</span>
<span id="cb26-161"><a href="#cb26-161"></a>avi ---&gt; e</span>
<span id="cb26-162"><a href="#cb26-162"></a><span class="in">```</span></span>
<span id="cb26-163"><a href="#cb26-163"></a></span>
<span id="cb26-164"><a href="#cb26-164"></a><span class="fu">#### initializing objects in networks</span></span>
<span id="cb26-165"><a href="#cb26-165"></a></span>
<span id="cb26-166"><a href="#cb26-166"></a>Near copy paste of the layers we have developed in Part 3, I added some docstring to the classes.</span>
<span id="cb26-167"><a href="#cb26-167"></a></span>
<span id="cb26-168"><a href="#cb26-168"></a><span class="fu">##### class `Linear`</span></span>
<span id="cb26-169"><a href="#cb26-169"></a></span>
<span id="cb26-172"><a href="#cb26-172"></a><span class="in">```{python}</span></span>
<span id="cb26-173"><a href="#cb26-173"></a><span class="co">#| code-fold: true</span></span>
<span id="cb26-174"><a href="#cb26-174"></a><span class="kw">class</span> Linear:</span>
<span id="cb26-175"><a href="#cb26-175"></a>  <span class="co">"""    </span></span>
<span id="cb26-176"><a href="#cb26-176"></a><span class="co">  Applies an affine linear transformation to the incoming data: y = xA^T + b.</span></span>
<span id="cb26-177"><a href="#cb26-177"></a></span>
<span id="cb26-178"><a href="#cb26-178"></a><span class="co">  This class implements a linear (fully connected) layer, which performs a linear </span></span>
<span id="cb26-179"><a href="#cb26-179"></a><span class="co">  transformation on the input tensor. It is typically used in neural network architectures </span></span>
<span id="cb26-180"><a href="#cb26-180"></a><span class="co">  to transform input features between layers.</span></span>
<span id="cb26-181"><a href="#cb26-181"></a></span>
<span id="cb26-182"><a href="#cb26-182"></a><span class="co">  Args:</span></span>
<span id="cb26-183"><a href="#cb26-183"></a><span class="co">      fan_in (int): Number of input features (input dimension).</span></span>
<span id="cb26-184"><a href="#cb26-184"></a><span class="co">      fan_out (int): Number of output features (output dimension).</span></span>
<span id="cb26-185"><a href="#cb26-185"></a><span class="co">      bias (bool, optional): Whether to include a learnable bias term. </span></span>
<span id="cb26-186"><a href="#cb26-186"></a><span class="co">          Defaults to True.</span></span>
<span id="cb26-187"><a href="#cb26-187"></a></span>
<span id="cb26-188"><a href="#cb26-188"></a><span class="co">  Attributes:</span></span>
<span id="cb26-189"><a href="#cb26-189"></a><span class="co">      weight (torch.Tensor): Weight matrix of shape (fan_in, fan_out), </span></span>
<span id="cb26-190"><a href="#cb26-190"></a><span class="co">          initialized using Kaiming initialization.</span></span>
<span id="cb26-191"><a href="#cb26-191"></a><span class="co">      bias (torch.Tensor or None): Bias vector of shape (fan_out), </span></span>
<span id="cb26-192"><a href="#cb26-192"></a><span class="co">          initialized to zeros if bias is True, otherwise None.</span></span>
<span id="cb26-193"><a href="#cb26-193"></a></span>
<span id="cb26-194"><a href="#cb26-194"></a><span class="co">  Methods:</span></span>
<span id="cb26-195"><a href="#cb26-195"></a><span class="co">      __call__(x): Applies the linear transformation to the input tensor x.</span></span>
<span id="cb26-196"><a href="#cb26-196"></a><span class="co">      parameters(): Returns a list of trainable parameters (weight and bias).</span></span>
<span id="cb26-197"><a href="#cb26-197"></a></span>
<span id="cb26-198"><a href="#cb26-198"></a><span class="co">  Example:</span></span>
<span id="cb26-199"><a href="#cb26-199"></a><span class="co">      &gt;&gt;&gt; layer = Linear(10, 5)  # Creates a linear layer with 10 input features and 5 output features</span></span>
<span id="cb26-200"><a href="#cb26-200"></a><span class="co">      &gt;&gt;&gt; x = torch.randn(3, 10)  # Input tensor with batch size 3 and 10 features</span></span>
<span id="cb26-201"><a href="#cb26-201"></a><span class="co">      &gt;&gt;&gt; output = layer(x)  # Applies linear transformation</span></span>
<span id="cb26-202"><a href="#cb26-202"></a><span class="co">      &gt;&gt;&gt; output.shape</span></span>
<span id="cb26-203"><a href="#cb26-203"></a><span class="co">      torch.Size([3, 5])</span></span>
<span id="cb26-204"><a href="#cb26-204"></a><span class="co">  """</span></span>
<span id="cb26-205"><a href="#cb26-205"></a></span>
<span id="cb26-206"><a href="#cb26-206"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, fan_in, fan_out, bias<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb26-207"><a href="#cb26-207"></a>    <span class="va">self</span>.weight <span class="op">=</span> torch.randn((fan_in, fan_out)) <span class="op">/</span> fan_in<span class="op">**</span><span class="fl">0.5</span> <span class="co"># note: kaiming init</span></span>
<span id="cb26-208"><a href="#cb26-208"></a>    <span class="va">self</span>.bias <span class="op">=</span> torch.zeros(fan_out) <span class="cf">if</span> bias <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb26-209"><a href="#cb26-209"></a>  </span>
<span id="cb26-210"><a href="#cb26-210"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb26-211"><a href="#cb26-211"></a>    <span class="va">self</span>.out <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.weight</span>
<span id="cb26-212"><a href="#cb26-212"></a>    <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb26-213"><a href="#cb26-213"></a>      <span class="va">self</span>.out <span class="op">+=</span> <span class="va">self</span>.bias</span>
<span id="cb26-214"><a href="#cb26-214"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb26-215"><a href="#cb26-215"></a>  </span>
<span id="cb26-216"><a href="#cb26-216"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb26-217"><a href="#cb26-217"></a>    <span class="cf">return</span> [<span class="va">self</span>.weight] <span class="op">+</span> ([] <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> [<span class="va">self</span>.bias])</span>
<span id="cb26-218"><a href="#cb26-218"></a><span class="in">```</span></span>
<span id="cb26-219"><a href="#cb26-219"></a></span>
<span id="cb26-220"><a href="#cb26-220"></a><span class="fu">##### class `BatchNorm1d`</span></span>
<span id="cb26-221"><a href="#cb26-221"></a></span>
<span id="cb26-224"><a href="#cb26-224"></a><span class="in">```{python}</span></span>
<span id="cb26-225"><a href="#cb26-225"></a><span class="co">#| code-fold: true</span></span>
<span id="cb26-226"><a href="#cb26-226"></a><span class="kw">class</span> BatchNorm1d:</span>
<span id="cb26-227"><a href="#cb26-227"></a>  <span class="co">"""</span></span>
<span id="cb26-228"><a href="#cb26-228"></a><span class="co">  Applies Batch Normalization to the input tensor, a technique to improve </span></span>
<span id="cb26-229"><a href="#cb26-229"></a><span class="co">  training stability and performance in deep neural networks.</span></span>
<span id="cb26-230"><a href="#cb26-230"></a></span>
<span id="cb26-231"><a href="#cb26-231"></a><span class="co">  Batch Normalization normalizes the input across the batch dimension, </span></span>
<span id="cb26-232"><a href="#cb26-232"></a><span class="co">  reducing internal covariate shift and allowing higher learning rates. </span></span>
<span id="cb26-233"><a href="#cb26-233"></a><span class="co">  This implementation supports both training and inference modes.</span></span>
<span id="cb26-234"><a href="#cb26-234"></a></span>
<span id="cb26-235"><a href="#cb26-235"></a><span class="co">  Args:</span></span>
<span id="cb26-236"><a href="#cb26-236"></a><span class="co">      dim (int): Number of features or channels to be normalized.</span></span>
<span id="cb26-237"><a href="#cb26-237"></a><span class="co">      eps (float, optional): A small constant added to the denominator for </span></span>
<span id="cb26-238"><a href="#cb26-238"></a><span class="co">          numerical stability to prevent division by zero. </span></span>
<span id="cb26-239"><a href="#cb26-239"></a><span class="co">          Defaults to 1e-5.</span></span>
<span id="cb26-240"><a href="#cb26-240"></a><span class="co">      momentum (float, optional): Momentum for updating running mean and </span></span>
<span id="cb26-241"><a href="#cb26-241"></a><span class="co">          variance during training. Controls the degree of exponential </span></span>
<span id="cb26-242"><a href="#cb26-242"></a><span class="co">          moving average. Defaults to 0.1.</span></span>
<span id="cb26-243"><a href="#cb26-243"></a></span>
<span id="cb26-244"><a href="#cb26-244"></a><span class="co">  Attributes:</span></span>
<span id="cb26-245"><a href="#cb26-245"></a><span class="co">      eps (float): Epsilon value for numerical stability.</span></span>
<span id="cb26-246"><a href="#cb26-246"></a><span class="co">      momentum (float): Momentum for running statistics update.</span></span>
<span id="cb26-247"><a href="#cb26-247"></a><span class="co">      training (bool): Indicates whether the layer is in training or inference mode.</span></span>
<span id="cb26-248"><a href="#cb26-248"></a><span class="co">      gamma (torch.Tensor): Learnable scale parameter of shape (dim,).</span></span>
<span id="cb26-249"><a href="#cb26-249"></a><span class="co">      beta (torch.Tensor): Learnable shift parameter of shape (dim,).</span></span>
<span id="cb26-250"><a href="#cb26-250"></a><span class="co">      running_mean (torch.Tensor): Exponential moving average of batch means.</span></span>
<span id="cb26-251"><a href="#cb26-251"></a><span class="co">      running_var (torch.Tensor): Exponential moving average of batch variances.</span></span>
<span id="cb26-252"><a href="#cb26-252"></a></span>
<span id="cb26-253"><a href="#cb26-253"></a><span class="co">  Methods:</span></span>
<span id="cb26-254"><a href="#cb26-254"></a><span class="co">      __call__(x): Applies batch normalization to the input tensor.</span></span>
<span id="cb26-255"><a href="#cb26-255"></a><span class="co">      parameters(): Returns learnable parameters (gamma and beta).</span></span>
<span id="cb26-256"><a href="#cb26-256"></a></span>
<span id="cb26-257"><a href="#cb26-257"></a><span class="co">  Key Normalization Steps:</span></span>
<span id="cb26-258"><a href="#cb26-258"></a><span class="co">  1. Compute batch mean and variance (in training mode)</span></span>
<span id="cb26-259"><a href="#cb26-259"></a><span class="co">  2. Normalize input by subtracting mean and dividing by standard deviation</span></span>
<span id="cb26-260"><a href="#cb26-260"></a><span class="co">  3. Apply learnable scale (gamma) and shift (beta) parameters</span></span>
<span id="cb26-261"><a href="#cb26-261"></a><span class="co">  4. Update running statistics during training</span></span>
<span id="cb26-262"><a href="#cb26-262"></a></span>
<span id="cb26-263"><a href="#cb26-263"></a><span class="co">  Example:</span></span>
<span id="cb26-264"><a href="#cb26-264"></a><span class="co">      &gt;&gt;&gt; batch_norm = BatchNorm1d(64)  # For 64-channel input</span></span>
<span id="cb26-265"><a href="#cb26-265"></a><span class="co">      &gt;&gt;&gt; x = torch.randn(32, 64)  # Batch of 32 samples with 64 features</span></span>
<span id="cb26-266"><a href="#cb26-266"></a><span class="co">      &gt;&gt;&gt; normalized_x = batch_norm(x)  # Apply batch normalization</span></span>
<span id="cb26-267"><a href="#cb26-267"></a><span class="co">      &gt;&gt;&gt; normalized_x.shape</span></span>
<span id="cb26-268"><a href="#cb26-268"></a><span class="co">      torch.Size([32, 64])</span></span>
<span id="cb26-269"><a href="#cb26-269"></a></span>
<span id="cb26-270"><a href="#cb26-270"></a><span class="co">  Note:</span></span>
<span id="cb26-271"><a href="#cb26-271"></a><span class="co">      - Supports both 2D (batch, features) and 3D (batch, channels, sequence) input tensors</span></span>
<span id="cb26-272"><a href="#cb26-272"></a><span class="co">      - During inference, uses running statistics instead of batch statistics</span></span>
<span id="cb26-273"><a href="#cb26-273"></a><span class="co">  """</span></span>
<span id="cb26-274"><a href="#cb26-274"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, eps<span class="op">=</span><span class="fl">1e-5</span>, momentum<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb26-275"><a href="#cb26-275"></a>    <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb26-276"><a href="#cb26-276"></a>    <span class="va">self</span>.momentum <span class="op">=</span> momentum</span>
<span id="cb26-277"><a href="#cb26-277"></a>    <span class="va">self</span>.training <span class="op">=</span> <span class="va">True</span></span>
<span id="cb26-278"><a href="#cb26-278"></a>    <span class="co"># parameters (trained with backprop)</span></span>
<span id="cb26-279"><a href="#cb26-279"></a>    <span class="va">self</span>.gamma <span class="op">=</span> torch.ones(dim)</span>
<span id="cb26-280"><a href="#cb26-280"></a>    <span class="va">self</span>.beta <span class="op">=</span> torch.zeros(dim)</span>
<span id="cb26-281"><a href="#cb26-281"></a>    <span class="co"># buffers (trained with a running 'momentum update')</span></span>
<span id="cb26-282"><a href="#cb26-282"></a>    <span class="va">self</span>.running_mean <span class="op">=</span> torch.zeros(dim)</span>
<span id="cb26-283"><a href="#cb26-283"></a>    <span class="va">self</span>.running_var <span class="op">=</span> torch.ones(dim)</span>
<span id="cb26-284"><a href="#cb26-284"></a>  </span>
<span id="cb26-285"><a href="#cb26-285"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb26-286"><a href="#cb26-286"></a>    <span class="co"># calculate the forward pass</span></span>
<span id="cb26-287"><a href="#cb26-287"></a>    <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb26-288"><a href="#cb26-288"></a>      xmean <span class="op">=</span> x.mean(dim, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch mean</span></span>
<span id="cb26-289"><a href="#cb26-289"></a>      xvar <span class="op">=</span> x.var(dim, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch variance</span></span>
<span id="cb26-290"><a href="#cb26-290"></a>    <span class="cf">else</span>:</span>
<span id="cb26-291"><a href="#cb26-291"></a>      xmean <span class="op">=</span> <span class="va">self</span>.running_mean</span>
<span id="cb26-292"><a href="#cb26-292"></a>      xvar <span class="op">=</span> <span class="va">self</span>.running_var</span>
<span id="cb26-293"><a href="#cb26-293"></a>    xhat <span class="op">=</span> (x <span class="op">-</span> xmean) <span class="op">/</span> torch.sqrt(xvar <span class="op">+</span> <span class="va">self</span>.eps) <span class="co"># normalize to unit variance</span></span>
<span id="cb26-294"><a href="#cb26-294"></a>    <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.gamma <span class="op">*</span> xhat <span class="op">+</span> <span class="va">self</span>.beta</span>
<span id="cb26-295"><a href="#cb26-295"></a>    <span class="co"># update the buffers</span></span>
<span id="cb26-296"><a href="#cb26-296"></a>    <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb26-297"><a href="#cb26-297"></a>      <span class="cf">with</span> torch.no_grad():</span>
<span id="cb26-298"><a href="#cb26-298"></a>        <span class="va">self</span>.running_mean <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> <span class="va">self</span>.running_mean <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> xmean</span>
<span id="cb26-299"><a href="#cb26-299"></a>        <span class="va">self</span>.running_var <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> <span class="va">self</span>.running_var <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> xvar</span>
<span id="cb26-300"><a href="#cb26-300"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb26-301"><a href="#cb26-301"></a>  </span>
<span id="cb26-302"><a href="#cb26-302"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb26-303"><a href="#cb26-303"></a>    <span class="cf">return</span> [<span class="va">self</span>.gamma, <span class="va">self</span>.beta]</span>
<span id="cb26-304"><a href="#cb26-304"></a><span class="in">```</span></span>
<span id="cb26-305"><a href="#cb26-305"></a></span>
<span id="cb26-306"><a href="#cb26-306"></a><span class="fu">#### class `Tanh`</span></span>
<span id="cb26-307"><a href="#cb26-307"></a></span>
<span id="cb26-310"><a href="#cb26-310"></a><span class="in">```{python}</span></span>
<span id="cb26-311"><a href="#cb26-311"></a><span class="co">#| code-fold: true</span></span>
<span id="cb26-312"><a href="#cb26-312"></a><span class="kw">class</span> Tanh:</span>
<span id="cb26-313"><a href="#cb26-313"></a>    <span class="co">"""</span></span>
<span id="cb26-314"><a href="#cb26-314"></a><span class="co">    Hyperbolic Tangent (Tanh) Activation Function</span></span>
<span id="cb26-315"><a href="#cb26-315"></a></span>
<span id="cb26-316"><a href="#cb26-316"></a><span class="co">    Applies the hyperbolic tangent activation function element-wise to the input tensor. </span></span>
<span id="cb26-317"><a href="#cb26-317"></a><span class="co">    Tanh maps input values to the range [-1, 1], providing a symmetric and non-linear </span></span>
<span id="cb26-318"><a href="#cb26-318"></a><span class="co">    transformation that helps neural networks learn complex patterns.</span></span>
<span id="cb26-319"><a href="#cb26-319"></a></span>
<span id="cb26-320"><a href="#cb26-320"></a><span class="co">    Mathematical Definition:</span></span>
<span id="cb26-321"><a href="#cb26-321"></a><span class="co">    tanh(x) = (e^x - e^-x) / (e^x + e^-x)</span></span>
<span id="cb26-322"><a href="#cb26-322"></a><span class="co">    </span></span>
<span id="cb26-323"><a href="#cb26-323"></a><span class="co">    Key Characteristics:</span></span>
<span id="cb26-324"><a href="#cb26-324"></a><span class="co">    - Output Range: [-1, 1]</span></span>
<span id="cb26-325"><a href="#cb26-325"></a><span class="co">    - Symmetric around the origin</span></span>
<span id="cb26-326"><a href="#cb26-326"></a><span class="co">    - Gradient is always less than 1, which helps mitigate the vanishing gradient problem</span></span>
<span id="cb26-327"><a href="#cb26-327"></a><span class="co">    - Commonly used in recurrent neural networks and hidden layers</span></span>
<span id="cb26-328"><a href="#cb26-328"></a></span>
<span id="cb26-329"><a href="#cb26-329"></a><span class="co">    Methods:</span></span>
<span id="cb26-330"><a href="#cb26-330"></a><span class="co">        __call__(x): Applies the Tanh activation to the input tensor.</span></span>
<span id="cb26-331"><a href="#cb26-331"></a><span class="co">        parameters(): Returns an empty list, as Tanh has no learnable parameters.</span></span>
<span id="cb26-332"><a href="#cb26-332"></a></span>
<span id="cb26-333"><a href="#cb26-333"></a><span class="co">    Attributes:</span></span>
<span id="cb26-334"><a href="#cb26-334"></a><span class="co">        out (torch.Tensor): Stores the output of the most recent forward pass.</span></span>
<span id="cb26-335"><a href="#cb26-335"></a></span>
<span id="cb26-336"><a href="#cb26-336"></a><span class="co">    Example:</span></span>
<span id="cb26-337"><a href="#cb26-337"></a><span class="co">        &gt;&gt;&gt; activation = Tanh()</span></span>
<span id="cb26-338"><a href="#cb26-338"></a><span class="co">        &gt;&gt;&gt; x = torch.tensor([-2.0, 0.0, 2.0])</span></span>
<span id="cb26-339"><a href="#cb26-339"></a><span class="co">        &gt;&gt;&gt; y = activation(x)</span></span>
<span id="cb26-340"><a href="#cb26-340"></a><span class="co">        &gt;&gt;&gt; y</span></span>
<span id="cb26-341"><a href="#cb26-341"></a><span class="co">        tensor([-0.9640, 0.0000, 0.9640])</span></span>
<span id="cb26-342"><a href="#cb26-342"></a></span>
<span id="cb26-343"><a href="#cb26-343"></a><span class="co">    Note:</span></span>
<span id="cb26-344"><a href="#cb26-344"></a><span class="co">        This implementation is stateless and does not modify the input tensor.</span></span>
<span id="cb26-345"><a href="#cb26-345"></a><span class="co">        The activation is applied element-wise, preserving the input tensor's shape.</span></span>
<span id="cb26-346"><a href="#cb26-346"></a><span class="co">    """</span></span>
<span id="cb26-347"><a href="#cb26-347"></a></span>
<span id="cb26-348"><a href="#cb26-348"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb26-349"><a href="#cb26-349"></a>        <span class="va">self</span>.out <span class="op">=</span> torch.tanh(x)</span>
<span id="cb26-350"><a href="#cb26-350"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb26-351"><a href="#cb26-351"></a></span>
<span id="cb26-352"><a href="#cb26-352"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb26-353"><a href="#cb26-353"></a>        <span class="cf">return</span> []</span>
<span id="cb26-354"><a href="#cb26-354"></a><span class="in">```</span></span>
<span id="cb26-355"><a href="#cb26-355"></a></span>
<span id="cb26-356"><a href="#cb26-356"></a><span class="fu">##### random number generator</span></span>
<span id="cb26-357"><a href="#cb26-357"></a></span>
<span id="cb26-360"><a href="#cb26-360"></a><span class="in">```{python}</span></span>
<span id="cb26-361"><a href="#cb26-361"></a>torch.manual_seed(<span class="dv">42</span>)<span class="op">;</span> <span class="co"># seed rng for reproducibility</span></span>
<span id="cb26-362"><a href="#cb26-362"></a><span class="in">```</span></span>
<span id="cb26-363"><a href="#cb26-363"></a></span>
<span id="cb26-364"><a href="#cb26-364"></a><span class="fu">##### network architecture</span></span>
<span id="cb26-365"><a href="#cb26-365"></a></span>
<span id="cb26-368"><a href="#cb26-368"></a><span class="in">```{python}</span></span>
<span id="cb26-369"><a href="#cb26-369"></a><span class="co">#| eval: false</span></span>
<span id="cb26-370"><a href="#cb26-370"></a><span class="co"># original network</span></span>
<span id="cb26-371"><a href="#cb26-371"></a>n_embd <span class="op">=</span> <span class="dv">10</span> <span class="co"># the dimensionality of the character embedding vectors</span></span>
<span id="cb26-372"><a href="#cb26-372"></a>n_hidden <span class="op">=</span> <span class="dv">200</span> <span class="co"># the number of neurons in the hidden layer of the MLP</span></span>
<span id="cb26-373"><a href="#cb26-373"></a></span>
<span id="cb26-374"><a href="#cb26-374"></a>C <span class="op">=</span> torch.rand((vocab_size, n_embd))</span>
<span id="cb26-375"><a href="#cb26-375"></a>layers <span class="op">=</span> [</span>
<span id="cb26-376"><a href="#cb26-376"></a>  Linear(n_embd <span class="op">*</span> block_size, n_hidden, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb26-377"><a href="#cb26-377"></a>  BatchNorm1d(n_hidden),</span>
<span id="cb26-378"><a href="#cb26-378"></a>  Tanh(),</span>
<span id="cb26-379"><a href="#cb26-379"></a>  Linear(n_hidden, vocab_size),</span>
<span id="cb26-380"><a href="#cb26-380"></a>]</span>
<span id="cb26-381"><a href="#cb26-381"></a></span>
<span id="cb26-382"><a href="#cb26-382"></a><span class="co"># parameter init</span></span>
<span id="cb26-383"><a href="#cb26-383"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb26-384"><a href="#cb26-384"></a>  layers[<span class="op">-</span><span class="dv">1</span>].weight <span class="op">*=</span> <span class="fl">0.1</span> <span class="co"># last layer make less confident</span></span>
<span id="cb26-385"><a href="#cb26-385"></a></span>
<span id="cb26-386"><a href="#cb26-386"></a>parameters <span class="op">=</span> [C] <span class="op">+</span> [p <span class="cf">for</span> layer <span class="kw">in</span> layers <span class="cf">for</span> p <span class="kw">in</span> layer.parameters()]</span>
<span id="cb26-387"><a href="#cb26-387"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters)) <span class="co"># number of parameters in total</span></span>
<span id="cb26-388"><a href="#cb26-388"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb26-389"><a href="#cb26-389"></a>  p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb26-390"><a href="#cb26-390"></a></span>
<span id="cb26-391"><a href="#cb26-391"></a><span class="co"># model params: 12097</span></span>
<span id="cb26-392"><a href="#cb26-392"></a><span class="in">```</span></span>
<span id="cb26-393"><a href="#cb26-393"></a></span>
<span id="cb26-394"><a href="#cb26-394"></a><span class="fu">#### optimization</span></span>
<span id="cb26-395"><a href="#cb26-395"></a></span>
<span id="cb26-398"><a href="#cb26-398"></a><span class="in">```{python}</span></span>
<span id="cb26-399"><a href="#cb26-399"></a><span class="co">#| eval: false</span></span>
<span id="cb26-400"><a href="#cb26-400"></a><span class="co"># same optimization as last time</span></span>
<span id="cb26-401"><a href="#cb26-401"></a>max_steps <span class="op">=</span> <span class="dv">200000</span></span>
<span id="cb26-402"><a href="#cb26-402"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb26-403"><a href="#cb26-403"></a>lossi <span class="op">=</span> []</span>
<span id="cb26-404"><a href="#cb26-404"></a></span>
<span id="cb26-405"><a href="#cb26-405"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb26-406"><a href="#cb26-406"></a>  </span>
<span id="cb26-407"><a href="#cb26-407"></a>  <span class="co"># minibatch construct</span></span>
<span id="cb26-408"><a href="#cb26-408"></a>  ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,))</span>
<span id="cb26-409"><a href="#cb26-409"></a>  Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix] <span class="co"># batch X,Y</span></span>
<span id="cb26-410"><a href="#cb26-410"></a>  </span>
<span id="cb26-411"><a href="#cb26-411"></a>  <span class="co"># forward pass</span></span>
<span id="cb26-412"><a href="#cb26-412"></a>  emb <span class="op">=</span> C[Xb] <span class="co"># embed the characters into vectors   </span></span>
<span id="cb26-413"><a href="#cb26-413"></a>  x <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate the vectors</span></span>
<span id="cb26-414"><a href="#cb26-414"></a>  <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb26-415"><a href="#cb26-415"></a>    x <span class="op">=</span> layer(x)</span>
<span id="cb26-416"><a href="#cb26-416"></a>  loss <span class="op">=</span> F.cross_entropy(x, Yb) <span class="co"># loss function</span></span>
<span id="cb26-417"><a href="#cb26-417"></a>  </span>
<span id="cb26-418"><a href="#cb26-418"></a>  <span class="co"># backward pass</span></span>
<span id="cb26-419"><a href="#cb26-419"></a>  <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb26-420"><a href="#cb26-420"></a>    p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb26-421"><a href="#cb26-421"></a>  loss.backward()</span>
<span id="cb26-422"><a href="#cb26-422"></a>  </span>
<span id="cb26-423"><a href="#cb26-423"></a>  <span class="co"># update: simple SGD</span></span>
<span id="cb26-424"><a href="#cb26-424"></a>  lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">150000</span> <span class="cf">else</span> <span class="fl">0.01</span> <span class="co"># step learning rate decay</span></span>
<span id="cb26-425"><a href="#cb26-425"></a>  <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb26-426"><a href="#cb26-426"></a>    p.data <span class="op">+=</span> <span class="op">-</span>lr <span class="op">*</span> p.grad</span>
<span id="cb26-427"><a href="#cb26-427"></a></span>
<span id="cb26-428"><a href="#cb26-428"></a>  <span class="co"># track stats</span></span>
<span id="cb26-429"><a href="#cb26-429"></a>  <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="co"># print every once in a while</span></span>
<span id="cb26-430"><a href="#cb26-430"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">:7d}</span><span class="ss">/</span><span class="sc">{</span>max_steps<span class="sc">:7d}</span><span class="ss">: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb26-431"><a href="#cb26-431"></a>  lossi.append(loss.log10().item())</span>
<span id="cb26-432"><a href="#cb26-432"></a><span class="in">```</span></span>
<span id="cb26-433"><a href="#cb26-433"></a></span>
<span id="cb26-434"><a href="#cb26-434"></a><span class="in">``` md</span></span>
<span id="cb26-435"><a href="#cb26-435"></a>     0/ 200000: 3.2885</span>
<span id="cb26-436"><a href="#cb26-436"></a>  10000/ 200000: 2.3938</span>
<span id="cb26-437"><a href="#cb26-437"></a>  20000/ 200000: 2.1235</span>
<span id="cb26-438"><a href="#cb26-438"></a>  30000/ 200000: 1.9222</span>
<span id="cb26-439"><a href="#cb26-439"></a>  40000/ 200000: 2.2440</span>
<span id="cb26-440"><a href="#cb26-440"></a>  50000/ 200000: 2.1108</span>
<span id="cb26-441"><a href="#cb26-441"></a>  60000/ 200000: 2.0624</span>
<span id="cb26-442"><a href="#cb26-442"></a>  70000/ 200000: 2.0893</span>
<span id="cb26-443"><a href="#cb26-443"></a>  80000/ 200000: 2.4173</span>
<span id="cb26-444"><a href="#cb26-444"></a>  90000/ 200000: 1.9744</span>
<span id="cb26-445"><a href="#cb26-445"></a> 100000/ 200000: 2.0883</span>
<span id="cb26-446"><a href="#cb26-446"></a> 110000/ 200000: 2.4538</span>
<span id="cb26-447"><a href="#cb26-447"></a> 120000/ 200000: 1.9535</span>
<span id="cb26-448"><a href="#cb26-448"></a> 130000/ 200000: 1.8980</span>
<span id="cb26-449"><a href="#cb26-449"></a> 140000/ 200000: 2.1196</span>
<span id="cb26-450"><a href="#cb26-450"></a> 150000/ 200000: 2.3550</span>
<span id="cb26-451"><a href="#cb26-451"></a> 160000/ 200000: 2.2957</span>
<span id="cb26-452"><a href="#cb26-452"></a> 170000/ 200000: 2.0286</span>
<span id="cb26-453"><a href="#cb26-453"></a> 180000/ 200000: 2.2379</span>
<span id="cb26-454"><a href="#cb26-454"></a> 190000/ 200000: 2.3866</span>
<span id="cb26-455"><a href="#cb26-455"></a><span class="in">```</span></span>
<span id="cb26-456"><a href="#cb26-456"></a></span>
<span id="cb26-457"><a href="#cb26-457"></a><span class="fu">#### observe training process/evaluation</span></span>
<span id="cb26-458"><a href="#cb26-458"></a></span>
<span id="cb26-461"><a href="#cb26-461"></a><span class="in">```{python}</span></span>
<span id="cb26-462"><a href="#cb26-462"></a>plt.plot(lossi)</span>
<span id="cb26-463"><a href="#cb26-463"></a><span class="in">```</span></span>
<span id="cb26-464"><a href="#cb26-464"></a></span>
<span id="cb26-465"><a href="#cb26-465"></a><span class="al">![`lossi` plot at the beginning](1_pre_lossi.png)</span></span>
<span id="cb26-466"><a href="#cb26-466"></a></span>
<span id="cb26-467"><a href="#cb26-467"></a><span class="fu">#### calibrate the `batchnorm` after training</span></span>
<span id="cb26-468"><a href="#cb26-468"></a></span>
<span id="cb26-469"><a href="#cb26-469"></a>We should be using the running mean/variance of the whole dataset splits rather than the last mini-batch.</span>
<span id="cb26-470"><a href="#cb26-470"></a></span>
<span id="cb26-473"><a href="#cb26-473"></a><span class="in">```{python}</span></span>
<span id="cb26-474"><a href="#cb26-474"></a><span class="co"># put layers into eval mode (needed for batchnorm especially)</span></span>
<span id="cb26-475"><a href="#cb26-475"></a><span class="cf">for</span> layer <span class="kw">in</span> model.layers:</span>
<span id="cb26-476"><a href="#cb26-476"></a>  layer.training <span class="op">=</span> <span class="va">False</span></span>
<span id="cb26-477"><a href="#cb26-477"></a><span class="in">```</span></span>
<span id="cb26-478"><a href="#cb26-478"></a></span>
<span id="cb26-479"><a href="#cb26-479"></a><span class="fu">#### calculate on whole training and validation splits</span></span>
<span id="cb26-480"><a href="#cb26-480"></a></span>
<span id="cb26-483"><a href="#cb26-483"></a><span class="in">```{python}</span></span>
<span id="cb26-484"><a href="#cb26-484"></a><span class="co"># evaluate the loss</span></span>
<span id="cb26-485"><a href="#cb26-485"></a><span class="at">@torch.no_grad</span>() <span class="co"># this decorator disables gradient tracking inside pytorch</span></span>
<span id="cb26-486"><a href="#cb26-486"></a><span class="kw">def</span> split_loss(split):</span>
<span id="cb26-487"><a href="#cb26-487"></a>  x,y <span class="op">=</span> {</span>
<span id="cb26-488"><a href="#cb26-488"></a>    <span class="st">'train'</span>: (Xtr, Ytr),</span>
<span id="cb26-489"><a href="#cb26-489"></a>    <span class="st">'val'</span>: (Xdev, Ydev),</span>
<span id="cb26-490"><a href="#cb26-490"></a>    <span class="st">'test'</span>: (Xte, Yte),</span>
<span id="cb26-491"><a href="#cb26-491"></a>  }[split]</span>
<span id="cb26-492"><a href="#cb26-492"></a>  logits <span class="op">=</span> model(x)</span>
<span id="cb26-493"><a href="#cb26-493"></a>  loss <span class="op">=</span> F.cross_entropy(logits, y)</span>
<span id="cb26-494"><a href="#cb26-494"></a>  <span class="bu">print</span>(split, loss.item())</span>
<span id="cb26-495"><a href="#cb26-495"></a></span>
<span id="cb26-496"><a href="#cb26-496"></a>split_loss(<span class="st">'train'</span>)</span>
<span id="cb26-497"><a href="#cb26-497"></a>split_loss(<span class="st">'val'</span>)</span>
<span id="cb26-498"><a href="#cb26-498"></a><span class="in">```</span></span>
<span id="cb26-499"><a href="#cb26-499"></a></span>
<span id="cb26-500"><a href="#cb26-500"></a>Pretty loss but there are still room for improve:</span>
<span id="cb26-501"><a href="#cb26-501"></a></span>
<span id="cb26-502"><a href="#cb26-502"></a><span class="in">```md</span></span>
<span id="cb26-503"><a href="#cb26-503"></a>train 2.0467958450317383</span>
<span id="cb26-504"><a href="#cb26-504"></a>val 2.0989298820495605</span>
<span id="cb26-505"><a href="#cb26-505"></a><span class="in">```</span></span>
<span id="cb26-506"><a href="#cb26-506"></a></span>
<span id="cb26-507"><a href="#cb26-507"></a><span class="fu">#### sample from the model</span></span>
<span id="cb26-508"><a href="#cb26-508"></a></span>
<span id="cb26-509"><a href="#cb26-509"></a>Here are Names generated by the model till now, we have relatively name-like results that do not exist in the training set.</span>
<span id="cb26-510"><a href="#cb26-510"></a></span>
<span id="cb26-511"><a href="#cb26-511"></a><span class="in">``` md</span></span>
<span id="cb26-512"><a href="#cb26-512"></a>liz.</span>
<span id="cb26-513"><a href="#cb26-513"></a>layah.</span>
<span id="cb26-514"><a href="#cb26-514"></a>dan.</span>
<span id="cb26-515"><a href="#cb26-515"></a>hilon.</span>
<span id="cb26-516"><a href="#cb26-516"></a>avani.</span>
<span id="cb26-517"><a href="#cb26-517"></a>korron.</span>
<span id="cb26-518"><a href="#cb26-518"></a>aua.</span>
<span id="cb26-519"><a href="#cb26-519"></a>noon.</span>
<span id="cb26-520"><a href="#cb26-520"></a>bethalyn.</span>
<span id="cb26-521"><a href="#cb26-521"></a>thia.</span>
<span id="cb26-522"><a href="#cb26-522"></a>bote.</span>
<span id="cb26-523"><a href="#cb26-523"></a>jereanail.</span>
<span id="cb26-524"><a href="#cb26-524"></a>vitorien.</span>
<span id="cb26-525"><a href="#cb26-525"></a>zarashivonna.</span>
<span id="cb26-526"><a href="#cb26-526"></a>yakurrren.</span>
<span id="cb26-527"><a href="#cb26-527"></a>jovon.</span>
<span id="cb26-528"><a href="#cb26-528"></a>malynn.</span>
<span id="cb26-529"><a href="#cb26-529"></a>vanna.</span>
<span id="cb26-530"><a href="#cb26-530"></a>caparmana.</span>
<span id="cb26-531"><a href="#cb26-531"></a>shantymonse.</span>
<span id="cb26-532"><a href="#cb26-532"></a><span class="in">```</span></span>
<span id="cb26-533"><a href="#cb26-533"></a></span>
<span id="cb26-534"><a href="#cb26-534"></a><span class="fu">## let’s fix the learning rate plot</span></span>
<span id="cb26-535"><a href="#cb26-535"></a></span>
<span id="cb26-536"><a href="#cb26-536"></a>The plot for <span class="in">`lossi`</span> looks very crazy, it's because the batch size of 32 is way too few so this time we got lucky, and next time we got un-lucky. And the mini-batch loss splashed too much. We should probaly fix it.</span>
<span id="cb26-537"><a href="#cb26-537"></a></span>
<span id="cb26-538"><a href="#cb26-538"></a>We pivot to a row for every 1000 observations of <span class="in">`lossi`</span> and calculate the mean, we end up have 200 oversevation which is easier to see.</span>
<span id="cb26-541"><a href="#cb26-541"></a><span class="in">```{python}</span></span>
<span id="cb26-542"><a href="#cb26-542"></a>plt.plot(torch.tensor(lossi).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1000</span>).mean(<span class="dv">1</span>))</span>
<span id="cb26-543"><a href="#cb26-543"></a><span class="in">```</span></span>
<span id="cb26-544"><a href="#cb26-544"></a>We can also observe the learning rate decay at 150k training loops.</span>
<span id="cb26-545"><a href="#cb26-545"></a></span>
<span id="cb26-546"><a href="#cb26-546"></a><span class="al">![`lossi` plot at the beginning](2_enhance_lossi.png)</span></span>
<span id="cb26-547"><a href="#cb26-547"></a></span>
<span id="cb26-548"><a href="#cb26-548"></a><span class="fu">## pytorchifying our code: layers, containers, `torch.nn`, fun bugs</span></span>
<span id="cb26-549"><a href="#cb26-549"></a></span>
<span id="cb26-550"><a href="#cb26-550"></a>Now we notice that we still have the embedding operation lying outside the pytorch-ified layers. It basically creating a lookup table <span class="in">`C`</span>, embedding it with our data <span class="in">`Y`</span> (or <span class="in">`Yb`</span>), then stretching out to row with <span class="in">`view()`</span> which is very cheap in PyTorch as no more memory creation is needed.</span>
<span id="cb26-551"><a href="#cb26-551"></a></span>
<span id="cb26-552"><a href="#cb26-552"></a>We modulize this by constructing 2 classes:</span>
<span id="cb26-553"><a href="#cb26-553"></a></span>
<span id="cb26-556"><a href="#cb26-556"></a><span class="in">```{python}</span></span>
<span id="cb26-557"><a href="#cb26-557"></a><span class="kw">class</span> Embedding:</span>
<span id="cb26-558"><a href="#cb26-558"></a>  </span>
<span id="cb26-559"><a href="#cb26-559"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_embeddings, embedding_dim):</span>
<span id="cb26-560"><a href="#cb26-560"></a>    <span class="va">self</span>.weight <span class="op">=</span> torch.randn((num_embeddings, embedding_dim))</span>
<span id="cb26-561"><a href="#cb26-561"></a>    </span>
<span id="cb26-562"><a href="#cb26-562"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, IX):</span>
<span id="cb26-563"><a href="#cb26-563"></a>    <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.weight[IX]</span>
<span id="cb26-564"><a href="#cb26-564"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb26-565"><a href="#cb26-565"></a>  </span>
<span id="cb26-566"><a href="#cb26-566"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb26-567"><a href="#cb26-567"></a>    <span class="cf">return</span> [<span class="va">self</span>.weight]</span>
<span id="cb26-568"><a href="#cb26-568"></a></span>
<span id="cb26-569"><a href="#cb26-569"></a><span class="co"># -----------------------------------------------------------------------------------------------</span></span>
<span id="cb26-570"><a href="#cb26-570"></a><span class="kw">class</span> Flatten:</span>
<span id="cb26-571"><a href="#cb26-571"></a></span>
<span id="cb26-572"><a href="#cb26-572"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb26-573"><a href="#cb26-573"></a>    <span class="va">self</span>.out <span class="op">=</span> x.view(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb26-574"><a href="#cb26-574"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb26-575"><a href="#cb26-575"></a>  </span>
<span id="cb26-576"><a href="#cb26-576"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb26-577"><a href="#cb26-577"></a>    <span class="cf">return</span> []</span>
<span id="cb26-578"><a href="#cb26-578"></a><span class="in">```</span></span>
<span id="cb26-579"><a href="#cb26-579"></a></span>
<span id="cb26-580"><a href="#cb26-580"></a>Now we can re-define the <span class="in">`layers`</span> like this:</span>
<span id="cb26-581"><a href="#cb26-581"></a></span>
<span id="cb26-584"><a href="#cb26-584"></a><span class="in">```{python}</span></span>
<span id="cb26-585"><a href="#cb26-585"></a>layers <span class="op">=</span> [</span>
<span id="cb26-586"><a href="#cb26-586"></a>  Embedding(vocab_size, n_embd),</span>
<span id="cb26-587"><a href="#cb26-587"></a>  Flatten(),</span>
<span id="cb26-588"><a href="#cb26-588"></a>  Linear(n_embd <span class="op">*</span> block_size, n_hidden, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb26-589"><a href="#cb26-589"></a>  BatchNorm1d(n_hidden),</span>
<span id="cb26-590"><a href="#cb26-590"></a>  Tanh(),</span>
<span id="cb26-591"><a href="#cb26-591"></a>  Linear(n_hidden, vocab_size),</span>
<span id="cb26-592"><a href="#cb26-592"></a>]</span>
<span id="cb26-593"><a href="#cb26-593"></a><span class="in">```</span></span>
<span id="cb26-594"><a href="#cb26-594"></a></span>
<span id="cb26-595"><a href="#cb26-595"></a>and also remove the <span class="in">`C`</span>, <span class="in">`emb`</span> definition in the forward pass construction. Going futher, we will be not only pytorchifying the elements of <span class="in">`layers`</span> only, but also the <span class="in">`layers`</span> itself. In PyTorch, we have term <span class="in">`containers`</span>, which specifying how we organize the layers in a network. And what are we doing here is constructing layers sequentially, which is equivalent to <span class="in">`Sequential`</span> in the <span class="in">`containers`</span>:</span>
<span id="cb26-596"><a href="#cb26-596"></a></span>
<span id="cb26-599"><a href="#cb26-599"></a><span class="in">```{python}</span></span>
<span id="cb26-600"><a href="#cb26-600"></a><span class="kw">class</span> Sequential:</span>
<span id="cb26-601"><a href="#cb26-601"></a>  </span>
<span id="cb26-602"><a href="#cb26-602"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, layers):</span>
<span id="cb26-603"><a href="#cb26-603"></a>    <span class="va">self</span>.layers <span class="op">=</span> layers</span>
<span id="cb26-604"><a href="#cb26-604"></a>  </span>
<span id="cb26-605"><a href="#cb26-605"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb26-606"><a href="#cb26-606"></a>    <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb26-607"><a href="#cb26-607"></a>      x <span class="op">=</span> layer(x)</span>
<span id="cb26-608"><a href="#cb26-608"></a>    <span class="va">self</span>.out <span class="op">=</span> x</span>
<span id="cb26-609"><a href="#cb26-609"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb26-610"><a href="#cb26-610"></a>  </span>
<span id="cb26-611"><a href="#cb26-611"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb26-612"><a href="#cb26-612"></a>    <span class="co"># get parameters of all layers and stretch them out into one list</span></span>
<span id="cb26-613"><a href="#cb26-613"></a>    <span class="cf">return</span> [p <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers <span class="cf">for</span> p <span class="kw">in</span> layer.parameters()]</span>
<span id="cb26-614"><a href="#cb26-614"></a><span class="in">```</span></span>
<span id="cb26-615"><a href="#cb26-615"></a></span>
<span id="cb26-616"><a href="#cb26-616"></a>and wrapp the layers into our <span class="in">`model`</span>:</span>
<span id="cb26-617"><a href="#cb26-617"></a></span>
<span id="cb26-620"><a href="#cb26-620"></a><span class="in">```{python}</span></span>
<span id="cb26-621"><a href="#cb26-621"></a>model <span class="op">=</span> Sequential([</span>
<span id="cb26-622"><a href="#cb26-622"></a>  Embedding(vocab_size, n_embd),</span>
<span id="cb26-623"><a href="#cb26-623"></a>  Flatten(),</span>
<span id="cb26-624"><a href="#cb26-624"></a>  Linear(n_embd <span class="op">*</span> block_size, n_hidden, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb26-625"><a href="#cb26-625"></a>  BatchNorm1d(n_hidden),</span>
<span id="cb26-626"><a href="#cb26-626"></a>  Tanh(),</span>
<span id="cb26-627"><a href="#cb26-627"></a>  Linear(n_hidden, vocab_size),</span>
<span id="cb26-628"><a href="#cb26-628"></a>])</span>
<span id="cb26-629"><a href="#cb26-629"></a><span class="in">```</span></span>
<span id="cb26-630"><a href="#cb26-630"></a></span>
<span id="cb26-631"><a href="#cb26-631"></a><span class="fu"># 2 implementing `WaveNet`</span></span>
<span id="cb26-632"><a href="#cb26-632"></a></span>
<span id="cb26-633"><a href="#cb26-633"></a>So far with the classical MLP following Bengio et al. (2003), we have a *embedding layer* followed by a *hidden layer* and end up with a *activation layer*. Although we added more layer after the embedding, we could not make a significant progress.</span>
<span id="cb26-634"><a href="#cb26-634"></a></span>
<span id="cb26-635"><a href="#cb26-635"></a>The problem is we dont have a **naive way** of making the model bigger in a productive way. We are still in the case that we crushing all the characters into a single all the way at the begining. And even if we make this a bigger layer and add neurons it's still like silly to squash all that information so fast into a single step.</span>
<span id="cb26-636"><a href="#cb26-636"></a></span>
<span id="cb26-637"><a href="#cb26-637"></a><span class="fu">## overview: `WaveNet`</span></span>
<span id="cb26-638"><a href="#cb26-638"></a></span>
<span id="cb26-639"><a href="#cb26-639"></a><span class="al">![Visualization of the `WaveNet` idea - Progressive Fusion](fig3_wavenet.png)</span></span>
<span id="cb26-640"><a href="#cb26-640"></a></span>
<span id="cb26-641"><a href="#cb26-641"></a><span class="fu">## dataset bump the context size to 8</span></span>
<span id="cb26-642"><a href="#cb26-642"></a></span>
<span id="cb26-643"><a href="#cb26-643"></a>first we change the <span class="in">`block_size`</span> into <span class="in">`8`</span> and now our dataset looks like:</span>
<span id="cb26-644"><a href="#cb26-644"></a></span>
<span id="cb26-645"><a href="#cb26-645"></a><span class="in">```md</span></span>
<span id="cb26-646"><a href="#cb26-646"></a>........ ---&gt; y</span>
<span id="cb26-647"><a href="#cb26-647"></a>.......y ---&gt; u</span>
<span id="cb26-648"><a href="#cb26-648"></a>......yu ---&gt; h</span>
<span id="cb26-649"><a href="#cb26-649"></a>.....yuh ---&gt; e</span>
<span id="cb26-650"><a href="#cb26-650"></a>....yuhe ---&gt; n</span>
<span id="cb26-651"><a href="#cb26-651"></a>...yuhen ---&gt; g</span>
<span id="cb26-652"><a href="#cb26-652"></a>..yuheng ---&gt; .</span>
<span id="cb26-653"><a href="#cb26-653"></a>........ ---&gt; d</span>
<span id="cb26-654"><a href="#cb26-654"></a>.......d ---&gt; i</span>
<span id="cb26-655"><a href="#cb26-655"></a>......di ---&gt; o</span>
<span id="cb26-656"><a href="#cb26-656"></a>.....dio ---&gt; n</span>
<span id="cb26-657"><a href="#cb26-657"></a>....dion ---&gt; d</span>
<span id="cb26-658"><a href="#cb26-658"></a>...diond ---&gt; r</span>
<span id="cb26-659"><a href="#cb26-659"></a>..diondr ---&gt; e</span>
<span id="cb26-660"><a href="#cb26-660"></a>.diondre ---&gt; .</span>
<span id="cb26-661"><a href="#cb26-661"></a>........ ---&gt; x</span>
<span id="cb26-662"><a href="#cb26-662"></a>.......x ---&gt; a</span>
<span id="cb26-663"><a href="#cb26-663"></a>......xa ---&gt; v</span>
<span id="cb26-664"><a href="#cb26-664"></a>.....xav ---&gt; i</span>
<span id="cb26-665"><a href="#cb26-665"></a>....xavi ---&gt; e</span>
<span id="cb26-666"><a href="#cb26-666"></a><span class="in">```</span></span>
<span id="cb26-667"><a href="#cb26-667"></a></span>
<span id="cb26-668"><a href="#cb26-668"></a>The model size now bumps to 22k.</span>
<span id="cb26-669"><a href="#cb26-669"></a></span>
<span id="cb26-670"><a href="#cb26-670"></a><span class="fu">## re-running baseline code on block_size 8</span></span>
<span id="cb26-671"><a href="#cb26-671"></a></span>
<span id="cb26-672"><a href="#cb26-672"></a><span class="fu">## implementing `WaveNet`</span></span>
<span id="cb26-673"><a href="#cb26-673"></a></span>
<span id="cb26-674"><a href="#cb26-674"></a><span class="fu">## training the `WaveNet`: first pass</span></span>
<span id="cb26-675"><a href="#cb26-675"></a></span>
<span id="cb26-676"><a href="#cb26-676"></a><span class="fu">## fixing `batchnorm1d` bug</span></span>
<span id="cb26-677"><a href="#cb26-677"></a></span>
<span id="cb26-678"><a href="#cb26-678"></a><span class="fu">## re-training `WaveNet` with bug fix</span></span>
<span id="cb26-679"><a href="#cb26-679"></a></span>
<span id="cb26-680"><a href="#cb26-680"></a><span class="fu">## scaling up our `WaveNet`</span></span>
<span id="cb26-681"><a href="#cb26-681"></a></span>
<span id="cb26-682"><a href="#cb26-682"></a><span class="fu"># 3 conclusions</span></span>
<span id="cb26-683"><a href="#cb26-683"></a></span>
<span id="cb26-684"><a href="#cb26-684"></a><span class="fu">## performance log</span></span>
<span id="cb26-685"><a href="#cb26-685"></a></span>
<span id="cb26-686"><a href="#cb26-686"></a>| Step | What we did                                                     | Loss we got (accum)      |</span>
<span id="cb26-687"><a href="#cb26-687"></a>|---------|------------------------------|----------------------------------|</span>
<span id="cb26-688"><a href="#cb26-688"></a>| 1    | original (3 character context + 200 hidden neurons, 12K params) | train 2.0467958450317383 </span>
<span id="cb26-689"><a href="#cb26-689"></a>                                                                          val 2.0989298820495605    |</span>
<span id="cb26-690"><a href="#cb26-690"></a>| 2    | context: 3 -<span class="sc">\&gt;</span> 8 (22K params)                                   |                          |</span>
<span id="cb26-691"><a href="#cb26-691"></a>| 3    | flat -<span class="sc">\&gt;</span> hierarchical (22K params)                              |                          |</span>
<span id="cb26-692"><a href="#cb26-692"></a>| 4    | fix bug in batchnorm                                            |                          |</span>
<span id="cb26-693"><a href="#cb26-693"></a>| 5    | scale up the network: n_embd 24, n_hidden 128 (76K params)      |                          |</span>
<span id="cb26-694"><a href="#cb26-694"></a></span>
<span id="cb26-695"><a href="#cb26-695"></a><span class="fu">## experimental harness</span></span>
<span id="cb26-696"><a href="#cb26-696"></a></span>
<span id="cb26-697"><a href="#cb26-697"></a><span class="fu">## `WaveNet` but with “dilated causal convolutions”</span></span>
<span id="cb26-698"><a href="#cb26-698"></a></span>
<span id="cb26-699"><a href="#cb26-699"></a><span class="fu">## `torch.nn`</span></span>
<span id="cb26-700"><a href="#cb26-700"></a></span>
<span id="cb26-701"><a href="#cb26-701"></a><span class="fu">## the development process of building deep neural nets</span></span>
<span id="cb26-702"><a href="#cb26-702"></a></span>
<span id="cb26-703"><a href="#cb26-703"></a><span class="fu">## going forward</span></span>
<span id="cb26-704"><a href="#cb26-704"></a></span>
<span id="cb26-705"><a href="#cb26-705"></a><span class="fu">## improve on my loss! how far can we improve a `WaveNet` on this data?</span></span>
<span id="cb26-706"><a href="#cb26-706"></a></span>
<span id="cb26-707"><a href="#cb26-707"></a><span class="fu"># 4 resources</span></span>
<span id="cb26-708"><a href="#cb26-708"></a></span>
<span id="cb26-709"><a href="#cb26-709"></a><span class="ss">1.  </span>WaveNet 2016 from DeepMind: <span class="ot">&lt;https://arxiv.org/abs/1609.03499&gt;</span>;</span>
<span id="cb26-710"><a href="#cb26-710"></a><span class="ss">2.  </span>Bengio et al. 2003 MLP LM: <span class="ot">&lt;https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&gt;</span>;</span>
<span id="cb26-711"><a href="#cb26-711"></a><span class="ss">3.  </span>Notebook: <span class="ot">&lt;https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part5_cnn1.ipynb&gt;</span>;</span>
<span id="cb26-712"><a href="#cb26-712"></a><span class="ss">4.  </span>DeepMind's blog post from 2016: <span class="ot">&lt;https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/&gt;</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block"><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"><i class="fa-brands fa-creative-commons" aria-label="creative-commons"></i></a> 2023-2024 Le Khac Tuan</span></p>
</div>   
    <div class="nav-footer-center">
<p><span class="faux-block">Designed with <i class="fa-solid fa-heart" aria-label="heart"></i></span></p>
</div>
    <div class="nav-footer-right">
<p><span class="faux-block">Made with <a href="https://quarto.org/">Quarto</a></span></p>
</div>
  </div>
</footer>




</body></html>