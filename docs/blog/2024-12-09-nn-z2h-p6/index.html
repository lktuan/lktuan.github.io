<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tuan Le Khac">
<meta name="dcterms.date" content="2024-12-09">
<meta name="description" content="This is Tuan’s blog">

<title>NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet – Le Khac Tuan</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../img/rocket_1613268.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-8e54dfbe729680b42f22c627ac27a053.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-81b089a3b74ed4cf194f083418e7130b.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-89cb849060b93fd12025e4f44aaa3f02.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-27750d9d09892f3d6a52d1fd788c41c6.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Le Khac Tuan</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../curriculum/index.html"> 
<span class="menu-text">Curriculum</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../学汉语的日记.html"> 
<span class="menu-text">学汉语的日记</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../jiu_jitsu_journal/index.html"> 
<span class="menu-text">Jiu Jitsu Journal</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lktuan"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/tuanlekhac/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.facebook.com/toilatuan.lk/"> <i class="bi bi-facebook" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/Halle4231"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/@tuan_lekhac"> <i class="bi bi-youtube" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:tuan.lekhac0905@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">NN-Z2H Lesson 6: Building makemore part 5 - Building a <code>WaveNet</code></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                  <div>
        <div class="description">
          CNN/<code>WaveNet</code> and a lot of reading of documentation, keeping track of multidimensional tensor shapes, moving between jupyter notebooks and repository code, …
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">til</div>
                <div class="quarto-category">python</div>
                <div class="quarto-category">andrej karpathy</div>
                <div class="quarto-category">nn-z2h</div>
                <div class="quarto-category">neural networks</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://lktuan.github.io/">Tuan Le Khac</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 9, 2024</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">December 9, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#intro" id="toc-intro" class="nav-link active" data-scroll-target="#intro">1 intro</a>
  <ul class="collapse">
  <li><a href="#starter-code-walk-through" id="toc-starter-code-walk-through" class="nav-link" data-scroll-target="#starter-code-walk-through">starter code walk through</a></li>
  <li><a href="#lets-fix-the-learning-rate-plot" id="toc-lets-fix-the-learning-rate-plot" class="nav-link" data-scroll-target="#lets-fix-the-learning-rate-plot">let’s fix the learning rate plot</a></li>
  <li><a href="#pytorchifying-our-code-layers-containers-torch.nn-fun-bugs" id="toc-pytorchifying-our-code-layers-containers-torch.nn-fun-bugs" class="nav-link" data-scroll-target="#pytorchifying-our-code-layers-containers-torch.nn-fun-bugs">pytorchifying our code: layers, containers, <code>torch.nn</code>, fun bugs</a></li>
  </ul></li>
  <li><a href="#implementing-wavenet" id="toc-implementing-wavenet" class="nav-link" data-scroll-target="#implementing-wavenet">2 implementing <code>WaveNet</code></a>
  <ul class="collapse">
  <li><a href="#overview-wavenet" id="toc-overview-wavenet" class="nav-link" data-scroll-target="#overview-wavenet">overview: <code>WaveNet</code></a></li>
  <li><a href="#dataset-bump-the-context-size-to-8" id="toc-dataset-bump-the-context-size-to-8" class="nav-link" data-scroll-target="#dataset-bump-the-context-size-to-8">dataset bump the context size to 8</a></li>
  <li><a href="#re-running-baseline-code-on-block_size-8" id="toc-re-running-baseline-code-on-block_size-8" class="nav-link" data-scroll-target="#re-running-baseline-code-on-block_size-8">re-running baseline code on <code>block_size = 8</code></a></li>
  <li><a href="#implementing-wavenet-1" id="toc-implementing-wavenet-1" class="nav-link" data-scroll-target="#implementing-wavenet-1">implementing <code>WaveNet</code></a></li>
  <li><a href="#training-the-wavenet-first-pass" id="toc-training-the-wavenet-first-pass" class="nav-link" data-scroll-target="#training-the-wavenet-first-pass">training the <code>WaveNet</code>: first pass</a></li>
  <li><a href="#fixing-batchnorm1d-bug" id="toc-fixing-batchnorm1d-bug" class="nav-link" data-scroll-target="#fixing-batchnorm1d-bug">fixing <code>batchnorm1d</code> bug</a></li>
  <li><a href="#re-training-wavenet-with-bug-fix" id="toc-re-training-wavenet-with-bug-fix" class="nav-link" data-scroll-target="#re-training-wavenet-with-bug-fix">re-training <code>WaveNet</code> with bug fix</a></li>
  <li><a href="#scaling-up-our-wavenet" id="toc-scaling-up-our-wavenet" class="nav-link" data-scroll-target="#scaling-up-our-wavenet">scaling up our <code>WaveNet</code></a></li>
  </ul></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions">3 conclusions</a>
  <ul class="collapse">
  <li><a href="#performance-log" id="toc-performance-log" class="nav-link" data-scroll-target="#performance-log">performance log</a></li>
  <li><a href="#experimental-harness" id="toc-experimental-harness" class="nav-link" data-scroll-target="#experimental-harness">experimental harness</a></li>
  <li><a href="#wavenet-but-with-dilated-causal-convolutions" id="toc-wavenet-but-with-dilated-causal-convolutions" class="nav-link" data-scroll-target="#wavenet-but-with-dilated-causal-convolutions"><code>WaveNet</code> but with “dilated causal convolutions”</a></li>
  <li><a href="#torch.nn" id="toc-torch.nn" class="nav-link" data-scroll-target="#torch.nn"><code>torch.nn</code></a></li>
  <li><a href="#the-development-process-of-building-deep-neural-nets-going-forward" id="toc-the-development-process-of-building-deep-neural-nets-going-forward" class="nav-link" data-scroll-target="#the-development-process-of-building-deep-neural-nets-going-forward">the development process of building deep neural nets &amp; going forward</a></li>
  </ul></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">4 resources</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="callout callout-style-default callout-important callout-titled" title="This is not orginal content!">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This is not orginal content!
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is my study notes / codes along with Andrej Karpathy’s “<a href="https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Neural Networks: Zero to Hero</a>” series.</p>
</div>
</div>
<p><em>Codes are executed in Colab, this calculation capacity exceeds my computer’s ability.</em></p>
<section id="intro" class="level1">
<h1>1 intro</h1>
<p>We are going to take the 2-layer MLP in the part 3 of <code>makemore</code> and complexify it by:</p>
<ul>
<li>extending the block size: from 3 to 8 characters;</li>
<li>making it deeper rather than 1 hidden layer.</li>
</ul>
<p>then end of with a Convoluntional Neural Network architecture similar to <code>WaveNet</code> (2016) by <a href="https://deepmind.google/">Google DeepMind</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="wavenet.png" class="img-fluid figure-img"></p>
<figcaption>WaveNet model architecture, <a href="https://www.researchgate.net/figure/WaveNet-Model-Architecture-38_fig2_380566531">source</a></figcaption>
</figure>
</div>
<section id="starter-code-walk-through" class="level2">
<h2 class="anchored" data-anchor-id="starter-code-walk-through">starter code walk through</h2>
<section id="import-libraries" class="level4">
<h4 class="anchored" data-anchor-id="import-libraries">import libraries</h4>
<div id="c0f047f6" class="cell" data-execution_count="1">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="reading-data" class="level4">
<h4 class="anchored" data-anchor-id="reading-data">reading data</h4>
<div id="e1d665af" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2"></a></span>
<span id="cb2-3"><a href="#cb2-3"></a>url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt"</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>words <span class="op">=</span> pd.read_csv(url, header<span class="op">=</span><span class="va">None</span>).iloc[:, <span class="dv">0</span>].tolist()</span>
<span id="cb2-5"><a href="#cb2-5"></a>words[:<span class="dv">8</span>]</span>
<span id="cb2-6"><a href="#cb2-6"></a></span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="co"># &gt;&gt;&gt; ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="building-vocab" class="level4">
<h4 class="anchored" data-anchor-id="building-vocab">building vocab</h4>
<div id="2647be2b" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># build the vocabulary of characters and mapping to/from integer</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(<span class="st">''</span>.join(words))))</span>
<span id="cb3-3"><a href="#cb3-3"></a>stoi <span class="op">=</span> {s:i<span class="op">+</span><span class="dv">1</span> <span class="cf">for</span> i, s <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb3-4"><a href="#cb3-4"></a>stoi[<span class="st">'.'</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-5"><a href="#cb3-5"></a>itos <span class="op">=</span> {i: s <span class="cf">for</span> s, i <span class="kw">in</span> stoi.items()}</span>
<span id="cb3-6"><a href="#cb3-6"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(itos)</span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="bu">print</span>(itos)</span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="bu">print</span>(vocab_size)</span>
<span id="cb3-9"><a href="#cb3-9"></a></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="co"># itos: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j',</span></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="co"># 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't',</span></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="co"># 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}</span></span>
<span id="cb3-13"><a href="#cb3-13"></a><span class="co"># vocab_size: 27</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="initializing-randomization" class="level4">
<h4 class="anchored" data-anchor-id="initializing-randomization">initializing randomization</h4>
<div id="226cb206" class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">import</span> random</span>
<span id="cb4-2"><a href="#cb4-2"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb4-3"><a href="#cb4-3"></a>random.shuffle(words)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="create-traindevtest-splits" class="level4">
<h4 class="anchored" data-anchor-id="create-traindevtest-splits">create train/dev/test splits</h4>
<div id="0f6eec99" class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>block_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># context length: how many characters do we take to predict the next one?</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="co"># build the dataset</span></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="kw">def</span> buid_dataset(words):</span>
<span id="cb5-4"><a href="#cb5-4"></a>    X, Y <span class="op">=</span> [], []</span>
<span id="cb5-5"><a href="#cb5-5"></a></span>
<span id="cb5-6"><a href="#cb5-6"></a>    <span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb5-7"><a href="#cb5-7"></a>        context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size</span>
<span id="cb5-8"><a href="#cb5-8"></a>        <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb5-9"><a href="#cb5-9"></a>            ix <span class="op">=</span> stoi[ch]</span>
<span id="cb5-10"><a href="#cb5-10"></a>            X.append(context)</span>
<span id="cb5-11"><a href="#cb5-11"></a>            Y.append(ix)</span>
<span id="cb5-12"><a href="#cb5-12"></a>            context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix]</span>
<span id="cb5-13"><a href="#cb5-13"></a></span>
<span id="cb5-14"><a href="#cb5-14"></a>    X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb5-15"><a href="#cb5-15"></a>    Y <span class="op">=</span> torch.tensor(Y)</span>
<span id="cb5-16"><a href="#cb5-16"></a>    <span class="bu">print</span>(X.shape, Y.shape)</span>
<span id="cb5-17"><a href="#cb5-17"></a>    <span class="cf">return</span> X, Y</span>
<span id="cb5-18"><a href="#cb5-18"></a></span>
<span id="cb5-19"><a href="#cb5-19"></a>n1 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb5-20"><a href="#cb5-20"></a>n2 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb5-21"><a href="#cb5-21"></a></span>
<span id="cb5-22"><a href="#cb5-22"></a>Xtr, Ytr <span class="op">=</span> buid_dataset(words[:n1])        <span class="co"># 80#</span></span>
<span id="cb5-23"><a href="#cb5-23"></a>Xdev, Ydev <span class="op">=</span> buid_dataset(words[n1:n2])    <span class="co"># 10%</span></span>
<span id="cb5-24"><a href="#cb5-24"></a>Xte, Yte <span class="op">=</span> buid_dataset(words[n2:])        <span class="co"># 10%</span></span>
<span id="cb5-25"><a href="#cb5-25"></a></span>
<span id="cb5-26"><a href="#cb5-26"></a><span class="co"># torch.Size([182625, 3]) torch.Size([182625])</span></span>
<span id="cb5-27"><a href="#cb5-27"></a><span class="co"># torch.Size([22655, 3]) torch.Size([22655])</span></span>
<span id="cb5-28"><a href="#cb5-28"></a><span class="co"># torch.Size([22866, 3]) torch.Size([22866])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="input-and-response-preview" class="level4">
<h4 class="anchored" data-anchor-id="input-and-response-preview">input and response preview</h4>
<div id="f9315522" class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(Xtr[:<span class="dv">20</span>], Ytr[:<span class="dv">20</span>]):</span>
<span id="cb6-2"><a href="#cb6-2"></a>  <span class="bu">print</span>(<span class="st">''</span>.join(itos[ix.item()] <span class="cf">for</span> ix <span class="kw">in</span> x), <span class="st">'---&gt;'</span>, itos[y.item()])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource md number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1"></a>... ---&gt; y</span>
<span id="cb7-2"><a href="#cb7-2"></a>..y ---&gt; u</span>
<span id="cb7-3"><a href="#cb7-3"></a>.yu ---&gt; h</span>
<span id="cb7-4"><a href="#cb7-4"></a>yuh ---&gt; e</span>
<span id="cb7-5"><a href="#cb7-5"></a>uhe ---&gt; n</span>
<span id="cb7-6"><a href="#cb7-6"></a>hen ---&gt; g</span>
<span id="cb7-7"><a href="#cb7-7"></a>eng ---&gt; .</span>
<span id="cb7-8"><a href="#cb7-8"></a>... ---&gt; d</span>
<span id="cb7-9"><a href="#cb7-9"></a>..d ---&gt; i</span>
<span id="cb7-10"><a href="#cb7-10"></a>.di ---&gt; o</span>
<span id="cb7-11"><a href="#cb7-11"></a>dio ---&gt; n</span>
<span id="cb7-12"><a href="#cb7-12"></a>ion ---&gt; d</span>
<span id="cb7-13"><a href="#cb7-13"></a>ond ---&gt; r</span>
<span id="cb7-14"><a href="#cb7-14"></a>ndr ---&gt; e</span>
<span id="cb7-15"><a href="#cb7-15"></a>dre ---&gt; .</span>
<span id="cb7-16"><a href="#cb7-16"></a>... ---&gt; x</span>
<span id="cb7-17"><a href="#cb7-17"></a>..x ---&gt; a</span>
<span id="cb7-18"><a href="#cb7-18"></a>.xa ---&gt; v</span>
<span id="cb7-19"><a href="#cb7-19"></a>xav ---&gt; i</span>
<span id="cb7-20"><a href="#cb7-20"></a>avi ---&gt; e</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="initializing-objects-in-networks" class="level4">
<h4 class="anchored" data-anchor-id="initializing-objects-in-networks">initializing objects in networks</h4>
<p>Near copy paste of the layers we have developed in Part 3, I added some docstring to the classes.</p>
<section id="class-linear" class="level5">
<h5 class="anchored" data-anchor-id="class-linear">class <code>Linear</code></h5>
<div id="5c4029f3" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="kw">class</span> Linear:</span>
<span id="cb8-2"><a href="#cb8-2"></a>  <span class="co">"""</span></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="co">  Applies an affine linear transformation to the incoming data: y = xA^T + b.</span></span>
<span id="cb8-4"><a href="#cb8-4"></a></span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="co">  This class implements a linear (fully connected) layer, which performs a linear</span></span>
<span id="cb8-6"><a href="#cb8-6"></a><span class="co">  transformation on the input tensor. It is typically used in neural network architectures</span></span>
<span id="cb8-7"><a href="#cb8-7"></a><span class="co">  to transform input features between layers.</span></span>
<span id="cb8-8"><a href="#cb8-8"></a></span>
<span id="cb8-9"><a href="#cb8-9"></a><span class="co">  Args:</span></span>
<span id="cb8-10"><a href="#cb8-10"></a><span class="co">      fan_in (int): Number of input features (input dimension).</span></span>
<span id="cb8-11"><a href="#cb8-11"></a><span class="co">      fan_out (int): Number of output features (output dimension).</span></span>
<span id="cb8-12"><a href="#cb8-12"></a><span class="co">      bias (bool, optional): Whether to include a learnable bias term.</span></span>
<span id="cb8-13"><a href="#cb8-13"></a><span class="co">          Defaults to True.</span></span>
<span id="cb8-14"><a href="#cb8-14"></a></span>
<span id="cb8-15"><a href="#cb8-15"></a><span class="co">  Attributes:</span></span>
<span id="cb8-16"><a href="#cb8-16"></a><span class="co">      weight (torch.Tensor): Weight matrix of shape (fan_in, fan_out),</span></span>
<span id="cb8-17"><a href="#cb8-17"></a><span class="co">          initialized using Kaiming initialization.</span></span>
<span id="cb8-18"><a href="#cb8-18"></a><span class="co">      bias (torch.Tensor or None): Bias vector of shape (fan_out),</span></span>
<span id="cb8-19"><a href="#cb8-19"></a><span class="co">          initialized to zeros if bias is True, otherwise None.</span></span>
<span id="cb8-20"><a href="#cb8-20"></a></span>
<span id="cb8-21"><a href="#cb8-21"></a><span class="co">  Methods:</span></span>
<span id="cb8-22"><a href="#cb8-22"></a><span class="co">      __call__(x): Applies the linear transformation to the input tensor x.</span></span>
<span id="cb8-23"><a href="#cb8-23"></a><span class="co">      parameters(): Returns a list of trainable parameters (weight and bias).</span></span>
<span id="cb8-24"><a href="#cb8-24"></a></span>
<span id="cb8-25"><a href="#cb8-25"></a><span class="co">  Example:</span></span>
<span id="cb8-26"><a href="#cb8-26"></a><span class="co">      &gt;&gt;&gt; layer = Linear(10, 5)  # Creates a linear layer with 10 input features and 5 output features</span></span>
<span id="cb8-27"><a href="#cb8-27"></a><span class="co">      &gt;&gt;&gt; x = torch.randn(3, 10)  # Input tensor with batch size 3 and 10 features</span></span>
<span id="cb8-28"><a href="#cb8-28"></a><span class="co">      &gt;&gt;&gt; output = layer(x)  # Applies linear transformation</span></span>
<span id="cb8-29"><a href="#cb8-29"></a><span class="co">      &gt;&gt;&gt; output.shape</span></span>
<span id="cb8-30"><a href="#cb8-30"></a><span class="co">      torch.Size([3, 5])</span></span>
<span id="cb8-31"><a href="#cb8-31"></a><span class="co">  """</span></span>
<span id="cb8-32"><a href="#cb8-32"></a></span>
<span id="cb8-33"><a href="#cb8-33"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, fan_in, fan_out, bias<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb8-34"><a href="#cb8-34"></a>    <span class="va">self</span>.weight <span class="op">=</span> torch.randn((fan_in, fan_out)) <span class="op">/</span> fan_in<span class="op">**</span><span class="fl">0.5</span> <span class="co"># note: kaiming init</span></span>
<span id="cb8-35"><a href="#cb8-35"></a>    <span class="va">self</span>.bias <span class="op">=</span> torch.zeros(fan_out) <span class="cf">if</span> bias <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb8-36"><a href="#cb8-36"></a></span>
<span id="cb8-37"><a href="#cb8-37"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb8-38"><a href="#cb8-38"></a>    <span class="va">self</span>.out <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.weight</span>
<span id="cb8-39"><a href="#cb8-39"></a>    <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb8-40"><a href="#cb8-40"></a>      <span class="va">self</span>.out <span class="op">+=</span> <span class="va">self</span>.bias</span>
<span id="cb8-41"><a href="#cb8-41"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb8-42"><a href="#cb8-42"></a></span>
<span id="cb8-43"><a href="#cb8-43"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb8-44"><a href="#cb8-44"></a>    <span class="cf">return</span> [<span class="va">self</span>.weight] <span class="op">+</span> ([] <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> [<span class="va">self</span>.bias])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="class-batchnorm1d" class="level5">
<h5 class="anchored" data-anchor-id="class-batchnorm1d">class <code>BatchNorm1d</code></h5>
<div id="964c549f" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="kw">class</span> BatchNorm1d:</span>
<span id="cb9-2"><a href="#cb9-2"></a>  <span class="co">"""</span></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="co">  Applies Batch Normalization to the input tensor, a technique to improve</span></span>
<span id="cb9-4"><a href="#cb9-4"></a><span class="co">  training stability and performance in deep neural networks.</span></span>
<span id="cb9-5"><a href="#cb9-5"></a></span>
<span id="cb9-6"><a href="#cb9-6"></a><span class="co">  Batch Normalization normalizes the input across the batch dimension,</span></span>
<span id="cb9-7"><a href="#cb9-7"></a><span class="co">  reducing internal covariate shift and allowing higher learning rates.</span></span>
<span id="cb9-8"><a href="#cb9-8"></a><span class="co">  This implementation supports both training and inference modes.</span></span>
<span id="cb9-9"><a href="#cb9-9"></a></span>
<span id="cb9-10"><a href="#cb9-10"></a><span class="co">  Args:</span></span>
<span id="cb9-11"><a href="#cb9-11"></a><span class="co">      dim (int): Number of features or channels to be normalized.</span></span>
<span id="cb9-12"><a href="#cb9-12"></a><span class="co">      eps (float, optional): A small constant added to the denominator for</span></span>
<span id="cb9-13"><a href="#cb9-13"></a><span class="co">          numerical stability to prevent division by zero.</span></span>
<span id="cb9-14"><a href="#cb9-14"></a><span class="co">          Defaults to 1e-5.</span></span>
<span id="cb9-15"><a href="#cb9-15"></a><span class="co">      momentum (float, optional): Momentum for updating running mean and</span></span>
<span id="cb9-16"><a href="#cb9-16"></a><span class="co">          variance during training. Controls the degree of exponential</span></span>
<span id="cb9-17"><a href="#cb9-17"></a><span class="co">          moving average. Defaults to 0.1.</span></span>
<span id="cb9-18"><a href="#cb9-18"></a></span>
<span id="cb9-19"><a href="#cb9-19"></a><span class="co">  Attributes:</span></span>
<span id="cb9-20"><a href="#cb9-20"></a><span class="co">      eps (float): Epsilon value for numerical stability.</span></span>
<span id="cb9-21"><a href="#cb9-21"></a><span class="co">      momentum (float): Momentum for running statistics update.</span></span>
<span id="cb9-22"><a href="#cb9-22"></a><span class="co">      training (bool): Indicates whether the layer is in training or inference mode.</span></span>
<span id="cb9-23"><a href="#cb9-23"></a><span class="co">      gamma (torch.Tensor): Learnable scale parameter of shape (dim,).</span></span>
<span id="cb9-24"><a href="#cb9-24"></a><span class="co">      beta (torch.Tensor): Learnable shift parameter of shape (dim,).</span></span>
<span id="cb9-25"><a href="#cb9-25"></a><span class="co">      running_mean (torch.Tensor): Exponential moving average of batch means.</span></span>
<span id="cb9-26"><a href="#cb9-26"></a><span class="co">      running_var (torch.Tensor): Exponential moving average of batch variances.</span></span>
<span id="cb9-27"><a href="#cb9-27"></a></span>
<span id="cb9-28"><a href="#cb9-28"></a><span class="co">  Methods:</span></span>
<span id="cb9-29"><a href="#cb9-29"></a><span class="co">      __call__(x): Applies batch normalization to the input tensor.</span></span>
<span id="cb9-30"><a href="#cb9-30"></a><span class="co">      parameters(): Returns learnable parameters (gamma and beta).</span></span>
<span id="cb9-31"><a href="#cb9-31"></a></span>
<span id="cb9-32"><a href="#cb9-32"></a><span class="co">  Key Normalization Steps:</span></span>
<span id="cb9-33"><a href="#cb9-33"></a><span class="co">  1. Compute batch mean and variance (in training mode)</span></span>
<span id="cb9-34"><a href="#cb9-34"></a><span class="co">  2. Normalize input by subtracting mean and dividing by standard deviation</span></span>
<span id="cb9-35"><a href="#cb9-35"></a><span class="co">  3. Apply learnable scale (gamma) and shift (beta) parameters</span></span>
<span id="cb9-36"><a href="#cb9-36"></a><span class="co">  4. Update running statistics during training</span></span>
<span id="cb9-37"><a href="#cb9-37"></a></span>
<span id="cb9-38"><a href="#cb9-38"></a><span class="co">  Example:</span></span>
<span id="cb9-39"><a href="#cb9-39"></a><span class="co">      &gt;&gt;&gt; batch_norm = BatchNorm1d(64)  # For 64-channel input</span></span>
<span id="cb9-40"><a href="#cb9-40"></a><span class="co">      &gt;&gt;&gt; x = torch.randn(32, 64)  # Batch of 32 samples with 64 features</span></span>
<span id="cb9-41"><a href="#cb9-41"></a><span class="co">      &gt;&gt;&gt; normalized_x = batch_norm(x)  # Apply batch normalization</span></span>
<span id="cb9-42"><a href="#cb9-42"></a><span class="co">      &gt;&gt;&gt; normalized_x.shape</span></span>
<span id="cb9-43"><a href="#cb9-43"></a><span class="co">      torch.Size([32, 64])</span></span>
<span id="cb9-44"><a href="#cb9-44"></a></span>
<span id="cb9-45"><a href="#cb9-45"></a><span class="co">  Note:</span></span>
<span id="cb9-46"><a href="#cb9-46"></a><span class="co">      - Supports both 2D (batch, features) and 3D (batch, channels, sequence) input tensors</span></span>
<span id="cb9-47"><a href="#cb9-47"></a><span class="co">      - During inference, uses running statistics instead of batch statistics</span></span>
<span id="cb9-48"><a href="#cb9-48"></a><span class="co">  """</span></span>
<span id="cb9-49"><a href="#cb9-49"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, eps<span class="op">=</span><span class="fl">1e-5</span>, momentum<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb9-50"><a href="#cb9-50"></a>    <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb9-51"><a href="#cb9-51"></a>    <span class="va">self</span>.momentum <span class="op">=</span> momentum</span>
<span id="cb9-52"><a href="#cb9-52"></a>    <span class="va">self</span>.training <span class="op">=</span> <span class="va">True</span></span>
<span id="cb9-53"><a href="#cb9-53"></a>    <span class="co"># parameters (trained with backprop)</span></span>
<span id="cb9-54"><a href="#cb9-54"></a>    <span class="va">self</span>.gamma <span class="op">=</span> torch.ones(dim)</span>
<span id="cb9-55"><a href="#cb9-55"></a>    <span class="va">self</span>.beta <span class="op">=</span> torch.zeros(dim)</span>
<span id="cb9-56"><a href="#cb9-56"></a>    <span class="co"># buffers (trained with a running 'momentum update')</span></span>
<span id="cb9-57"><a href="#cb9-57"></a>    <span class="va">self</span>.running_mean <span class="op">=</span> torch.zeros(dim)</span>
<span id="cb9-58"><a href="#cb9-58"></a>    <span class="va">self</span>.running_var <span class="op">=</span> torch.ones(dim)</span>
<span id="cb9-59"><a href="#cb9-59"></a></span>
<span id="cb9-60"><a href="#cb9-60"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb9-61"><a href="#cb9-61"></a>    <span class="co"># calculate the forward pass</span></span>
<span id="cb9-62"><a href="#cb9-62"></a>    <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb9-63"><a href="#cb9-63"></a>      xmean <span class="op">=</span> x.mean(dim, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch mean</span></span>
<span id="cb9-64"><a href="#cb9-64"></a>      xvar <span class="op">=</span> x.var(dim, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch variance</span></span>
<span id="cb9-65"><a href="#cb9-65"></a>    <span class="cf">else</span>:</span>
<span id="cb9-66"><a href="#cb9-66"></a>      xmean <span class="op">=</span> <span class="va">self</span>.running_mean</span>
<span id="cb9-67"><a href="#cb9-67"></a>      xvar <span class="op">=</span> <span class="va">self</span>.running_var</span>
<span id="cb9-68"><a href="#cb9-68"></a>    xhat <span class="op">=</span> (x <span class="op">-</span> xmean) <span class="op">/</span> torch.sqrt(xvar <span class="op">+</span> <span class="va">self</span>.eps) <span class="co"># normalize to unit variance</span></span>
<span id="cb9-69"><a href="#cb9-69"></a>    <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.gamma <span class="op">*</span> xhat <span class="op">+</span> <span class="va">self</span>.beta</span>
<span id="cb9-70"><a href="#cb9-70"></a>    <span class="co"># update the buffers</span></span>
<span id="cb9-71"><a href="#cb9-71"></a>    <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb9-72"><a href="#cb9-72"></a>      <span class="cf">with</span> torch.no_grad():</span>
<span id="cb9-73"><a href="#cb9-73"></a>        <span class="va">self</span>.running_mean <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> <span class="va">self</span>.running_mean <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> xmean</span>
<span id="cb9-74"><a href="#cb9-74"></a>        <span class="va">self</span>.running_var <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> <span class="va">self</span>.running_var <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> xvar</span>
<span id="cb9-75"><a href="#cb9-75"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb9-76"><a href="#cb9-76"></a></span>
<span id="cb9-77"><a href="#cb9-77"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb9-78"><a href="#cb9-78"></a>    <span class="cf">return</span> [<span class="va">self</span>.gamma, <span class="va">self</span>.beta]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="class-tanh" class="level4">
<h4 class="anchored" data-anchor-id="class-tanh">class <code>Tanh</code></h4>
<div id="e66a8d61" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="kw">class</span> Tanh:</span>
<span id="cb10-2"><a href="#cb10-2"></a>    <span class="co">"""</span></span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="co">    Hyperbolic Tangent (Tanh) Activation Function</span></span>
<span id="cb10-4"><a href="#cb10-4"></a></span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="co">    Applies the hyperbolic tangent activation function element-wise to the input tensor.</span></span>
<span id="cb10-6"><a href="#cb10-6"></a><span class="co">    Tanh maps input values to the range [-1, 1], providing a symmetric and non-linear</span></span>
<span id="cb10-7"><a href="#cb10-7"></a><span class="co">    transformation that helps neural networks learn complex patterns.</span></span>
<span id="cb10-8"><a href="#cb10-8"></a></span>
<span id="cb10-9"><a href="#cb10-9"></a><span class="co">    Mathematical Definition:</span></span>
<span id="cb10-10"><a href="#cb10-10"></a><span class="co">    tanh(x) = (e^x - e^-x) / (e^x + e^-x)</span></span>
<span id="cb10-11"><a href="#cb10-11"></a></span>
<span id="cb10-12"><a href="#cb10-12"></a><span class="co">    Key Characteristics:</span></span>
<span id="cb10-13"><a href="#cb10-13"></a><span class="co">    - Output Range: [-1, 1]</span></span>
<span id="cb10-14"><a href="#cb10-14"></a><span class="co">    - Symmetric around the origin</span></span>
<span id="cb10-15"><a href="#cb10-15"></a><span class="co">    - Gradient is always less than 1, which helps mitigate the vanishing gradient problem</span></span>
<span id="cb10-16"><a href="#cb10-16"></a><span class="co">    - Commonly used in recurrent neural networks and hidden layers</span></span>
<span id="cb10-17"><a href="#cb10-17"></a></span>
<span id="cb10-18"><a href="#cb10-18"></a><span class="co">    Methods:</span></span>
<span id="cb10-19"><a href="#cb10-19"></a><span class="co">        __call__(x): Applies the Tanh activation to the input tensor.</span></span>
<span id="cb10-20"><a href="#cb10-20"></a><span class="co">        parameters(): Returns an empty list, as Tanh has no learnable parameters.</span></span>
<span id="cb10-21"><a href="#cb10-21"></a></span>
<span id="cb10-22"><a href="#cb10-22"></a><span class="co">    Attributes:</span></span>
<span id="cb10-23"><a href="#cb10-23"></a><span class="co">        out (torch.Tensor): Stores the output of the most recent forward pass.</span></span>
<span id="cb10-24"><a href="#cb10-24"></a></span>
<span id="cb10-25"><a href="#cb10-25"></a><span class="co">    Example:</span></span>
<span id="cb10-26"><a href="#cb10-26"></a><span class="co">        &gt;&gt;&gt; activation = Tanh()</span></span>
<span id="cb10-27"><a href="#cb10-27"></a><span class="co">        &gt;&gt;&gt; x = torch.tensor([-2.0, 0.0, 2.0])</span></span>
<span id="cb10-28"><a href="#cb10-28"></a><span class="co">        &gt;&gt;&gt; y = activation(x)</span></span>
<span id="cb10-29"><a href="#cb10-29"></a><span class="co">        &gt;&gt;&gt; y</span></span>
<span id="cb10-30"><a href="#cb10-30"></a><span class="co">        tensor([-0.9640, 0.0000, 0.9640])</span></span>
<span id="cb10-31"><a href="#cb10-31"></a></span>
<span id="cb10-32"><a href="#cb10-32"></a><span class="co">    Note:</span></span>
<span id="cb10-33"><a href="#cb10-33"></a><span class="co">        This implementation is stateless and does not modify the input tensor.</span></span>
<span id="cb10-34"><a href="#cb10-34"></a><span class="co">        The activation is applied element-wise, preserving the input tensor's shape.</span></span>
<span id="cb10-35"><a href="#cb10-35"></a><span class="co">    """</span></span>
<span id="cb10-36"><a href="#cb10-36"></a></span>
<span id="cb10-37"><a href="#cb10-37"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb10-38"><a href="#cb10-38"></a>        <span class="va">self</span>.out <span class="op">=</span> torch.tanh(x)</span>
<span id="cb10-39"><a href="#cb10-39"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb10-40"><a href="#cb10-40"></a></span>
<span id="cb10-41"><a href="#cb10-41"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb10-42"><a href="#cb10-42"></a>        <span class="cf">return</span> []</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="random-number-generator" class="level5">
<h5 class="anchored" data-anchor-id="random-number-generator">random number generator</h5>
<div id="812115b1" class="cell" data-execution_count="10">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>torch.manual_seed(<span class="dv">42</span>)<span class="op">;</span> <span class="co"># seed rng for reproducibility</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="network-architecture" class="level5">
<h5 class="anchored" data-anchor-id="network-architecture">network architecture</h5>
<div id="a5a47931" class="cell" data-execution_count="11">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="co"># original network</span></span>
<span id="cb12-2"><a href="#cb12-2"></a>n_embd <span class="op">=</span> <span class="dv">10</span> <span class="co"># the dimensionality of the character embedding vectors</span></span>
<span id="cb12-3"><a href="#cb12-3"></a>n_hidden <span class="op">=</span> <span class="dv">200</span> <span class="co"># the number of neurons in the hidden layer of the MLP</span></span>
<span id="cb12-4"><a href="#cb12-4"></a></span>
<span id="cb12-5"><a href="#cb12-5"></a>C <span class="op">=</span> torch.rand((vocab_size, n_embd))</span>
<span id="cb12-6"><a href="#cb12-6"></a>layers <span class="op">=</span> [</span>
<span id="cb12-7"><a href="#cb12-7"></a>  Linear(n_embd <span class="op">*</span> block_size, n_hidden, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb12-8"><a href="#cb12-8"></a>  BatchNorm1d(n_hidden),</span>
<span id="cb12-9"><a href="#cb12-9"></a>  Tanh(),</span>
<span id="cb12-10"><a href="#cb12-10"></a>  Linear(n_hidden, vocab_size),</span>
<span id="cb12-11"><a href="#cb12-11"></a>]</span>
<span id="cb12-12"><a href="#cb12-12"></a></span>
<span id="cb12-13"><a href="#cb12-13"></a><span class="co"># parameter init</span></span>
<span id="cb12-14"><a href="#cb12-14"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-15"><a href="#cb12-15"></a>  layers[<span class="op">-</span><span class="dv">1</span>].weight <span class="op">*=</span> <span class="fl">0.1</span> <span class="co"># last layer make less confident</span></span>
<span id="cb12-16"><a href="#cb12-16"></a></span>
<span id="cb12-17"><a href="#cb12-17"></a>parameters <span class="op">=</span> [C] <span class="op">+</span> [p <span class="cf">for</span> layer <span class="kw">in</span> layers <span class="cf">for</span> p <span class="kw">in</span> layer.parameters()]</span>
<span id="cb12-18"><a href="#cb12-18"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters)) <span class="co"># number of parameters in total</span></span>
<span id="cb12-19"><a href="#cb12-19"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb12-20"><a href="#cb12-20"></a>  p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb12-21"><a href="#cb12-21"></a></span>
<span id="cb12-22"><a href="#cb12-22"></a><span class="co"># model params: 12097</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="optimization" class="level4">
<h4 class="anchored" data-anchor-id="optimization">optimization</h4>
<div id="dc2af5a3" class="cell" data-execution_count="12">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># same optimization as last time</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>max_steps <span class="op">=</span> <span class="dv">200000</span></span>
<span id="cb13-3"><a href="#cb13-3"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb13-4"><a href="#cb13-4"></a>lossi <span class="op">=</span> []</span>
<span id="cb13-5"><a href="#cb13-5"></a></span>
<span id="cb13-6"><a href="#cb13-6"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb13-7"><a href="#cb13-7"></a></span>
<span id="cb13-8"><a href="#cb13-8"></a>  <span class="co"># minibatch construct</span></span>
<span id="cb13-9"><a href="#cb13-9"></a>  ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,))</span>
<span id="cb13-10"><a href="#cb13-10"></a>  Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix] <span class="co"># batch X,Y</span></span>
<span id="cb13-11"><a href="#cb13-11"></a></span>
<span id="cb13-12"><a href="#cb13-12"></a>  <span class="co"># forward pass</span></span>
<span id="cb13-13"><a href="#cb13-13"></a>  emb <span class="op">=</span> C[Xb] <span class="co"># embed the characters into vectors</span></span>
<span id="cb13-14"><a href="#cb13-14"></a>  x <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate the vectors</span></span>
<span id="cb13-15"><a href="#cb13-15"></a>  <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb13-16"><a href="#cb13-16"></a>    x <span class="op">=</span> layer(x)</span>
<span id="cb13-17"><a href="#cb13-17"></a>  loss <span class="op">=</span> F.cross_entropy(x, Yb) <span class="co"># loss function</span></span>
<span id="cb13-18"><a href="#cb13-18"></a></span>
<span id="cb13-19"><a href="#cb13-19"></a>  <span class="co"># backward pass</span></span>
<span id="cb13-20"><a href="#cb13-20"></a>  <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb13-21"><a href="#cb13-21"></a>    p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-22"><a href="#cb13-22"></a>  loss.backward()</span>
<span id="cb13-23"><a href="#cb13-23"></a></span>
<span id="cb13-24"><a href="#cb13-24"></a>  <span class="co"># update: simple SGD</span></span>
<span id="cb13-25"><a href="#cb13-25"></a>  lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">150000</span> <span class="cf">else</span> <span class="fl">0.01</span> <span class="co"># step learning rate decay</span></span>
<span id="cb13-26"><a href="#cb13-26"></a>  <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb13-27"><a href="#cb13-27"></a>    p.data <span class="op">+=</span> <span class="op">-</span>lr <span class="op">*</span> p.grad</span>
<span id="cb13-28"><a href="#cb13-28"></a></span>
<span id="cb13-29"><a href="#cb13-29"></a>  <span class="co"># track stats</span></span>
<span id="cb13-30"><a href="#cb13-30"></a>  <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="co"># print every once in a while</span></span>
<span id="cb13-31"><a href="#cb13-31"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">:7d}</span><span class="ss">/</span><span class="sc">{</span>max_steps<span class="sc">:7d}</span><span class="ss">: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb13-32"><a href="#cb13-32"></a>  lossi.append(loss.log10().item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="sourceCode" id="cb14"><pre class="sourceCode numberSource md number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb14-1"><a href="#cb14-1"></a><span class="in">     0/ 200000: 3.2885</span></span>
<span id="cb14-2"><a href="#cb14-2"></a>  10000/ 200000: 2.3938</span>
<span id="cb14-3"><a href="#cb14-3"></a>  20000/ 200000: 2.1235</span>
<span id="cb14-4"><a href="#cb14-4"></a>  30000/ 200000: 1.9222</span>
<span id="cb14-5"><a href="#cb14-5"></a>  40000/ 200000: 2.2440</span>
<span id="cb14-6"><a href="#cb14-6"></a>  50000/ 200000: 2.1108</span>
<span id="cb14-7"><a href="#cb14-7"></a>  60000/ 200000: 2.0624</span>
<span id="cb14-8"><a href="#cb14-8"></a>  70000/ 200000: 2.0893</span>
<span id="cb14-9"><a href="#cb14-9"></a>  80000/ 200000: 2.4173</span>
<span id="cb14-10"><a href="#cb14-10"></a>  90000/ 200000: 1.9744</span>
<span id="cb14-11"><a href="#cb14-11"></a> 100000/ 200000: 2.0883</span>
<span id="cb14-12"><a href="#cb14-12"></a> 110000/ 200000: 2.4538</span>
<span id="cb14-13"><a href="#cb14-13"></a> 120000/ 200000: 1.9535</span>
<span id="cb14-14"><a href="#cb14-14"></a> 130000/ 200000: 1.8980</span>
<span id="cb14-15"><a href="#cb14-15"></a> 140000/ 200000: 2.1196</span>
<span id="cb14-16"><a href="#cb14-16"></a> 150000/ 200000: 2.3550</span>
<span id="cb14-17"><a href="#cb14-17"></a> 160000/ 200000: 2.2957</span>
<span id="cb14-18"><a href="#cb14-18"></a> 170000/ 200000: 2.0286</span>
<span id="cb14-19"><a href="#cb14-19"></a> 180000/ 200000: 2.2379</span>
<span id="cb14-20"><a href="#cb14-20"></a> 190000/ 200000: 2.3866</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="observe-training-processevaluation" class="level4">
<h4 class="anchored" data-anchor-id="observe-training-processevaluation">observe training process/evaluation</h4>
<div id="f2a87323" class="cell" data-execution_count="13">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>plt.plot(lossi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="1_pre_lossi.png" class="img-fluid figure-img"></p>
<figcaption><code>lossi</code> plot at the beginning</figcaption>
</figure>
</div>
</section>
<section id="calibrate-the-batchnorm-after-training" class="level4">
<h4 class="anchored" data-anchor-id="calibrate-the-batchnorm-after-training">calibrate the <code>batchnorm</code> after training</h4>
<p>We should be using the running mean/variance of the whole dataset splits rather than the last mini-batch.</p>
<div id="05be87a1" class="cell" data-execution_count="14">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="co"># put layers into eval mode (needed for batchnorm especially)</span></span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="cf">for</span> layer <span class="kw">in</span> model.layers:</span>
<span id="cb16-3"><a href="#cb16-3"></a>  layer.training <span class="op">=</span> <span class="va">False</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="calculate-on-whole-training-and-validation-splits" class="level4">
<h4 class="anchored" data-anchor-id="calculate-on-whole-training-and-validation-splits">calculate on whole training and validation splits</h4>
<div id="954d7379" class="cell" data-execution_count="15">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="co"># evaluate the loss</span></span>
<span id="cb17-2"><a href="#cb17-2"></a><span class="at">@torch.no_grad</span>() <span class="co"># this decorator disables gradient tracking inside pytorch</span></span>
<span id="cb17-3"><a href="#cb17-3"></a><span class="kw">def</span> split_loss(split):</span>
<span id="cb17-4"><a href="#cb17-4"></a>  x,y <span class="op">=</span> {</span>
<span id="cb17-5"><a href="#cb17-5"></a>    <span class="st">'train'</span>: (Xtr, Ytr),</span>
<span id="cb17-6"><a href="#cb17-6"></a>    <span class="st">'val'</span>: (Xdev, Ydev),</span>
<span id="cb17-7"><a href="#cb17-7"></a>    <span class="st">'test'</span>: (Xte, Yte),</span>
<span id="cb17-8"><a href="#cb17-8"></a>  }[split]</span>
<span id="cb17-9"><a href="#cb17-9"></a>  logits <span class="op">=</span> model(x)</span>
<span id="cb17-10"><a href="#cb17-10"></a>  loss <span class="op">=</span> F.cross_entropy(logits, y)</span>
<span id="cb17-11"><a href="#cb17-11"></a>  <span class="bu">print</span>(split, loss.item())</span>
<span id="cb17-12"><a href="#cb17-12"></a></span>
<span id="cb17-13"><a href="#cb17-13"></a>split_loss(<span class="st">'train'</span>)</span>
<span id="cb17-14"><a href="#cb17-14"></a>split_loss(<span class="st">'val'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Pretty loss but there are still room for improve:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode numberSource md number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb18-1"><a href="#cb18-1"></a>train 2.0467958450317383</span>
<span id="cb18-2"><a href="#cb18-2"></a>val 2.0989298820495605</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="sample-from-the-model" class="level4">
<h4 class="anchored" data-anchor-id="sample-from-the-model">sample from the model</h4>
<p>Here are Names generated by the model till now, we have relatively name-like results that do not exist in the training set.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode numberSource md number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb19-1"><a href="#cb19-1"></a>liz.</span>
<span id="cb19-2"><a href="#cb19-2"></a>layah.</span>
<span id="cb19-3"><a href="#cb19-3"></a>dan.</span>
<span id="cb19-4"><a href="#cb19-4"></a>hilon.</span>
<span id="cb19-5"><a href="#cb19-5"></a>avani.</span>
<span id="cb19-6"><a href="#cb19-6"></a>korron.</span>
<span id="cb19-7"><a href="#cb19-7"></a>aua.</span>
<span id="cb19-8"><a href="#cb19-8"></a>noon.</span>
<span id="cb19-9"><a href="#cb19-9"></a>bethalyn.</span>
<span id="cb19-10"><a href="#cb19-10"></a>thia.</span>
<span id="cb19-11"><a href="#cb19-11"></a>bote.</span>
<span id="cb19-12"><a href="#cb19-12"></a>jereanail.</span>
<span id="cb19-13"><a href="#cb19-13"></a>vitorien.</span>
<span id="cb19-14"><a href="#cb19-14"></a>zarashivonna.</span>
<span id="cb19-15"><a href="#cb19-15"></a>yakurrren.</span>
<span id="cb19-16"><a href="#cb19-16"></a>jovon.</span>
<span id="cb19-17"><a href="#cb19-17"></a>malynn.</span>
<span id="cb19-18"><a href="#cb19-18"></a>vanna.</span>
<span id="cb19-19"><a href="#cb19-19"></a>caparmana.</span>
<span id="cb19-20"><a href="#cb19-20"></a>shantymonse.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="lets-fix-the-learning-rate-plot" class="level2">
<h2 class="anchored" data-anchor-id="lets-fix-the-learning-rate-plot">let’s fix the learning rate plot</h2>
<p>The plot for <code>lossi</code> looks very crazy, it’s because the batch size of 32 is way too few so this time we got lucky, and next time we got unlucky. And the mini-batch loss splashed too much. We should probably fix it.</p>
<p>We pivot to a row for every 1000 observations of <code>lossi</code> and calculate the mean, we end up have 200 observations which is easier to see.</p>
<div id="6f3f4a90" class="cell" data-execution_count="16">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>plt.plot(torch.tensor(lossi).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1000</span>).mean(<span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We can also observe the learning rate decay at 150k training loops.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2_enhance_lossi.png" class="img-fluid figure-img"></p>
<figcaption><code>lossi</code> plot fixed</figcaption>
</figure>
</div>
</section>
<section id="pytorchifying-our-code-layers-containers-torch.nn-fun-bugs" class="level2">
<h2 class="anchored" data-anchor-id="pytorchifying-our-code-layers-containers-torch.nn-fun-bugs">pytorchifying our code: layers, containers, <code>torch.nn</code>, fun bugs</h2>
<p>Now we notice that we still have the embedding operation lying outside the pytorch-ified layers. It basically creating a lookup table <code>C</code>, embedding it with our data <code>Y</code> (or <code>Yb</code>), then stretching out to row with <code>view()</code> which is very cheap in PyTorch as no more memory creation is needed.</p>
<p>We modulize this by constructing 2 classes:</p>
<div id="ed88f230" class="cell" data-execution_count="17">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="kw">class</span> Embedding:</span>
<span id="cb21-2"><a href="#cb21-2"></a></span>
<span id="cb21-3"><a href="#cb21-3"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_embeddings, embedding_dim):</span>
<span id="cb21-4"><a href="#cb21-4"></a>    <span class="va">self</span>.weight <span class="op">=</span> torch.randn((num_embeddings, embedding_dim))</span>
<span id="cb21-5"><a href="#cb21-5"></a></span>
<span id="cb21-6"><a href="#cb21-6"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, IX):</span>
<span id="cb21-7"><a href="#cb21-7"></a>    <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.weight[IX]</span>
<span id="cb21-8"><a href="#cb21-8"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb21-9"><a href="#cb21-9"></a></span>
<span id="cb21-10"><a href="#cb21-10"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb21-11"><a href="#cb21-11"></a>    <span class="cf">return</span> [<span class="va">self</span>.weight]</span>
<span id="cb21-12"><a href="#cb21-12"></a></span>
<span id="cb21-13"><a href="#cb21-13"></a><span class="co"># -----------------------------------------------------------------------------------------------</span></span>
<span id="cb21-14"><a href="#cb21-14"></a><span class="kw">class</span> Flatten:</span>
<span id="cb21-15"><a href="#cb21-15"></a></span>
<span id="cb21-16"><a href="#cb21-16"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb21-17"><a href="#cb21-17"></a>    <span class="va">self</span>.out <span class="op">=</span> x.view(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb21-18"><a href="#cb21-18"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb21-19"><a href="#cb21-19"></a></span>
<span id="cb21-20"><a href="#cb21-20"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb21-21"><a href="#cb21-21"></a>    <span class="cf">return</span> []</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now we can re-define the <code>layers</code> like this:</p>
<div id="720dbdaa" class="cell" data-execution_count="18">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>layers <span class="op">=</span> [</span>
<span id="cb22-2"><a href="#cb22-2"></a>  Embedding(vocab_size, n_embd),</span>
<span id="cb22-3"><a href="#cb22-3"></a>  Flatten(),</span>
<span id="cb22-4"><a href="#cb22-4"></a>  Linear(n_embd <span class="op">*</span> block_size, n_hidden, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb22-5"><a href="#cb22-5"></a>  BatchNorm1d(n_hidden),</span>
<span id="cb22-6"><a href="#cb22-6"></a>  Tanh(),</span>
<span id="cb22-7"><a href="#cb22-7"></a>  Linear(n_hidden, vocab_size),</span>
<span id="cb22-8"><a href="#cb22-8"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>and also remove the <code>C</code>, <code>emb</code> definition in the forward pass construction. Going futher, we will be not only pytorchifying the elements of <code>layers</code> only, but also the <code>layers</code> itself. In PyTorch, we have term <code>containers</code>, which specifying how we organize the layers in a network. And what are we doing here is constructing layers sequentially, which is equivalent to <code>Sequential</code> in the <code>containers</code>:</p>
<div id="e127f831" class="cell" data-execution_count="19">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="kw">class</span> Sequential:</span>
<span id="cb23-2"><a href="#cb23-2"></a></span>
<span id="cb23-3"><a href="#cb23-3"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, layers):</span>
<span id="cb23-4"><a href="#cb23-4"></a>    <span class="va">self</span>.layers <span class="op">=</span> layers</span>
<span id="cb23-5"><a href="#cb23-5"></a></span>
<span id="cb23-6"><a href="#cb23-6"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb23-7"><a href="#cb23-7"></a>    <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb23-8"><a href="#cb23-8"></a>      x <span class="op">=</span> layer(x)</span>
<span id="cb23-9"><a href="#cb23-9"></a>    <span class="va">self</span>.out <span class="op">=</span> x</span>
<span id="cb23-10"><a href="#cb23-10"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb23-11"><a href="#cb23-11"></a></span>
<span id="cb23-12"><a href="#cb23-12"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb23-13"><a href="#cb23-13"></a>    <span class="co"># get parameters of all layers and stretch them out into one list</span></span>
<span id="cb23-14"><a href="#cb23-14"></a>    <span class="cf">return</span> [p <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers <span class="cf">for</span> p <span class="kw">in</span> layer.parameters()]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>and wrapp the layers into our <code>model</code>:</p>
<div id="560f5a5d" class="cell" data-execution_count="20">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a>model <span class="op">=</span> Sequential([</span>
<span id="cb24-2"><a href="#cb24-2"></a>  Embedding(vocab_size, n_embd),</span>
<span id="cb24-3"><a href="#cb24-3"></a>  Flatten(),</span>
<span id="cb24-4"><a href="#cb24-4"></a>  Linear(n_embd <span class="op">*</span> block_size, n_hidden, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb24-5"><a href="#cb24-5"></a>  BatchNorm1d(n_hidden),</span>
<span id="cb24-6"><a href="#cb24-6"></a>  Tanh(),</span>
<span id="cb24-7"><a href="#cb24-7"></a>  Linear(n_hidden, vocab_size),</span>
<span id="cb24-8"><a href="#cb24-8"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="implementing-wavenet" class="level1">
<h1>2 implementing <code>WaveNet</code></h1>
<p>So far with the classical MLP following Bengio et al.&nbsp;(2003), we have a <em>embedding layer</em> followed by a <em>hidden layer</em> and end up with a <em>activation layer</em>. Although we added more layer after the embedding, we could not make a significant progress.</p>
<p>The problem is we dont have a <strong>naive way</strong> of making the model bigger in a productive way. We are still in the case that we crushing all the characters into a single all the way at the begining. And even if we make this a bigger layer and add neurons it’s still like silly to squash all that information so fast into a single step.</p>
<section id="overview-wavenet" class="level2">
<h2 class="anchored" data-anchor-id="overview-wavenet">overview: <code>WaveNet</code></h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig3_wavenet.png" class="img-fluid figure-img"></p>
<figcaption>Visualization of the <code>WaveNet</code> idea - Progressive Fusion</figcaption>
</figure>
</div>
</section>
<section id="dataset-bump-the-context-size-to-8" class="level2">
<h2 class="anchored" data-anchor-id="dataset-bump-the-context-size-to-8">dataset bump the context size to 8</h2>
<p>first we change the <code>block_size</code> into <code>8</code> and now our dataset looks like:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode numberSource md number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb25-1"><a href="#cb25-1"></a>........ ---&gt; y</span>
<span id="cb25-2"><a href="#cb25-2"></a>.......y ---&gt; u</span>
<span id="cb25-3"><a href="#cb25-3"></a>......yu ---&gt; h</span>
<span id="cb25-4"><a href="#cb25-4"></a>.....yuh ---&gt; e</span>
<span id="cb25-5"><a href="#cb25-5"></a>....yuhe ---&gt; n</span>
<span id="cb25-6"><a href="#cb25-6"></a>...yuhen ---&gt; g</span>
<span id="cb25-7"><a href="#cb25-7"></a>..yuheng ---&gt; .</span>
<span id="cb25-8"><a href="#cb25-8"></a>........ ---&gt; d</span>
<span id="cb25-9"><a href="#cb25-9"></a>.......d ---&gt; i</span>
<span id="cb25-10"><a href="#cb25-10"></a>......di ---&gt; o</span>
<span id="cb25-11"><a href="#cb25-11"></a>.....dio ---&gt; n</span>
<span id="cb25-12"><a href="#cb25-12"></a>....dion ---&gt; d</span>
<span id="cb25-13"><a href="#cb25-13"></a>...diond ---&gt; r</span>
<span id="cb25-14"><a href="#cb25-14"></a>..diondr ---&gt; e</span>
<span id="cb25-15"><a href="#cb25-15"></a>.diondre ---&gt; .</span>
<span id="cb25-16"><a href="#cb25-16"></a>........ ---&gt; x</span>
<span id="cb25-17"><a href="#cb25-17"></a>.......x ---&gt; a</span>
<span id="cb25-18"><a href="#cb25-18"></a>......xa ---&gt; v</span>
<span id="cb25-19"><a href="#cb25-19"></a>.....xav ---&gt; i</span>
<span id="cb25-20"><a href="#cb25-20"></a>....xavi ---&gt; e</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The model size now bumps up to 22k.</p>
</section>
<section id="re-running-baseline-code-on-block_size-8" class="level2">
<h2 class="anchored" data-anchor-id="re-running-baseline-code-on-block_size-8">re-running baseline code on <code>block_size = 8</code></h2>
<p>Just by lazily extending the context size to 8, we can already improve the model a little bit, the loss on validation split now is around <code>2.045</code>. The names generated now look prettier:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode numberSource md number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb26-1"><a href="#cb26-1"></a>zamari.</span>
<span id="cb26-2"><a href="#cb26-2"></a>brennis.</span>
<span id="cb26-3"><a href="#cb26-3"></a>shavia.</span>
<span id="cb26-4"><a href="#cb26-4"></a>wililke.</span>
<span id="cb26-5"><a href="#cb26-5"></a>obalyid.</span>
<span id="cb26-6"><a href="#cb26-6"></a>leenoluja.</span>
<span id="cb26-7"><a href="#cb26-7"></a>rianny.</span>
<span id="cb26-8"><a href="#cb26-8"></a>jordanoe.</span>
<span id="cb26-9"><a href="#cb26-9"></a>yuvalfue.</span>
<span id="cb26-10"><a href="#cb26-10"></a>ozleega.</span>
<span id="cb26-11"><a href="#cb26-11"></a>jemirene.</span>
<span id="cb26-12"><a href="#cb26-12"></a>polton.</span>
<span id="cb26-13"><a href="#cb26-13"></a>jawi.</span>
<span id="cb26-14"><a href="#cb26-14"></a>meyah.</span>
<span id="cb26-15"><a href="#cb26-15"></a>gekiniq.</span>
<span id="cb26-16"><a href="#cb26-16"></a>angelinne.</span>
<span id="cb26-17"><a href="#cb26-17"></a>tayler.</span>
<span id="cb26-18"><a href="#cb26-18"></a>catrician.</span>
<span id="cb26-19"><a href="#cb26-19"></a>kyearie.</span>
<span id="cb26-20"><a href="#cb26-20"></a>anderias.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s deem this as a baseline then we can start to implement <code>WaveNet</code> and see how far we can go!</p>
</section>
<section id="implementing-wavenet-1" class="level2">
<h2 class="anchored" data-anchor-id="implementing-wavenet-1">implementing <code>WaveNet</code></h2>
<p>First, let’s revisit the shape of the tensors along the way of the forward pass in our neural net:</p>
<div id="24a32dbb" class="cell" data-execution_count="21">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="co"># Look at a batch of just 4 examples</span></span>
<span id="cb27-2"><a href="#cb27-2"></a>ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (<span class="dv">4</span>,))</span>
<span id="cb27-3"><a href="#cb27-3"></a>Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix]</span>
<span id="cb27-4"><a href="#cb27-4"></a>logits <span class="op">=</span> model(Xb)</span>
<span id="cb27-5"><a href="#cb27-5"></a><span class="bu">print</span>(Xb.shape)</span>
<span id="cb27-6"><a href="#cb27-6"></a>Xb</span>
<span id="cb27-7"><a href="#cb27-7"></a></span>
<span id="cb27-8"><a href="#cb27-8"></a><span class="co"># &gt; torch.Size([4, 8]) # because the context length is now 8</span></span>
<span id="cb27-9"><a href="#cb27-9"></a><span class="co"># &gt; tensor([[ 0,  0,  0,  0,  0,  0, 13,  9],</span></span>
<span id="cb27-10"><a href="#cb27-10"></a><span class="co">#         [ 0,  0,  0,  0,  0,  0,  0,  0],</span></span>
<span id="cb27-11"><a href="#cb27-11"></a><span class="co">#         [ 0,  0,  0, 11,  5, 18, 15, 12],</span></span>
<span id="cb27-12"><a href="#cb27-12"></a><span class="co">#         [ 0,  0,  4, 15, 13,  9, 14,  9]])</span></span>
<span id="cb27-13"><a href="#cb27-13"></a></span>
<span id="cb27-14"><a href="#cb27-14"></a><span class="co"># Output of the embedding layer, each input is translated</span></span>
<span id="cb27-15"><a href="#cb27-15"></a><span class="co"># to 10 dimensional vector</span></span>
<span id="cb27-16"><a href="#cb27-16"></a>model.layers[<span class="dv">0</span>].out.shape</span>
<span id="cb27-17"><a href="#cb27-17"></a><span class="co"># &gt; torch.Size([4, 8, 10])</span></span>
<span id="cb27-18"><a href="#cb27-18"></a></span>
<span id="cb27-19"><a href="#cb27-19"></a><span class="co"># Output of Flatten layer, each 10-dim vector is concatenated</span></span>
<span id="cb27-20"><a href="#cb27-20"></a><span class="co"># to each other for all 8-dim context vectors</span></span>
<span id="cb27-21"><a href="#cb27-21"></a>model.layers[<span class="dv">1</span>].out.shape</span>
<span id="cb27-22"><a href="#cb27-22"></a><span class="co"># &gt; torch.Size([4, 80])</span></span>
<span id="cb27-23"><a href="#cb27-23"></a></span>
<span id="cb27-24"><a href="#cb27-24"></a><span class="co"># Output of the Linear layer, take 80 and create 200 channels,</span></span>
<span id="cb27-25"><a href="#cb27-25"></a><span class="co"># just via matrix mult</span></span>
<span id="cb27-26"><a href="#cb27-26"></a>model.layers[<span class="dv">2</span>].out.shape</span>
<span id="cb27-27"><a href="#cb27-27"></a><span class="co"># &gt; torch.Size([4, 200])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now look into the Linear layer, which take the input <code>x</code> in the forward pass, multiply by <code>weight</code> and add the <code>bias</code> in (there is broadcasting here). So the transformation in this layer looks like:</p>
<div id="ed056309" class="cell" data-execution_count="22">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a>(torch.randn(<span class="dv">4</span>, <span class="dv">80</span>) <span class="op">@</span> torch.randn(<span class="dv">80</span>, <span class="dv">200</span>) <span class="op">+</span> torch.randn(<span class="dv">200</span>)).shape</span>
<span id="cb28-2"><a href="#cb28-2"></a></span>
<span id="cb28-3"><a href="#cb28-3"></a><span class="co"># &gt; torch.Size([4, 200])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Input <code>x</code> matrix here does not need to be 2 dimensional array. The matrix multiplication in PyTorch is quite powerfull, you can pass more than 2 dimensional array. And all dimensions will be preserved except the last one. Like this:</p>
<div id="60f6327e" class="cell" data-execution_count="23">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a>(torch.randn(<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">80</span>) <span class="op">@</span> torch.randn(<span class="dv">80</span>, <span class="dv">200</span>) <span class="op">+</span> torch.randn(<span class="dv">200</span>)).shape</span>
<span id="cb29-2"><a href="#cb29-2"></a></span>
<span id="cb29-3"><a href="#cb29-3"></a><span class="co"># &gt; torch.Size([4, 5, 200])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Which we want to improve now is not just flatten the 8 characters input too fast at the beginning, we want to group them pair by pair to process them in parallel.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode numberSource md number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb30-1"><a href="#cb30-1"></a>(x1 x2) (x3 x4) (x5 x6) (x7 x8)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Particularly for 8 characters block size we want to divide it into 4 groups ~ 4 pair of <em>bigrams</em>. We are increasing <strong>the dimensions of the batch</strong>.</p>
<div id="9df4e45c" class="cell" data-execution_count="24">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a>(torch.randn(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">20</span>) <span class="op">@</span> torch.randn(<span class="dv">20</span>, <span class="dv">200</span>) <span class="op">+</span> torch.randn(<span class="dv">200</span>)).shape</span>
<span id="cb31-2"><a href="#cb31-2"></a></span>
<span id="cb31-3"><a href="#cb31-3"></a><span class="co"># &gt; torch.Size([4, 4, 200])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>How can we achieve this in PyTorch, we can index the odd and even indexes then pair them up.</p>
<div id="888b1a66" class="cell" data-execution_count="25">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a>e <span class="op">=</span> torch.randn(<span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">10</span>) <span class="co"># 4 examples, 8 chars context size, 10-d embedding</span></span>
<span id="cb32-2"><a href="#cb32-2"></a><span class="co"># goal: want this to be (4, 4, 20) where consecutive 10-d vectors get concatenated</span></span>
<span id="cb32-3"><a href="#cb32-3"></a></span>
<span id="cb32-4"><a href="#cb32-4"></a>explicit <span class="op">=</span> torch.cat([e[:, ::<span class="dv">2</span>, :], e[:, <span class="dv">1</span>::<span class="dv">2</span>, :]], dim<span class="op">=</span><span class="dv">2</span>) <span class="co"># cat in the third dim</span></span>
<span id="cb32-5"><a href="#cb32-5"></a>explicit.shape</span>
<span id="cb32-6"><a href="#cb32-6"></a><span class="co"># &gt; torch.Size([4, 4, 20])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Of course, PyTorch provide a more efficient way to do this, using <code>view()</code>:</p>
<div id="7f949bb6" class="cell" data-execution_count="26">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a>(e.view(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">20</span>) <span class="op">==</span> explicit).<span class="bu">all</span>()</span>
<span id="cb33-2"><a href="#cb33-2"></a><span class="co"># &gt; tensor(True)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We are going to modulize the <code>FlattenConsecutive</code>:</p>
<div id="a763eb71" class="cell" data-execution_count="27">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a><span class="kw">class</span> FlattenConsecutive:</span>
<span id="cb34-2"><a href="#cb34-2"></a></span>
<span id="cb34-3"><a href="#cb34-3"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n):</span>
<span id="cb34-4"><a href="#cb34-4"></a>    <span class="va">self</span>.n <span class="op">=</span> n</span>
<span id="cb34-5"><a href="#cb34-5"></a></span>
<span id="cb34-6"><a href="#cb34-6"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb34-7"><a href="#cb34-7"></a>    B, T, C <span class="op">=</span> x.shape</span>
<span id="cb34-8"><a href="#cb34-8"></a>    x <span class="op">=</span> x.view(B, T<span class="op">//</span><span class="va">self</span>.n, C<span class="op">*</span><span class="va">self</span>.n)</span>
<span id="cb34-9"><a href="#cb34-9"></a>    <span class="cf">if</span> x.shape[<span class="dv">1</span>] <span class="op">==</span> <span class="dv">1</span>: <span class="co"># spurious tensor, if it's 1, squeeze it</span></span>
<span id="cb34-10"><a href="#cb34-10"></a>      x <span class="op">=</span> x.squeeze(<span class="dv">1</span>)</span>
<span id="cb34-11"><a href="#cb34-11"></a>    <span class="va">self</span>.out <span class="op">=</span> x</span>
<span id="cb34-12"><a href="#cb34-12"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb34-13"><a href="#cb34-13"></a></span>
<span id="cb34-14"><a href="#cb34-14"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb34-15"><a href="#cb34-15"></a>    <span class="cf">return</span> []</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>and update the flatten layer to <code>FlattenConsecutive(block_size)</code> (8) in our <code>model</code>. We can observe the dimension of tensors in all layers:</p>
<div id="39c004d9" class="cell" data-execution_count="28">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a><span class="cf">for</span> layer <span class="kw">in</span> model.layers:</span>
<span id="cb35-2"><a href="#cb35-2"></a>  <span class="bu">print</span>(layer.__class__.<span class="va">__name__</span>,<span class="st">": "</span>, <span class="bu">tuple</span>(layer.out.shape))</span>
<span id="cb35-3"><a href="#cb35-3"></a></span>
<span id="cb35-4"><a href="#cb35-4"></a><span class="co"># Embedding :  (4, 8, 10)</span></span>
<span id="cb35-5"><a href="#cb35-5"></a><span class="co"># Flatten :  (4, 80)</span></span>
<span id="cb35-6"><a href="#cb35-6"></a><span class="co"># Linear :  (4, 200)</span></span>
<span id="cb35-7"><a href="#cb35-7"></a><span class="co"># BatchNorm1d :  (4, 200)</span></span>
<span id="cb35-8"><a href="#cb35-8"></a><span class="co"># Tanh :  (4, 200)</span></span>
<span id="cb35-9"><a href="#cb35-9"></a><span class="co"># Linear :  (4, 27)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This is what we have currently. But as said, we dont want to flatten too fast, so we gonna flatten by 2 character, here is the update of the <code>model</code> - 3 layers present the consecutive flatten <code>4 -&gt; 2 -&gt; 1</code>:</p>
<div id="19f6dec8" class="cell" data-execution_count="29">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a>model <span class="op">=</span> Sequential([</span>
<span id="cb36-2"><a href="#cb36-2"></a>  Embedding(vocab_size, n_embd),</span>
<span id="cb36-3"><a href="#cb36-3"></a>  FlattenConsecutive(<span class="dv">2</span>), Linear(n_embd <span class="op">*</span> <span class="dv">2</span>, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb36-4"><a href="#cb36-4"></a>  FlattenConsecutive(<span class="dv">2</span>), Linear(n_hidden<span class="op">*</span><span class="dv">2</span>, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb36-5"><a href="#cb36-5"></a>  FlattenConsecutive(<span class="dv">2</span>), Linear(n_hidden<span class="op">*</span><span class="dv">2</span>, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb36-6"><a href="#cb36-6"></a>  Linear(n_hidden, vocab_size),</span>
<span id="cb36-7"><a href="#cb36-7"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>and here is tensors dimension flowing in forward pass:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode numberSource md number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb37-1"><a href="#cb37-1"></a><span class="an">Embedding :</span><span class="co">  (4, 8, 10)</span></span>
<span id="cb37-2"><a href="#cb37-2"></a><span class="an">FlattenConsecutive :</span><span class="co">  (4, 4, 20)</span></span>
<span id="cb37-3"><a href="#cb37-3"></a><span class="an">Linear :</span><span class="co">  (4, 4, 200)</span></span>
<span id="cb37-4"><a href="#cb37-4"></a><span class="an">BatchNorm1d :</span><span class="co">  (4, 4, 200)</span></span>
<span id="cb37-5"><a href="#cb37-5"></a><span class="an">Tanh :</span><span class="co">  (4, 4, 200)</span></span>
<span id="cb37-6"><a href="#cb37-6"></a><span class="an">FlattenConsecutive :</span><span class="co">  (4, 2, 400)</span></span>
<span id="cb37-7"><a href="#cb37-7"></a><span class="an">Linear :</span><span class="co">  (4, 2, 200)</span></span>
<span id="cb37-8"><a href="#cb37-8"></a><span class="an">BatchNorm1d :</span><span class="co">  (4, 2, 200)</span></span>
<span id="cb37-9"><a href="#cb37-9"></a><span class="an">Tanh :</span><span class="co">  (4, 2, 200)</span></span>
<span id="cb37-10"><a href="#cb37-10"></a><span class="an">FlattenConsecutive :</span><span class="co">  (4, 400)</span></span>
<span id="cb37-11"><a href="#cb37-11"></a><span class="an">Linear :</span><span class="co">  (4, 200)</span></span>
<span id="cb37-12"><a href="#cb37-12"></a><span class="an">BatchNorm1d :</span><span class="co">  (4, 200)</span></span>
<span id="cb37-13"><a href="#cb37-13"></a><span class="an">Tanh :</span><span class="co">  (4, 200)</span></span>
<span id="cb37-14"><a href="#cb37-14"></a><span class="an">Linear :</span><span class="co">  (4, 27)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>That’s is, we have successfully implemented the <code>WaveNet</code>.</p>
</section>
<section id="training-the-wavenet-first-pass" class="level2">
<h2 class="anchored" data-anchor-id="training-the-wavenet-first-pass">training the <code>WaveNet</code>: first pass</h2>
<p>Now assume we use the same size of network (number of neurons), let’s see if the <code>loss</code> can be improved. We change the <code>n_hidden = 68</code>, so that the total parameters of our network remain 22k. Below is update tensor dims for a batch (32):</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode numberSource md number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb38-1"><a href="#cb38-1"></a><span class="an">Embedding :</span><span class="co">  (32, 8, 10)</span></span>
<span id="cb38-2"><a href="#cb38-2"></a><span class="an">FlattenConsecutive :</span><span class="co">  (32, 4, 20)</span></span>
<span id="cb38-3"><a href="#cb38-3"></a><span class="an">Linear :</span><span class="co">  (32, 4, 68)</span></span>
<span id="cb38-4"><a href="#cb38-4"></a><span class="an">BatchNorm1d :</span><span class="co">  (32, 4, 68)</span></span>
<span id="cb38-5"><a href="#cb38-5"></a><span class="an">Tanh :</span><span class="co">  (32, 4, 68)</span></span>
<span id="cb38-6"><a href="#cb38-6"></a><span class="an">FlattenConsecutive :</span><span class="co">  (32, 2, 136)</span></span>
<span id="cb38-7"><a href="#cb38-7"></a><span class="an">Linear :</span><span class="co">  (32, 2, 68)</span></span>
<span id="cb38-8"><a href="#cb38-8"></a><span class="an">BatchNorm1d :</span><span class="co">  (32, 2, 68)</span></span>
<span id="cb38-9"><a href="#cb38-9"></a><span class="an">Tanh :</span><span class="co">  (32, 2, 68)</span></span>
<span id="cb38-10"><a href="#cb38-10"></a><span class="an">FlattenConsecutive :</span><span class="co">  (32, 136)</span></span>
<span id="cb38-11"><a href="#cb38-11"></a><span class="an">Linear :</span><span class="co">  (32, 68)</span></span>
<span id="cb38-12"><a href="#cb38-12"></a><span class="an">BatchNorm1d :</span><span class="co">  (32, 68)</span></span>
<span id="cb38-13"><a href="#cb38-13"></a><span class="an">Tanh :</span><span class="co">  (32, 68)</span></span>
<span id="cb38-14"><a href="#cb38-14"></a><span class="an">Linear :</span><span class="co">  (32, 27)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>It turns out that we got almost identical result. There are 2 things:</p>
<ol type="1">
<li>We just constructed the architecture of <code>WaveNet</code> but not tortured the model enough to find best set of hyperparameters; and</li>
<li>We may have a bug in <code>BatchNorm1d</code> layer, let’s take a look into this.</li>
</ol>
</section>
<section id="fixing-batchnorm1d-bug" class="level2">
<h2 class="anchored" data-anchor-id="fixing-batchnorm1d-bug">fixing <code>batchnorm1d</code> bug</h2>
<p>Let’s look at the <code>BatchNorm1d</code> happen in the first flatten layer:</p>
<div id="926b8f86" class="cell" data-execution_count="30">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a>e <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">4</span> , <span class="dv">68</span>)</span>
<span id="cb39-2"><a href="#cb39-2"></a>emean <span class="op">=</span> e.mean(<span class="dv">0</span>, keepdim <span class="op">=</span> <span class="va">True</span>) <span class="co"># 1, 4, 68</span></span>
<span id="cb39-3"><a href="#cb39-3"></a>evar <span class="op">=</span> e.var(<span class="dv">0</span>, keepdim <span class="op">=</span> <span class="va">True</span>) <span class="co"># 1, 4, 68</span></span>
<span id="cb39-4"><a href="#cb39-4"></a></span>
<span id="cb39-5"><a href="#cb39-5"></a>ehat <span class="op">=</span> (e <span class="op">-</span> emean) <span class="op">/</span> torch.sqrt(evar <span class="op">+</span> <span class="fl">1e-5</span>)</span>
<span id="cb39-6"><a href="#cb39-6"></a>ehat.shape</span>
<span id="cb39-7"><a href="#cb39-7"></a></span>
<span id="cb39-8"><a href="#cb39-8"></a><span class="co"># &gt; torch.Size([32, 4, 68])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>For <code>ehat</code>, everything is calculated properly, mean and variance are calculated to the batch and the 2nd dim <code>4</code> is preserved. But for the <code>running_mean</code>:</p>
<div id="abfeda82" class="cell" data-execution_count="31">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a>model.layers[<span class="dv">3</span>].running_mean.shape</span>
<span id="cb40-2"><a href="#cb40-2"></a></span>
<span id="cb40-3"><a href="#cb40-3"></a><span class="co"># &gt; torch.Size([1, 4, 68])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We see it is (1, 4, 68) while we are expected it’s 1 dimensional only which is defined in the init method (<code>torch.zeros(dim)</code>). We are maintaining the batch norm in parallel over 4 x 68 channels individually and independently instead of just 68 channels. We want to treat this <code>4</code> just like a batch norm dimension, ie everaging of <code>32 * 4</code> numbers for 68 channels. Fortunately PyTorch <code>mean()</code> method offer the reducing dimension not only for integer but also tuple.</p>
<div id="71eb0522" class="cell" data-execution_count="32">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1"></a>e <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">4</span> , <span class="dv">68</span>)</span>
<span id="cb41-2"><a href="#cb41-2"></a>emean <span class="op">=</span> e.mean((<span class="dv">0</span>,<span class="dv">1</span>), keepdim <span class="op">=</span> <span class="va">True</span>) <span class="co"># 1, 1, 68</span></span>
<span id="cb41-3"><a href="#cb41-3"></a>evar <span class="op">=</span> e.var((<span class="dv">0</span>,<span class="dv">1</span>), keepdim <span class="op">=</span> <span class="va">True</span>) <span class="co"># 1, 1, 68</span></span>
<span id="cb41-4"><a href="#cb41-4"></a></span>
<span id="cb41-5"><a href="#cb41-5"></a>ehat <span class="op">=</span> (e <span class="op">-</span> emean) <span class="op">/</span> torch.sqrt(evar <span class="op">+</span> <span class="fl">1e-5</span>)</span>
<span id="cb41-6"><a href="#cb41-6"></a>ehat.shape</span>
<span id="cb41-7"><a href="#cb41-7"></a></span>
<span id="cb41-8"><a href="#cb41-8"></a><span class="co"># &gt; torch.Size([32, 4, 68])</span></span>
<span id="cb41-9"><a href="#cb41-9"></a></span>
<span id="cb41-10"><a href="#cb41-10"></a>model.layers[<span class="dv">3</span>].running_mean.shape</span>
<span id="cb41-11"><a href="#cb41-11"></a></span>
<span id="cb41-12"><a href="#cb41-12"></a><span class="co"># &gt; torch.Size([1, 1, 68])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We now modify the <code>BatchNorm1d</code> definition accordingly, only the training mean/var in the <code>__call__</code> method:</p>
<div id="264b096d" class="cell" data-execution_count="33">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a><span class="co"># ---- remains the same</span></span>
<span id="cb42-2"><a href="#cb42-2"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb42-3"><a href="#cb42-3"></a>    <span class="co"># calculate the forward pass</span></span>
<span id="cb42-4"><a href="#cb42-4"></a>    <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb42-5"><a href="#cb42-5"></a>      <span class="cf">if</span> x.ndim <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb42-6"><a href="#cb42-6"></a>        dim <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb42-7"><a href="#cb42-7"></a>      <span class="cf">elif</span> x.ndim <span class="op">==</span> <span class="dv">3</span>:</span>
<span id="cb42-8"><a href="#cb42-8"></a>        dim <span class="op">=</span> (<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb42-9"><a href="#cb42-9"></a>      xmean <span class="op">=</span> x.mean(dim, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch mean</span></span>
<span id="cb42-10"><a href="#cb42-10"></a>      xvar <span class="op">=</span> x.var(dim, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch variance</span></span>
<span id="cb42-11"><a href="#cb42-11"></a>    <span class="cf">else</span>:</span>
<span id="cb42-12"><a href="#cb42-12"></a>      xmean <span class="op">=</span> <span class="va">self</span>.running_mean</span>
<span id="cb42-13"><a href="#cb42-13"></a>      xvar <span class="op">=</span> <span class="va">self</span>.running_var</span>
<span id="cb42-14"><a href="#cb42-14"></a>    xhat <span class="op">=</span> (x <span class="op">-</span> xmean) <span class="op">/</span> torch.sqrt(xvar <span class="op">+</span> <span class="va">self</span>.eps) <span class="co"># normalize to unit variance</span></span>
<span id="cb42-15"><a href="#cb42-15"></a>    <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.gamma <span class="op">*</span> xhat <span class="op">+</span> <span class="va">self</span>.beta</span>
<span id="cb42-16"><a href="#cb42-16"></a><span class="co"># ---- remaind the same</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="re-training-wavenet-with-bug-fix" class="level2">
<h2 class="anchored" data-anchor-id="re-training-wavenet-with-bug-fix">re-training <code>WaveNet</code> with bug fix</h2>
<p>Now retraining the network with bug fixed, we obtain a slightly better loss of <code>2.022</code>. We just fixed the normalization term inside the network so they did not thrush too much so a little improvement only is expected.</p>
</section>
<section id="scaling-up-our-wavenet" class="level2">
<h2 class="anchored" data-anchor-id="scaling-up-our-wavenet">scaling up our <code>WaveNet</code></h2>
<p>Now we’re ready to scale up our network and retrain everything, the model now have roughly 76k paramters. We finally passed the <code>2.0</code> threshold and achieved the loss of <code>1.99</code> on the validation split.</p>
<div id="4b813a84" class="cell" data-execution_count="34">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a>n_embd <span class="op">=</span> <span class="dv">24</span></span>
<span id="cb43-2"><a href="#cb43-2"></a>n_hidden <span class="op">=</span> <span class="dv">128</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>And here is final loss plot:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="final_lossi.png" class="img-fluid figure-img"></p>
<figcaption><code>lossi</code> final</figcaption>
</figure>
</div>
</section>
</section>
<section id="conclusions" class="level1">
<h1>3 conclusions</h1>
<section id="performance-log" class="level2">
<h2 class="anchored" data-anchor-id="performance-log">performance log</h2>
<table class="table-striped table-hover caption-top table">
<caption>Loss logs</caption>
<colgroup>
<col style="width: 10%">
<col style="width: 40%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>What we did</th>
<th>Loss we got (accum)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>original (3 character context + 200 hidden neurons, 12K params)</td>
<td><p>train 2.0467958450317383</p>
<p>val 2.0989298820495605</p></td>
</tr>
<tr class="even">
<td>2</td>
<td>context: 3 -&gt; 8 (22K params)</td>
<td><p>train 1.9028635025024414</p>
<p>val 2.044949769973755</p></td>
</tr>
<tr class="odd">
<td>3</td>
<td>flat -&gt; hierarchical (22K params)</td>
<td><p>train 1.9366059303283691</p>
<p>val 2.017268419265747</p></td>
</tr>
<tr class="even">
<td>4</td>
<td>fix bug in <code>batchnorm1d</code></td>
<td><p>train 1.9156142473220825</p>
<p>val 2.0228867530822754</p></td>
</tr>
<tr class="odd">
<td>5</td>
<td>scale up the network: <code>n_embd</code> 24, <code>n_hidden</code> 128 (76K params)</td>
<td><p>train 1.7680459022521973</p>
<p>val 1.994154691696167</p></td>
</tr>
</tbody>
</table>
</section>
<section id="experimental-harness" class="level2">
<h2 class="anchored" data-anchor-id="experimental-harness">experimental harness</h2>
<p>The “harness” metaphor is apt because it’s like a structured support system that allows researchers to systematically explore and optimize neural network configurations, much like a harness helps guide and support an athlete during training.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>An experimental harness typically includes several key components:</p>
<ol type="1">
<li>Hyperparameter Search Space Definition: This involves specifying the range of hyperparameters to be explored, such as:</li>
</ol>
<ul>
<li>Learning rates</li>
<li>Batch sizes</li>
<li>Network architecture depths</li>
<li>Activation functions</li>
<li>Regularization techniques</li>
<li>Dropout rates</li>
</ul>
<ol start="2" type="1">
<li>Search Strategy: Methods for exploring the hyperparameter space, which can include:</li>
</ol>
<ul>
<li>Grid search</li>
<li>Random search</li>
<li>Bayesian optimization</li>
<li>Evolutionary algorithms</li>
<li>Gradient-based optimization techniques</li>
</ul>
<ol start="3" type="1">
<li>Evaluation Metrics: Predefined metrics to assess model performance, such as:</li>
</ol>
<ul>
<li>Validation accuracy</li>
<li>Loss function values</li>
<li>Precision and recall</li>
<li>F1 score</li>
<li>Computational efficiency</li>
</ul>
<ol start="4" type="1">
<li>Automated Experiment Management: Tools and scripts that can:</li>
</ol>
<ul>
<li>Automatically generate and run different model configurations</li>
<li>Log results</li>
<li>Track experiments</li>
<li>Compare performance across different hyperparameter settings</li>
</ul>
<ol start="5" type="1">
<li>Reproducibility Mechanisms: Ensuring that experiments can be repeated and validated, which includes:</li>
</ol>
<ul>
<li>Fixed random seeds</li>
<li>Consistent data splitting</li>
<li>Versioning of datasets and configurations</li>
</ul>
</div>
</div>
</section>
<section id="wavenet-but-with-dilated-causal-convolutions" class="level2">
<h2 class="anchored" data-anchor-id="wavenet-but-with-dilated-causal-convolutions"><code>WaveNet</code> but with “dilated causal convolutions”</h2>
<ul>
<li>Convolution is a “for loop” applying a linear filter over space of some input sequence;</li>
<li>Not happen only in Python but also in Kernel</li>
</ul>
</section>
<section id="torch.nn" class="level2">
<h2 class="anchored" data-anchor-id="torch.nn"><code>torch.nn</code></h2>
<p>We have implement alot of concepts in <code>torch.nn</code>:</p>
<ul>
<li>containers: <code>Sequential</code></li>
<li><code>Linear</code>, <code>BatchNorm1d</code>, <code>Tanh</code>, <code>FlattenConsecutive</code>, …</li>
</ul>
</section>
<section id="the-development-process-of-building-deep-neural-nets-going-forward" class="level2">
<h2 class="anchored" data-anchor-id="the-development-process-of-building-deep-neural-nets-going-forward">the development process of building deep neural nets &amp; going forward</h2>
<ul>
<li>Spending a ton of time exploring PyTorch documentation, unfortunately it’s not a good one;</li>
<li>Ton of time to make the shapes work: fan in, fan out, NLC or NLC, broadcasting, viewing, etc;</li>
<li>What we:
<ul>
<li>done: implemented dilated causal convoluntional network;</li>
<li>to be explores: residual and skip connections;</li>
<li>to be explores: experimental harness;</li>
<li>more mordern networks: RNN, LSTM, Transformer.</li>
</ul></li>
</ul>
</section>
</section>
<section id="resources" class="level1">
<h1>4 resources</h1>
<ol type="1">
<li>WaveNet 2016 from DeepMind: <a href="https://arxiv.org/abs/1609.03499" class="uri">https://arxiv.org/abs/1609.03499</a>;</li>
<li>Bengio et al.&nbsp;2003 MLP LM: <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" class="uri">https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf</a>;</li>
<li>Notebook: <a href="https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part5_cnn1.ipynb" class="uri">https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part5_cnn1.ipynb</a>;</li>
<li>DeepMind’s blog post from 2016: <a href="https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/" class="uri">https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/</a></li>
</ol>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/lktuan\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb44" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb44-1"><a href="#cb44-1"></a><span class="co">---</span></span>
<span id="cb44-2"><a href="#cb44-2"></a><span class="an">title:</span><span class="co"> "NN-Z2H Lesson 6: Building makemore part 5 - Building a `WaveNet`"</span></span>
<span id="cb44-3"><a href="#cb44-3"></a><span class="an">description:</span><span class="co"> "CNN/`WaveNet` and a lot of reading of documentation, keeping track of multidimensional tensor shapes, moving between jupyter notebooks and repository code, ..."</span></span>
<span id="cb44-4"><a href="#cb44-4"></a><span class="an">author:</span></span>
<span id="cb44-5"><a href="#cb44-5"></a><span class="co">  - name: "Tuan Le Khac"</span></span>
<span id="cb44-6"><a href="#cb44-6"></a><span class="co">    url: https://lktuan.github.io/</span></span>
<span id="cb44-7"><a href="#cb44-7"></a><span class="an">categories:</span><span class="co"> [til, python, andrej karpathy, nn-z2h, neural networks]</span></span>
<span id="cb44-8"><a href="#cb44-8"></a><span class="an">date:</span><span class="co"> 12-09-2024</span></span>
<span id="cb44-9"><a href="#cb44-9"></a><span class="an">date-modified:</span><span class="co"> 12-09-2024</span></span>
<span id="cb44-10"><a href="#cb44-10"></a><span class="an">image:</span><span class="co"> wavenet.png</span></span>
<span id="cb44-11"><a href="#cb44-11"></a><span class="an">draft:</span><span class="co"> false</span></span>
<span id="cb44-12"><a href="#cb44-12"></a><span class="an">format:</span></span>
<span id="cb44-13"><a href="#cb44-13"></a><span class="co">  html:</span></span>
<span id="cb44-14"><a href="#cb44-14"></a><span class="co">    code-overflow: wrap</span></span>
<span id="cb44-15"><a href="#cb44-15"></a><span class="co">    code-tools: true</span></span>
<span id="cb44-16"><a href="#cb44-16"></a><span class="co">    code-fold: show</span></span>
<span id="cb44-17"><a href="#cb44-17"></a><span class="co">    code-annotations: hover</span></span>
<span id="cb44-18"><a href="#cb44-18"></a><span class="an">execute:</span></span>
<span id="cb44-19"><a href="#cb44-19"></a><span class="co">  eval: false</span></span>
<span id="cb44-20"><a href="#cb44-20"></a><span class="co">---</span></span>
<span id="cb44-21"><a href="#cb44-21"></a></span>
<span id="cb44-22"><a href="#cb44-22"></a>::: {.callout-important title="This is not orginal content!"}</span>
<span id="cb44-23"><a href="#cb44-23"></a>This is my study notes / codes along with Andrej Karpathy's "<span class="co">[</span><span class="ot">Neural Networks: Zero to Hero</span><span class="co">](https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)</span>" series.</span>
<span id="cb44-24"><a href="#cb44-24"></a>:::</span>
<span id="cb44-25"><a href="#cb44-25"></a></span>
<span id="cb44-26"><a href="#cb44-26"></a>*Codes are executed in Colab, this calculation capacity exceeds my computer's ability.*</span>
<span id="cb44-27"><a href="#cb44-27"></a></span>
<span id="cb44-28"><a href="#cb44-28"></a><span class="fu"># 1 intro</span></span>
<span id="cb44-29"><a href="#cb44-29"></a></span>
<span id="cb44-30"><a href="#cb44-30"></a>We are going to take the 2-layer MLP in the part 3 of <span class="in">`makemore`</span> and complexify it by:</span>
<span id="cb44-31"><a href="#cb44-31"></a></span>
<span id="cb44-32"><a href="#cb44-32"></a><span class="ss">-   </span>extending the block size: from 3 to 8 characters;</span>
<span id="cb44-33"><a href="#cb44-33"></a><span class="ss">-   </span>making it deeper rather than 1 hidden layer.</span>
<span id="cb44-34"><a href="#cb44-34"></a></span>
<span id="cb44-35"><a href="#cb44-35"></a>then end of with a Convoluntional Neural Network architecture similar to <span class="in">`WaveNet`</span> (2016) by <span class="co">[</span><span class="ot">Google DeepMind</span><span class="co">](https://deepmind.google/)</span>.</span>
<span id="cb44-36"><a href="#cb44-36"></a></span>
<span id="cb44-37"><a href="#cb44-37"></a>!<span class="co">[</span><span class="ot">WaveNet model architecture, [source](https://www.researchgate.net/figure/WaveNet-Model-Architecture-38_fig2_380566531)</span><span class="co">](wavenet.png)</span></span>
<span id="cb44-38"><a href="#cb44-38"></a></span>
<span id="cb44-39"><a href="#cb44-39"></a><span class="fu">## starter code walk through</span></span>
<span id="cb44-40"><a href="#cb44-40"></a></span>
<span id="cb44-41"><a href="#cb44-41"></a><span class="fu">#### import libraries</span></span>
<span id="cb44-42"><a href="#cb44-42"></a></span>
<span id="cb44-45"><a href="#cb44-45"></a><span class="in">```{python}</span></span>
<span id="cb44-46"><a href="#cb44-46"></a><span class="im">import</span> torch</span>
<span id="cb44-47"><a href="#cb44-47"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb44-48"><a href="#cb44-48"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb44-49"><a href="#cb44-49"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb44-50"><a href="#cb44-50"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb44-51"><a href="#cb44-51"></a><span class="in">```</span></span>
<span id="cb44-52"><a href="#cb44-52"></a></span>
<span id="cb44-53"><a href="#cb44-53"></a><span class="fu">#### reading data</span></span>
<span id="cb44-54"><a href="#cb44-54"></a></span>
<span id="cb44-57"><a href="#cb44-57"></a><span class="in">```{python}</span></span>
<span id="cb44-58"><a href="#cb44-58"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb44-59"><a href="#cb44-59"></a></span>
<span id="cb44-60"><a href="#cb44-60"></a>url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt"</span></span>
<span id="cb44-61"><a href="#cb44-61"></a>words <span class="op">=</span> pd.read_csv(url, header<span class="op">=</span><span class="va">None</span>).iloc[:, <span class="dv">0</span>].tolist()</span>
<span id="cb44-62"><a href="#cb44-62"></a>words[:<span class="dv">8</span>]</span>
<span id="cb44-63"><a href="#cb44-63"></a></span>
<span id="cb44-64"><a href="#cb44-64"></a><span class="co"># &gt;&gt;&gt; ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']</span></span>
<span id="cb44-65"><a href="#cb44-65"></a><span class="in">```</span></span>
<span id="cb44-66"><a href="#cb44-66"></a></span>
<span id="cb44-67"><a href="#cb44-67"></a><span class="fu">#### building vocab</span></span>
<span id="cb44-68"><a href="#cb44-68"></a></span>
<span id="cb44-71"><a href="#cb44-71"></a><span class="in">```{python}</span></span>
<span id="cb44-72"><a href="#cb44-72"></a><span class="co"># build the vocabulary of characters and mapping to/from integer</span></span>
<span id="cb44-73"><a href="#cb44-73"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(<span class="st">''</span>.join(words))))</span>
<span id="cb44-74"><a href="#cb44-74"></a>stoi <span class="op">=</span> {s:i<span class="op">+</span><span class="dv">1</span> <span class="cf">for</span> i, s <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb44-75"><a href="#cb44-75"></a>stoi[<span class="st">'.'</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb44-76"><a href="#cb44-76"></a>itos <span class="op">=</span> {i: s <span class="cf">for</span> s, i <span class="kw">in</span> stoi.items()}</span>
<span id="cb44-77"><a href="#cb44-77"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(itos)</span>
<span id="cb44-78"><a href="#cb44-78"></a><span class="bu">print</span>(itos)</span>
<span id="cb44-79"><a href="#cb44-79"></a><span class="bu">print</span>(vocab_size)</span>
<span id="cb44-80"><a href="#cb44-80"></a></span>
<span id="cb44-81"><a href="#cb44-81"></a><span class="co"># itos: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j',</span></span>
<span id="cb44-82"><a href="#cb44-82"></a><span class="co"># 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't',</span></span>
<span id="cb44-83"><a href="#cb44-83"></a><span class="co"># 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}</span></span>
<span id="cb44-84"><a href="#cb44-84"></a><span class="co"># vocab_size: 27</span></span>
<span id="cb44-85"><a href="#cb44-85"></a><span class="in">```</span></span>
<span id="cb44-86"><a href="#cb44-86"></a></span>
<span id="cb44-87"><a href="#cb44-87"></a><span class="fu">#### initializing randomization</span></span>
<span id="cb44-88"><a href="#cb44-88"></a></span>
<span id="cb44-91"><a href="#cb44-91"></a><span class="in">```{python}</span></span>
<span id="cb44-92"><a href="#cb44-92"></a><span class="im">import</span> random</span>
<span id="cb44-93"><a href="#cb44-93"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb44-94"><a href="#cb44-94"></a>random.shuffle(words)</span>
<span id="cb44-95"><a href="#cb44-95"></a><span class="in">```</span></span>
<span id="cb44-96"><a href="#cb44-96"></a></span>
<span id="cb44-97"><a href="#cb44-97"></a><span class="fu">#### create train/dev/test splits</span></span>
<span id="cb44-98"><a href="#cb44-98"></a></span>
<span id="cb44-101"><a href="#cb44-101"></a><span class="in">```{python}</span></span>
<span id="cb44-102"><a href="#cb44-102"></a>block_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># context length: how many characters do we take to predict the next one?</span></span>
<span id="cb44-103"><a href="#cb44-103"></a><span class="co"># build the dataset</span></span>
<span id="cb44-104"><a href="#cb44-104"></a><span class="kw">def</span> buid_dataset(words):</span>
<span id="cb44-105"><a href="#cb44-105"></a>    X, Y <span class="op">=</span> [], []</span>
<span id="cb44-106"><a href="#cb44-106"></a></span>
<span id="cb44-107"><a href="#cb44-107"></a>    <span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb44-108"><a href="#cb44-108"></a>        context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size</span>
<span id="cb44-109"><a href="#cb44-109"></a>        <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb44-110"><a href="#cb44-110"></a>            ix <span class="op">=</span> stoi[ch]</span>
<span id="cb44-111"><a href="#cb44-111"></a>            X.append(context)</span>
<span id="cb44-112"><a href="#cb44-112"></a>            Y.append(ix)</span>
<span id="cb44-113"><a href="#cb44-113"></a>            context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix]</span>
<span id="cb44-114"><a href="#cb44-114"></a></span>
<span id="cb44-115"><a href="#cb44-115"></a>    X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb44-116"><a href="#cb44-116"></a>    Y <span class="op">=</span> torch.tensor(Y)</span>
<span id="cb44-117"><a href="#cb44-117"></a>    <span class="bu">print</span>(X.shape, Y.shape)</span>
<span id="cb44-118"><a href="#cb44-118"></a>    <span class="cf">return</span> X, Y</span>
<span id="cb44-119"><a href="#cb44-119"></a></span>
<span id="cb44-120"><a href="#cb44-120"></a>n1 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb44-121"><a href="#cb44-121"></a>n2 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span> <span class="op">*</span> <span class="bu">len</span>(words))</span>
<span id="cb44-122"><a href="#cb44-122"></a></span>
<span id="cb44-123"><a href="#cb44-123"></a>Xtr, Ytr <span class="op">=</span> buid_dataset(words[:n1])        <span class="co"># 80#</span></span>
<span id="cb44-124"><a href="#cb44-124"></a>Xdev, Ydev <span class="op">=</span> buid_dataset(words[n1:n2])    <span class="co"># 10%</span></span>
<span id="cb44-125"><a href="#cb44-125"></a>Xte, Yte <span class="op">=</span> buid_dataset(words[n2:])        <span class="co"># 10%</span></span>
<span id="cb44-126"><a href="#cb44-126"></a></span>
<span id="cb44-127"><a href="#cb44-127"></a><span class="co"># torch.Size([182625, 3]) torch.Size([182625])</span></span>
<span id="cb44-128"><a href="#cb44-128"></a><span class="co"># torch.Size([22655, 3]) torch.Size([22655])</span></span>
<span id="cb44-129"><a href="#cb44-129"></a><span class="co"># torch.Size([22866, 3]) torch.Size([22866])</span></span>
<span id="cb44-130"><a href="#cb44-130"></a><span class="in">```</span></span>
<span id="cb44-131"><a href="#cb44-131"></a></span>
<span id="cb44-132"><a href="#cb44-132"></a><span class="fu">#### input and response preview</span></span>
<span id="cb44-133"><a href="#cb44-133"></a></span>
<span id="cb44-136"><a href="#cb44-136"></a><span class="in">```{python}</span></span>
<span id="cb44-137"><a href="#cb44-137"></a><span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(Xtr[:<span class="dv">20</span>], Ytr[:<span class="dv">20</span>]):</span>
<span id="cb44-138"><a href="#cb44-138"></a>  <span class="bu">print</span>(<span class="st">''</span>.join(itos[ix.item()] <span class="cf">for</span> ix <span class="kw">in</span> x), <span class="st">'---&gt;'</span>, itos[y.item()])</span>
<span id="cb44-139"><a href="#cb44-139"></a><span class="in">```</span></span>
<span id="cb44-140"><a href="#cb44-140"></a></span>
<span id="cb44-141"><a href="#cb44-141"></a><span class="in">``` md</span></span>
<span id="cb44-142"><a href="#cb44-142"></a>... ---&gt; y</span>
<span id="cb44-143"><a href="#cb44-143"></a>..y ---&gt; u</span>
<span id="cb44-144"><a href="#cb44-144"></a>.yu ---&gt; h</span>
<span id="cb44-145"><a href="#cb44-145"></a>yuh ---&gt; e</span>
<span id="cb44-146"><a href="#cb44-146"></a>uhe ---&gt; n</span>
<span id="cb44-147"><a href="#cb44-147"></a>hen ---&gt; g</span>
<span id="cb44-148"><a href="#cb44-148"></a>eng ---&gt; .</span>
<span id="cb44-149"><a href="#cb44-149"></a>... ---&gt; d</span>
<span id="cb44-150"><a href="#cb44-150"></a>..d ---&gt; i</span>
<span id="cb44-151"><a href="#cb44-151"></a>.di ---&gt; o</span>
<span id="cb44-152"><a href="#cb44-152"></a>dio ---&gt; n</span>
<span id="cb44-153"><a href="#cb44-153"></a>ion ---&gt; d</span>
<span id="cb44-154"><a href="#cb44-154"></a>ond ---&gt; r</span>
<span id="cb44-155"><a href="#cb44-155"></a>ndr ---&gt; e</span>
<span id="cb44-156"><a href="#cb44-156"></a>dre ---&gt; .</span>
<span id="cb44-157"><a href="#cb44-157"></a>... ---&gt; x</span>
<span id="cb44-158"><a href="#cb44-158"></a>..x ---&gt; a</span>
<span id="cb44-159"><a href="#cb44-159"></a>.xa ---&gt; v</span>
<span id="cb44-160"><a href="#cb44-160"></a>xav ---&gt; i</span>
<span id="cb44-161"><a href="#cb44-161"></a>avi ---&gt; e</span>
<span id="cb44-162"><a href="#cb44-162"></a><span class="in">```</span></span>
<span id="cb44-163"><a href="#cb44-163"></a></span>
<span id="cb44-164"><a href="#cb44-164"></a><span class="fu">#### initializing objects in networks</span></span>
<span id="cb44-165"><a href="#cb44-165"></a></span>
<span id="cb44-166"><a href="#cb44-166"></a>Near copy paste of the layers we have developed in Part 3, I added some docstring to the classes.</span>
<span id="cb44-167"><a href="#cb44-167"></a></span>
<span id="cb44-168"><a href="#cb44-168"></a><span class="fu">##### class `Linear`</span></span>
<span id="cb44-169"><a href="#cb44-169"></a></span>
<span id="cb44-172"><a href="#cb44-172"></a><span class="in">```{python}</span></span>
<span id="cb44-173"><a href="#cb44-173"></a><span class="co">#| code-fold: true</span></span>
<span id="cb44-174"><a href="#cb44-174"></a><span class="kw">class</span> Linear:</span>
<span id="cb44-175"><a href="#cb44-175"></a>  <span class="co">"""</span></span>
<span id="cb44-176"><a href="#cb44-176"></a><span class="co">  Applies an affine linear transformation to the incoming data: y = xA^T + b.</span></span>
<span id="cb44-177"><a href="#cb44-177"></a></span>
<span id="cb44-178"><a href="#cb44-178"></a><span class="co">  This class implements a linear (fully connected) layer, which performs a linear</span></span>
<span id="cb44-179"><a href="#cb44-179"></a><span class="co">  transformation on the input tensor. It is typically used in neural network architectures</span></span>
<span id="cb44-180"><a href="#cb44-180"></a><span class="co">  to transform input features between layers.</span></span>
<span id="cb44-181"><a href="#cb44-181"></a></span>
<span id="cb44-182"><a href="#cb44-182"></a><span class="co">  Args:</span></span>
<span id="cb44-183"><a href="#cb44-183"></a><span class="co">      fan_in (int): Number of input features (input dimension).</span></span>
<span id="cb44-184"><a href="#cb44-184"></a><span class="co">      fan_out (int): Number of output features (output dimension).</span></span>
<span id="cb44-185"><a href="#cb44-185"></a><span class="co">      bias (bool, optional): Whether to include a learnable bias term.</span></span>
<span id="cb44-186"><a href="#cb44-186"></a><span class="co">          Defaults to True.</span></span>
<span id="cb44-187"><a href="#cb44-187"></a></span>
<span id="cb44-188"><a href="#cb44-188"></a><span class="co">  Attributes:</span></span>
<span id="cb44-189"><a href="#cb44-189"></a><span class="co">      weight (torch.Tensor): Weight matrix of shape (fan_in, fan_out),</span></span>
<span id="cb44-190"><a href="#cb44-190"></a><span class="co">          initialized using Kaiming initialization.</span></span>
<span id="cb44-191"><a href="#cb44-191"></a><span class="co">      bias (torch.Tensor or None): Bias vector of shape (fan_out),</span></span>
<span id="cb44-192"><a href="#cb44-192"></a><span class="co">          initialized to zeros if bias is True, otherwise None.</span></span>
<span id="cb44-193"><a href="#cb44-193"></a></span>
<span id="cb44-194"><a href="#cb44-194"></a><span class="co">  Methods:</span></span>
<span id="cb44-195"><a href="#cb44-195"></a><span class="co">      __call__(x): Applies the linear transformation to the input tensor x.</span></span>
<span id="cb44-196"><a href="#cb44-196"></a><span class="co">      parameters(): Returns a list of trainable parameters (weight and bias).</span></span>
<span id="cb44-197"><a href="#cb44-197"></a></span>
<span id="cb44-198"><a href="#cb44-198"></a><span class="co">  Example:</span></span>
<span id="cb44-199"><a href="#cb44-199"></a><span class="co">      &gt;&gt;&gt; layer = Linear(10, 5)  # Creates a linear layer with 10 input features and 5 output features</span></span>
<span id="cb44-200"><a href="#cb44-200"></a><span class="co">      &gt;&gt;&gt; x = torch.randn(3, 10)  # Input tensor with batch size 3 and 10 features</span></span>
<span id="cb44-201"><a href="#cb44-201"></a><span class="co">      &gt;&gt;&gt; output = layer(x)  # Applies linear transformation</span></span>
<span id="cb44-202"><a href="#cb44-202"></a><span class="co">      &gt;&gt;&gt; output.shape</span></span>
<span id="cb44-203"><a href="#cb44-203"></a><span class="co">      torch.Size([3, 5])</span></span>
<span id="cb44-204"><a href="#cb44-204"></a><span class="co">  """</span></span>
<span id="cb44-205"><a href="#cb44-205"></a></span>
<span id="cb44-206"><a href="#cb44-206"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, fan_in, fan_out, bias<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb44-207"><a href="#cb44-207"></a>    <span class="va">self</span>.weight <span class="op">=</span> torch.randn((fan_in, fan_out)) <span class="op">/</span> fan_in<span class="op">**</span><span class="fl">0.5</span> <span class="co"># note: kaiming init</span></span>
<span id="cb44-208"><a href="#cb44-208"></a>    <span class="va">self</span>.bias <span class="op">=</span> torch.zeros(fan_out) <span class="cf">if</span> bias <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb44-209"><a href="#cb44-209"></a></span>
<span id="cb44-210"><a href="#cb44-210"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb44-211"><a href="#cb44-211"></a>    <span class="va">self</span>.out <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.weight</span>
<span id="cb44-212"><a href="#cb44-212"></a>    <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb44-213"><a href="#cb44-213"></a>      <span class="va">self</span>.out <span class="op">+=</span> <span class="va">self</span>.bias</span>
<span id="cb44-214"><a href="#cb44-214"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb44-215"><a href="#cb44-215"></a></span>
<span id="cb44-216"><a href="#cb44-216"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb44-217"><a href="#cb44-217"></a>    <span class="cf">return</span> [<span class="va">self</span>.weight] <span class="op">+</span> ([] <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> [<span class="va">self</span>.bias])</span>
<span id="cb44-218"><a href="#cb44-218"></a><span class="in">```</span></span>
<span id="cb44-219"><a href="#cb44-219"></a></span>
<span id="cb44-220"><a href="#cb44-220"></a><span class="fu">##### class `BatchNorm1d`</span></span>
<span id="cb44-221"><a href="#cb44-221"></a></span>
<span id="cb44-224"><a href="#cb44-224"></a><span class="in">```{python}</span></span>
<span id="cb44-225"><a href="#cb44-225"></a><span class="co">#| code-fold: true</span></span>
<span id="cb44-226"><a href="#cb44-226"></a><span class="kw">class</span> BatchNorm1d:</span>
<span id="cb44-227"><a href="#cb44-227"></a>  <span class="co">"""</span></span>
<span id="cb44-228"><a href="#cb44-228"></a><span class="co">  Applies Batch Normalization to the input tensor, a technique to improve</span></span>
<span id="cb44-229"><a href="#cb44-229"></a><span class="co">  training stability and performance in deep neural networks.</span></span>
<span id="cb44-230"><a href="#cb44-230"></a></span>
<span id="cb44-231"><a href="#cb44-231"></a><span class="co">  Batch Normalization normalizes the input across the batch dimension,</span></span>
<span id="cb44-232"><a href="#cb44-232"></a><span class="co">  reducing internal covariate shift and allowing higher learning rates.</span></span>
<span id="cb44-233"><a href="#cb44-233"></a><span class="co">  This implementation supports both training and inference modes.</span></span>
<span id="cb44-234"><a href="#cb44-234"></a></span>
<span id="cb44-235"><a href="#cb44-235"></a><span class="co">  Args:</span></span>
<span id="cb44-236"><a href="#cb44-236"></a><span class="co">      dim (int): Number of features or channels to be normalized.</span></span>
<span id="cb44-237"><a href="#cb44-237"></a><span class="co">      eps (float, optional): A small constant added to the denominator for</span></span>
<span id="cb44-238"><a href="#cb44-238"></a><span class="co">          numerical stability to prevent division by zero.</span></span>
<span id="cb44-239"><a href="#cb44-239"></a><span class="co">          Defaults to 1e-5.</span></span>
<span id="cb44-240"><a href="#cb44-240"></a><span class="co">      momentum (float, optional): Momentum for updating running mean and</span></span>
<span id="cb44-241"><a href="#cb44-241"></a><span class="co">          variance during training. Controls the degree of exponential</span></span>
<span id="cb44-242"><a href="#cb44-242"></a><span class="co">          moving average. Defaults to 0.1.</span></span>
<span id="cb44-243"><a href="#cb44-243"></a></span>
<span id="cb44-244"><a href="#cb44-244"></a><span class="co">  Attributes:</span></span>
<span id="cb44-245"><a href="#cb44-245"></a><span class="co">      eps (float): Epsilon value for numerical stability.</span></span>
<span id="cb44-246"><a href="#cb44-246"></a><span class="co">      momentum (float): Momentum for running statistics update.</span></span>
<span id="cb44-247"><a href="#cb44-247"></a><span class="co">      training (bool): Indicates whether the layer is in training or inference mode.</span></span>
<span id="cb44-248"><a href="#cb44-248"></a><span class="co">      gamma (torch.Tensor): Learnable scale parameter of shape (dim,).</span></span>
<span id="cb44-249"><a href="#cb44-249"></a><span class="co">      beta (torch.Tensor): Learnable shift parameter of shape (dim,).</span></span>
<span id="cb44-250"><a href="#cb44-250"></a><span class="co">      running_mean (torch.Tensor): Exponential moving average of batch means.</span></span>
<span id="cb44-251"><a href="#cb44-251"></a><span class="co">      running_var (torch.Tensor): Exponential moving average of batch variances.</span></span>
<span id="cb44-252"><a href="#cb44-252"></a></span>
<span id="cb44-253"><a href="#cb44-253"></a><span class="co">  Methods:</span></span>
<span id="cb44-254"><a href="#cb44-254"></a><span class="co">      __call__(x): Applies batch normalization to the input tensor.</span></span>
<span id="cb44-255"><a href="#cb44-255"></a><span class="co">      parameters(): Returns learnable parameters (gamma and beta).</span></span>
<span id="cb44-256"><a href="#cb44-256"></a></span>
<span id="cb44-257"><a href="#cb44-257"></a><span class="co">  Key Normalization Steps:</span></span>
<span id="cb44-258"><a href="#cb44-258"></a><span class="co">  1. Compute batch mean and variance (in training mode)</span></span>
<span id="cb44-259"><a href="#cb44-259"></a><span class="co">  2. Normalize input by subtracting mean and dividing by standard deviation</span></span>
<span id="cb44-260"><a href="#cb44-260"></a><span class="co">  3. Apply learnable scale (gamma) and shift (beta) parameters</span></span>
<span id="cb44-261"><a href="#cb44-261"></a><span class="co">  4. Update running statistics during training</span></span>
<span id="cb44-262"><a href="#cb44-262"></a></span>
<span id="cb44-263"><a href="#cb44-263"></a><span class="co">  Example:</span></span>
<span id="cb44-264"><a href="#cb44-264"></a><span class="co">      &gt;&gt;&gt; batch_norm = BatchNorm1d(64)  # For 64-channel input</span></span>
<span id="cb44-265"><a href="#cb44-265"></a><span class="co">      &gt;&gt;&gt; x = torch.randn(32, 64)  # Batch of 32 samples with 64 features</span></span>
<span id="cb44-266"><a href="#cb44-266"></a><span class="co">      &gt;&gt;&gt; normalized_x = batch_norm(x)  # Apply batch normalization</span></span>
<span id="cb44-267"><a href="#cb44-267"></a><span class="co">      &gt;&gt;&gt; normalized_x.shape</span></span>
<span id="cb44-268"><a href="#cb44-268"></a><span class="co">      torch.Size([32, 64])</span></span>
<span id="cb44-269"><a href="#cb44-269"></a></span>
<span id="cb44-270"><a href="#cb44-270"></a><span class="co">  Note:</span></span>
<span id="cb44-271"><a href="#cb44-271"></a><span class="co">      - Supports both 2D (batch, features) and 3D (batch, channels, sequence) input tensors</span></span>
<span id="cb44-272"><a href="#cb44-272"></a><span class="co">      - During inference, uses running statistics instead of batch statistics</span></span>
<span id="cb44-273"><a href="#cb44-273"></a><span class="co">  """</span></span>
<span id="cb44-274"><a href="#cb44-274"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, eps<span class="op">=</span><span class="fl">1e-5</span>, momentum<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb44-275"><a href="#cb44-275"></a>    <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb44-276"><a href="#cb44-276"></a>    <span class="va">self</span>.momentum <span class="op">=</span> momentum</span>
<span id="cb44-277"><a href="#cb44-277"></a>    <span class="va">self</span>.training <span class="op">=</span> <span class="va">True</span></span>
<span id="cb44-278"><a href="#cb44-278"></a>    <span class="co"># parameters (trained with backprop)</span></span>
<span id="cb44-279"><a href="#cb44-279"></a>    <span class="va">self</span>.gamma <span class="op">=</span> torch.ones(dim)</span>
<span id="cb44-280"><a href="#cb44-280"></a>    <span class="va">self</span>.beta <span class="op">=</span> torch.zeros(dim)</span>
<span id="cb44-281"><a href="#cb44-281"></a>    <span class="co"># buffers (trained with a running 'momentum update')</span></span>
<span id="cb44-282"><a href="#cb44-282"></a>    <span class="va">self</span>.running_mean <span class="op">=</span> torch.zeros(dim)</span>
<span id="cb44-283"><a href="#cb44-283"></a>    <span class="va">self</span>.running_var <span class="op">=</span> torch.ones(dim)</span>
<span id="cb44-284"><a href="#cb44-284"></a></span>
<span id="cb44-285"><a href="#cb44-285"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb44-286"><a href="#cb44-286"></a>    <span class="co"># calculate the forward pass</span></span>
<span id="cb44-287"><a href="#cb44-287"></a>    <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb44-288"><a href="#cb44-288"></a>      xmean <span class="op">=</span> x.mean(dim, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch mean</span></span>
<span id="cb44-289"><a href="#cb44-289"></a>      xvar <span class="op">=</span> x.var(dim, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch variance</span></span>
<span id="cb44-290"><a href="#cb44-290"></a>    <span class="cf">else</span>:</span>
<span id="cb44-291"><a href="#cb44-291"></a>      xmean <span class="op">=</span> <span class="va">self</span>.running_mean</span>
<span id="cb44-292"><a href="#cb44-292"></a>      xvar <span class="op">=</span> <span class="va">self</span>.running_var</span>
<span id="cb44-293"><a href="#cb44-293"></a>    xhat <span class="op">=</span> (x <span class="op">-</span> xmean) <span class="op">/</span> torch.sqrt(xvar <span class="op">+</span> <span class="va">self</span>.eps) <span class="co"># normalize to unit variance</span></span>
<span id="cb44-294"><a href="#cb44-294"></a>    <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.gamma <span class="op">*</span> xhat <span class="op">+</span> <span class="va">self</span>.beta</span>
<span id="cb44-295"><a href="#cb44-295"></a>    <span class="co"># update the buffers</span></span>
<span id="cb44-296"><a href="#cb44-296"></a>    <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb44-297"><a href="#cb44-297"></a>      <span class="cf">with</span> torch.no_grad():</span>
<span id="cb44-298"><a href="#cb44-298"></a>        <span class="va">self</span>.running_mean <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> <span class="va">self</span>.running_mean <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> xmean</span>
<span id="cb44-299"><a href="#cb44-299"></a>        <span class="va">self</span>.running_var <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> <span class="va">self</span>.running_var <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> xvar</span>
<span id="cb44-300"><a href="#cb44-300"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb44-301"><a href="#cb44-301"></a></span>
<span id="cb44-302"><a href="#cb44-302"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb44-303"><a href="#cb44-303"></a>    <span class="cf">return</span> [<span class="va">self</span>.gamma, <span class="va">self</span>.beta]</span>
<span id="cb44-304"><a href="#cb44-304"></a><span class="in">```</span></span>
<span id="cb44-305"><a href="#cb44-305"></a></span>
<span id="cb44-306"><a href="#cb44-306"></a><span class="fu">#### class `Tanh`</span></span>
<span id="cb44-307"><a href="#cb44-307"></a></span>
<span id="cb44-310"><a href="#cb44-310"></a><span class="in">```{python}</span></span>
<span id="cb44-311"><a href="#cb44-311"></a><span class="co">#| code-fold: true</span></span>
<span id="cb44-312"><a href="#cb44-312"></a><span class="kw">class</span> Tanh:</span>
<span id="cb44-313"><a href="#cb44-313"></a>    <span class="co">"""</span></span>
<span id="cb44-314"><a href="#cb44-314"></a><span class="co">    Hyperbolic Tangent (Tanh) Activation Function</span></span>
<span id="cb44-315"><a href="#cb44-315"></a></span>
<span id="cb44-316"><a href="#cb44-316"></a><span class="co">    Applies the hyperbolic tangent activation function element-wise to the input tensor.</span></span>
<span id="cb44-317"><a href="#cb44-317"></a><span class="co">    Tanh maps input values to the range [-1, 1], providing a symmetric and non-linear</span></span>
<span id="cb44-318"><a href="#cb44-318"></a><span class="co">    transformation that helps neural networks learn complex patterns.</span></span>
<span id="cb44-319"><a href="#cb44-319"></a></span>
<span id="cb44-320"><a href="#cb44-320"></a><span class="co">    Mathematical Definition:</span></span>
<span id="cb44-321"><a href="#cb44-321"></a><span class="co">    tanh(x) = (e^x - e^-x) / (e^x + e^-x)</span></span>
<span id="cb44-322"><a href="#cb44-322"></a></span>
<span id="cb44-323"><a href="#cb44-323"></a><span class="co">    Key Characteristics:</span></span>
<span id="cb44-324"><a href="#cb44-324"></a><span class="co">    - Output Range: [-1, 1]</span></span>
<span id="cb44-325"><a href="#cb44-325"></a><span class="co">    - Symmetric around the origin</span></span>
<span id="cb44-326"><a href="#cb44-326"></a><span class="co">    - Gradient is always less than 1, which helps mitigate the vanishing gradient problem</span></span>
<span id="cb44-327"><a href="#cb44-327"></a><span class="co">    - Commonly used in recurrent neural networks and hidden layers</span></span>
<span id="cb44-328"><a href="#cb44-328"></a></span>
<span id="cb44-329"><a href="#cb44-329"></a><span class="co">    Methods:</span></span>
<span id="cb44-330"><a href="#cb44-330"></a><span class="co">        __call__(x): Applies the Tanh activation to the input tensor.</span></span>
<span id="cb44-331"><a href="#cb44-331"></a><span class="co">        parameters(): Returns an empty list, as Tanh has no learnable parameters.</span></span>
<span id="cb44-332"><a href="#cb44-332"></a></span>
<span id="cb44-333"><a href="#cb44-333"></a><span class="co">    Attributes:</span></span>
<span id="cb44-334"><a href="#cb44-334"></a><span class="co">        out (torch.Tensor): Stores the output of the most recent forward pass.</span></span>
<span id="cb44-335"><a href="#cb44-335"></a></span>
<span id="cb44-336"><a href="#cb44-336"></a><span class="co">    Example:</span></span>
<span id="cb44-337"><a href="#cb44-337"></a><span class="co">        &gt;&gt;&gt; activation = Tanh()</span></span>
<span id="cb44-338"><a href="#cb44-338"></a><span class="co">        &gt;&gt;&gt; x = torch.tensor([-2.0, 0.0, 2.0])</span></span>
<span id="cb44-339"><a href="#cb44-339"></a><span class="co">        &gt;&gt;&gt; y = activation(x)</span></span>
<span id="cb44-340"><a href="#cb44-340"></a><span class="co">        &gt;&gt;&gt; y</span></span>
<span id="cb44-341"><a href="#cb44-341"></a><span class="co">        tensor([-0.9640, 0.0000, 0.9640])</span></span>
<span id="cb44-342"><a href="#cb44-342"></a></span>
<span id="cb44-343"><a href="#cb44-343"></a><span class="co">    Note:</span></span>
<span id="cb44-344"><a href="#cb44-344"></a><span class="co">        This implementation is stateless and does not modify the input tensor.</span></span>
<span id="cb44-345"><a href="#cb44-345"></a><span class="co">        The activation is applied element-wise, preserving the input tensor's shape.</span></span>
<span id="cb44-346"><a href="#cb44-346"></a><span class="co">    """</span></span>
<span id="cb44-347"><a href="#cb44-347"></a></span>
<span id="cb44-348"><a href="#cb44-348"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb44-349"><a href="#cb44-349"></a>        <span class="va">self</span>.out <span class="op">=</span> torch.tanh(x)</span>
<span id="cb44-350"><a href="#cb44-350"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb44-351"><a href="#cb44-351"></a></span>
<span id="cb44-352"><a href="#cb44-352"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb44-353"><a href="#cb44-353"></a>        <span class="cf">return</span> []</span>
<span id="cb44-354"><a href="#cb44-354"></a><span class="in">```</span></span>
<span id="cb44-355"><a href="#cb44-355"></a></span>
<span id="cb44-356"><a href="#cb44-356"></a><span class="fu">##### random number generator</span></span>
<span id="cb44-357"><a href="#cb44-357"></a></span>
<span id="cb44-360"><a href="#cb44-360"></a><span class="in">```{python}</span></span>
<span id="cb44-361"><a href="#cb44-361"></a>torch.manual_seed(<span class="dv">42</span>)<span class="op">;</span> <span class="co"># seed rng for reproducibility</span></span>
<span id="cb44-362"><a href="#cb44-362"></a><span class="in">```</span></span>
<span id="cb44-363"><a href="#cb44-363"></a></span>
<span id="cb44-364"><a href="#cb44-364"></a><span class="fu">##### network architecture</span></span>
<span id="cb44-365"><a href="#cb44-365"></a></span>
<span id="cb44-368"><a href="#cb44-368"></a><span class="in">```{python}</span></span>
<span id="cb44-369"><a href="#cb44-369"></a><span class="co">#| eval: false</span></span>
<span id="cb44-370"><a href="#cb44-370"></a><span class="co"># original network</span></span>
<span id="cb44-371"><a href="#cb44-371"></a>n_embd <span class="op">=</span> <span class="dv">10</span> <span class="co"># the dimensionality of the character embedding vectors</span></span>
<span id="cb44-372"><a href="#cb44-372"></a>n_hidden <span class="op">=</span> <span class="dv">200</span> <span class="co"># the number of neurons in the hidden layer of the MLP</span></span>
<span id="cb44-373"><a href="#cb44-373"></a></span>
<span id="cb44-374"><a href="#cb44-374"></a>C <span class="op">=</span> torch.rand((vocab_size, n_embd))</span>
<span id="cb44-375"><a href="#cb44-375"></a>layers <span class="op">=</span> [</span>
<span id="cb44-376"><a href="#cb44-376"></a>  Linear(n_embd <span class="op">*</span> block_size, n_hidden, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb44-377"><a href="#cb44-377"></a>  BatchNorm1d(n_hidden),</span>
<span id="cb44-378"><a href="#cb44-378"></a>  Tanh(),</span>
<span id="cb44-379"><a href="#cb44-379"></a>  Linear(n_hidden, vocab_size),</span>
<span id="cb44-380"><a href="#cb44-380"></a>]</span>
<span id="cb44-381"><a href="#cb44-381"></a></span>
<span id="cb44-382"><a href="#cb44-382"></a><span class="co"># parameter init</span></span>
<span id="cb44-383"><a href="#cb44-383"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb44-384"><a href="#cb44-384"></a>  layers[<span class="op">-</span><span class="dv">1</span>].weight <span class="op">*=</span> <span class="fl">0.1</span> <span class="co"># last layer make less confident</span></span>
<span id="cb44-385"><a href="#cb44-385"></a></span>
<span id="cb44-386"><a href="#cb44-386"></a>parameters <span class="op">=</span> [C] <span class="op">+</span> [p <span class="cf">for</span> layer <span class="kw">in</span> layers <span class="cf">for</span> p <span class="kw">in</span> layer.parameters()]</span>
<span id="cb44-387"><a href="#cb44-387"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters)) <span class="co"># number of parameters in total</span></span>
<span id="cb44-388"><a href="#cb44-388"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb44-389"><a href="#cb44-389"></a>  p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb44-390"><a href="#cb44-390"></a></span>
<span id="cb44-391"><a href="#cb44-391"></a><span class="co"># model params: 12097</span></span>
<span id="cb44-392"><a href="#cb44-392"></a><span class="in">```</span></span>
<span id="cb44-393"><a href="#cb44-393"></a></span>
<span id="cb44-394"><a href="#cb44-394"></a><span class="fu">#### optimization</span></span>
<span id="cb44-395"><a href="#cb44-395"></a></span>
<span id="cb44-398"><a href="#cb44-398"></a><span class="in">```{python}</span></span>
<span id="cb44-399"><a href="#cb44-399"></a><span class="co">#| eval: false</span></span>
<span id="cb44-400"><a href="#cb44-400"></a><span class="co"># same optimization as last time</span></span>
<span id="cb44-401"><a href="#cb44-401"></a>max_steps <span class="op">=</span> <span class="dv">200000</span></span>
<span id="cb44-402"><a href="#cb44-402"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb44-403"><a href="#cb44-403"></a>lossi <span class="op">=</span> []</span>
<span id="cb44-404"><a href="#cb44-404"></a></span>
<span id="cb44-405"><a href="#cb44-405"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb44-406"><a href="#cb44-406"></a></span>
<span id="cb44-407"><a href="#cb44-407"></a>  <span class="co"># minibatch construct</span></span>
<span id="cb44-408"><a href="#cb44-408"></a>  ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,))</span>
<span id="cb44-409"><a href="#cb44-409"></a>  Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix] <span class="co"># batch X,Y</span></span>
<span id="cb44-410"><a href="#cb44-410"></a></span>
<span id="cb44-411"><a href="#cb44-411"></a>  <span class="co"># forward pass</span></span>
<span id="cb44-412"><a href="#cb44-412"></a>  emb <span class="op">=</span> C[Xb] <span class="co"># embed the characters into vectors</span></span>
<span id="cb44-413"><a href="#cb44-413"></a>  x <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate the vectors</span></span>
<span id="cb44-414"><a href="#cb44-414"></a>  <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb44-415"><a href="#cb44-415"></a>    x <span class="op">=</span> layer(x)</span>
<span id="cb44-416"><a href="#cb44-416"></a>  loss <span class="op">=</span> F.cross_entropy(x, Yb) <span class="co"># loss function</span></span>
<span id="cb44-417"><a href="#cb44-417"></a></span>
<span id="cb44-418"><a href="#cb44-418"></a>  <span class="co"># backward pass</span></span>
<span id="cb44-419"><a href="#cb44-419"></a>  <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb44-420"><a href="#cb44-420"></a>    p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb44-421"><a href="#cb44-421"></a>  loss.backward()</span>
<span id="cb44-422"><a href="#cb44-422"></a></span>
<span id="cb44-423"><a href="#cb44-423"></a>  <span class="co"># update: simple SGD</span></span>
<span id="cb44-424"><a href="#cb44-424"></a>  lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">150000</span> <span class="cf">else</span> <span class="fl">0.01</span> <span class="co"># step learning rate decay</span></span>
<span id="cb44-425"><a href="#cb44-425"></a>  <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb44-426"><a href="#cb44-426"></a>    p.data <span class="op">+=</span> <span class="op">-</span>lr <span class="op">*</span> p.grad</span>
<span id="cb44-427"><a href="#cb44-427"></a></span>
<span id="cb44-428"><a href="#cb44-428"></a>  <span class="co"># track stats</span></span>
<span id="cb44-429"><a href="#cb44-429"></a>  <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="co"># print every once in a while</span></span>
<span id="cb44-430"><a href="#cb44-430"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">:7d}</span><span class="ss">/</span><span class="sc">{</span>max_steps<span class="sc">:7d}</span><span class="ss">: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb44-431"><a href="#cb44-431"></a>  lossi.append(loss.log10().item())</span>
<span id="cb44-432"><a href="#cb44-432"></a><span class="in">```</span></span>
<span id="cb44-433"><a href="#cb44-433"></a></span>
<span id="cb44-434"><a href="#cb44-434"></a><span class="in">``` md</span></span>
<span id="cb44-435"><a href="#cb44-435"></a>     0/ 200000: 3.2885</span>
<span id="cb44-436"><a href="#cb44-436"></a>  10000/ 200000: 2.3938</span>
<span id="cb44-437"><a href="#cb44-437"></a>  20000/ 200000: 2.1235</span>
<span id="cb44-438"><a href="#cb44-438"></a>  30000/ 200000: 1.9222</span>
<span id="cb44-439"><a href="#cb44-439"></a>  40000/ 200000: 2.2440</span>
<span id="cb44-440"><a href="#cb44-440"></a>  50000/ 200000: 2.1108</span>
<span id="cb44-441"><a href="#cb44-441"></a>  60000/ 200000: 2.0624</span>
<span id="cb44-442"><a href="#cb44-442"></a>  70000/ 200000: 2.0893</span>
<span id="cb44-443"><a href="#cb44-443"></a>  80000/ 200000: 2.4173</span>
<span id="cb44-444"><a href="#cb44-444"></a>  90000/ 200000: 1.9744</span>
<span id="cb44-445"><a href="#cb44-445"></a> 100000/ 200000: 2.0883</span>
<span id="cb44-446"><a href="#cb44-446"></a> 110000/ 200000: 2.4538</span>
<span id="cb44-447"><a href="#cb44-447"></a> 120000/ 200000: 1.9535</span>
<span id="cb44-448"><a href="#cb44-448"></a> 130000/ 200000: 1.8980</span>
<span id="cb44-449"><a href="#cb44-449"></a> 140000/ 200000: 2.1196</span>
<span id="cb44-450"><a href="#cb44-450"></a> 150000/ 200000: 2.3550</span>
<span id="cb44-451"><a href="#cb44-451"></a> 160000/ 200000: 2.2957</span>
<span id="cb44-452"><a href="#cb44-452"></a> 170000/ 200000: 2.0286</span>
<span id="cb44-453"><a href="#cb44-453"></a> 180000/ 200000: 2.2379</span>
<span id="cb44-454"><a href="#cb44-454"></a> 190000/ 200000: 2.3866</span>
<span id="cb44-455"><a href="#cb44-455"></a><span class="in">```</span></span>
<span id="cb44-456"><a href="#cb44-456"></a></span>
<span id="cb44-457"><a href="#cb44-457"></a><span class="fu">#### observe training process/evaluation</span></span>
<span id="cb44-458"><a href="#cb44-458"></a></span>
<span id="cb44-461"><a href="#cb44-461"></a><span class="in">```{python}</span></span>
<span id="cb44-462"><a href="#cb44-462"></a>plt.plot(lossi)</span>
<span id="cb44-463"><a href="#cb44-463"></a><span class="in">```</span></span>
<span id="cb44-464"><a href="#cb44-464"></a></span>
<span id="cb44-465"><a href="#cb44-465"></a><span class="al">![`lossi` plot at the beginning](1_pre_lossi.png)</span></span>
<span id="cb44-466"><a href="#cb44-466"></a></span>
<span id="cb44-467"><a href="#cb44-467"></a><span class="fu">#### calibrate the `batchnorm` after training</span></span>
<span id="cb44-468"><a href="#cb44-468"></a></span>
<span id="cb44-469"><a href="#cb44-469"></a>We should be using the running mean/variance of the whole dataset splits rather than the last mini-batch.</span>
<span id="cb44-470"><a href="#cb44-470"></a></span>
<span id="cb44-473"><a href="#cb44-473"></a><span class="in">```{python}</span></span>
<span id="cb44-474"><a href="#cb44-474"></a><span class="co"># put layers into eval mode (needed for batchnorm especially)</span></span>
<span id="cb44-475"><a href="#cb44-475"></a><span class="cf">for</span> layer <span class="kw">in</span> model.layers:</span>
<span id="cb44-476"><a href="#cb44-476"></a>  layer.training <span class="op">=</span> <span class="va">False</span></span>
<span id="cb44-477"><a href="#cb44-477"></a><span class="in">```</span></span>
<span id="cb44-478"><a href="#cb44-478"></a></span>
<span id="cb44-479"><a href="#cb44-479"></a><span class="fu">#### calculate on whole training and validation splits</span></span>
<span id="cb44-480"><a href="#cb44-480"></a></span>
<span id="cb44-483"><a href="#cb44-483"></a><span class="in">```{python}</span></span>
<span id="cb44-484"><a href="#cb44-484"></a><span class="co"># evaluate the loss</span></span>
<span id="cb44-485"><a href="#cb44-485"></a><span class="at">@torch.no_grad</span>() <span class="co"># this decorator disables gradient tracking inside pytorch</span></span>
<span id="cb44-486"><a href="#cb44-486"></a><span class="kw">def</span> split_loss(split):</span>
<span id="cb44-487"><a href="#cb44-487"></a>  x,y <span class="op">=</span> {</span>
<span id="cb44-488"><a href="#cb44-488"></a>    <span class="st">'train'</span>: (Xtr, Ytr),</span>
<span id="cb44-489"><a href="#cb44-489"></a>    <span class="st">'val'</span>: (Xdev, Ydev),</span>
<span id="cb44-490"><a href="#cb44-490"></a>    <span class="st">'test'</span>: (Xte, Yte),</span>
<span id="cb44-491"><a href="#cb44-491"></a>  }[split]</span>
<span id="cb44-492"><a href="#cb44-492"></a>  logits <span class="op">=</span> model(x)</span>
<span id="cb44-493"><a href="#cb44-493"></a>  loss <span class="op">=</span> F.cross_entropy(logits, y)</span>
<span id="cb44-494"><a href="#cb44-494"></a>  <span class="bu">print</span>(split, loss.item())</span>
<span id="cb44-495"><a href="#cb44-495"></a></span>
<span id="cb44-496"><a href="#cb44-496"></a>split_loss(<span class="st">'train'</span>)</span>
<span id="cb44-497"><a href="#cb44-497"></a>split_loss(<span class="st">'val'</span>)</span>
<span id="cb44-498"><a href="#cb44-498"></a><span class="in">```</span></span>
<span id="cb44-499"><a href="#cb44-499"></a></span>
<span id="cb44-500"><a href="#cb44-500"></a>Pretty loss but there are still room for improve:</span>
<span id="cb44-501"><a href="#cb44-501"></a></span>
<span id="cb44-502"><a href="#cb44-502"></a><span class="in">``` md</span></span>
<span id="cb44-503"><a href="#cb44-503"></a>train 2.0467958450317383</span>
<span id="cb44-504"><a href="#cb44-504"></a>val 2.0989298820495605</span>
<span id="cb44-505"><a href="#cb44-505"></a><span class="in">```</span></span>
<span id="cb44-506"><a href="#cb44-506"></a></span>
<span id="cb44-507"><a href="#cb44-507"></a><span class="fu">#### sample from the model</span></span>
<span id="cb44-508"><a href="#cb44-508"></a></span>
<span id="cb44-509"><a href="#cb44-509"></a>Here are Names generated by the model till now, we have relatively name-like results that do not exist in the training set.</span>
<span id="cb44-510"><a href="#cb44-510"></a></span>
<span id="cb44-511"><a href="#cb44-511"></a><span class="in">``` md</span></span>
<span id="cb44-512"><a href="#cb44-512"></a>liz.</span>
<span id="cb44-513"><a href="#cb44-513"></a>layah.</span>
<span id="cb44-514"><a href="#cb44-514"></a>dan.</span>
<span id="cb44-515"><a href="#cb44-515"></a>hilon.</span>
<span id="cb44-516"><a href="#cb44-516"></a>avani.</span>
<span id="cb44-517"><a href="#cb44-517"></a>korron.</span>
<span id="cb44-518"><a href="#cb44-518"></a>aua.</span>
<span id="cb44-519"><a href="#cb44-519"></a>noon.</span>
<span id="cb44-520"><a href="#cb44-520"></a>bethalyn.</span>
<span id="cb44-521"><a href="#cb44-521"></a>thia.</span>
<span id="cb44-522"><a href="#cb44-522"></a>bote.</span>
<span id="cb44-523"><a href="#cb44-523"></a>jereanail.</span>
<span id="cb44-524"><a href="#cb44-524"></a>vitorien.</span>
<span id="cb44-525"><a href="#cb44-525"></a>zarashivonna.</span>
<span id="cb44-526"><a href="#cb44-526"></a>yakurrren.</span>
<span id="cb44-527"><a href="#cb44-527"></a>jovon.</span>
<span id="cb44-528"><a href="#cb44-528"></a>malynn.</span>
<span id="cb44-529"><a href="#cb44-529"></a>vanna.</span>
<span id="cb44-530"><a href="#cb44-530"></a>caparmana.</span>
<span id="cb44-531"><a href="#cb44-531"></a>shantymonse.</span>
<span id="cb44-532"><a href="#cb44-532"></a><span class="in">```</span></span>
<span id="cb44-533"><a href="#cb44-533"></a></span>
<span id="cb44-534"><a href="#cb44-534"></a><span class="fu">## let’s fix the learning rate plot</span></span>
<span id="cb44-535"><a href="#cb44-535"></a></span>
<span id="cb44-536"><a href="#cb44-536"></a>The plot for <span class="in">`lossi`</span> looks very crazy, it's because the batch size of 32 is way too few so this time we got lucky, and next time we got unlucky. And the mini-batch loss splashed too much. We should probably fix it.</span>
<span id="cb44-537"><a href="#cb44-537"></a></span>
<span id="cb44-538"><a href="#cb44-538"></a>We pivot to a row for every 1000 observations of <span class="in">`lossi`</span> and calculate the mean, we end up have 200 observations which is easier to see.</span>
<span id="cb44-539"><a href="#cb44-539"></a></span>
<span id="cb44-542"><a href="#cb44-542"></a><span class="in">```{python}</span></span>
<span id="cb44-543"><a href="#cb44-543"></a>plt.plot(torch.tensor(lossi).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1000</span>).mean(<span class="dv">1</span>))</span>
<span id="cb44-544"><a href="#cb44-544"></a><span class="in">```</span></span>
<span id="cb44-545"><a href="#cb44-545"></a></span>
<span id="cb44-546"><a href="#cb44-546"></a>We can also observe the learning rate decay at 150k training loops.</span>
<span id="cb44-547"><a href="#cb44-547"></a></span>
<span id="cb44-548"><a href="#cb44-548"></a><span class="al">![`lossi` plot fixed](2_enhance_lossi.png)</span></span>
<span id="cb44-549"><a href="#cb44-549"></a></span>
<span id="cb44-550"><a href="#cb44-550"></a><span class="fu">## pytorchifying our code: layers, containers, `torch.nn`, fun bugs</span></span>
<span id="cb44-551"><a href="#cb44-551"></a></span>
<span id="cb44-552"><a href="#cb44-552"></a>Now we notice that we still have the embedding operation lying outside the pytorch-ified layers. It basically creating a lookup table <span class="in">`C`</span>, embedding it with our data <span class="in">`Y`</span> (or <span class="in">`Yb`</span>), then stretching out to row with <span class="in">`view()`</span> which is very cheap in PyTorch as no more memory creation is needed.</span>
<span id="cb44-553"><a href="#cb44-553"></a></span>
<span id="cb44-554"><a href="#cb44-554"></a>We modulize this by constructing 2 classes:</span>
<span id="cb44-555"><a href="#cb44-555"></a></span>
<span id="cb44-558"><a href="#cb44-558"></a><span class="in">```{python}</span></span>
<span id="cb44-559"><a href="#cb44-559"></a><span class="kw">class</span> Embedding:</span>
<span id="cb44-560"><a href="#cb44-560"></a></span>
<span id="cb44-561"><a href="#cb44-561"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_embeddings, embedding_dim):</span>
<span id="cb44-562"><a href="#cb44-562"></a>    <span class="va">self</span>.weight <span class="op">=</span> torch.randn((num_embeddings, embedding_dim))</span>
<span id="cb44-563"><a href="#cb44-563"></a></span>
<span id="cb44-564"><a href="#cb44-564"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, IX):</span>
<span id="cb44-565"><a href="#cb44-565"></a>    <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.weight[IX]</span>
<span id="cb44-566"><a href="#cb44-566"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb44-567"><a href="#cb44-567"></a></span>
<span id="cb44-568"><a href="#cb44-568"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb44-569"><a href="#cb44-569"></a>    <span class="cf">return</span> [<span class="va">self</span>.weight]</span>
<span id="cb44-570"><a href="#cb44-570"></a></span>
<span id="cb44-571"><a href="#cb44-571"></a><span class="co"># -----------------------------------------------------------------------------------------------</span></span>
<span id="cb44-572"><a href="#cb44-572"></a><span class="kw">class</span> Flatten:</span>
<span id="cb44-573"><a href="#cb44-573"></a></span>
<span id="cb44-574"><a href="#cb44-574"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb44-575"><a href="#cb44-575"></a>    <span class="va">self</span>.out <span class="op">=</span> x.view(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb44-576"><a href="#cb44-576"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb44-577"><a href="#cb44-577"></a></span>
<span id="cb44-578"><a href="#cb44-578"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb44-579"><a href="#cb44-579"></a>    <span class="cf">return</span> []</span>
<span id="cb44-580"><a href="#cb44-580"></a><span class="in">```</span></span>
<span id="cb44-581"><a href="#cb44-581"></a></span>
<span id="cb44-582"><a href="#cb44-582"></a>Now we can re-define the <span class="in">`layers`</span> like this:</span>
<span id="cb44-583"><a href="#cb44-583"></a></span>
<span id="cb44-586"><a href="#cb44-586"></a><span class="in">```{python}</span></span>
<span id="cb44-587"><a href="#cb44-587"></a>layers <span class="op">=</span> [</span>
<span id="cb44-588"><a href="#cb44-588"></a>  Embedding(vocab_size, n_embd),</span>
<span id="cb44-589"><a href="#cb44-589"></a>  Flatten(),</span>
<span id="cb44-590"><a href="#cb44-590"></a>  Linear(n_embd <span class="op">*</span> block_size, n_hidden, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb44-591"><a href="#cb44-591"></a>  BatchNorm1d(n_hidden),</span>
<span id="cb44-592"><a href="#cb44-592"></a>  Tanh(),</span>
<span id="cb44-593"><a href="#cb44-593"></a>  Linear(n_hidden, vocab_size),</span>
<span id="cb44-594"><a href="#cb44-594"></a>]</span>
<span id="cb44-595"><a href="#cb44-595"></a><span class="in">```</span></span>
<span id="cb44-596"><a href="#cb44-596"></a></span>
<span id="cb44-597"><a href="#cb44-597"></a>and also remove the <span class="in">`C`</span>, <span class="in">`emb`</span> definition in the forward pass construction. Going futher, we will be not only pytorchifying the elements of <span class="in">`layers`</span> only, but also the <span class="in">`layers`</span> itself. In PyTorch, we have term <span class="in">`containers`</span>, which specifying how we organize the layers in a network. And what are we doing here is constructing layers sequentially, which is equivalent to <span class="in">`Sequential`</span> in the <span class="in">`containers`</span>:</span>
<span id="cb44-598"><a href="#cb44-598"></a></span>
<span id="cb44-601"><a href="#cb44-601"></a><span class="in">```{python}</span></span>
<span id="cb44-602"><a href="#cb44-602"></a><span class="kw">class</span> Sequential:</span>
<span id="cb44-603"><a href="#cb44-603"></a></span>
<span id="cb44-604"><a href="#cb44-604"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, layers):</span>
<span id="cb44-605"><a href="#cb44-605"></a>    <span class="va">self</span>.layers <span class="op">=</span> layers</span>
<span id="cb44-606"><a href="#cb44-606"></a></span>
<span id="cb44-607"><a href="#cb44-607"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb44-608"><a href="#cb44-608"></a>    <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb44-609"><a href="#cb44-609"></a>      x <span class="op">=</span> layer(x)</span>
<span id="cb44-610"><a href="#cb44-610"></a>    <span class="va">self</span>.out <span class="op">=</span> x</span>
<span id="cb44-611"><a href="#cb44-611"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb44-612"><a href="#cb44-612"></a></span>
<span id="cb44-613"><a href="#cb44-613"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb44-614"><a href="#cb44-614"></a>    <span class="co"># get parameters of all layers and stretch them out into one list</span></span>
<span id="cb44-615"><a href="#cb44-615"></a>    <span class="cf">return</span> [p <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers <span class="cf">for</span> p <span class="kw">in</span> layer.parameters()]</span>
<span id="cb44-616"><a href="#cb44-616"></a><span class="in">```</span></span>
<span id="cb44-617"><a href="#cb44-617"></a></span>
<span id="cb44-618"><a href="#cb44-618"></a>and wrapp the layers into our <span class="in">`model`</span>:</span>
<span id="cb44-619"><a href="#cb44-619"></a></span>
<span id="cb44-622"><a href="#cb44-622"></a><span class="in">```{python}</span></span>
<span id="cb44-623"><a href="#cb44-623"></a>model <span class="op">=</span> Sequential([</span>
<span id="cb44-624"><a href="#cb44-624"></a>  Embedding(vocab_size, n_embd),</span>
<span id="cb44-625"><a href="#cb44-625"></a>  Flatten(),</span>
<span id="cb44-626"><a href="#cb44-626"></a>  Linear(n_embd <span class="op">*</span> block_size, n_hidden, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb44-627"><a href="#cb44-627"></a>  BatchNorm1d(n_hidden),</span>
<span id="cb44-628"><a href="#cb44-628"></a>  Tanh(),</span>
<span id="cb44-629"><a href="#cb44-629"></a>  Linear(n_hidden, vocab_size),</span>
<span id="cb44-630"><a href="#cb44-630"></a>])</span>
<span id="cb44-631"><a href="#cb44-631"></a><span class="in">```</span></span>
<span id="cb44-632"><a href="#cb44-632"></a></span>
<span id="cb44-633"><a href="#cb44-633"></a><span class="fu"># 2 implementing `WaveNet`</span></span>
<span id="cb44-634"><a href="#cb44-634"></a></span>
<span id="cb44-635"><a href="#cb44-635"></a>So far with the classical MLP following Bengio et al. (2003), we have a *embedding layer* followed by a *hidden layer* and end up with a *activation layer*. Although we added more layer after the embedding, we could not make a significant progress.</span>
<span id="cb44-636"><a href="#cb44-636"></a></span>
<span id="cb44-637"><a href="#cb44-637"></a>The problem is we dont have a **naive way** of making the model bigger in a productive way. We are still in the case that we crushing all the characters into a single all the way at the begining. And even if we make this a bigger layer and add neurons it's still like silly to squash all that information so fast into a single step.</span>
<span id="cb44-638"><a href="#cb44-638"></a></span>
<span id="cb44-639"><a href="#cb44-639"></a><span class="fu">## overview: `WaveNet`</span></span>
<span id="cb44-640"><a href="#cb44-640"></a></span>
<span id="cb44-641"><a href="#cb44-641"></a><span class="al">![Visualization of the `WaveNet` idea - Progressive Fusion](fig3_wavenet.png)</span></span>
<span id="cb44-642"><a href="#cb44-642"></a></span>
<span id="cb44-643"><a href="#cb44-643"></a><span class="fu">## dataset bump the context size to 8</span></span>
<span id="cb44-644"><a href="#cb44-644"></a></span>
<span id="cb44-645"><a href="#cb44-645"></a>first we change the <span class="in">`block_size`</span> into <span class="in">`8`</span> and now our dataset looks like:</span>
<span id="cb44-646"><a href="#cb44-646"></a></span>
<span id="cb44-647"><a href="#cb44-647"></a><span class="in">``` md</span></span>
<span id="cb44-648"><a href="#cb44-648"></a>........ ---&gt; y</span>
<span id="cb44-649"><a href="#cb44-649"></a>.......y ---&gt; u</span>
<span id="cb44-650"><a href="#cb44-650"></a>......yu ---&gt; h</span>
<span id="cb44-651"><a href="#cb44-651"></a>.....yuh ---&gt; e</span>
<span id="cb44-652"><a href="#cb44-652"></a>....yuhe ---&gt; n</span>
<span id="cb44-653"><a href="#cb44-653"></a>...yuhen ---&gt; g</span>
<span id="cb44-654"><a href="#cb44-654"></a>..yuheng ---&gt; .</span>
<span id="cb44-655"><a href="#cb44-655"></a>........ ---&gt; d</span>
<span id="cb44-656"><a href="#cb44-656"></a>.......d ---&gt; i</span>
<span id="cb44-657"><a href="#cb44-657"></a>......di ---&gt; o</span>
<span id="cb44-658"><a href="#cb44-658"></a>.....dio ---&gt; n</span>
<span id="cb44-659"><a href="#cb44-659"></a>....dion ---&gt; d</span>
<span id="cb44-660"><a href="#cb44-660"></a>...diond ---&gt; r</span>
<span id="cb44-661"><a href="#cb44-661"></a>..diondr ---&gt; e</span>
<span id="cb44-662"><a href="#cb44-662"></a>.diondre ---&gt; .</span>
<span id="cb44-663"><a href="#cb44-663"></a>........ ---&gt; x</span>
<span id="cb44-664"><a href="#cb44-664"></a>.......x ---&gt; a</span>
<span id="cb44-665"><a href="#cb44-665"></a>......xa ---&gt; v</span>
<span id="cb44-666"><a href="#cb44-666"></a>.....xav ---&gt; i</span>
<span id="cb44-667"><a href="#cb44-667"></a>....xavi ---&gt; e</span>
<span id="cb44-668"><a href="#cb44-668"></a><span class="in">```</span></span>
<span id="cb44-669"><a href="#cb44-669"></a></span>
<span id="cb44-670"><a href="#cb44-670"></a>The model size now bumps up to 22k.</span>
<span id="cb44-671"><a href="#cb44-671"></a></span>
<span id="cb44-672"><a href="#cb44-672"></a><span class="fu">## re-running baseline code on `block_size = 8`</span></span>
<span id="cb44-673"><a href="#cb44-673"></a></span>
<span id="cb44-674"><a href="#cb44-674"></a>Just by lazily extending the context size to 8, we can already improve the model a little bit, the loss on validation split now is around <span class="in">`2.045`</span>. The names generated now look prettier:</span>
<span id="cb44-675"><a href="#cb44-675"></a></span>
<span id="cb44-676"><a href="#cb44-676"></a><span class="in">``` md</span></span>
<span id="cb44-677"><a href="#cb44-677"></a>zamari.</span>
<span id="cb44-678"><a href="#cb44-678"></a>brennis.</span>
<span id="cb44-679"><a href="#cb44-679"></a>shavia.</span>
<span id="cb44-680"><a href="#cb44-680"></a>wililke.</span>
<span id="cb44-681"><a href="#cb44-681"></a>obalyid.</span>
<span id="cb44-682"><a href="#cb44-682"></a>leenoluja.</span>
<span id="cb44-683"><a href="#cb44-683"></a>rianny.</span>
<span id="cb44-684"><a href="#cb44-684"></a>jordanoe.</span>
<span id="cb44-685"><a href="#cb44-685"></a>yuvalfue.</span>
<span id="cb44-686"><a href="#cb44-686"></a>ozleega.</span>
<span id="cb44-687"><a href="#cb44-687"></a>jemirene.</span>
<span id="cb44-688"><a href="#cb44-688"></a>polton.</span>
<span id="cb44-689"><a href="#cb44-689"></a>jawi.</span>
<span id="cb44-690"><a href="#cb44-690"></a>meyah.</span>
<span id="cb44-691"><a href="#cb44-691"></a>gekiniq.</span>
<span id="cb44-692"><a href="#cb44-692"></a>angelinne.</span>
<span id="cb44-693"><a href="#cb44-693"></a>tayler.</span>
<span id="cb44-694"><a href="#cb44-694"></a>catrician.</span>
<span id="cb44-695"><a href="#cb44-695"></a>kyearie.</span>
<span id="cb44-696"><a href="#cb44-696"></a>anderias.</span>
<span id="cb44-697"><a href="#cb44-697"></a><span class="in">```</span></span>
<span id="cb44-698"><a href="#cb44-698"></a></span>
<span id="cb44-699"><a href="#cb44-699"></a>Let's deem this as a baseline then we can start to implement <span class="in">`WaveNet`</span> and see how far we can go!</span>
<span id="cb44-700"><a href="#cb44-700"></a></span>
<span id="cb44-701"><a href="#cb44-701"></a><span class="fu">## implementing `WaveNet`</span></span>
<span id="cb44-702"><a href="#cb44-702"></a></span>
<span id="cb44-703"><a href="#cb44-703"></a>First, let's revisit the shape of the tensors along the way of the forward pass in our neural net:</span>
<span id="cb44-704"><a href="#cb44-704"></a></span>
<span id="cb44-707"><a href="#cb44-707"></a><span class="in">```{python}</span></span>
<span id="cb44-708"><a href="#cb44-708"></a><span class="co"># Look at a batch of just 4 examples</span></span>
<span id="cb44-709"><a href="#cb44-709"></a>ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (<span class="dv">4</span>,))</span>
<span id="cb44-710"><a href="#cb44-710"></a>Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix]</span>
<span id="cb44-711"><a href="#cb44-711"></a>logits <span class="op">=</span> model(Xb)</span>
<span id="cb44-712"><a href="#cb44-712"></a><span class="bu">print</span>(Xb.shape)</span>
<span id="cb44-713"><a href="#cb44-713"></a>Xb</span>
<span id="cb44-714"><a href="#cb44-714"></a></span>
<span id="cb44-715"><a href="#cb44-715"></a><span class="co"># &gt; torch.Size([4, 8]) # because the context length is now 8</span></span>
<span id="cb44-716"><a href="#cb44-716"></a><span class="co"># &gt; tensor([[ 0,  0,  0,  0,  0,  0, 13,  9],</span></span>
<span id="cb44-717"><a href="#cb44-717"></a><span class="co">#         [ 0,  0,  0,  0,  0,  0,  0,  0],</span></span>
<span id="cb44-718"><a href="#cb44-718"></a><span class="co">#         [ 0,  0,  0, 11,  5, 18, 15, 12],</span></span>
<span id="cb44-719"><a href="#cb44-719"></a><span class="co">#         [ 0,  0,  4, 15, 13,  9, 14,  9]])</span></span>
<span id="cb44-720"><a href="#cb44-720"></a></span>
<span id="cb44-721"><a href="#cb44-721"></a><span class="co"># Output of the embedding layer, each input is translated</span></span>
<span id="cb44-722"><a href="#cb44-722"></a><span class="co"># to 10 dimensional vector</span></span>
<span id="cb44-723"><a href="#cb44-723"></a>model.layers[<span class="dv">0</span>].out.shape</span>
<span id="cb44-724"><a href="#cb44-724"></a><span class="co"># &gt; torch.Size([4, 8, 10])</span></span>
<span id="cb44-725"><a href="#cb44-725"></a></span>
<span id="cb44-726"><a href="#cb44-726"></a><span class="co"># Output of Flatten layer, each 10-dim vector is concatenated</span></span>
<span id="cb44-727"><a href="#cb44-727"></a><span class="co"># to each other for all 8-dim context vectors</span></span>
<span id="cb44-728"><a href="#cb44-728"></a>model.layers[<span class="dv">1</span>].out.shape</span>
<span id="cb44-729"><a href="#cb44-729"></a><span class="co"># &gt; torch.Size([4, 80])</span></span>
<span id="cb44-730"><a href="#cb44-730"></a></span>
<span id="cb44-731"><a href="#cb44-731"></a><span class="co"># Output of the Linear layer, take 80 and create 200 channels,</span></span>
<span id="cb44-732"><a href="#cb44-732"></a><span class="co"># just via matrix mult</span></span>
<span id="cb44-733"><a href="#cb44-733"></a>model.layers[<span class="dv">2</span>].out.shape</span>
<span id="cb44-734"><a href="#cb44-734"></a><span class="co"># &gt; torch.Size([4, 200])</span></span>
<span id="cb44-735"><a href="#cb44-735"></a><span class="in">```</span></span>
<span id="cb44-736"><a href="#cb44-736"></a></span>
<span id="cb44-737"><a href="#cb44-737"></a>Now look into the Linear layer, which take the input <span class="in">`x`</span> in the forward pass, multiply by <span class="in">`weight`</span> and add the <span class="in">`bias`</span> in (there is broadcasting here). So the transformation in this layer looks like:</span>
<span id="cb44-738"><a href="#cb44-738"></a></span>
<span id="cb44-741"><a href="#cb44-741"></a><span class="in">```{python}</span></span>
<span id="cb44-742"><a href="#cb44-742"></a>(torch.randn(<span class="dv">4</span>, <span class="dv">80</span>) <span class="op">@</span> torch.randn(<span class="dv">80</span>, <span class="dv">200</span>) <span class="op">+</span> torch.randn(<span class="dv">200</span>)).shape</span>
<span id="cb44-743"><a href="#cb44-743"></a></span>
<span id="cb44-744"><a href="#cb44-744"></a><span class="co"># &gt; torch.Size([4, 200])</span></span>
<span id="cb44-745"><a href="#cb44-745"></a><span class="in">```</span></span>
<span id="cb44-746"><a href="#cb44-746"></a></span>
<span id="cb44-747"><a href="#cb44-747"></a>Input <span class="in">`x`</span> matrix here does not need to be 2 dimensional array. The matrix multiplication in PyTorch is quite powerfull, you can pass more than 2 dimensional array. And all dimensions will be preserved except the last one. Like this:</span>
<span id="cb44-748"><a href="#cb44-748"></a></span>
<span id="cb44-751"><a href="#cb44-751"></a><span class="in">```{python}</span></span>
<span id="cb44-752"><a href="#cb44-752"></a>(torch.randn(<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">80</span>) <span class="op">@</span> torch.randn(<span class="dv">80</span>, <span class="dv">200</span>) <span class="op">+</span> torch.randn(<span class="dv">200</span>)).shape</span>
<span id="cb44-753"><a href="#cb44-753"></a></span>
<span id="cb44-754"><a href="#cb44-754"></a><span class="co"># &gt; torch.Size([4, 5, 200])</span></span>
<span id="cb44-755"><a href="#cb44-755"></a><span class="in">```</span></span>
<span id="cb44-756"><a href="#cb44-756"></a></span>
<span id="cb44-757"><a href="#cb44-757"></a>Which we want to improve now is not just flatten the 8 characters input too fast at the beginning, we want to group them pair by pair to process them in parallel.</span>
<span id="cb44-758"><a href="#cb44-758"></a></span>
<span id="cb44-759"><a href="#cb44-759"></a><span class="in">``` md</span></span>
<span id="cb44-760"><a href="#cb44-760"></a>(x1 x2) (x3 x4) (x5 x6) (x7 x8)</span>
<span id="cb44-761"><a href="#cb44-761"></a><span class="in">```</span></span>
<span id="cb44-762"><a href="#cb44-762"></a></span>
<span id="cb44-763"><a href="#cb44-763"></a>Particularly for 8 characters block size we want to divide it into 4 groups \~ 4 pair of *bigrams*. We are increasing **the dimensions of the batch**.</span>
<span id="cb44-764"><a href="#cb44-764"></a></span>
<span id="cb44-767"><a href="#cb44-767"></a><span class="in">```{python}</span></span>
<span id="cb44-768"><a href="#cb44-768"></a>(torch.randn(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">20</span>) <span class="op">@</span> torch.randn(<span class="dv">20</span>, <span class="dv">200</span>) <span class="op">+</span> torch.randn(<span class="dv">200</span>)).shape</span>
<span id="cb44-769"><a href="#cb44-769"></a></span>
<span id="cb44-770"><a href="#cb44-770"></a><span class="co"># &gt; torch.Size([4, 4, 200])</span></span>
<span id="cb44-771"><a href="#cb44-771"></a><span class="in">```</span></span>
<span id="cb44-772"><a href="#cb44-772"></a></span>
<span id="cb44-773"><a href="#cb44-773"></a>How can we achieve this in PyTorch, we can index the odd and even indexes then pair them up.</span>
<span id="cb44-774"><a href="#cb44-774"></a></span>
<span id="cb44-777"><a href="#cb44-777"></a><span class="in">```{python}</span></span>
<span id="cb44-778"><a href="#cb44-778"></a>e <span class="op">=</span> torch.randn(<span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">10</span>) <span class="co"># 4 examples, 8 chars context size, 10-d embedding</span></span>
<span id="cb44-779"><a href="#cb44-779"></a><span class="co"># goal: want this to be (4, 4, 20) where consecutive 10-d vectors get concatenated</span></span>
<span id="cb44-780"><a href="#cb44-780"></a></span>
<span id="cb44-781"><a href="#cb44-781"></a>explicit <span class="op">=</span> torch.cat([e[:, ::<span class="dv">2</span>, :], e[:, <span class="dv">1</span>::<span class="dv">2</span>, :]], dim<span class="op">=</span><span class="dv">2</span>) <span class="co"># cat in the third dim</span></span>
<span id="cb44-782"><a href="#cb44-782"></a>explicit.shape</span>
<span id="cb44-783"><a href="#cb44-783"></a><span class="co"># &gt; torch.Size([4, 4, 20])</span></span>
<span id="cb44-784"><a href="#cb44-784"></a><span class="in">```</span></span>
<span id="cb44-785"><a href="#cb44-785"></a></span>
<span id="cb44-786"><a href="#cb44-786"></a>Of course, PyTorch provide a more efficient way to do this, using <span class="in">`view()`</span>:</span>
<span id="cb44-787"><a href="#cb44-787"></a></span>
<span id="cb44-790"><a href="#cb44-790"></a><span class="in">```{python}</span></span>
<span id="cb44-791"><a href="#cb44-791"></a>(e.view(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">20</span>) <span class="op">==</span> explicit).<span class="bu">all</span>()</span>
<span id="cb44-792"><a href="#cb44-792"></a><span class="co"># &gt; tensor(True)</span></span>
<span id="cb44-793"><a href="#cb44-793"></a><span class="in">```</span></span>
<span id="cb44-794"><a href="#cb44-794"></a></span>
<span id="cb44-795"><a href="#cb44-795"></a>We are going to modulize the <span class="in">`FlattenConsecutive`</span>:</span>
<span id="cb44-796"><a href="#cb44-796"></a></span>
<span id="cb44-799"><a href="#cb44-799"></a><span class="in">```{python}</span></span>
<span id="cb44-800"><a href="#cb44-800"></a><span class="kw">class</span> FlattenConsecutive:</span>
<span id="cb44-801"><a href="#cb44-801"></a></span>
<span id="cb44-802"><a href="#cb44-802"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n):</span>
<span id="cb44-803"><a href="#cb44-803"></a>    <span class="va">self</span>.n <span class="op">=</span> n</span>
<span id="cb44-804"><a href="#cb44-804"></a></span>
<span id="cb44-805"><a href="#cb44-805"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb44-806"><a href="#cb44-806"></a>    B, T, C <span class="op">=</span> x.shape</span>
<span id="cb44-807"><a href="#cb44-807"></a>    x <span class="op">=</span> x.view(B, T<span class="op">//</span><span class="va">self</span>.n, C<span class="op">*</span><span class="va">self</span>.n)</span>
<span id="cb44-808"><a href="#cb44-808"></a>    <span class="cf">if</span> x.shape[<span class="dv">1</span>] <span class="op">==</span> <span class="dv">1</span>: <span class="co"># spurious tensor, if it's 1, squeeze it</span></span>
<span id="cb44-809"><a href="#cb44-809"></a>      x <span class="op">=</span> x.squeeze(<span class="dv">1</span>)</span>
<span id="cb44-810"><a href="#cb44-810"></a>    <span class="va">self</span>.out <span class="op">=</span> x</span>
<span id="cb44-811"><a href="#cb44-811"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb44-812"><a href="#cb44-812"></a></span>
<span id="cb44-813"><a href="#cb44-813"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb44-814"><a href="#cb44-814"></a>    <span class="cf">return</span> []</span>
<span id="cb44-815"><a href="#cb44-815"></a><span class="in">```</span></span>
<span id="cb44-816"><a href="#cb44-816"></a></span>
<span id="cb44-817"><a href="#cb44-817"></a>and update the flatten layer to <span class="in">`FlattenConsecutive(block_size)`</span> (8) in our <span class="in">`model`</span>. We can observe the dimension of tensors in all layers:</span>
<span id="cb44-818"><a href="#cb44-818"></a></span>
<span id="cb44-821"><a href="#cb44-821"></a><span class="in">```{python}</span></span>
<span id="cb44-822"><a href="#cb44-822"></a><span class="cf">for</span> layer <span class="kw">in</span> model.layers:</span>
<span id="cb44-823"><a href="#cb44-823"></a>  <span class="bu">print</span>(layer.__class__.<span class="va">__name__</span>,<span class="st">": "</span>, <span class="bu">tuple</span>(layer.out.shape))</span>
<span id="cb44-824"><a href="#cb44-824"></a></span>
<span id="cb44-825"><a href="#cb44-825"></a><span class="co"># Embedding :  (4, 8, 10)</span></span>
<span id="cb44-826"><a href="#cb44-826"></a><span class="co"># Flatten :  (4, 80)</span></span>
<span id="cb44-827"><a href="#cb44-827"></a><span class="co"># Linear :  (4, 200)</span></span>
<span id="cb44-828"><a href="#cb44-828"></a><span class="co"># BatchNorm1d :  (4, 200)</span></span>
<span id="cb44-829"><a href="#cb44-829"></a><span class="co"># Tanh :  (4, 200)</span></span>
<span id="cb44-830"><a href="#cb44-830"></a><span class="co"># Linear :  (4, 27)</span></span>
<span id="cb44-831"><a href="#cb44-831"></a><span class="in">```</span></span>
<span id="cb44-832"><a href="#cb44-832"></a></span>
<span id="cb44-833"><a href="#cb44-833"></a>This is what we have currently. But as said, we dont want to flatten too fast, so we gonna flatten by 2 character, here is the update of the <span class="in">`model`</span> - 3 layers present the consecutive flatten <span class="in">`4 -&gt; 2 -&gt; 1`</span>:</span>
<span id="cb44-834"><a href="#cb44-834"></a></span>
<span id="cb44-837"><a href="#cb44-837"></a><span class="in">```{python}</span></span>
<span id="cb44-838"><a href="#cb44-838"></a>model <span class="op">=</span> Sequential([</span>
<span id="cb44-839"><a href="#cb44-839"></a>  Embedding(vocab_size, n_embd),</span>
<span id="cb44-840"><a href="#cb44-840"></a>  FlattenConsecutive(<span class="dv">2</span>), Linear(n_embd <span class="op">*</span> <span class="dv">2</span>, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb44-841"><a href="#cb44-841"></a>  FlattenConsecutive(<span class="dv">2</span>), Linear(n_hidden<span class="op">*</span><span class="dv">2</span>, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb44-842"><a href="#cb44-842"></a>  FlattenConsecutive(<span class="dv">2</span>), Linear(n_hidden<span class="op">*</span><span class="dv">2</span>, n_hidden, bias<span class="op">=</span><span class="va">False</span>), BatchNorm1d(n_hidden), Tanh(),</span>
<span id="cb44-843"><a href="#cb44-843"></a>  Linear(n_hidden, vocab_size),</span>
<span id="cb44-844"><a href="#cb44-844"></a>])</span>
<span id="cb44-845"><a href="#cb44-845"></a><span class="in">```</span></span>
<span id="cb44-846"><a href="#cb44-846"></a></span>
<span id="cb44-847"><a href="#cb44-847"></a>and here is tensors dimension flowing in forward pass:</span>
<span id="cb44-848"><a href="#cb44-848"></a></span>
<span id="cb44-849"><a href="#cb44-849"></a><span class="in">``` md</span></span>
<span id="cb44-850"><a href="#cb44-850"></a>Embedding :  (4, 8, 10)</span>
<span id="cb44-851"><a href="#cb44-851"></a>FlattenConsecutive :  (4, 4, 20)</span>
<span id="cb44-852"><a href="#cb44-852"></a>Linear :  (4, 4, 200)</span>
<span id="cb44-853"><a href="#cb44-853"></a>BatchNorm1d :  (4, 4, 200)</span>
<span id="cb44-854"><a href="#cb44-854"></a>Tanh :  (4, 4, 200)</span>
<span id="cb44-855"><a href="#cb44-855"></a>FlattenConsecutive :  (4, 2, 400)</span>
<span id="cb44-856"><a href="#cb44-856"></a>Linear :  (4, 2, 200)</span>
<span id="cb44-857"><a href="#cb44-857"></a>BatchNorm1d :  (4, 2, 200)</span>
<span id="cb44-858"><a href="#cb44-858"></a>Tanh :  (4, 2, 200)</span>
<span id="cb44-859"><a href="#cb44-859"></a>FlattenConsecutive :  (4, 400)</span>
<span id="cb44-860"><a href="#cb44-860"></a>Linear :  (4, 200)</span>
<span id="cb44-861"><a href="#cb44-861"></a>BatchNorm1d :  (4, 200)</span>
<span id="cb44-862"><a href="#cb44-862"></a>Tanh :  (4, 200)</span>
<span id="cb44-863"><a href="#cb44-863"></a>Linear :  (4, 27)</span>
<span id="cb44-864"><a href="#cb44-864"></a><span class="in">```</span></span>
<span id="cb44-865"><a href="#cb44-865"></a></span>
<span id="cb44-866"><a href="#cb44-866"></a>That's is, we have successfully implemented the <span class="in">`WaveNet`</span>.</span>
<span id="cb44-867"><a href="#cb44-867"></a></span>
<span id="cb44-868"><a href="#cb44-868"></a><span class="fu">## training the `WaveNet`: first pass</span></span>
<span id="cb44-869"><a href="#cb44-869"></a></span>
<span id="cb44-870"><a href="#cb44-870"></a>Now assume we use the same size of network (number of neurons), let's see if the <span class="in">`loss`</span> can be improved. We change the <span class="in">`n_hidden = 68`</span>, so that the total parameters of our network remain 22k. Below is update tensor dims for a batch (32):</span>
<span id="cb44-871"><a href="#cb44-871"></a></span>
<span id="cb44-872"><a href="#cb44-872"></a><span class="in">``` md</span></span>
<span id="cb44-873"><a href="#cb44-873"></a>Embedding :  (32, 8, 10)</span>
<span id="cb44-874"><a href="#cb44-874"></a>FlattenConsecutive :  (32, 4, 20)</span>
<span id="cb44-875"><a href="#cb44-875"></a>Linear :  (32, 4, 68)</span>
<span id="cb44-876"><a href="#cb44-876"></a>BatchNorm1d :  (32, 4, 68)</span>
<span id="cb44-877"><a href="#cb44-877"></a>Tanh :  (32, 4, 68)</span>
<span id="cb44-878"><a href="#cb44-878"></a>FlattenConsecutive :  (32, 2, 136)</span>
<span id="cb44-879"><a href="#cb44-879"></a>Linear :  (32, 2, 68)</span>
<span id="cb44-880"><a href="#cb44-880"></a>BatchNorm1d :  (32, 2, 68)</span>
<span id="cb44-881"><a href="#cb44-881"></a>Tanh :  (32, 2, 68)</span>
<span id="cb44-882"><a href="#cb44-882"></a>FlattenConsecutive :  (32, 136)</span>
<span id="cb44-883"><a href="#cb44-883"></a>Linear :  (32, 68)</span>
<span id="cb44-884"><a href="#cb44-884"></a>BatchNorm1d :  (32, 68)</span>
<span id="cb44-885"><a href="#cb44-885"></a>Tanh :  (32, 68)</span>
<span id="cb44-886"><a href="#cb44-886"></a>Linear :  (32, 27)</span>
<span id="cb44-887"><a href="#cb44-887"></a><span class="in">```</span></span>
<span id="cb44-888"><a href="#cb44-888"></a></span>
<span id="cb44-889"><a href="#cb44-889"></a>It turns out that we got almost identical result. There are 2 things:</span>
<span id="cb44-890"><a href="#cb44-890"></a></span>
<span id="cb44-891"><a href="#cb44-891"></a><span class="ss">1.  </span>We just constructed the architecture of <span class="in">`WaveNet`</span> but not tortured the model enough to find best set of hyperparameters; and</span>
<span id="cb44-892"><a href="#cb44-892"></a><span class="ss">2.  </span>We may have a bug in <span class="in">`BatchNorm1d`</span> layer, let's take a look into this.</span>
<span id="cb44-893"><a href="#cb44-893"></a></span>
<span id="cb44-894"><a href="#cb44-894"></a><span class="fu">## fixing `batchnorm1d` bug</span></span>
<span id="cb44-895"><a href="#cb44-895"></a></span>
<span id="cb44-896"><a href="#cb44-896"></a>Let's look at the <span class="in">`BatchNorm1d`</span> happen in the first flatten layer:</span>
<span id="cb44-897"><a href="#cb44-897"></a></span>
<span id="cb44-900"><a href="#cb44-900"></a><span class="in">```{python}</span></span>
<span id="cb44-901"><a href="#cb44-901"></a>e <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">4</span> , <span class="dv">68</span>)</span>
<span id="cb44-902"><a href="#cb44-902"></a>emean <span class="op">=</span> e.mean(<span class="dv">0</span>, keepdim <span class="op">=</span> <span class="va">True</span>) <span class="co"># 1, 4, 68</span></span>
<span id="cb44-903"><a href="#cb44-903"></a>evar <span class="op">=</span> e.var(<span class="dv">0</span>, keepdim <span class="op">=</span> <span class="va">True</span>) <span class="co"># 1, 4, 68</span></span>
<span id="cb44-904"><a href="#cb44-904"></a></span>
<span id="cb44-905"><a href="#cb44-905"></a>ehat <span class="op">=</span> (e <span class="op">-</span> emean) <span class="op">/</span> torch.sqrt(evar <span class="op">+</span> <span class="fl">1e-5</span>)</span>
<span id="cb44-906"><a href="#cb44-906"></a>ehat.shape</span>
<span id="cb44-907"><a href="#cb44-907"></a></span>
<span id="cb44-908"><a href="#cb44-908"></a><span class="co"># &gt; torch.Size([32, 4, 68])</span></span>
<span id="cb44-909"><a href="#cb44-909"></a><span class="in">```</span></span>
<span id="cb44-910"><a href="#cb44-910"></a></span>
<span id="cb44-911"><a href="#cb44-911"></a>For <span class="in">`ehat`</span>, everything is calculated properly, mean and variance are calculated to the batch and the 2nd dim <span class="in">`4`</span> is preserved. But for the <span class="in">`running_mean`</span>:</span>
<span id="cb44-912"><a href="#cb44-912"></a></span>
<span id="cb44-915"><a href="#cb44-915"></a><span class="in">```{python}</span></span>
<span id="cb44-916"><a href="#cb44-916"></a>model.layers[<span class="dv">3</span>].running_mean.shape</span>
<span id="cb44-917"><a href="#cb44-917"></a></span>
<span id="cb44-918"><a href="#cb44-918"></a><span class="co"># &gt; torch.Size([1, 4, 68])</span></span>
<span id="cb44-919"><a href="#cb44-919"></a><span class="in">```</span></span>
<span id="cb44-920"><a href="#cb44-920"></a></span>
<span id="cb44-921"><a href="#cb44-921"></a>We see it is (1, 4, 68) while we are expected it's 1 dimensional only which is defined in the init method (<span class="in">`torch.zeros(dim)`</span>). We are maintaining the batch norm in parallel over 4 x 68 channels individually and independently instead of just 68 channels. We want to treat this <span class="in">`4`</span> just like a batch norm dimension, ie everaging of <span class="in">`32 * 4`</span> numbers for 68 channels. Fortunately PyTorch <span class="in">`mean()`</span> method offer the reducing dimension not only for integer but also tuple.</span>
<span id="cb44-922"><a href="#cb44-922"></a></span>
<span id="cb44-925"><a href="#cb44-925"></a><span class="in">```{python}</span></span>
<span id="cb44-926"><a href="#cb44-926"></a>e <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">4</span> , <span class="dv">68</span>)</span>
<span id="cb44-927"><a href="#cb44-927"></a>emean <span class="op">=</span> e.mean((<span class="dv">0</span>,<span class="dv">1</span>), keepdim <span class="op">=</span> <span class="va">True</span>) <span class="co"># 1, 1, 68</span></span>
<span id="cb44-928"><a href="#cb44-928"></a>evar <span class="op">=</span> e.var((<span class="dv">0</span>,<span class="dv">1</span>), keepdim <span class="op">=</span> <span class="va">True</span>) <span class="co"># 1, 1, 68</span></span>
<span id="cb44-929"><a href="#cb44-929"></a></span>
<span id="cb44-930"><a href="#cb44-930"></a>ehat <span class="op">=</span> (e <span class="op">-</span> emean) <span class="op">/</span> torch.sqrt(evar <span class="op">+</span> <span class="fl">1e-5</span>)</span>
<span id="cb44-931"><a href="#cb44-931"></a>ehat.shape</span>
<span id="cb44-932"><a href="#cb44-932"></a></span>
<span id="cb44-933"><a href="#cb44-933"></a><span class="co"># &gt; torch.Size([32, 4, 68])</span></span>
<span id="cb44-934"><a href="#cb44-934"></a></span>
<span id="cb44-935"><a href="#cb44-935"></a>model.layers[<span class="dv">3</span>].running_mean.shape</span>
<span id="cb44-936"><a href="#cb44-936"></a></span>
<span id="cb44-937"><a href="#cb44-937"></a><span class="co"># &gt; torch.Size([1, 1, 68])</span></span>
<span id="cb44-938"><a href="#cb44-938"></a><span class="in">```</span></span>
<span id="cb44-939"><a href="#cb44-939"></a></span>
<span id="cb44-940"><a href="#cb44-940"></a>We now modify the <span class="in">`BatchNorm1d`</span> definition accordingly, only the training mean/var in the <span class="in">`__call__`</span> method:</span>
<span id="cb44-941"><a href="#cb44-941"></a></span>
<span id="cb44-944"><a href="#cb44-944"></a><span class="in">```{python}</span></span>
<span id="cb44-945"><a href="#cb44-945"></a><span class="co"># ---- remains the same</span></span>
<span id="cb44-946"><a href="#cb44-946"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb44-947"><a href="#cb44-947"></a>    <span class="co"># calculate the forward pass</span></span>
<span id="cb44-948"><a href="#cb44-948"></a>    <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb44-949"><a href="#cb44-949"></a>      <span class="cf">if</span> x.ndim <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb44-950"><a href="#cb44-950"></a>        dim <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb44-951"><a href="#cb44-951"></a>      <span class="cf">elif</span> x.ndim <span class="op">==</span> <span class="dv">3</span>:</span>
<span id="cb44-952"><a href="#cb44-952"></a>        dim <span class="op">=</span> (<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb44-953"><a href="#cb44-953"></a>      xmean <span class="op">=</span> x.mean(dim, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch mean</span></span>
<span id="cb44-954"><a href="#cb44-954"></a>      xvar <span class="op">=</span> x.var(dim, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch variance</span></span>
<span id="cb44-955"><a href="#cb44-955"></a>    <span class="cf">else</span>:</span>
<span id="cb44-956"><a href="#cb44-956"></a>      xmean <span class="op">=</span> <span class="va">self</span>.running_mean</span>
<span id="cb44-957"><a href="#cb44-957"></a>      xvar <span class="op">=</span> <span class="va">self</span>.running_var</span>
<span id="cb44-958"><a href="#cb44-958"></a>    xhat <span class="op">=</span> (x <span class="op">-</span> xmean) <span class="op">/</span> torch.sqrt(xvar <span class="op">+</span> <span class="va">self</span>.eps) <span class="co"># normalize to unit variance</span></span>
<span id="cb44-959"><a href="#cb44-959"></a>    <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.gamma <span class="op">*</span> xhat <span class="op">+</span> <span class="va">self</span>.beta</span>
<span id="cb44-960"><a href="#cb44-960"></a><span class="co"># ---- remaind the same</span></span>
<span id="cb44-961"><a href="#cb44-961"></a><span class="in">```</span></span>
<span id="cb44-962"><a href="#cb44-962"></a></span>
<span id="cb44-963"><a href="#cb44-963"></a><span class="fu">## re-training `WaveNet` with bug fix</span></span>
<span id="cb44-964"><a href="#cb44-964"></a></span>
<span id="cb44-965"><a href="#cb44-965"></a>Now retraining the network with bug fixed, we obtain a slightly better loss of <span class="in">`2.022`</span>. We just fixed the normalization term inside the network so they did not thrush too much so a little improvement only is expected.</span>
<span id="cb44-966"><a href="#cb44-966"></a></span>
<span id="cb44-967"><a href="#cb44-967"></a><span class="fu">## scaling up our `WaveNet`</span></span>
<span id="cb44-968"><a href="#cb44-968"></a></span>
<span id="cb44-969"><a href="#cb44-969"></a>Now we're ready to scale up our network and retrain everything, the model now have roughly 76k paramters. We finally passed the <span class="in">`2.0`</span> threshold and achieved the loss of <span class="in">`1.99`</span> on the validation split.</span>
<span id="cb44-970"><a href="#cb44-970"></a></span>
<span id="cb44-973"><a href="#cb44-973"></a><span class="in">```{python}</span></span>
<span id="cb44-974"><a href="#cb44-974"></a>n_embd <span class="op">=</span> <span class="dv">24</span></span>
<span id="cb44-975"><a href="#cb44-975"></a>n_hidden <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb44-976"><a href="#cb44-976"></a><span class="in">```</span></span>
<span id="cb44-977"><a href="#cb44-977"></a></span>
<span id="cb44-978"><a href="#cb44-978"></a>And here is final loss plot:</span>
<span id="cb44-979"><a href="#cb44-979"></a></span>
<span id="cb44-980"><a href="#cb44-980"></a><span class="al">![`lossi` final](final_lossi.png)</span></span>
<span id="cb44-981"><a href="#cb44-981"></a></span>
<span id="cb44-982"><a href="#cb44-982"></a><span class="fu"># 3 conclusions</span></span>
<span id="cb44-983"><a href="#cb44-983"></a></span>
<span id="cb44-984"><a href="#cb44-984"></a><span class="fu">## performance log</span></span>
<span id="cb44-985"><a href="#cb44-985"></a></span>
<span id="cb44-986"><a href="#cb44-986"></a>+--------------+-----------------------------------------------------------------+--------------------------+</span>
<span id="cb44-987"><a href="#cb44-987"></a>| Step         | What we did                                                     | Loss we got (accum)      |</span>
<span id="cb44-988"><a href="#cb44-988"></a>+==============+=================================================================+==========================+</span>
<span id="cb44-989"><a href="#cb44-989"></a>| 1            | original (3 character context + 200 hidden neurons, 12K params) | train 2.0467958450317383 |</span>
<span id="cb44-990"><a href="#cb44-990"></a>|              |                                                                 |                          |</span>
<span id="cb44-991"><a href="#cb44-991"></a>|              |                                                                 | val 2.0989298820495605   |</span>
<span id="cb44-992"><a href="#cb44-992"></a>+--------------+-----------------------------------------------------------------+--------------------------+</span>
<span id="cb44-993"><a href="#cb44-993"></a>| 2            | context: 3 -<span class="sc">\&gt;</span> 8 (22K params)                                   | train 1.9028635025024414 |</span>
<span id="cb44-994"><a href="#cb44-994"></a>|              |                                                                 |                          |</span>
<span id="cb44-995"><a href="#cb44-995"></a>|              |                                                                 | val 2.044949769973755    |</span>
<span id="cb44-996"><a href="#cb44-996"></a>+--------------+-----------------------------------------------------------------+--------------------------+</span>
<span id="cb44-997"><a href="#cb44-997"></a>| 3            | flat -<span class="sc">\&gt;</span> hierarchical (22K params)                              | train 1.9366059303283691 |</span>
<span id="cb44-998"><a href="#cb44-998"></a>|              |                                                                 |                          |</span>
<span id="cb44-999"><a href="#cb44-999"></a>|              |                                                                 | val 2.017268419265747    |</span>
<span id="cb44-1000"><a href="#cb44-1000"></a>+--------------+-----------------------------------------------------------------+--------------------------+</span>
<span id="cb44-1001"><a href="#cb44-1001"></a>| 4            | fix bug in <span class="in">`batchnorm1d`</span>                                        | train 1.9156142473220825 |</span>
<span id="cb44-1002"><a href="#cb44-1002"></a>|              |                                                                 |                          |</span>
<span id="cb44-1003"><a href="#cb44-1003"></a>|              |                                                                 | val 2.0228867530822754   |</span>
<span id="cb44-1004"><a href="#cb44-1004"></a>+--------------+-----------------------------------------------------------------+--------------------------+</span>
<span id="cb44-1005"><a href="#cb44-1005"></a>| 5            | scale up the network: <span class="in">`n_embd`</span> 24, <span class="in">`n_hidden`</span> 128 (76K params)  | train 1.7680459022521973 |</span>
<span id="cb44-1006"><a href="#cb44-1006"></a>|              |                                                                 |                          |</span>
<span id="cb44-1007"><a href="#cb44-1007"></a>|              |                                                                 | val 1.994154691696167    |</span>
<span id="cb44-1008"><a href="#cb44-1008"></a>+--------------+-----------------------------------------------------------------+--------------------------+</span>
<span id="cb44-1009"><a href="#cb44-1009"></a></span>
<span id="cb44-1010"><a href="#cb44-1010"></a>: Loss logs {tbl-colwidths="<span class="sc">\[</span>10,40,50<span class="sc">\]</span>" .striped .hover}</span>
<span id="cb44-1011"><a href="#cb44-1011"></a></span>
<span id="cb44-1012"><a href="#cb44-1012"></a><span class="fu">## experimental harness</span></span>
<span id="cb44-1013"><a href="#cb44-1013"></a></span>
<span id="cb44-1014"><a href="#cb44-1014"></a>The "harness" metaphor is apt because it's like a structured support system that allows researchers to systematically explore and optimize neural network configurations, much like a harness helps guide and support an athlete during training.</span>
<span id="cb44-1015"><a href="#cb44-1015"></a></span>
<span id="cb44-1016"><a href="#cb44-1016"></a>::: callout-note</span>
<span id="cb44-1017"><a href="#cb44-1017"></a>An experimental harness typically includes several key components:</span>
<span id="cb44-1018"><a href="#cb44-1018"></a></span>
<span id="cb44-1019"><a href="#cb44-1019"></a><span class="ss">1.  </span>Hyperparameter Search Space Definition: This involves specifying the range of hyperparameters to be explored, such as:</span>
<span id="cb44-1020"><a href="#cb44-1020"></a></span>
<span id="cb44-1021"><a href="#cb44-1021"></a><span class="ss">-   </span>Learning rates</span>
<span id="cb44-1022"><a href="#cb44-1022"></a><span class="ss">-   </span>Batch sizes</span>
<span id="cb44-1023"><a href="#cb44-1023"></a><span class="ss">-   </span>Network architecture depths</span>
<span id="cb44-1024"><a href="#cb44-1024"></a><span class="ss">-   </span>Activation functions</span>
<span id="cb44-1025"><a href="#cb44-1025"></a><span class="ss">-   </span>Regularization techniques</span>
<span id="cb44-1026"><a href="#cb44-1026"></a><span class="ss">-   </span>Dropout rates</span>
<span id="cb44-1027"><a href="#cb44-1027"></a></span>
<span id="cb44-1028"><a href="#cb44-1028"></a><span class="ss">2.  </span>Search Strategy: Methods for exploring the hyperparameter space, which can include:</span>
<span id="cb44-1029"><a href="#cb44-1029"></a></span>
<span id="cb44-1030"><a href="#cb44-1030"></a><span class="ss">-   </span>Grid search</span>
<span id="cb44-1031"><a href="#cb44-1031"></a><span class="ss">-   </span>Random search</span>
<span id="cb44-1032"><a href="#cb44-1032"></a><span class="ss">-   </span>Bayesian optimization</span>
<span id="cb44-1033"><a href="#cb44-1033"></a><span class="ss">-   </span>Evolutionary algorithms</span>
<span id="cb44-1034"><a href="#cb44-1034"></a><span class="ss">-   </span>Gradient-based optimization techniques</span>
<span id="cb44-1035"><a href="#cb44-1035"></a></span>
<span id="cb44-1036"><a href="#cb44-1036"></a><span class="ss">3.  </span>Evaluation Metrics: Predefined metrics to assess model performance, such as:</span>
<span id="cb44-1037"><a href="#cb44-1037"></a></span>
<span id="cb44-1038"><a href="#cb44-1038"></a><span class="ss">-   </span>Validation accuracy</span>
<span id="cb44-1039"><a href="#cb44-1039"></a><span class="ss">-   </span>Loss function values</span>
<span id="cb44-1040"><a href="#cb44-1040"></a><span class="ss">-   </span>Precision and recall</span>
<span id="cb44-1041"><a href="#cb44-1041"></a><span class="ss">-   </span>F1 score</span>
<span id="cb44-1042"><a href="#cb44-1042"></a><span class="ss">-   </span>Computational efficiency</span>
<span id="cb44-1043"><a href="#cb44-1043"></a></span>
<span id="cb44-1044"><a href="#cb44-1044"></a><span class="ss">4.  </span>Automated Experiment Management: Tools and scripts that can:</span>
<span id="cb44-1045"><a href="#cb44-1045"></a></span>
<span id="cb44-1046"><a href="#cb44-1046"></a><span class="ss">-   </span>Automatically generate and run different model configurations</span>
<span id="cb44-1047"><a href="#cb44-1047"></a><span class="ss">-   </span>Log results</span>
<span id="cb44-1048"><a href="#cb44-1048"></a><span class="ss">-   </span>Track experiments</span>
<span id="cb44-1049"><a href="#cb44-1049"></a><span class="ss">-   </span>Compare performance across different hyperparameter settings</span>
<span id="cb44-1050"><a href="#cb44-1050"></a></span>
<span id="cb44-1051"><a href="#cb44-1051"></a><span class="ss">5.  </span>Reproducibility Mechanisms: Ensuring that experiments can be repeated and validated, which includes:</span>
<span id="cb44-1052"><a href="#cb44-1052"></a></span>
<span id="cb44-1053"><a href="#cb44-1053"></a><span class="ss">-   </span>Fixed random seeds</span>
<span id="cb44-1054"><a href="#cb44-1054"></a><span class="ss">-   </span>Consistent data splitting</span>
<span id="cb44-1055"><a href="#cb44-1055"></a><span class="ss">-   </span>Versioning of datasets and configurations</span>
<span id="cb44-1056"><a href="#cb44-1056"></a>:::</span>
<span id="cb44-1057"><a href="#cb44-1057"></a></span>
<span id="cb44-1058"><a href="#cb44-1058"></a><span class="fu">## `WaveNet` but with “dilated causal convolutions”</span></span>
<span id="cb44-1059"><a href="#cb44-1059"></a></span>
<span id="cb44-1060"><a href="#cb44-1060"></a><span class="ss">-   </span>Convolution is a "for loop" applying a linear filter over space of some input sequence;</span>
<span id="cb44-1061"><a href="#cb44-1061"></a><span class="ss">-   </span>Not happen only in Python but also in Kernel</span>
<span id="cb44-1062"><a href="#cb44-1062"></a></span>
<span id="cb44-1063"><a href="#cb44-1063"></a><span class="fu">## `torch.nn`</span></span>
<span id="cb44-1064"><a href="#cb44-1064"></a></span>
<span id="cb44-1065"><a href="#cb44-1065"></a>We have implement alot of concepts in <span class="in">`torch.nn`</span>:</span>
<span id="cb44-1066"><a href="#cb44-1066"></a></span>
<span id="cb44-1067"><a href="#cb44-1067"></a><span class="ss">-   </span>containers: <span class="in">`Sequential`</span></span>
<span id="cb44-1068"><a href="#cb44-1068"></a><span class="ss">-   </span><span class="in">`Linear`</span>, <span class="in">`BatchNorm1d`</span>, <span class="in">`Tanh`</span>, <span class="in">`FlattenConsecutive`</span>, ...</span>
<span id="cb44-1069"><a href="#cb44-1069"></a></span>
<span id="cb44-1070"><a href="#cb44-1070"></a><span class="fu">## the development process of building deep neural nets &amp; going forward</span></span>
<span id="cb44-1071"><a href="#cb44-1071"></a></span>
<span id="cb44-1072"><a href="#cb44-1072"></a><span class="ss">-   </span>Spending a ton of time exploring PyTorch documentation, unfortunately it's not a good one;</span>
<span id="cb44-1073"><a href="#cb44-1073"></a><span class="ss">-   </span>Ton of time to make the shapes work: fan in, fan out, NLC or NLC, broadcasting, viewing, etc;</span>
<span id="cb44-1074"><a href="#cb44-1074"></a><span class="ss">-   </span>What we:</span>
<span id="cb44-1075"><a href="#cb44-1075"></a><span class="ss">    -   </span>done: implemented dilated causal convoluntional network;</span>
<span id="cb44-1076"><a href="#cb44-1076"></a><span class="ss">    -   </span>to be explores: residual and skip connections;</span>
<span id="cb44-1077"><a href="#cb44-1077"></a><span class="ss">    -   </span>to be explores: experimental harness;</span>
<span id="cb44-1078"><a href="#cb44-1078"></a><span class="ss">    -   </span>more mordern networks: RNN, LSTM, Transformer.</span>
<span id="cb44-1079"><a href="#cb44-1079"></a></span>
<span id="cb44-1080"><a href="#cb44-1080"></a><span class="fu"># 4 resources</span></span>
<span id="cb44-1081"><a href="#cb44-1081"></a></span>
<span id="cb44-1082"><a href="#cb44-1082"></a><span class="ss">1.  </span>WaveNet 2016 from DeepMind: <span class="ot">&lt;https://arxiv.org/abs/1609.03499&gt;</span>;</span>
<span id="cb44-1083"><a href="#cb44-1083"></a><span class="ss">2.  </span>Bengio et al. 2003 MLP LM: <span class="ot">&lt;https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&gt;</span>;</span>
<span id="cb44-1084"><a href="#cb44-1084"></a><span class="ss">3.  </span>Notebook: <span class="ot">&lt;https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part5_cnn1.ipynb&gt;</span>;</span>
<span id="cb44-1085"><a href="#cb44-1085"></a><span class="ss">4.  </span>DeepMind's blog post from 2016: <span class="ot">&lt;https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/&gt;</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block"><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"><i class="fa-brands fa-creative-commons" aria-label="creative-commons"></i></a> 2023-2025 Le Khac Tuan</span></p>
</div>   
    <div class="nav-footer-center">
<p><span class="faux-block"> Designed with <i class="fa-solid fa-heart" aria-label="heart"></i>, <span id="commit-info">Loading last commit…</span> </span></p>
</div>
    <div class="nav-footer-right">
<p><span class="faux-block">Made with <a href="https://quarto.org/">Quarto</a></span></p>
</div>
  </div>
</footer>
<script type="application/javascript" src="commit_info.js"></script>




</body></html>