<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tuan Le Khac">
<meta name="dcterms.date" content="2025-04-27">
<meta name="description" content="This is Tuan’s blog">

<title>Let’s build GPT, in code, spelled out! – Le Khac Tuan</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../img/rocket_1613268.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-8e54dfbe729680b42f22c627ac27a053.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-81b089a3b74ed4cf194f083418e7130b.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-89cb849060b93fd12025e4f44aaa3f02.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-27750d9d09892f3d6a52d1fd788c41c6.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Le Khac Tuan</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../curriculum/index.html"> 
<span class="menu-text">Curriculum</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../学汉语的日记.html"> 
<span class="menu-text">学汉语的日记</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../jiu_jitsu_journal/index.html"> 
<span class="menu-text">Jiu Jitsu Journal</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lktuan"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/tuanlekhac/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.facebook.com/toilatuan.lk/"> <i class="bi bi-facebook" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/Halle4231"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/@tuan_lekhac"> <i class="bi bi-youtube" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:tuan.lekhac0905@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Let’s build GPT, in code, spelled out!</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          Build a Generatively Pretrained Transformer (GPT), following the paper ‘Attention is All You Need’ and OpenAI’s GPT-2 / GPT-3
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">til</div>
                <div class="quarto-category">python</div>
                <div class="quarto-category">andrej karpathy</div>
                <div class="quarto-category">nn-z2h</div>
                <div class="quarto-category">neural networks</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://lktuan.github.io/">Tuan Le Khac</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 27, 2025</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">May 4, 2025</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#intro-chatgpt-transformers-nanogpt-shakespeare" id="toc-intro-chatgpt-transformers-nanogpt-shakespeare" class="nav-link active" data-scroll-target="#intro-chatgpt-transformers-nanogpt-shakespeare">1 intro: ChatGPT, Transformers, <code>nanoGPT</code>, Shakespeare</a></li>
  <li><a href="#baseline-language-modeling-code-setup" id="toc-baseline-language-modeling-code-setup" class="nav-link" data-scroll-target="#baseline-language-modeling-code-setup">2 baseline language modeling, code setup</a>
  <ul class="collapse">
  <li><a href="#reading-and-exploring-the-data" id="toc-reading-and-exploring-the-data" class="nav-link" data-scroll-target="#reading-and-exploring-the-data">2.1 reading and exploring the data</a></li>
  <li><a href="#tokenization-trainval-split" id="toc-tokenization-trainval-split" class="nav-link" data-scroll-target="#tokenization-trainval-split">2.2 tokenization, train/val split</a></li>
  <li><a href="#data-loader-batches-of-chunks-of-data" id="toc-data-loader-batches-of-chunks-of-data" class="nav-link" data-scroll-target="#data-loader-batches-of-chunks-of-data">2.3 data loader, batches of chunks of data</a></li>
  <li><a href="#simplest-baseline-bigram-language-model-loss-generation" id="toc-simplest-baseline-bigram-language-model-loss-generation" class="nav-link" data-scroll-target="#simplest-baseline-bigram-language-model-loss-generation">2.4 simplest baseline: bigram language model, loss, generation</a></li>
  <li><a href="#training-the-bigram-model" id="toc-training-the-bigram-model" class="nav-link" data-scroll-target="#training-the-bigram-model">2.5 training the bigram model</a></li>
  <li><a href="#port-our-code-to-a-script" id="toc-port-our-code-to-a-script" class="nav-link" data-scroll-target="#port-our-code-to-a-script">2.6 port our code to a script</a></li>
  </ul></li>
  <li><a href="#building-the-self-attention" id="toc-building-the-self-attention" class="nav-link" data-scroll-target="#building-the-self-attention">3 Building the “self-attention”</a>
  <ul class="collapse">
  <li><a href="#version-1-averaging-past-context-with-for-loops-the-weakest-form-of-aggregation" id="toc-version-1-averaging-past-context-with-for-loops-the-weakest-form-of-aggregation" class="nav-link" data-scroll-target="#version-1-averaging-past-context-with-for-loops-the-weakest-form-of-aggregation">3.1 version 1: averaging past context with for loops, the weakest form of aggregation</a></li>
  <li><a href="#the-trick-in-self-attention-matrix-multiply-as-weighted-aggregation" id="toc-the-trick-in-self-attention-matrix-multiply-as-weighted-aggregation" class="nav-link" data-scroll-target="#the-trick-in-self-attention-matrix-multiply-as-weighted-aggregation">3.2 the trick in self-attention: matrix multiply as weighted aggregation</a></li>
  <li><a href="#version-2-using-matrix-multiply" id="toc-version-2-using-matrix-multiply" class="nav-link" data-scroll-target="#version-2-using-matrix-multiply">3.3 version 2: using matrix multiply</a></li>
  <li><a href="#version-3-adding-softmax" id="toc-version-3-adding-softmax" class="nav-link" data-scroll-target="#version-3-adding-softmax">3.4 version 3: adding softmax</a></li>
  <li><a href="#minor-code-cleanup" id="toc-minor-code-cleanup" class="nav-link" data-scroll-target="#minor-code-cleanup">3.5 minor code cleanup</a></li>
  <li><a href="#positional-encoding" id="toc-positional-encoding" class="nav-link" data-scroll-target="#positional-encoding">3.6 positional encoding</a></li>
  <li><a href="#the-crux-of-the-video-version-4-self-attention" id="toc-the-crux-of-the-video-version-4-self-attention" class="nav-link" data-scroll-target="#the-crux-of-the-video-version-4-self-attention">3.7 THE CRUX OF THE VIDEO: version 4: self-attention</a></li>
  <li><a href="#note-1-attention-as-communication" id="toc-note-1-attention-as-communication" class="nav-link" data-scroll-target="#note-1-attention-as-communication">3.8 note 1: attention as communication</a></li>
  <li><a href="#note-2-attention-has-no-notion-of-space-operates-over-sets" id="toc-note-2-attention-has-no-notion-of-space-operates-over-sets" class="nav-link" data-scroll-target="#note-2-attention-has-no-notion-of-space-operates-over-sets">3.9 note 2: attention has no notion of space, operates over sets</a></li>
  <li><a href="#note-3-there-is-no-communication-across-batch-dimension" id="toc-note-3-there-is-no-communication-across-batch-dimension" class="nav-link" data-scroll-target="#note-3-there-is-no-communication-across-batch-dimension">3.10 note 3: there is no communication across batch dimension</a></li>
  <li><a href="#note-4-encoder-blocks-vs.-decoder-blocks" id="toc-note-4-encoder-blocks-vs.-decoder-blocks" class="nav-link" data-scroll-target="#note-4-encoder-blocks-vs.-decoder-blocks">3.11 note 4: encoder blocks vs.&nbsp;decoder blocks</a></li>
  <li><a href="#note-5-attention-vs.-self-attention-vs.-cross-attention" id="toc-note-5-attention-vs.-self-attention-vs.-cross-attention" class="nav-link" data-scroll-target="#note-5-attention-vs.-self-attention-vs.-cross-attention">3.12 note 5: attention vs.&nbsp;self-attention vs.&nbsp;cross-attention</a></li>
  <li><a href="#note-6-scaled-self-attention.-why-divide-by-sqrthead_size" id="toc-note-6-scaled-self-attention.-why-divide-by-sqrthead_size" class="nav-link" data-scroll-target="#note-6-scaled-self-attention.-why-divide-by-sqrthead_size">3.13 note 6: “scaled” self-attention. why divide by sqrt(head_size)</a></li>
  </ul></li>
  <li><a href="#building-the-transformer" id="toc-building-the-transformer" class="nav-link" data-scroll-target="#building-the-transformer">4 Building the Transformer</a>
  <ul class="collapse">
  <li><a href="#inserting-a-single-self-attention-block-to-our-network" id="toc-inserting-a-single-self-attention-block-to-our-network" class="nav-link" data-scroll-target="#inserting-a-single-self-attention-block-to-our-network">4.1 inserting a single self-attention block to our network</a></li>
  <li><a href="#multi-headed-self-attention" id="toc-multi-headed-self-attention" class="nav-link" data-scroll-target="#multi-headed-self-attention">4.2 multi-headed self-attention</a></li>
  <li><a href="#feedforward-layers-of-transformer-block" id="toc-feedforward-layers-of-transformer-block" class="nav-link" data-scroll-target="#feedforward-layers-of-transformer-block">4.3 feedforward layers of transformer block</a></li>
  <li><a href="#residual-connections" id="toc-residual-connections" class="nav-link" data-scroll-target="#residual-connections">4.4 residual connections</a></li>
  <li><a href="#layernorm-and-its-relationship-to-our-previous-batchnorm" id="toc-layernorm-and-its-relationship-to-our-previous-batchnorm" class="nav-link" data-scroll-target="#layernorm-and-its-relationship-to-our-previous-batchnorm">4.5 layernorm (and its relationship to our previous batchnorm)</a></li>
  <li><a href="#scaling-up-the-model-creating-a-few-variables.-adding-dropout" id="toc-scaling-up-the-model-creating-a-few-variables.-adding-dropout" class="nav-link" data-scroll-target="#scaling-up-the-model-creating-a-few-variables.-adding-dropout">4.6 scaling up the model! creating a few variables. adding dropout</a></li>
  </ul></li>
  <li><a href="#notes-on-transformer" id="toc-notes-on-transformer" class="nav-link" data-scroll-target="#notes-on-transformer">5 Notes on Transformer</a>
  <ul class="collapse">
  <li><a href="#encoder-vs.-decoder-vs.-both-transformers" id="toc-encoder-vs.-decoder-vs.-both-transformers" class="nav-link" data-scroll-target="#encoder-vs.-decoder-vs.-both-transformers">5.1 encoder vs.&nbsp;decoder vs.&nbsp;both (?) Transformers</a></li>
  <li><a href="#super-quick-walkthrough-of-nanogpt-batched-multi-headed-self-attention" id="toc-super-quick-walkthrough-of-nanogpt-batched-multi-headed-self-attention" class="nav-link" data-scroll-target="#super-quick-walkthrough-of-nanogpt-batched-multi-headed-self-attention">5.2 super quick walkthrough of nanoGPT, batched multi-headed self-attention</a></li>
  <li><a href="#back-to-chatgpt-gpt-3-pretraining-vs.-finetuning-rlhf" id="toc-back-to-chatgpt-gpt-3-pretraining-vs.-finetuning-rlhf" class="nav-link" data-scroll-target="#back-to-chatgpt-gpt-3-pretraining-vs.-finetuning-rlhf">5.3 back to ChatGPT, GPT-3, pretraining vs.&nbsp;finetuning, RLHF</a></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions">5.4 conclusions</a></li>
  </ul></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">6 resources</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="callout callout-style-default callout-important callout-titled" title="This is not orginal content!">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This is not orginal content!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Long time no LLMs, past months struggled with job-search and personal stuffs distracted me from learning AI. No I am continuing my favorite AI series Neural Networks, from Zero to Hero by Andrej Karpathy.</p>
<p>I will be learning to build a GPT from scratch. This is my note and hope I’ll survive :)</p>
<p>Links: <a href="https://youtu.be/kCc8FmEb1nY?si=4Fa3EAjuTQ5UbOFk" class="uri">https://youtu.be/kCc8FmEb1nY?si=4Fa3EAjuTQ5UbOFk</a></p>
</div>
</div>
<section id="intro-chatgpt-transformers-nanogpt-shakespeare" class="level1">
<h1>1 intro: ChatGPT, Transformers, <code>nanoGPT</code>, Shakespeare</h1>
<ul>
<li>ChatGPT (GPT stands for Generative Pre-trained Transformer) is a <em>probabilistic</em> system that for anyone’s prompt it can give us multiple answers. It models sequence of words (or token) and tries to predict the next word to complete the prompt we give;</li>
<li>Transformers, proposed by Vaswani et al.&nbsp;in the landmark paper “Attention is All You Need” back in 2017, is an architecture that did all those heavy lifting under the hood of ChatGPT;</li>
<li><code>nanoGPT</code> is “the simplest, fastest repository for training/finetuning medium-sized GPTs” written as a side project by Andrej. We gonna follow this code structure in this lecture, but will not rebuild the whole 124M params GPT-2;</li>
<li>Shakespearse tiny dataset will be the text data that we’ll be working on.</li>
</ul>
</section>
<section id="baseline-language-modeling-code-setup" class="level1">
<h1>2 baseline language modeling, code setup</h1>
<section id="reading-and-exploring-the-data" class="level2">
<h2 class="anchored" data-anchor-id="reading-and-exploring-the-data">2.1 reading and exploring the data</h2>
<p>The <em>tinyshakepeare</em> dataset contains 1,115,394 characters, here are the first 1000 ones look like:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource text number-lines code-with-copy"><code class="sourceCode"><span id="cb1-1"><a href="#cb1-1"></a>First Citizen:</span>
<span id="cb1-2"><a href="#cb1-2"></a>Before we proceed any further, hear me speak.</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a>All:</span>
<span id="cb1-5"><a href="#cb1-5"></a>Speak, speak.</span>
<span id="cb1-6"><a href="#cb1-6"></a></span>
<span id="cb1-7"><a href="#cb1-7"></a>First Citizen:</span>
<span id="cb1-8"><a href="#cb1-8"></a>You are all resolved rather to die than to famish?</span>
<span id="cb1-9"><a href="#cb1-9"></a></span>
<span id="cb1-10"><a href="#cb1-10"></a>All:</span>
<span id="cb1-11"><a href="#cb1-11"></a>Resolved. resolved.</span>
<span id="cb1-12"><a href="#cb1-12"></a></span>
<span id="cb1-13"><a href="#cb1-13"></a>First Citizen:</span>
<span id="cb1-14"><a href="#cb1-14"></a>First, you know Caius Marcius is chief enemy to the people.</span>
<span id="cb1-15"><a href="#cb1-15"></a></span>
<span id="cb1-16"><a href="#cb1-16"></a>All:</span>
<span id="cb1-17"><a href="#cb1-17"></a>We know't, we know't.</span>
<span id="cb1-18"><a href="#cb1-18"></a></span>
<span id="cb1-19"><a href="#cb1-19"></a>First Citizen:</span>
<span id="cb1-20"><a href="#cb1-20"></a>Let us kill him, and we'll have corn at our own price.</span>
<span id="cb1-21"><a href="#cb1-21"></a>Is't a verdict?</span>
<span id="cb1-22"><a href="#cb1-22"></a></span>
<span id="cb1-23"><a href="#cb1-23"></a>All:</span>
<span id="cb1-24"><a href="#cb1-24"></a>No more talking on't; let it be done: away, away!</span>
<span id="cb1-25"><a href="#cb1-25"></a></span>
<span id="cb1-26"><a href="#cb1-26"></a>Second Citizen:</span>
<span id="cb1-27"><a href="#cb1-27"></a>One word, good citizens.</span>
<span id="cb1-28"><a href="#cb1-28"></a></span>
<span id="cb1-29"><a href="#cb1-29"></a>First Citizen:</span>
<span id="cb1-30"><a href="#cb1-30"></a>We are accounted poor citizens, the patricians good.</span>
<span id="cb1-31"><a href="#cb1-31"></a>What authority surfeits on would relieve us: if they</span>
<span id="cb1-32"><a href="#cb1-32"></a>would yield us but the superfluity, while it were</span>
<span id="cb1-33"><a href="#cb1-33"></a>wholesome, we might guess they relieved us humanely;</span>
<span id="cb1-34"><a href="#cb1-34"></a>but they think we are too dear: the leanness that</span>
<span id="cb1-35"><a href="#cb1-35"></a>afflicts us, the object of our misery, is as an</span>
<span id="cb1-36"><a href="#cb1-36"></a>inventory to particularise their abundance; our</span>
<span id="cb1-37"><a href="#cb1-37"></a>sufferance is a gain to them Let us revenge this with</span>
<span id="cb1-38"><a href="#cb1-38"></a>our pikes, ere we become rakes: for the gods know I</span>
<span id="cb1-39"><a href="#cb1-39"></a>speak this in hunger for bread, not in thirst for revenge.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In contains 65 different characters, the first one was <em>space</em>:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource md number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1"></a> !$&amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This list of characters can be call the <em>dictionary</em>, same with previous lectures, we create mapping from characters to integers/indicesindices <code>stoi</code> and vice versa <code>itos</code>.</p>
</section>
<section id="tokenization-trainval-split" class="level2">
<h2 class="anchored" data-anchor-id="tokenization-trainval-split">2.2 tokenization, train/val split</h2>
<p>The two functions <code>encode</code> and <code>decode</code> can let us transition from string (for human reading) to number (for machine reading) and vice versa.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="bu">print</span>(encode(<span class="st">"hii there"</span>))</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="bu">print</span>(decode(encode(<span class="st">"hii there"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript"><span id="cb4-1"><a href="#cb4-1"></a>[<span class="dv">46</span><span class="op">,</span> <span class="dv">47</span><span class="op">,</span> <span class="dv">47</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">43</span>]</span>
<span id="cb4-2"><a href="#cb4-2"></a>hii there</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is a very simple <em>tokenization and detokenization</em>, there are more complex/effective ones out there in the industry, for eg., <a href="https://github.com/google/sentencepiece">SentencePiece</a> by Google, and <a href="https://github.com/openai/tiktoken">tiktoken</a> by OpenAI, which implement more sophisticated paradigm like <a href="https://aclanthology.org/P16-1162/">BPE</a>.</p>
<p>Then use this set of utility we can convert our dataset to PyTorch tensor:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">import</span> torch <span class="co"># we use PyTorch: https://pytorch.org</span></span>
<span id="cb5-2"><a href="#cb5-2"></a>data <span class="op">=</span> torch.tensor(encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="bu">print</span>(data.shape, data.dtype)</span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="bu">print</span>(data[:<span class="dv">100</span>]) <span class="co"># the first 100 characters we looked at earier will to the GPT look like this</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript"><span id="cb6-1"><a href="#cb6-1"></a>torch<span class="op">.</span><span class="fu">Size</span>([<span class="dv">1115394</span>]) torch<span class="op">.</span><span class="at">int64</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="fu">tensor</span>([<span class="dv">18</span><span class="op">,</span> <span class="dv">47</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">57</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">15</span><span class="op">,</span> <span class="dv">47</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">47</span><span class="op">,</span> <span class="dv">64</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">52</span><span class="op">,</span> <span class="dv">10</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span> <span class="dv">14</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">44</span><span class="op">,</span></span>
<span id="cb6-3"><a href="#cb6-3"></a>        <span class="dv">53</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">61</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">54</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">53</span><span class="op">,</span> <span class="dv">41</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">42</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">52</span><span class="op">,</span> <span class="dv">63</span><span class="op">,</span></span>
<span id="cb6-4"><a href="#cb6-4"></a>         <span class="dv">1</span><span class="op">,</span> <span class="dv">44</span><span class="op">,</span> <span class="dv">59</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span>  <span class="dv">6</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">51</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span></span>
<span id="cb6-5"><a href="#cb6-5"></a>        <span class="dv">57</span><span class="op">,</span> <span class="dv">54</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">49</span><span class="op">,</span>  <span class="dv">8</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span> <span class="dv">13</span><span class="op">,</span> <span class="dv">50</span><span class="op">,</span> <span class="dv">50</span><span class="op">,</span> <span class="dv">10</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span> <span class="dv">31</span><span class="op">,</span> <span class="dv">54</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">49</span><span class="op">,</span></span>
<span id="cb6-6"><a href="#cb6-6"></a>         <span class="dv">6</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">57</span><span class="op">,</span> <span class="dv">54</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">49</span><span class="op">,</span>  <span class="dv">8</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span> <span class="dv">18</span><span class="op">,</span> <span class="dv">47</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">57</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">15</span><span class="op">,</span> <span class="dv">47</span><span class="op">,</span></span>
<span id="cb6-7"><a href="#cb6-7"></a>        <span class="dv">58</span><span class="op">,</span> <span class="dv">47</span><span class="op">,</span> <span class="dv">64</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">52</span><span class="op">,</span> <span class="dv">10</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span> <span class="dv">37</span><span class="op">,</span> <span class="dv">53</span><span class="op">,</span> <span class="dv">59</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Finally for the very first pre-processing we want to use the last 10% of out dataset as the validation/development split. We dont want to build a model that mimick the Shakepeare’s tone, we want it to be generative/creative.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(data)) <span class="co"># first 90% will be train, rest val</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>train_data <span class="op">=</span> data[:n]</span>
<span id="cb7-3"><a href="#cb7-3"></a>val_data <span class="op">=</span> data[n:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="data-loader-batches-of-chunks-of-data" class="level2">
<h2 class="anchored" data-anchor-id="data-loader-batches-of-chunks-of-data">2.3 data loader, batches of chunks of data</h2>
<p>Now the idea of our character-level language model is to predict the next character(s) given a sequence of characters, with a certain maximum length, not the whole previous part of the dataset. Same with previos lecture, we can set it (<code>block_size</code>, or <code>context_length</code>) to <em>8</em>.</p>
<p>And the second constraint that we want to setup is the <em>batch size</em> of chunk that we feed to the <em>transformer</em>, it will reduce the cost of calculation while retain the efficiency of training. You can imagine the <code>context</code> and <code>target</code> like this:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb8-2"><a href="#cb8-2"></a>batch_size <span class="op">=</span> <span class="dv">4</span> <span class="co"># how many independent sequences will we process in parallel?</span></span>
<span id="cb8-3"><a href="#cb8-3"></a>block_size <span class="op">=</span> <span class="dv">8</span> <span class="co"># what is the maximum context length for predictions?</span></span>
<span id="cb8-4"><a href="#cb8-4"></a></span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb8-6"><a href="#cb8-6"></a>    <span class="co"># generate a small batch of data of inputs x and targets y</span></span>
<span id="cb8-7"><a href="#cb8-7"></a>    data <span class="op">=</span> train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span> <span class="cf">else</span> val_data</span>
<span id="cb8-8"><a href="#cb8-8"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb8-9"><a href="#cb8-9"></a>    x <span class="op">=</span> torch.stack([data[i:i<span class="op">+</span>block_size] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb8-10"><a href="#cb8-10"></a>    y <span class="op">=</span> torch.stack([data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>block_size<span class="op">+</span><span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb8-11"><a href="#cb8-11"></a>    <span class="cf">return</span> x, y</span>
<span id="cb8-12"><a href="#cb8-12"></a></span>
<span id="cb8-13"><a href="#cb8-13"></a>xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb8-14"><a href="#cb8-14"></a><span class="bu">print</span>(<span class="st">'inputs:'</span>)</span>
<span id="cb8-15"><a href="#cb8-15"></a><span class="bu">print</span>(xb.shape)</span>
<span id="cb8-16"><a href="#cb8-16"></a><span class="bu">print</span>(xb)</span>
<span id="cb8-17"><a href="#cb8-17"></a><span class="bu">print</span>(<span class="st">'targets:'</span>)</span>
<span id="cb8-18"><a href="#cb8-18"></a><span class="bu">print</span>(yb.shape)</span>
<span id="cb8-19"><a href="#cb8-19"></a><span class="bu">print</span>(yb)</span>
<span id="cb8-20"><a href="#cb8-20"></a></span>
<span id="cb8-21"><a href="#cb8-21"></a><span class="bu">print</span>(<span class="st">'----'</span>)</span>
<span id="cb8-22"><a href="#cb8-22"></a></span>
<span id="cb8-23"><a href="#cb8-23"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(batch_size): <span class="co"># batch dimension</span></span>
<span id="cb8-24"><a href="#cb8-24"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(block_size): <span class="co"># time dimension</span></span>
<span id="cb8-25"><a href="#cb8-25"></a>        context <span class="op">=</span> xb[b, :t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb8-26"><a href="#cb8-26"></a>        target <span class="op">=</span> yb[b,t]</span>
<span id="cb8-27"><a href="#cb8-27"></a>        <span class="bu">print</span>(<span class="ss">f"when input is </span><span class="sc">{</span>context<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss"> the target: </span><span class="sc">{</span>target<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript"><span id="cb9-1"><a href="#cb9-1"></a>inputs<span class="op">:</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>torch<span class="op">.</span><span class="fu">Size</span>([<span class="dv">4</span><span class="op">,</span> <span class="dv">8</span>])</span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="fu">tensor</span>([[<span class="dv">24</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span>  <span class="dv">5</span><span class="op">,</span> <span class="dv">57</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">43</span>]<span class="op">,</span></span>
<span id="cb9-4"><a href="#cb9-4"></a>        [<span class="dv">44</span><span class="op">,</span> <span class="dv">53</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">58</span>]<span class="op">,</span></span>
<span id="cb9-5"><a href="#cb9-5"></a>        [<span class="dv">52</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span>  <span class="dv">1</span>]<span class="op">,</span></span>
<span id="cb9-6"><a href="#cb9-6"></a>        [<span class="dv">25</span><span class="op">,</span> <span class="dv">17</span><span class="op">,</span> <span class="dv">27</span><span class="op">,</span> <span class="dv">10</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span> <span class="dv">21</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">54</span>]])</span>
<span id="cb9-7"><a href="#cb9-7"></a>targets<span class="op">:</span></span>
<span id="cb9-8"><a href="#cb9-8"></a>torch<span class="op">.</span><span class="fu">Size</span>([<span class="dv">4</span><span class="op">,</span> <span class="dv">8</span>])</span>
<span id="cb9-9"><a href="#cb9-9"></a><span class="fu">tensor</span>([[<span class="dv">43</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span>  <span class="dv">5</span><span class="op">,</span> <span class="dv">57</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">39</span>]<span class="op">,</span></span>
<span id="cb9-10"><a href="#cb9-10"></a>        [<span class="dv">53</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span>  <span class="dv">1</span>]<span class="op">,</span></span>
<span id="cb9-11"><a href="#cb9-11"></a>        [<span class="dv">58</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">46</span>]<span class="op">,</span></span>
<span id="cb9-12"><a href="#cb9-12"></a>        [<span class="dv">17</span><span class="op">,</span> <span class="dv">27</span><span class="op">,</span> <span class="dv">10</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span> <span class="dv">21</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">54</span><span class="op">,</span> <span class="dv">39</span>]])</span>
<span id="cb9-13"><a href="#cb9-13"></a><span class="op">----</span></span>
<span id="cb9-14"><a href="#cb9-14"></a>when input is [<span class="dv">24</span>] the target<span class="op">:</span> <span class="dv">43</span></span>
<span id="cb9-15"><a href="#cb9-15"></a>when input is [<span class="dv">24</span><span class="op">,</span> <span class="dv">43</span>] the target<span class="op">:</span> <span class="dv">58</span></span>
<span id="cb9-16"><a href="#cb9-16"></a>when input is [<span class="dv">24</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">58</span>] the target<span class="op">:</span> <span class="dv">5</span></span>
<span id="cb9-17"><a href="#cb9-17"></a>when input is [<span class="dv">24</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">5</span>] the target<span class="op">:</span> <span class="dv">57</span></span>
<span id="cb9-18"><a href="#cb9-18"></a>when input is [<span class="dv">24</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">5</span><span class="op">,</span> <span class="dv">57</span>] the target<span class="op">:</span> <span class="dv">1</span></span>
<span id="cb9-19"><a href="#cb9-19"></a>when input is [<span class="dv">24</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">5</span><span class="op">,</span> <span class="dv">57</span><span class="op">,</span> <span class="dv">1</span>] the target<span class="op">:</span> <span class="dv">46</span></span>
<span id="cb9-20"><a href="#cb9-20"></a>when input is [<span class="dv">24</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">5</span><span class="op">,</span> <span class="dv">57</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">46</span>] the target<span class="op">:</span> <span class="dv">43</span></span>
<span id="cb9-21"><a href="#cb9-21"></a>when input is [<span class="dv">24</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">5</span><span class="op">,</span> <span class="dv">57</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">43</span>] the target<span class="op">:</span> <span class="dv">39</span></span>
<span id="cb9-22"><a href="#cb9-22"></a>when input is [<span class="dv">44</span>] the target<span class="op">:</span> <span class="dv">53</span></span>
<span id="cb9-23"><a href="#cb9-23"></a>when input is [<span class="dv">44</span><span class="op">,</span> <span class="dv">53</span>] the target<span class="op">:</span> <span class="dv">56</span></span>
<span id="cb9-24"><a href="#cb9-24"></a>when input is [<span class="dv">44</span><span class="op">,</span> <span class="dv">53</span><span class="op">,</span> <span class="dv">56</span>] the target<span class="op">:</span> <span class="dv">1</span></span>
<span id="cb9-25"><a href="#cb9-25"></a>when input is [<span class="dv">44</span><span class="op">,</span> <span class="dv">53</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">1</span>] the target<span class="op">:</span> <span class="dv">58</span></span>
<span id="cb9-26"><a href="#cb9-26"></a>when input is [<span class="dv">44</span><span class="op">,</span> <span class="dv">53</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span>] the target<span class="op">:</span> <span class="dv">46</span></span>
<span id="cb9-27"><a href="#cb9-27"></a>when input is [<span class="dv">44</span><span class="op">,</span> <span class="dv">53</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span>] the target<span class="op">:</span> <span class="dv">39</span></span>
<span id="cb9-28"><a href="#cb9-28"></a>when input is [<span class="dv">44</span><span class="op">,</span> <span class="dv">53</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">39</span>] the target<span class="op">:</span> <span class="dv">58</span></span>
<span id="cb9-29"><a href="#cb9-29"></a>when input is [<span class="dv">44</span><span class="op">,</span> <span class="dv">53</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">58</span>] the target<span class="op">:</span> <span class="dv">1</span></span>
<span id="cb9-30"><a href="#cb9-30"></a>when input is [<span class="dv">52</span>] the target<span class="op">:</span> <span class="dv">58</span></span>
<span id="cb9-31"><a href="#cb9-31"></a>when input is [<span class="dv">52</span><span class="op">,</span> <span class="dv">58</span>] the target<span class="op">:</span> <span class="dv">1</span></span>
<span id="cb9-32"><a href="#cb9-32"></a>when input is [<span class="dv">52</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">1</span>] the target<span class="op">:</span> <span class="dv">58</span></span>
<span id="cb9-33"><a href="#cb9-33"></a>when input is [<span class="dv">52</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span>] the target<span class="op">:</span> <span class="dv">46</span></span>
<span id="cb9-34"><a href="#cb9-34"></a>when input is [<span class="dv">52</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span>] the target<span class="op">:</span> <span class="dv">39</span></span>
<span id="cb9-35"><a href="#cb9-35"></a>when input is [<span class="dv">52</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">39</span>] the target<span class="op">:</span> <span class="dv">58</span></span>
<span id="cb9-36"><a href="#cb9-36"></a>when input is [<span class="dv">52</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">58</span>] the target<span class="op">:</span> <span class="dv">1</span></span>
<span id="cb9-37"><a href="#cb9-37"></a>when input is [<span class="dv">52</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">1</span>] the target<span class="op">:</span> <span class="dv">46</span></span>
<span id="cb9-38"><a href="#cb9-38"></a>when input is [<span class="dv">25</span>] the target<span class="op">:</span> <span class="dv">17</span></span>
<span id="cb9-39"><a href="#cb9-39"></a>when input is [<span class="dv">25</span><span class="op">,</span> <span class="dv">17</span>] the target<span class="op">:</span> <span class="dv">27</span></span>
<span id="cb9-40"><a href="#cb9-40"></a>when input is [<span class="dv">25</span><span class="op">,</span> <span class="dv">17</span><span class="op">,</span> <span class="dv">27</span>] the target<span class="op">:</span> <span class="dv">10</span></span>
<span id="cb9-41"><a href="#cb9-41"></a>when input is [<span class="dv">25</span><span class="op">,</span> <span class="dv">17</span><span class="op">,</span> <span class="dv">27</span><span class="op">,</span> <span class="dv">10</span>] the target<span class="op">:</span> <span class="dv">0</span></span>
<span id="cb9-42"><a href="#cb9-42"></a>when input is [<span class="dv">25</span><span class="op">,</span> <span class="dv">17</span><span class="op">,</span> <span class="dv">27</span><span class="op">,</span> <span class="dv">10</span><span class="op">,</span> <span class="dv">0</span>] the target<span class="op">:</span> <span class="dv">21</span></span>
<span id="cb9-43"><a href="#cb9-43"></a>when input is [<span class="dv">25</span><span class="op">,</span> <span class="dv">17</span><span class="op">,</span> <span class="dv">27</span><span class="op">,</span> <span class="dv">10</span><span class="op">,</span> <span class="dv">0</span><span class="op">,</span> <span class="dv">21</span>] the target<span class="op">:</span> <span class="dv">1</span></span>
<span id="cb9-44"><a href="#cb9-44"></a>when input is [<span class="dv">25</span><span class="op">,</span> <span class="dv">17</span><span class="op">,</span> <span class="dv">27</span><span class="op">,</span> <span class="dv">10</span><span class="op">,</span> <span class="dv">0</span><span class="op">,</span> <span class="dv">21</span><span class="op">,</span> <span class="dv">1</span>] the target<span class="op">:</span> <span class="dv">54</span></span>
<span id="cb9-45"><a href="#cb9-45"></a>when input is [<span class="dv">25</span><span class="op">,</span> <span class="dv">17</span><span class="op">,</span> <span class="dv">27</span><span class="op">,</span> <span class="dv">10</span><span class="op">,</span> <span class="dv">0</span><span class="op">,</span> <span class="dv">21</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">54</span>] the target<span class="op">:</span> <span class="dv">39</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="simplest-baseline-bigram-language-model-loss-generation" class="level2">
<h2 class="anchored" data-anchor-id="simplest-baseline-bigram-language-model-loss-generation">2.4 simplest baseline: bigram language model, loss, generation</h2>
<p>We’ll go into the most simplest thing in the realm of NLP - the <code>bigram</code> language model ~ where the next character is predicted based on 1 single previous character.</p>
<p>Below we are constructing a child class of <code>nn.Module</code> which is a <code>bigram</code> language model under a <code>vocab_size</code> = 65 language space. The state of the model is presented by <code>token_embedding_table</code> ~ how it thinks about the next character given one.</p>
<p>We constructed a <code>loss</code> function based on <code>cross_entropy</code> to quantify the quality of the model, and also implemented a <code>generate</code> function to sample from the model.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="im">import</span> torch</span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb10-5"><a href="#cb10-5"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb10-6"><a href="#cb10-6"></a></span>
<span id="cb10-7"><a href="#cb10-7"></a><span class="kw">class</span> BigramLanguageModel(nn.Module):</span>
<span id="cb10-8"><a href="#cb10-8"></a></span>
<span id="cb10-9"><a href="#cb10-9"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size):</span>
<span id="cb10-10"><a href="#cb10-10"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-11"><a href="#cb10-11"></a>        <span class="co"># each token directly reads off the logits for the next token from a lookup table</span></span>
<span id="cb10-12"><a href="#cb10-12"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, vocab_size)</span>
<span id="cb10-13"><a href="#cb10-13"></a>        <span class="co"># 26x26 2D Tensor, represents possibility of each char right after a char.</span></span>
<span id="cb10-14"><a href="#cb10-14"></a></span>
<span id="cb10-15"><a href="#cb10-15"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb10-16"><a href="#cb10-16"></a></span>
<span id="cb10-17"><a href="#cb10-17"></a>        <span class="co"># idx and targets are both (B,T) tensor of integers</span></span>
<span id="cb10-18"><a href="#cb10-18"></a>        logits <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx) <span class="co"># (B,T,C)</span></span>
<span id="cb10-19"><a href="#cb10-19"></a>        <span class="co"># (Batch = 4,Time = 8,Channel = 65)</span></span>
<span id="cb10-20"><a href="#cb10-20"></a></span>
<span id="cb10-21"><a href="#cb10-21"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb10-22"><a href="#cb10-22"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-23"><a href="#cb10-23"></a>        <span class="cf">else</span>:</span>
<span id="cb10-24"><a href="#cb10-24"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb10-25"><a href="#cb10-25"></a>            logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C) <span class="co"># stretch this out of Time dimension</span></span>
<span id="cb10-26"><a href="#cb10-26"></a>            targets <span class="op">=</span> targets.view(B<span class="op">*</span>T)</span>
<span id="cb10-27"><a href="#cb10-27"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb10-28"><a href="#cb10-28"></a>            <span class="co"># https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html</span></span>
<span id="cb10-29"><a href="#cb10-29"></a>            <span class="co"># C = number of classes</span></span>
<span id="cb10-30"><a href="#cb10-30"></a>            <span class="co"># B or N = Batch size</span></span>
<span id="cb10-31"><a href="#cb10-31"></a></span>
<span id="cb10-32"><a href="#cb10-32"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb10-33"><a href="#cb10-33"></a></span>
<span id="cb10-34"><a href="#cb10-34"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb10-35"><a href="#cb10-35"></a>        <span class="co"># idx is (B, T) array of indices in the current context</span></span>
<span id="cb10-36"><a href="#cb10-36"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb10-37"><a href="#cb10-37"></a>            <span class="co"># get the predictions</span></span>
<span id="cb10-38"><a href="#cb10-38"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx)</span>
<span id="cb10-39"><a href="#cb10-39"></a>            <span class="co"># focus only on the last time step</span></span>
<span id="cb10-40"><a href="#cb10-40"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># becomes (B, C)</span></span>
<span id="cb10-41"><a href="#cb10-41"></a>            <span class="co"># apply softmax to get probabilities</span></span>
<span id="cb10-42"><a href="#cb10-42"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># (B, C)</span></span>
<span id="cb10-43"><a href="#cb10-43"></a>            <span class="co"># sample from the distribution</span></span>
<span id="cb10-44"><a href="#cb10-44"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, 1)</span></span>
<span id="cb10-45"><a href="#cb10-45"></a>            <span class="co"># append sampled index to the running sequence</span></span>
<span id="cb10-46"><a href="#cb10-46"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, T+1)</span></span>
<span id="cb10-47"><a href="#cb10-47"></a>        <span class="cf">return</span> idx</span>
<span id="cb10-48"><a href="#cb10-48"></a></span>
<span id="cb10-49"><a href="#cb10-49"></a>m <span class="op">=</span> BigramLanguageModel(vocab_size)</span>
<span id="cb10-50"><a href="#cb10-50"></a>logits, loss <span class="op">=</span> m(xb, yb)</span>
<span id="cb10-51"><a href="#cb10-51"></a><span class="bu">print</span>(logits.shape)</span>
<span id="cb10-52"><a href="#cb10-52"></a><span class="bu">print</span>(loss)</span>
<span id="cb10-53"><a href="#cb10-53"></a></span>
<span id="cb10-54"><a href="#cb10-54"></a><span class="bu">print</span>(decode(m.generate(idx <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>), max_new_tokens<span class="op">=</span><span class="dv">100</span>)[<span class="dv">0</span>].tolist()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript"><span id="cb11-1"><a href="#cb11-1"></a>torch<span class="op">.</span><span class="fu">Size</span>([<span class="dv">32</span><span class="op">,</span> <span class="dv">65</span>])</span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="fu">tensor</span>(<span class="fl">4.8786</span><span class="op">,</span> grad_fn<span class="op">=&lt;</span>NllLossBackward0<span class="op">&gt;</span>)</span>
<span id="cb11-3"><a href="#cb11-3"></a></span>
<span id="cb11-4"><a href="#cb11-4"></a>SKIcLT<span class="op">;</span>AcELMoTbvZv C<span class="op">?</span>nq<span class="op">-</span>QE33<span class="op">:</span>CJqkOKH<span class="op">-</span>q<span class="op">;:</span>la<span class="op">!</span>oiywkHjgChzbQ<span class="op">?</span>u<span class="op">!</span><span class="dv">3</span><span class="er">bLIgwevmyFJGUGp</span></span>
<span id="cb11-5"><a href="#cb11-5"></a>wnYWmnxKWWev<span class="op">-</span>tDqXErVKLgJ</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The sampling is garbage, since our model are in a completely random state right now - the loss is <code>4.88</code> which is near to probability of each character in the <code>vocab_size</code> space (<code>1/65</code>). We are going to train the model!</p>
</section>
<section id="training-the-bigram-model" class="level2">
<h2 class="anchored" data-anchor-id="training-the-bigram-model">2.5 training the bigram model</h2>
</section>
<section id="port-our-code-to-a-script" class="level2">
<h2 class="anchored" data-anchor-id="port-our-code-to-a-script">2.6 port our code to a script</h2>
</section>
</section>
<section id="building-the-self-attention" class="level1">
<h1>3 Building the “self-attention”</h1>
<section id="version-1-averaging-past-context-with-for-loops-the-weakest-form-of-aggregation" class="level2">
<h2 class="anchored" data-anchor-id="version-1-averaging-past-context-with-for-loops-the-weakest-form-of-aggregation">3.1 version 1: averaging past context with for loops, the weakest form of aggregation</h2>
</section>
<section id="the-trick-in-self-attention-matrix-multiply-as-weighted-aggregation" class="level2">
<h2 class="anchored" data-anchor-id="the-trick-in-self-attention-matrix-multiply-as-weighted-aggregation">3.2 the trick in self-attention: matrix multiply as weighted aggregation</h2>
</section>
<section id="version-2-using-matrix-multiply" class="level2">
<h2 class="anchored" data-anchor-id="version-2-using-matrix-multiply">3.3 version 2: using matrix multiply</h2>
</section>
<section id="version-3-adding-softmax" class="level2">
<h2 class="anchored" data-anchor-id="version-3-adding-softmax">3.4 version 3: adding softmax</h2>
</section>
<section id="minor-code-cleanup" class="level2">
<h2 class="anchored" data-anchor-id="minor-code-cleanup">3.5 minor code cleanup</h2>
</section>
<section id="positional-encoding" class="level2">
<h2 class="anchored" data-anchor-id="positional-encoding">3.6 positional encoding</h2>
</section>
<section id="the-crux-of-the-video-version-4-self-attention" class="level2">
<h2 class="anchored" data-anchor-id="the-crux-of-the-video-version-4-self-attention">3.7 THE CRUX OF THE VIDEO: version 4: self-attention</h2>
</section>
<section id="note-1-attention-as-communication" class="level2">
<h2 class="anchored" data-anchor-id="note-1-attention-as-communication">3.8 note 1: attention as communication</h2>
</section>
<section id="note-2-attention-has-no-notion-of-space-operates-over-sets" class="level2">
<h2 class="anchored" data-anchor-id="note-2-attention-has-no-notion-of-space-operates-over-sets">3.9 note 2: attention has no notion of space, operates over sets</h2>
</section>
<section id="note-3-there-is-no-communication-across-batch-dimension" class="level2">
<h2 class="anchored" data-anchor-id="note-3-there-is-no-communication-across-batch-dimension">3.10 note 3: there is no communication across batch dimension</h2>
</section>
<section id="note-4-encoder-blocks-vs.-decoder-blocks" class="level2">
<h2 class="anchored" data-anchor-id="note-4-encoder-blocks-vs.-decoder-blocks">3.11 note 4: encoder blocks vs.&nbsp;decoder blocks</h2>
</section>
<section id="note-5-attention-vs.-self-attention-vs.-cross-attention" class="level2">
<h2 class="anchored" data-anchor-id="note-5-attention-vs.-self-attention-vs.-cross-attention">3.12 note 5: attention vs.&nbsp;self-attention vs.&nbsp;cross-attention</h2>
</section>
<section id="note-6-scaled-self-attention.-why-divide-by-sqrthead_size" class="level2">
<h2 class="anchored" data-anchor-id="note-6-scaled-self-attention.-why-divide-by-sqrthead_size">3.13 note 6: “scaled” self-attention. why divide by sqrt(head_size)</h2>
</section>
</section>
<section id="building-the-transformer" class="level1">
<h1>4 Building the Transformer</h1>
<section id="inserting-a-single-self-attention-block-to-our-network" class="level2">
<h2 class="anchored" data-anchor-id="inserting-a-single-self-attention-block-to-our-network">4.1 inserting a single self-attention block to our network</h2>
</section>
<section id="multi-headed-self-attention" class="level2">
<h2 class="anchored" data-anchor-id="multi-headed-self-attention">4.2 multi-headed self-attention</h2>
</section>
<section id="feedforward-layers-of-transformer-block" class="level2">
<h2 class="anchored" data-anchor-id="feedforward-layers-of-transformer-block">4.3 feedforward layers of transformer block</h2>
</section>
<section id="residual-connections" class="level2">
<h2 class="anchored" data-anchor-id="residual-connections">4.4 residual connections</h2>
</section>
<section id="layernorm-and-its-relationship-to-our-previous-batchnorm" class="level2">
<h2 class="anchored" data-anchor-id="layernorm-and-its-relationship-to-our-previous-batchnorm">4.5 layernorm (and its relationship to our previous batchnorm)</h2>
</section>
<section id="scaling-up-the-model-creating-a-few-variables.-adding-dropout" class="level2">
<h2 class="anchored" data-anchor-id="scaling-up-the-model-creating-a-few-variables.-adding-dropout">4.6 scaling up the model! creating a few variables. adding dropout</h2>
</section>
</section>
<section id="notes-on-transformer" class="level1">
<h1>5 Notes on Transformer</h1>
<section id="encoder-vs.-decoder-vs.-both-transformers" class="level2">
<h2 class="anchored" data-anchor-id="encoder-vs.-decoder-vs.-both-transformers">5.1 encoder vs.&nbsp;decoder vs.&nbsp;both (?) Transformers</h2>
</section>
<section id="super-quick-walkthrough-of-nanogpt-batched-multi-headed-self-attention" class="level2">
<h2 class="anchored" data-anchor-id="super-quick-walkthrough-of-nanogpt-batched-multi-headed-self-attention">5.2 super quick walkthrough of nanoGPT, batched multi-headed self-attention</h2>
</section>
<section id="back-to-chatgpt-gpt-3-pretraining-vs.-finetuning-rlhf" class="level2">
<h2 class="anchored" data-anchor-id="back-to-chatgpt-gpt-3-pretraining-vs.-finetuning-rlhf">5.3 back to ChatGPT, GPT-3, pretraining vs.&nbsp;finetuning, RLHF</h2>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">5.4 conclusions</h2>
</section>
</section>
<section id="resources" class="level1">
<h1>6 resources</h1>
<ul>
<li>Colab notebook: <a href="https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing" class="uri">https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing</a>;</li>
<li><code>nanoGPT</code>: <a href="https://github.com/karpathy/nanoGPT" class="uri">https://github.com/karpathy/nanoGPT</a>;</li>
<li>TinyShakepeare dataset: <a href="https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt" class="uri">https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt</a>;</li>
</ul>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/lktuan\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb12" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb12-1"><a href="#cb12-1"></a><span class="co">---</span></span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="an">title:</span><span class="co"> "Let's build GPT, in code, spelled out!"</span></span>
<span id="cb12-3"><a href="#cb12-3"></a><span class="an">description:</span><span class="co"> "Build a Generatively Pretrained Transformer (GPT), following the paper 'Attention is All You Need' and OpenAI's GPT-2 / GPT-3"</span></span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="an">author:</span></span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="co">  - name: "Tuan Le Khac"</span></span>
<span id="cb12-6"><a href="#cb12-6"></a><span class="co">    url: https://lktuan.github.io/</span></span>
<span id="cb12-7"><a href="#cb12-7"></a><span class="an">categories:</span><span class="co"> [til, python, andrej karpathy, nn-z2h, neural networks]</span></span>
<span id="cb12-8"><a href="#cb12-8"></a><span class="an">date:</span><span class="co"> 04-27-2025</span></span>
<span id="cb12-9"><a href="#cb12-9"></a><span class="an">date-modified:</span><span class="co"> 05-04-2025</span></span>
<span id="cb12-10"><a href="#cb12-10"></a><span class="an">image:</span><span class="co"> attention.jpg</span></span>
<span id="cb12-11"><a href="#cb12-11"></a><span class="an">draft:</span><span class="co"> false</span></span>
<span id="cb12-12"><a href="#cb12-12"></a><span class="an">format:</span></span>
<span id="cb12-13"><a href="#cb12-13"></a><span class="co">  html:</span></span>
<span id="cb12-14"><a href="#cb12-14"></a><span class="co">    code-overflow: wrap</span></span>
<span id="cb12-15"><a href="#cb12-15"></a><span class="co">    code-tools: true</span></span>
<span id="cb12-16"><a href="#cb12-16"></a><span class="co">    code-fold: show</span></span>
<span id="cb12-17"><a href="#cb12-17"></a><span class="co">    code-annotations: hover</span></span>
<span id="cb12-18"><a href="#cb12-18"></a><span class="an">execute:</span></span>
<span id="cb12-19"><a href="#cb12-19"></a><span class="co">  eval: false</span></span>
<span id="cb12-20"><a href="#cb12-20"></a><span class="co">---</span></span>
<span id="cb12-21"><a href="#cb12-21"></a></span>
<span id="cb12-22"><a href="#cb12-22"></a>::: {.callout-important title="This is not orginal content!"}</span>
<span id="cb12-23"><a href="#cb12-23"></a>Long time no LLMs, past months struggled with job-search and personal stuffs distracted me from learning AI. No I am continuing my favorite AI series Neural Networks, from Zero to Hero by Andrej Karpathy.</span>
<span id="cb12-24"><a href="#cb12-24"></a></span>
<span id="cb12-25"><a href="#cb12-25"></a>I will be learning to build a GPT from scratch. This is my note and hope I'll survive :)</span>
<span id="cb12-26"><a href="#cb12-26"></a></span>
<span id="cb12-27"><a href="#cb12-27"></a>Links: <span class="ot">&lt;https://youtu.be/kCc8FmEb1nY?si=4Fa3EAjuTQ5UbOFk&gt;</span></span>
<span id="cb12-28"><a href="#cb12-28"></a>:::</span>
<span id="cb12-29"><a href="#cb12-29"></a></span>
<span id="cb12-30"><a href="#cb12-30"></a><span class="fu"># 1 intro: ChatGPT, Transformers, `nanoGPT`, Shakespeare</span></span>
<span id="cb12-31"><a href="#cb12-31"></a></span>
<span id="cb12-32"><a href="#cb12-32"></a><span class="ss">- </span>ChatGPT (GPT stands for Generative Pre-trained Transformer) is a *probabilistic* system that for anyone's prompt it can give us multiple answers.</span>
<span id="cb12-33"><a href="#cb12-33"></a>It models sequence of words (or token) and tries to predict the next word to complete the prompt we give;</span>
<span id="cb12-34"><a href="#cb12-34"></a><span class="ss">- </span>Transformers, proposed by Vaswani et al. in the landmark paper "Attention is All You Need" back in 2017, is an architecture that did all those heavy lifting under the hood of ChatGPT;</span>
<span id="cb12-35"><a href="#cb12-35"></a><span class="ss">- </span><span class="in">`nanoGPT`</span> is "the simplest, fastest repository for training/finetuning medium-sized GPTs" written as a side project by Andrej. We gonna follow this code structure in this lecture, but will not rebuild the whole 124M params GPT-2;</span>
<span id="cb12-36"><a href="#cb12-36"></a><span class="ss">- </span>Shakespearse tiny dataset will be the text data that we'll be working on.</span>
<span id="cb12-37"><a href="#cb12-37"></a></span>
<span id="cb12-38"><a href="#cb12-38"></a><span class="fu"># 2 baseline language modeling, code setup</span></span>
<span id="cb12-39"><a href="#cb12-39"></a></span>
<span id="cb12-40"><a href="#cb12-40"></a><span class="fu">## 2.1 reading and exploring the data</span></span>
<span id="cb12-41"><a href="#cb12-41"></a></span>
<span id="cb12-42"><a href="#cb12-42"></a>The *tinyshakepeare* dataset contains 1,115,394 characters, here are the first 1000 ones look like:</span>
<span id="cb12-43"><a href="#cb12-43"></a></span>
<span id="cb12-44"><a href="#cb12-44"></a><span class="in">```text</span></span>
<span id="cb12-45"><a href="#cb12-45"></a><span class="in">First Citizen:</span></span>
<span id="cb12-46"><a href="#cb12-46"></a><span class="in">Before we proceed any further, hear me speak.</span></span>
<span id="cb12-47"><a href="#cb12-47"></a></span>
<span id="cb12-48"><a href="#cb12-48"></a><span class="in">All:</span></span>
<span id="cb12-49"><a href="#cb12-49"></a><span class="in">Speak, speak.</span></span>
<span id="cb12-50"><a href="#cb12-50"></a></span>
<span id="cb12-51"><a href="#cb12-51"></a><span class="in">First Citizen:</span></span>
<span id="cb12-52"><a href="#cb12-52"></a><span class="in">You are all resolved rather to die than to famish?</span></span>
<span id="cb12-53"><a href="#cb12-53"></a></span>
<span id="cb12-54"><a href="#cb12-54"></a><span class="in">All:</span></span>
<span id="cb12-55"><a href="#cb12-55"></a><span class="in">Resolved. resolved.</span></span>
<span id="cb12-56"><a href="#cb12-56"></a></span>
<span id="cb12-57"><a href="#cb12-57"></a><span class="in">First Citizen:</span></span>
<span id="cb12-58"><a href="#cb12-58"></a><span class="in">First, you know Caius Marcius is chief enemy to the people.</span></span>
<span id="cb12-59"><a href="#cb12-59"></a></span>
<span id="cb12-60"><a href="#cb12-60"></a><span class="in">All:</span></span>
<span id="cb12-61"><a href="#cb12-61"></a><span class="in">We know't, we know't.</span></span>
<span id="cb12-62"><a href="#cb12-62"></a></span>
<span id="cb12-63"><a href="#cb12-63"></a><span class="in">First Citizen:</span></span>
<span id="cb12-64"><a href="#cb12-64"></a><span class="in">Let us kill him, and we'll have corn at our own price.</span></span>
<span id="cb12-65"><a href="#cb12-65"></a><span class="in">Is't a verdict?</span></span>
<span id="cb12-66"><a href="#cb12-66"></a></span>
<span id="cb12-67"><a href="#cb12-67"></a><span class="in">All:</span></span>
<span id="cb12-68"><a href="#cb12-68"></a><span class="in">No more talking on't; let it be done: away, away!</span></span>
<span id="cb12-69"><a href="#cb12-69"></a></span>
<span id="cb12-70"><a href="#cb12-70"></a><span class="in">Second Citizen:</span></span>
<span id="cb12-71"><a href="#cb12-71"></a><span class="in">One word, good citizens.</span></span>
<span id="cb12-72"><a href="#cb12-72"></a></span>
<span id="cb12-73"><a href="#cb12-73"></a><span class="in">First Citizen:</span></span>
<span id="cb12-74"><a href="#cb12-74"></a><span class="in">We are accounted poor citizens, the patricians good.</span></span>
<span id="cb12-75"><a href="#cb12-75"></a><span class="in">What authority surfeits on would relieve us: if they</span></span>
<span id="cb12-76"><a href="#cb12-76"></a><span class="in">would yield us but the superfluity, while it were</span></span>
<span id="cb12-77"><a href="#cb12-77"></a><span class="in">wholesome, we might guess they relieved us humanely;</span></span>
<span id="cb12-78"><a href="#cb12-78"></a><span class="in">but they think we are too dear: the leanness that</span></span>
<span id="cb12-79"><a href="#cb12-79"></a><span class="in">afflicts us, the object of our misery, is as an</span></span>
<span id="cb12-80"><a href="#cb12-80"></a><span class="in">inventory to particularise their abundance; our</span></span>
<span id="cb12-81"><a href="#cb12-81"></a><span class="in">sufferance is a gain to them Let us revenge this with</span></span>
<span id="cb12-82"><a href="#cb12-82"></a><span class="in">our pikes, ere we become rakes: for the gods know I</span></span>
<span id="cb12-83"><a href="#cb12-83"></a><span class="in">speak this in hunger for bread, not in thirst for revenge.</span></span>
<span id="cb12-84"><a href="#cb12-84"></a><span class="in">```</span></span>
<span id="cb12-85"><a href="#cb12-85"></a></span>
<span id="cb12-86"><a href="#cb12-86"></a>In contains 65 different characters, the first one was *space*:</span>
<span id="cb12-87"><a href="#cb12-87"></a></span>
<span id="cb12-88"><a href="#cb12-88"></a><span class="in">```md</span></span>
<span id="cb12-89"><a href="#cb12-89"></a> !$&amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz</span>
<span id="cb12-90"><a href="#cb12-90"></a><span class="in">```</span></span>
<span id="cb12-91"><a href="#cb12-91"></a></span>
<span id="cb12-92"><a href="#cb12-92"></a>This list of characters can be call the *dictionary*, same with previous lectures, we create mapping from characters to integers/indicesindices <span class="in">`stoi`</span> and vice versa <span class="in">`itos`</span>.</span>
<span id="cb12-93"><a href="#cb12-93"></a></span>
<span id="cb12-94"><a href="#cb12-94"></a><span class="fu">## 2.2 tokenization, train/val split</span></span>
<span id="cb12-95"><a href="#cb12-95"></a></span>
<span id="cb12-96"><a href="#cb12-96"></a>The two functions <span class="in">`encode`</span> and <span class="in">`decode`</span> can let us transition from string (for human reading) to number (for machine reading) and vice versa.</span>
<span id="cb12-97"><a href="#cb12-97"></a></span>
<span id="cb12-98"><a href="#cb12-98"></a><span class="in">```python</span></span>
<span id="cb12-99"><a href="#cb12-99"></a><span class="bu">print</span>(encode(<span class="st">"hii there"</span>))</span>
<span id="cb12-100"><a href="#cb12-100"></a><span class="bu">print</span>(decode(encode(<span class="st">"hii there"</span>)))</span>
<span id="cb12-101"><a href="#cb12-101"></a><span class="in">```</span></span>
<span id="cb12-102"><a href="#cb12-102"></a><span class="in">```js</span></span>
<span id="cb12-103"><a href="#cb12-103"></a>[<span class="dv">46</span><span class="op">,</span> <span class="dv">47</span><span class="op">,</span> <span class="dv">47</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">43</span>]</span>
<span id="cb12-104"><a href="#cb12-104"></a>hii there</span>
<span id="cb12-105"><a href="#cb12-105"></a><span class="in">```</span></span>
<span id="cb12-106"><a href="#cb12-106"></a></span>
<span id="cb12-107"><a href="#cb12-107"></a>This is a very simple *tokenization and detokenization*, there are more complex/effective ones out there in the industry, for eg., <span class="co">[</span><span class="ot">SentencePiece</span><span class="co">](https://github.com/google/sentencepiece)</span> by Google, and <span class="co">[</span><span class="ot">tiktoken</span><span class="co">](https://github.com/openai/tiktoken)</span> by OpenAI, which implement more sophisticated paradigm like <span class="co">[</span><span class="ot">BPE</span><span class="co">](https://aclanthology.org/P16-1162/)</span>.</span>
<span id="cb12-108"><a href="#cb12-108"></a></span>
<span id="cb12-109"><a href="#cb12-109"></a>Then use this set of utility we can convert our dataset to PyTorch tensor:</span>
<span id="cb12-110"><a href="#cb12-110"></a></span>
<span id="cb12-111"><a href="#cb12-111"></a><span class="in">```python</span></span>
<span id="cb12-112"><a href="#cb12-112"></a><span class="im">import</span> torch <span class="co"># we use PyTorch: https://pytorch.org</span></span>
<span id="cb12-113"><a href="#cb12-113"></a>data <span class="op">=</span> torch.tensor(encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb12-114"><a href="#cb12-114"></a><span class="bu">print</span>(data.shape, data.dtype)</span>
<span id="cb12-115"><a href="#cb12-115"></a><span class="bu">print</span>(data[:<span class="dv">100</span>]) <span class="co"># the first 100 characters we looked at earier will to the GPT look like this</span></span>
<span id="cb12-116"><a href="#cb12-116"></a><span class="in">```</span></span>
<span id="cb12-117"><a href="#cb12-117"></a><span class="in">```js</span></span>
<span id="cb12-118"><a href="#cb12-118"></a>torch<span class="op">.</span><span class="fu">Size</span>([<span class="dv">1115394</span>]) torch<span class="op">.</span><span class="at">int64</span></span>
<span id="cb12-119"><a href="#cb12-119"></a><span class="fu">tensor</span>([<span class="dv">18</span><span class="op">,</span> <span class="dv">47</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">57</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">15</span><span class="op">,</span> <span class="dv">47</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">47</span><span class="op">,</span> <span class="dv">64</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">52</span><span class="op">,</span> <span class="dv">10</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span> <span class="dv">14</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">44</span><span class="op">,</span></span>
<span id="cb12-120"><a href="#cb12-120"></a>        <span class="dv">53</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">61</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">54</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">53</span><span class="op">,</span> <span class="dv">41</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">42</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">52</span><span class="op">,</span> <span class="dv">63</span><span class="op">,</span></span>
<span id="cb12-121"><a href="#cb12-121"></a>         <span class="dv">1</span><span class="op">,</span> <span class="dv">44</span><span class="op">,</span> <span class="dv">59</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span>  <span class="dv">6</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">51</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span></span>
<span id="cb12-122"><a href="#cb12-122"></a>        <span class="dv">57</span><span class="op">,</span> <span class="dv">54</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">49</span><span class="op">,</span>  <span class="dv">8</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span> <span class="dv">13</span><span class="op">,</span> <span class="dv">50</span><span class="op">,</span> <span class="dv">50</span><span class="op">,</span> <span class="dv">10</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span> <span class="dv">31</span><span class="op">,</span> <span class="dv">54</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">49</span><span class="op">,</span></span>
<span id="cb12-123"><a href="#cb12-123"></a>         <span class="dv">6</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">57</span><span class="op">,</span> <span class="dv">54</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">49</span><span class="op">,</span>  <span class="dv">8</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span> <span class="dv">18</span><span class="op">,</span> <span class="dv">47</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">57</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">15</span><span class="op">,</span> <span class="dv">47</span><span class="op">,</span></span>
<span id="cb12-124"><a href="#cb12-124"></a>        <span class="dv">58</span><span class="op">,</span> <span class="dv">47</span><span class="op">,</span> <span class="dv">64</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">52</span><span class="op">,</span> <span class="dv">10</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span> <span class="dv">37</span><span class="op">,</span> <span class="dv">53</span><span class="op">,</span> <span class="dv">59</span>])</span>
<span id="cb12-125"><a href="#cb12-125"></a><span class="in">```</span></span>
<span id="cb12-126"><a href="#cb12-126"></a></span>
<span id="cb12-127"><a href="#cb12-127"></a>Finally for the very first pre-processing we want to use the last 10% of out dataset as the validation/development split. We dont want to build a model that mimick the Shakepeare's tone, we want it to be generative/creative.</span>
<span id="cb12-128"><a href="#cb12-128"></a></span>
<span id="cb12-129"><a href="#cb12-129"></a><span class="in">```python</span></span>
<span id="cb12-130"><a href="#cb12-130"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(data)) <span class="co"># first 90% will be train, rest val</span></span>
<span id="cb12-131"><a href="#cb12-131"></a>train_data <span class="op">=</span> data[:n]</span>
<span id="cb12-132"><a href="#cb12-132"></a>val_data <span class="op">=</span> data[n:]</span>
<span id="cb12-133"><a href="#cb12-133"></a><span class="in">```</span></span>
<span id="cb12-134"><a href="#cb12-134"></a></span>
<span id="cb12-135"><a href="#cb12-135"></a><span class="fu">## 2.3 data loader, batches of chunks of data</span></span>
<span id="cb12-136"><a href="#cb12-136"></a></span>
<span id="cb12-137"><a href="#cb12-137"></a>Now the idea of our character-level language model is to predict the next character(s) given a sequence of characters, with a certain maximum length, not the whole previous part of the dataset. Same with previos lecture, we can set it (<span class="in">`block_size`</span>, or <span class="in">`context_length`</span>) to *8*.</span>
<span id="cb12-138"><a href="#cb12-138"></a></span>
<span id="cb12-139"><a href="#cb12-139"></a>And the second constraint that we want to setup is the *batch size* of chunk that we feed to the *transformer*, it will reduce the cost of calculation while retain the efficiency of training. You can imagine the <span class="in">`context`</span> and <span class="in">`target`</span> like this:</span>
<span id="cb12-140"><a href="#cb12-140"></a></span>
<span id="cb12-141"><a href="#cb12-141"></a><span class="in">```python</span></span>
<span id="cb12-142"><a href="#cb12-142"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb12-143"><a href="#cb12-143"></a>batch_size <span class="op">=</span> <span class="dv">4</span> <span class="co"># how many independent sequences will we process in parallel?</span></span>
<span id="cb12-144"><a href="#cb12-144"></a>block_size <span class="op">=</span> <span class="dv">8</span> <span class="co"># what is the maximum context length for predictions?</span></span>
<span id="cb12-145"><a href="#cb12-145"></a></span>
<span id="cb12-146"><a href="#cb12-146"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb12-147"><a href="#cb12-147"></a>    <span class="co"># generate a small batch of data of inputs x and targets y</span></span>
<span id="cb12-148"><a href="#cb12-148"></a>    data <span class="op">=</span> train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span> <span class="cf">else</span> val_data</span>
<span id="cb12-149"><a href="#cb12-149"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb12-150"><a href="#cb12-150"></a>    x <span class="op">=</span> torch.stack([data[i:i<span class="op">+</span>block_size] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb12-151"><a href="#cb12-151"></a>    y <span class="op">=</span> torch.stack([data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>block_size<span class="op">+</span><span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb12-152"><a href="#cb12-152"></a>    <span class="cf">return</span> x, y</span>
<span id="cb12-153"><a href="#cb12-153"></a></span>
<span id="cb12-154"><a href="#cb12-154"></a>xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb12-155"><a href="#cb12-155"></a><span class="bu">print</span>(<span class="st">'inputs:'</span>)</span>
<span id="cb12-156"><a href="#cb12-156"></a><span class="bu">print</span>(xb.shape)</span>
<span id="cb12-157"><a href="#cb12-157"></a><span class="bu">print</span>(xb)</span>
<span id="cb12-158"><a href="#cb12-158"></a><span class="bu">print</span>(<span class="st">'targets:'</span>)</span>
<span id="cb12-159"><a href="#cb12-159"></a><span class="bu">print</span>(yb.shape)</span>
<span id="cb12-160"><a href="#cb12-160"></a><span class="bu">print</span>(yb)</span>
<span id="cb12-161"><a href="#cb12-161"></a></span>
<span id="cb12-162"><a href="#cb12-162"></a><span class="bu">print</span>(<span class="st">'----'</span>)</span>
<span id="cb12-163"><a href="#cb12-163"></a></span>
<span id="cb12-164"><a href="#cb12-164"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(batch_size): <span class="co"># batch dimension</span></span>
<span id="cb12-165"><a href="#cb12-165"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(block_size): <span class="co"># time dimension</span></span>
<span id="cb12-166"><a href="#cb12-166"></a>        context <span class="op">=</span> xb[b, :t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb12-167"><a href="#cb12-167"></a>        target <span class="op">=</span> yb[b,t]</span>
<span id="cb12-168"><a href="#cb12-168"></a>        <span class="bu">print</span>(<span class="ss">f"when input is </span><span class="sc">{</span>context<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss"> the target: </span><span class="sc">{</span>target<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-169"><a href="#cb12-169"></a><span class="in">```</span></span>
<span id="cb12-170"><a href="#cb12-170"></a><span class="in">```js</span></span>
<span id="cb12-171"><a href="#cb12-171"></a>inputs<span class="op">:</span></span>
<span id="cb12-172"><a href="#cb12-172"></a>torch<span class="op">.</span><span class="fu">Size</span>([<span class="dv">4</span><span class="op">,</span> <span class="dv">8</span>])</span>
<span id="cb12-173"><a href="#cb12-173"></a><span class="fu">tensor</span>([[<span class="dv">24</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span>  <span class="dv">5</span><span class="op">,</span> <span class="dv">57</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">43</span>]<span class="op">,</span></span>
<span id="cb12-174"><a href="#cb12-174"></a>        [<span class="dv">44</span><span class="op">,</span> <span class="dv">53</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">58</span>]<span class="op">,</span></span>
<span id="cb12-175"><a href="#cb12-175"></a>        [<span class="dv">52</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span>  <span class="dv">1</span>]<span class="op">,</span></span>
<span id="cb12-176"><a href="#cb12-176"></a>        [<span class="dv">25</span><span class="op">,</span> <span class="dv">17</span><span class="op">,</span> <span class="dv">27</span><span class="op">,</span> <span class="dv">10</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span> <span class="dv">21</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">54</span>]])</span>
<span id="cb12-177"><a href="#cb12-177"></a>targets<span class="op">:</span></span>
<span id="cb12-178"><a href="#cb12-178"></a>torch<span class="op">.</span><span class="fu">Size</span>([<span class="dv">4</span><span class="op">,</span> <span class="dv">8</span>])</span>
<span id="cb12-179"><a href="#cb12-179"></a><span class="fu">tensor</span>([[<span class="dv">43</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span>  <span class="dv">5</span><span class="op">,</span> <span class="dv">57</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">39</span>]<span class="op">,</span></span>
<span id="cb12-180"><a href="#cb12-180"></a>        [<span class="dv">53</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span>  <span class="dv">1</span>]<span class="op">,</span></span>
<span id="cb12-181"><a href="#cb12-181"></a>        [<span class="dv">58</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">46</span>]<span class="op">,</span></span>
<span id="cb12-182"><a href="#cb12-182"></a>        [<span class="dv">17</span><span class="op">,</span> <span class="dv">27</span><span class="op">,</span> <span class="dv">10</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span> <span class="dv">21</span><span class="op">,</span>  <span class="dv">1</span><span class="op">,</span> <span class="dv">54</span><span class="op">,</span> <span class="dv">39</span>]])</span>
<span id="cb12-183"><a href="#cb12-183"></a><span class="op">----</span></span>
<span id="cb12-184"><a href="#cb12-184"></a>when input is [<span class="dv">24</span>] the target<span class="op">:</span> <span class="dv">43</span></span>
<span id="cb12-185"><a href="#cb12-185"></a>when input is [<span class="dv">24</span><span class="op">,</span> <span class="dv">43</span>] the target<span class="op">:</span> <span class="dv">58</span></span>
<span id="cb12-186"><a href="#cb12-186"></a>when input is [<span class="dv">24</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">58</span>] the target<span class="op">:</span> <span class="dv">5</span></span>
<span id="cb12-187"><a href="#cb12-187"></a>when input is [<span class="dv">24</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">5</span>] the target<span class="op">:</span> <span class="dv">57</span></span>
<span id="cb12-188"><a href="#cb12-188"></a>when input is [<span class="dv">24</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">5</span><span class="op">,</span> <span class="dv">57</span>] the target<span class="op">:</span> <span class="dv">1</span></span>
<span id="cb12-189"><a href="#cb12-189"></a>when input is [<span class="dv">24</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">5</span><span class="op">,</span> <span class="dv">57</span><span class="op">,</span> <span class="dv">1</span>] the target<span class="op">:</span> <span class="dv">46</span></span>
<span id="cb12-190"><a href="#cb12-190"></a>when input is [<span class="dv">24</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">5</span><span class="op">,</span> <span class="dv">57</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">46</span>] the target<span class="op">:</span> <span class="dv">43</span></span>
<span id="cb12-191"><a href="#cb12-191"></a>when input is [<span class="dv">24</span><span class="op">,</span> <span class="dv">43</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">5</span><span class="op">,</span> <span class="dv">57</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">43</span>] the target<span class="op">:</span> <span class="dv">39</span></span>
<span id="cb12-192"><a href="#cb12-192"></a>when input is [<span class="dv">44</span>] the target<span class="op">:</span> <span class="dv">53</span></span>
<span id="cb12-193"><a href="#cb12-193"></a>when input is [<span class="dv">44</span><span class="op">,</span> <span class="dv">53</span>] the target<span class="op">:</span> <span class="dv">56</span></span>
<span id="cb12-194"><a href="#cb12-194"></a>when input is [<span class="dv">44</span><span class="op">,</span> <span class="dv">53</span><span class="op">,</span> <span class="dv">56</span>] the target<span class="op">:</span> <span class="dv">1</span></span>
<span id="cb12-195"><a href="#cb12-195"></a>when input is [<span class="dv">44</span><span class="op">,</span> <span class="dv">53</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">1</span>] the target<span class="op">:</span> <span class="dv">58</span></span>
<span id="cb12-196"><a href="#cb12-196"></a>when input is [<span class="dv">44</span><span class="op">,</span> <span class="dv">53</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span>] the target<span class="op">:</span> <span class="dv">46</span></span>
<span id="cb12-197"><a href="#cb12-197"></a>when input is [<span class="dv">44</span><span class="op">,</span> <span class="dv">53</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span>] the target<span class="op">:</span> <span class="dv">39</span></span>
<span id="cb12-198"><a href="#cb12-198"></a>when input is [<span class="dv">44</span><span class="op">,</span> <span class="dv">53</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">39</span>] the target<span class="op">:</span> <span class="dv">58</span></span>
<span id="cb12-199"><a href="#cb12-199"></a>when input is [<span class="dv">44</span><span class="op">,</span> <span class="dv">53</span><span class="op">,</span> <span class="dv">56</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">58</span>] the target<span class="op">:</span> <span class="dv">1</span></span>
<span id="cb12-200"><a href="#cb12-200"></a>when input is [<span class="dv">52</span>] the target<span class="op">:</span> <span class="dv">58</span></span>
<span id="cb12-201"><a href="#cb12-201"></a>when input is [<span class="dv">52</span><span class="op">,</span> <span class="dv">58</span>] the target<span class="op">:</span> <span class="dv">1</span></span>
<span id="cb12-202"><a href="#cb12-202"></a>when input is [<span class="dv">52</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">1</span>] the target<span class="op">:</span> <span class="dv">58</span></span>
<span id="cb12-203"><a href="#cb12-203"></a>when input is [<span class="dv">52</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span>] the target<span class="op">:</span> <span class="dv">46</span></span>
<span id="cb12-204"><a href="#cb12-204"></a>when input is [<span class="dv">52</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span>] the target<span class="op">:</span> <span class="dv">39</span></span>
<span id="cb12-205"><a href="#cb12-205"></a>when input is [<span class="dv">52</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">39</span>] the target<span class="op">:</span> <span class="dv">58</span></span>
<span id="cb12-206"><a href="#cb12-206"></a>when input is [<span class="dv">52</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">58</span>] the target<span class="op">:</span> <span class="dv">1</span></span>
<span id="cb12-207"><a href="#cb12-207"></a>when input is [<span class="dv">52</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">46</span><span class="op">,</span> <span class="dv">39</span><span class="op">,</span> <span class="dv">58</span><span class="op">,</span> <span class="dv">1</span>] the target<span class="op">:</span> <span class="dv">46</span></span>
<span id="cb12-208"><a href="#cb12-208"></a>when input is [<span class="dv">25</span>] the target<span class="op">:</span> <span class="dv">17</span></span>
<span id="cb12-209"><a href="#cb12-209"></a>when input is [<span class="dv">25</span><span class="op">,</span> <span class="dv">17</span>] the target<span class="op">:</span> <span class="dv">27</span></span>
<span id="cb12-210"><a href="#cb12-210"></a>when input is [<span class="dv">25</span><span class="op">,</span> <span class="dv">17</span><span class="op">,</span> <span class="dv">27</span>] the target<span class="op">:</span> <span class="dv">10</span></span>
<span id="cb12-211"><a href="#cb12-211"></a>when input is [<span class="dv">25</span><span class="op">,</span> <span class="dv">17</span><span class="op">,</span> <span class="dv">27</span><span class="op">,</span> <span class="dv">10</span>] the target<span class="op">:</span> <span class="dv">0</span></span>
<span id="cb12-212"><a href="#cb12-212"></a>when input is [<span class="dv">25</span><span class="op">,</span> <span class="dv">17</span><span class="op">,</span> <span class="dv">27</span><span class="op">,</span> <span class="dv">10</span><span class="op">,</span> <span class="dv">0</span>] the target<span class="op">:</span> <span class="dv">21</span></span>
<span id="cb12-213"><a href="#cb12-213"></a>when input is [<span class="dv">25</span><span class="op">,</span> <span class="dv">17</span><span class="op">,</span> <span class="dv">27</span><span class="op">,</span> <span class="dv">10</span><span class="op">,</span> <span class="dv">0</span><span class="op">,</span> <span class="dv">21</span>] the target<span class="op">:</span> <span class="dv">1</span></span>
<span id="cb12-214"><a href="#cb12-214"></a>when input is [<span class="dv">25</span><span class="op">,</span> <span class="dv">17</span><span class="op">,</span> <span class="dv">27</span><span class="op">,</span> <span class="dv">10</span><span class="op">,</span> <span class="dv">0</span><span class="op">,</span> <span class="dv">21</span><span class="op">,</span> <span class="dv">1</span>] the target<span class="op">:</span> <span class="dv">54</span></span>
<span id="cb12-215"><a href="#cb12-215"></a>when input is [<span class="dv">25</span><span class="op">,</span> <span class="dv">17</span><span class="op">,</span> <span class="dv">27</span><span class="op">,</span> <span class="dv">10</span><span class="op">,</span> <span class="dv">0</span><span class="op">,</span> <span class="dv">21</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">54</span>] the target<span class="op">:</span> <span class="dv">39</span></span>
<span id="cb12-216"><a href="#cb12-216"></a><span class="in">```</span></span>
<span id="cb12-217"><a href="#cb12-217"></a></span>
<span id="cb12-218"><a href="#cb12-218"></a><span class="fu">## 2.4 simplest baseline: bigram language model, loss, generation</span></span>
<span id="cb12-219"><a href="#cb12-219"></a></span>
<span id="cb12-220"><a href="#cb12-220"></a>We'll go into the most simplest thing in the realm of NLP - the <span class="in">`bigram`</span> language model ~ where the next character is predicted based on 1 single previous character.</span>
<span id="cb12-221"><a href="#cb12-221"></a></span>
<span id="cb12-222"><a href="#cb12-222"></a>Below we are constructing a child class of <span class="in">`nn.Module`</span> which is a <span class="in">`bigram`</span> language model under a <span class="in">`vocab_size`</span> = 65 language space. The state of the model is presented by <span class="in">`token_embedding_table`</span> ~ how it thinks about the next character given one.</span>
<span id="cb12-223"><a href="#cb12-223"></a></span>
<span id="cb12-224"><a href="#cb12-224"></a>We constructed a <span class="in">`loss`</span> function based on <span class="in">`cross_entropy`</span> to quantify the quality of the model, and also implemented a <span class="in">`generate`</span> function to sample from the model.</span>
<span id="cb12-225"><a href="#cb12-225"></a></span>
<span id="cb12-226"><a href="#cb12-226"></a></span>
<span id="cb12-227"><a href="#cb12-227"></a><span class="in">```python</span></span>
<span id="cb12-228"><a href="#cb12-228"></a><span class="co">#| code-fold: true</span></span>
<span id="cb12-229"><a href="#cb12-229"></a><span class="im">import</span> torch</span>
<span id="cb12-230"><a href="#cb12-230"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb12-231"><a href="#cb12-231"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb12-232"><a href="#cb12-232"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb12-233"><a href="#cb12-233"></a></span>
<span id="cb12-234"><a href="#cb12-234"></a><span class="kw">class</span> BigramLanguageModel(nn.Module):</span>
<span id="cb12-235"><a href="#cb12-235"></a></span>
<span id="cb12-236"><a href="#cb12-236"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size):</span>
<span id="cb12-237"><a href="#cb12-237"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-238"><a href="#cb12-238"></a>        <span class="co"># each token directly reads off the logits for the next token from a lookup table</span></span>
<span id="cb12-239"><a href="#cb12-239"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, vocab_size)</span>
<span id="cb12-240"><a href="#cb12-240"></a>        <span class="co"># 26x26 2D Tensor, represents possibility of each char right after a char.</span></span>
<span id="cb12-241"><a href="#cb12-241"></a></span>
<span id="cb12-242"><a href="#cb12-242"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb12-243"><a href="#cb12-243"></a></span>
<span id="cb12-244"><a href="#cb12-244"></a>        <span class="co"># idx and targets are both (B,T) tensor of integers</span></span>
<span id="cb12-245"><a href="#cb12-245"></a>        logits <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx) <span class="co"># (B,T,C)</span></span>
<span id="cb12-246"><a href="#cb12-246"></a>        <span class="co"># (Batch = 4,Time = 8,Channel = 65)</span></span>
<span id="cb12-247"><a href="#cb12-247"></a></span>
<span id="cb12-248"><a href="#cb12-248"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb12-249"><a href="#cb12-249"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb12-250"><a href="#cb12-250"></a>        <span class="cf">else</span>:</span>
<span id="cb12-251"><a href="#cb12-251"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb12-252"><a href="#cb12-252"></a>            logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C) <span class="co"># stretch this out of Time dimension</span></span>
<span id="cb12-253"><a href="#cb12-253"></a>            targets <span class="op">=</span> targets.view(B<span class="op">*</span>T)</span>
<span id="cb12-254"><a href="#cb12-254"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb12-255"><a href="#cb12-255"></a>            <span class="co"># https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html</span></span>
<span id="cb12-256"><a href="#cb12-256"></a>            <span class="co"># C = number of classes</span></span>
<span id="cb12-257"><a href="#cb12-257"></a>            <span class="co"># B or N = Batch size</span></span>
<span id="cb12-258"><a href="#cb12-258"></a></span>
<span id="cb12-259"><a href="#cb12-259"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb12-260"><a href="#cb12-260"></a></span>
<span id="cb12-261"><a href="#cb12-261"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb12-262"><a href="#cb12-262"></a>        <span class="co"># idx is (B, T) array of indices in the current context</span></span>
<span id="cb12-263"><a href="#cb12-263"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb12-264"><a href="#cb12-264"></a>            <span class="co"># get the predictions</span></span>
<span id="cb12-265"><a href="#cb12-265"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx)</span>
<span id="cb12-266"><a href="#cb12-266"></a>            <span class="co"># focus only on the last time step</span></span>
<span id="cb12-267"><a href="#cb12-267"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># becomes (B, C)</span></span>
<span id="cb12-268"><a href="#cb12-268"></a>            <span class="co"># apply softmax to get probabilities</span></span>
<span id="cb12-269"><a href="#cb12-269"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># (B, C)</span></span>
<span id="cb12-270"><a href="#cb12-270"></a>            <span class="co"># sample from the distribution</span></span>
<span id="cb12-271"><a href="#cb12-271"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, 1)</span></span>
<span id="cb12-272"><a href="#cb12-272"></a>            <span class="co"># append sampled index to the running sequence</span></span>
<span id="cb12-273"><a href="#cb12-273"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, T+1)</span></span>
<span id="cb12-274"><a href="#cb12-274"></a>        <span class="cf">return</span> idx</span>
<span id="cb12-275"><a href="#cb12-275"></a></span>
<span id="cb12-276"><a href="#cb12-276"></a>m <span class="op">=</span> BigramLanguageModel(vocab_size)</span>
<span id="cb12-277"><a href="#cb12-277"></a>logits, loss <span class="op">=</span> m(xb, yb)</span>
<span id="cb12-278"><a href="#cb12-278"></a><span class="bu">print</span>(logits.shape)</span>
<span id="cb12-279"><a href="#cb12-279"></a><span class="bu">print</span>(loss)</span>
<span id="cb12-280"><a href="#cb12-280"></a></span>
<span id="cb12-281"><a href="#cb12-281"></a><span class="bu">print</span>(decode(m.generate(idx <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>), max_new_tokens<span class="op">=</span><span class="dv">100</span>)[<span class="dv">0</span>].tolist()))</span>
<span id="cb12-282"><a href="#cb12-282"></a><span class="in">```</span></span>
<span id="cb12-283"><a href="#cb12-283"></a><span class="in">```js</span></span>
<span id="cb12-284"><a href="#cb12-284"></a>torch<span class="op">.</span><span class="fu">Size</span>([<span class="dv">32</span><span class="op">,</span> <span class="dv">65</span>])</span>
<span id="cb12-285"><a href="#cb12-285"></a><span class="fu">tensor</span>(<span class="fl">4.8786</span><span class="op">,</span> grad_fn<span class="op">=&lt;</span>NllLossBackward0<span class="op">&gt;</span>)</span>
<span id="cb12-286"><a href="#cb12-286"></a></span>
<span id="cb12-287"><a href="#cb12-287"></a>SKIcLT<span class="op">;</span>AcELMoTbvZv C<span class="op">?</span>nq<span class="op">-</span>QE33<span class="op">:</span>CJqkOKH<span class="op">-</span>q<span class="op">;:</span>la<span class="op">!</span>oiywkHjgChzbQ<span class="op">?</span>u<span class="op">!</span><span class="dv">3</span><span class="er">bLIgwevmyFJGUGp</span></span>
<span id="cb12-288"><a href="#cb12-288"></a>wnYWmnxKWWev<span class="op">-</span>tDqXErVKLgJ</span>
<span id="cb12-289"><a href="#cb12-289"></a><span class="in">```</span></span>
<span id="cb12-290"><a href="#cb12-290"></a></span>
<span id="cb12-291"><a href="#cb12-291"></a>The sampling is garbage, since our model are in a completely random state right now - the loss is <span class="in">`4.88`</span> which is near to probability of each character in the <span class="in">`vocab_size`</span> space (<span class="in">`1/65`</span>). We are going to train the model!</span>
<span id="cb12-292"><a href="#cb12-292"></a></span>
<span id="cb12-293"><a href="#cb12-293"></a><span class="fu">## 2.5 training the bigram model</span></span>
<span id="cb12-294"><a href="#cb12-294"></a><span class="fu">## 2.6 port our code to a script</span></span>
<span id="cb12-295"><a href="#cb12-295"></a><span class="fu"># 3 Building the "self-attention"</span></span>
<span id="cb12-296"><a href="#cb12-296"></a><span class="fu">## 3.1 version 1: averaging past context with for loops, the weakest form of aggregation</span></span>
<span id="cb12-297"><a href="#cb12-297"></a><span class="fu">## 3.2 the trick in self-attention: matrix multiply as weighted aggregation</span></span>
<span id="cb12-298"><a href="#cb12-298"></a><span class="fu">## 3.3 version 2: using matrix multiply</span></span>
<span id="cb12-299"><a href="#cb12-299"></a><span class="fu">## 3.4 version 3: adding softmax</span></span>
<span id="cb12-300"><a href="#cb12-300"></a><span class="fu">## 3.5 minor code cleanup</span></span>
<span id="cb12-301"><a href="#cb12-301"></a><span class="fu">## 3.6 positional encoding</span></span>
<span id="cb12-302"><a href="#cb12-302"></a><span class="fu">## 3.7 THE CRUX OF THE VIDEO: version 4: self-attention</span></span>
<span id="cb12-303"><a href="#cb12-303"></a><span class="fu">## 3.8 note 1: attention as communication</span></span>
<span id="cb12-304"><a href="#cb12-304"></a><span class="fu">## 3.9 note 2: attention has no notion of space, operates over sets</span></span>
<span id="cb12-305"><a href="#cb12-305"></a><span class="fu">## 3.10 note 3: there is no communication across batch dimension</span></span>
<span id="cb12-306"><a href="#cb12-306"></a><span class="fu">## 3.11 note 4: encoder blocks vs. decoder blocks</span></span>
<span id="cb12-307"><a href="#cb12-307"></a><span class="fu">## 3.12 note 5: attention vs. self-attention vs. cross-attention</span></span>
<span id="cb12-308"><a href="#cb12-308"></a><span class="fu">## 3.13 note 6: "scaled" self-attention. why divide by sqrt(head_size)</span></span>
<span id="cb12-309"><a href="#cb12-309"></a><span class="fu"># 4 Building the Transformer</span></span>
<span id="cb12-310"><a href="#cb12-310"></a><span class="fu">## 4.1 inserting a single self-attention block to our network</span></span>
<span id="cb12-311"><a href="#cb12-311"></a><span class="fu">## 4.2 multi-headed self-attention</span></span>
<span id="cb12-312"><a href="#cb12-312"></a><span class="fu">## 4.3 feedforward layers of transformer block</span></span>
<span id="cb12-313"><a href="#cb12-313"></a><span class="fu">## 4.4 residual connections</span></span>
<span id="cb12-314"><a href="#cb12-314"></a><span class="fu">## 4.5 layernorm (and its relationship to our previous batchnorm)</span></span>
<span id="cb12-315"><a href="#cb12-315"></a><span class="fu">## 4.6 scaling up the model! creating a few variables. adding dropout</span></span>
<span id="cb12-316"><a href="#cb12-316"></a><span class="fu"># 5 Notes on Transformer</span></span>
<span id="cb12-317"><a href="#cb12-317"></a><span class="fu">## 5.1 encoder vs. decoder vs. both (?) Transformers</span></span>
<span id="cb12-318"><a href="#cb12-318"></a><span class="fu">## 5.2 super quick walkthrough of nanoGPT, batched multi-headed self-attention</span></span>
<span id="cb12-319"><a href="#cb12-319"></a><span class="fu">## 5.3 back to ChatGPT, GPT-3, pretraining vs. finetuning, RLHF</span></span>
<span id="cb12-320"><a href="#cb12-320"></a><span class="fu">## 5.4 conclusions</span></span>
<span id="cb12-321"><a href="#cb12-321"></a><span class="fu"># 6 resources</span></span>
<span id="cb12-322"><a href="#cb12-322"></a></span>
<span id="cb12-323"><a href="#cb12-323"></a><span class="ss">- </span>Colab notebook: <span class="ot">&lt;https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing&gt;</span>;</span>
<span id="cb12-324"><a href="#cb12-324"></a><span class="ss">- </span><span class="in">`nanoGPT`</span>: <span class="ot">&lt;https://github.com/karpathy/nanoGPT&gt;</span>;</span>
<span id="cb12-325"><a href="#cb12-325"></a><span class="ss">- </span>TinyShakepeare dataset: <span class="ot">&lt;https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt&gt;</span>;</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block"><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"><i class="fa-brands fa-creative-commons" aria-label="creative-commons"></i></a> 2023-2025 Le Khac Tuan</span></p>
</div>   
    <div class="nav-footer-center">
<p><span class="faux-block"> Designed with <i class="fa-solid fa-heart" aria-label="heart"></i>, <span id="commit-info">Loading last commit…</span> </span></p>
</div>
    <div class="nav-footer-right">
<p><span class="faux-block">Made with <a href="https://quarto.org/">Quarto</a></span></p>
</div>
  </div>
</footer>
<script type="application/javascript" src="commit_info.js"></script>




</body></html>