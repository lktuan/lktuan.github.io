[
  {
    "objectID": "学汉语的日记.html",
    "href": "学汉语的日记.html",
    "title": "学汉语的日记",
    "section": "",
    "text": "HSK5上 | 第01课： 爱的细节\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n。。。有人认为夫妻之间最重要的是恩爱，有人说是诚实，也有人说是关爱和理解。。。\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第20课： 小人书摊\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n。。。一些印刷精美、有特色的作品则身价大涨，成了收藏品，甚至进了博物馆。小人书和小人书摊已成为历史的记忆。\n\n\n\n\n\nJan 6, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第19课： 家乡的萝卜饼\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n家乡的众多美食中，萝卜饼是最让我怀念的。它那丰富的色彩、微甜的口感，至今仍让我十分想念。\n\n\n\n\n\nDec 30, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第21课： 汉字叔叔：一个美国人的汉字情缘\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n他说：“我不会退休，我还要继续追求我的梦想，我要‘活到老，学到老’。\n\n\n\n\n\nJan 20, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第22课： 阅读与思考\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n“知识”两个字我始终认为它是要分开来谈的，“知”就是知感，“识”就是认识\n\n\n\n\n\nJan 30, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第23课： 放手\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n文文沉默了很久，最后吻了妈妈一下，轻轻地说：“妈妈，我真的很喜欢现在的你。”妈妈忍𣎴住流下了眼泪。她说：“幸亏那晚天色很暗。\n\n\n\n\n\nMar 10, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第26课： 你属于哪种“忙”\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n我有一个体会：现实中，我们不一定知道正确的道路是什么，但时时反省、总结，却可以使我们不会在错误的道路上走得太远。\n\n\n\n\n\nApr 17, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第25课： 给自己加满水\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n那些胸怀理想、肩上有责任感的人，才能承受住压力，从历史的风雨中走过“鬼谷”；而那些没有理想，没有一点压力，做一天和尚撞一天钟的人，就像一艘风暴中的空船，往往一场人生的狂风巨浪便会把他们彻底地打翻在地。\n\n\n\n\n\nMar 28, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第24课： 支教行动\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n不管以后在哪儿，我都会继续用我的力量影响山里的孩子们，因为他们是国家的未来与希望。\n\n\n\n\n\nMar 16, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下|第27课：下棋\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n每走一步，你事先都应该想清楚：为了赢得什么，你愿意失去什么，这样才可能赢。\n\n\n\n\n\nApr 20, 2024\n\n\nTuanLeKhac\n\n\n\n\n\n\n\n\n\n\n\n\nHSKK, 朗读：江南的春雨\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\nHSKK\n\n\n朗读\n\n\n\n江南的人，江南的风俗，甚至窗前的那盆水仙，连我铺在书桌上准备写江南雨的稿予，连我童年的记忆都湿了!\n\n\n\n\n\nJul 23, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第36课： 老舍与养花\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n我不知道花草们受我的照顾，感谢我不感谢，反正我要感谢它们。\n\n\n\n\n\nJul 20, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第29课： 培养对手\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n能够适应环境的人才能很好的存在，生活下去\n\n\n\n\n\nMay 17, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第28课： 最受欢迎的毕业生\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n无论在哪个行业，最缺乏的永远都是专注的人，专注的人永远不缺机会\n\n\n\n\n\nMay 6, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第35课： 植物会出汗\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n我们都知道这样的常识一植物的根会吸收养分和水分，但是你有没有想过，植物是怎么控制这些成分，把它们运输到十几米甚至上百米的树梢的呢？\n\n\n\n\n\nJul 12, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第34课： 鸟儿的护肤术\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n大家都接触过鸟儿吧？那你知道鸟儿最重要的特征是什么吗？是有翅膀会飞？还是吃昆虫？\n\n\n\n\n\nJun 30, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第33课： 以堵治堵-缓解交通有妙招\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n城市本是为人而建，如今却被汽车占有，詹森的目标很明确，就是期待能够解放城市，使之更适合人类生活。\n\n\n\n\n\nJun 22, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第32课： 身边的环保\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n地球是人类共同的家园，我们应该把它看作一个属于自己的大房间\n\n\n\n\n\nJun 17, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSKK, 朗读：狐假虎威\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\nHSKK\n\n\n朗读\n\n\n\nThe fox borrows the tiger’s fierceness\n\n\n\n\n\nJun 20, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第31课： 登门槛效应\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n有人说，接受或帮助与否，要看对方是什么人，或者自己当时的心情如何；也有人认为要看对方的请求是否合理，或者自己是否有能力帮助他\n\n\n\n\n\nMay 31, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSKK, 朗读：武侠小说\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\nHSKK\n\n\n朗读\n\n\n\n优秀的武侠小说不仅从另一个角度反映了时代风貌和各色人等的心理历程\n\n\n\n\n\nJun 3, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSKK, 朗读：北京\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\nHSKK\n\n\n朗读\n\n\n\n人类最伟大的个体工程\n\n\n\n\n\nJun 2, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第30课： 竞争让市场更高效\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n竞争的好处确实不少，它有利于运动员创造好成绩，可以加强企业的活力，能够推动经济和社会的发展\n\n\n\n\n\nMay 23, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5上 | 第18课：抽象艺术美不美\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5上 | 第17课：在最美好的时刻离开\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5上 | 第16课：体重与节食\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5上 | 第15课：纸上谈兵\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5上 | 第14课：北京的四合院\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5上 | 第13课：据掉生活的“筐底”\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5上 | 第12课：外国用户玩儿微信\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5上 | 第11课：闹钟的危害\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5上 | 第10课：争论的奇迹\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5上 | 第09课：别样鲁迅\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5上 | 第08课：朝三暮四\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5上 | 第07课：成语故事两则\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5上 | 第06课：除夕的由来\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5上 | 第05课：济南的泉水\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5上 | 第04课：子路背米\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5上 | 第03课：一生有选择，一切可改变\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5上 | 第02课：留串钥匙给父母\n\n\n\n\n\n\nHSK5\n\n\n学汉语的日记\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html",
    "href": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html",
    "title": "HSK5下 | 第35课： 植物会出汗",
    "section": "",
    "text": "植物会出汗 | Plants also sweat\n炎热的夏天，踢完一场球赛，每个队员都已经是汗如雨下。如果这个时候，能到大树下歇一歇，喝口凉开水，吃个冰激凌，放松放松肌肉，缓解一下疲劳，那一定是件美事，可以很快恢复活力。不过，你知道吗，我们之所以能在大树下享受这种湿润荫凉，也是因为大树在“出汗”呢！\n人体要保持相对稳定的温度，一旦温度上升，大脑就会指挥我们的身体赶快出汗，这时所有汗腺开始工作，汗水就从毛孔里冒了出来。大树出的“汗”，通常是从叶片的气孔里冒出来的，不过，这种“出汗〞可不是为了降低温度，而是为了运输养分。\n我们都知道这样的常识一植物的根会吸收养分和水分，但是你有没有想过，植物是怎么控制这些成分，把它们运输到十几米甚至上百米的树梢的呢？\n最初人们认为大树是通过毛细作用来提水的。所谓“毛细作用”简单来说，就是水会顺着很细很细的管子向上“爬”，我们在家可以用个比较细的玻璃管体验一下。玻璃管越细，水爬升的高度就越高。可是，经过测验计算发现，以大树输送管道的尺寸产生的毛细作用，根本无法把水分送到几十米高的地方。\n实际上，大树利用的是枝千顶端的那些叶片。叶子通过不停地向空气中释放水汽，迫使树干中的水分自动前来补充，这样节节传递，就像是把树根吸收的水分给抽了上来。因为跟蒸腾作用有关，这种特殊的提升力就被称为“蒸腾拉力”。不过，这个大树内部的供水系统具体的运转状况是怎么样的，它们遵守的是一种什么样的秩序，为什么会产生如此巨大的拉力，到目前还是个谜。\n改编自《科学松鼠会》，作者：史军\n\n\n\n\n\n\n\n\n\nTranspiration\n\n\n\n\n\n\n\n\n植物会出汗\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n炎热\nyán rè - adj. scorching, burning hot\n\n\n2\n歇\nxiē - v. to rest, to take a rest\n\n\n3\n开水\nkāi shuǐ - n. boiled water\n\n\n4\n冰激凌\nbīng jī líng - n. ice cream\n\n\n5\n肌肉\njī ròu - n. muscle\n\n\n6\n恢复\nhuī fù - v. to recover, too regain\n\n\n7\n湿润\nshī rùn - adj. moist, humid\n\n\n8\n荫凉\nyìnliáng - adj. shady and cool\n\n\n9\n指挥\nzhǐhuī - v. to command, to direct\n\n\n10\n赶快\ngǎnkuài - adv. at once, hurriedly\n\n\n11\n汗腺\nhànxiàn - n. sweat gland\n\n\n12\n毛孔\nmáo kǒng - n. pore\n\n\n13\n冒\nmào - v. to emit, to give off, to send out\n\n\n14\n片\npiàn - n. flat and thin piece\n\n\n15\n常识\ncháng shí - n. common knowledge\n\n\n16\n吸收\nxī shōu - v. to absorb, to take in\n\n\n17\n控制\nkòngzhì - v. to control, to take command of\n\n\n18\n成分\nchéng fèn - n. element, component, ingredient\n\n\n19\n梢\nshāo - n. tip, thin end of a twic, etc.\n\n\n20\n管子\nguǎn zi - n. tube, pipe\n\n\n21\n玻璃\nbō li - n. glass\n\n\n22\n根本\ngēn běn - adv. (often used in the negative) at all, simply\n\n\n23\n枝干\nzhī gàn - n. branch, limb\n\n\n24\n释放\nshì fàng - v. to release, to emit\n\n\n25\n自动\nzì dòng - adv. voluntarily, spontaneously, automatic\n\n\n26\n补充\nbǔchōng - v. to supplement, to replenish\n\n\n27\n抽\nchōu - v. to draw, to obtain by drawing\n\n\n28\n蒸腾\nzhēngténg - v. (of steam) to rise, to vaporize\n\n\n29\n特殊\ntè shū - adj. special, particular\n\n\n30\n内部\nnèibù - n. inside, interior, inner part\n\n\n31\n系统\nxì tǒng - n. system\n\n\n32\n状况\nzhuàng kuàng - n. condition, situation, state\n\n\n33\n秩序\nzhìxù - n. order, orderly state\n\n\n34\n测验\ncè yàn - v. to test\n\n\n\n\n\n\n\n赶快\n\nphó từ, ý nghĩa là “抓紧时间、加快速度”（bắt kịp thời gian, gia tăng tốc độ）.Ví dụ:\n\n我下个月要搬家，得赶快找房子。\n这份材料下午开会要用，你赶快把它复印一下。\n·····一旦温度上升，大脑就会指挥我们的身体赶快出汗，······\n\n（1）你要赶快出发，要不该赶上堵车了。\n（2）A：我们快要到了，你赶快收拾吧。\nB：好，我马上就收拾好了\n（3）A：你怎么还在玩儿游戏？火车票预定了吗？\nB：还没呢，我现在就赶快预定。\n\n片\n\n“片”，danh từ, đồ vật có bề mặt phẳng, mỏng, thường không quá lớn. Ví dụ:\n\n瓶子里装着满满的石头、玻璃碎片和沙子。\n大树出的“汗”，通常是从叶片的气孔里冒出来的，······\n\n“片”，còn là lượng từ, dùng để chỉ những đồ vật thành phiến (成片); cũng có thể dùng với âm thanh, cảnh sắc…Ví dụ:\n\n窗外有一棵大树，秋风中，叶子一片片地掉落下来。\n同学们听了，发出一片热烈的欢呼声。\n\n（1）A：我来切菜吧，土豆怎么切？\nB：你可以把土豆切成片。\n（2）今天早上我吃了片西瓜，现在还不太饿。\n（3）A：这药怎么吃？\nB：每天吃饭前你应该吃两片药。\n\n根本\n\n“根本”, danh từ, bộ phận quan trọng nhất của sự vật. Ví dụ:\n\n教育是国家的根本。\n这个办法只能救急，不能从根本上解决问题。\n\n“根本”, còn là tính từ, ý nghĩa là “主要的、最重要的、起决定作用的”（chủ yếu, quan trọng nhất, đóng vai trò quyết định）\n\n谈判还算顺利，一些根本的问题都谈好了。\n政府工作应从人民的根本利益出发。\n\n“根本”, còn là phó từ, biểu thị từ đầu đến cuối, trước sau như một, thường dùng trong câu phủ định. Ví dụ:\n\n有时候我会梦见参加考试，可是却发现自己根本读不懂考试的题目。\n可是，经过测验计算发现，以大树输送管道的尺寸产生的毛细作用，根本无法把水分送到几十米高的地方。\n\n“根本”, khi làm phó từ, còn biểu thị triệt để, hoàn toàn. Ví dụ:\n\n事情已经根本解决了。\n他根本就是在故意找我们的麻烦。\n\n（1）A：你觉得他说的话有道理吗？\nB：他一开始说话的时候根本没有道理，我觉得你还是听听玛丽的建议吧\n（2）搬家要整理的东西太多了，我根本拿不动。你帮帮我吧。\n（3）秦国人在赵国四处散布谣言，说秦军最怕赵括，那个谣言，根本就是故意想让赵括对他自己的实力过分自信。\n\nPhân biệt 特殊 và 特别\n\n共同点：Khi làm tính từ, đều có nghĩa là không giống với bình thường.\n\n如：对我来说， 他是一个特殊/特别的人。\n\n不同点：\n\n\n\n\n\n\n\n\n\n特殊\n特别\n\n\n\n\n1\nThường dùng trong văn viết.\n如：因为跟蒸腾作用有关，这种特殊的提升力就被称为“蒸腾拉力“\nVăn viết hay khẩu ngữ đều có thể sử dụng.\n如：她穿衣服总是很特别。\n\n\n2\nKhông có cách dùng này.\nCòn có thể làm phó từ. Ý nghĩa là 格外 “ đặc biệt“.\n如：我特别喜欢学中文，尤其是汉子。\n\n\n\n\n\n\n问题：动、植物\n\n动、植物\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n老鼠\nlao3shu3 - mouse\n\n\n2\n蜜蜂\nmi4feng1 - honey bee\n\n\n3\n蛇\nshe2 - snake\n\n\n4\n狮子\nshi1zi - lion\n\n\n5\n兔子\ntu4zi - rabbit\n\n\n6\n大象\nda4xiang1 - elephant\n\n\n7\n猴子\nhou2zi - monkey\n\n\n8\n猪\nzhu1 - pig\n\n\n9\n蝴蝶\nhu2die2 - butterfly\n\n\n10\n昆虫\nkun1chong2 - insect\n\n\n11\n小麦\nxiao3mai4 - wheat\n\n\n12\n竹子\nzhu2zi - bamboo\n\n\n13\n根\ngen1 - root\n\n\n14\n果实\nguo3shi2 - fruit\n\n\n\n\n\n\n你所知道的一种植物\n越南大米（Oryza sativa）是稻属一年生草本植物，属于禾本科（Poaceae）。这种植物在越南的湿热气候中茁壮成长，特别适合湄公河三角洲和红河三角洲等低地地区。越南全年高温多雨的气候为水稻生长提供了理想条件。\n越南的水稻种植独具特色，每年可以收获三季稻。这种密集的种植模式充分利用了越南的气候优势。该国气候的显著特点是高温（平均20-35°C）和充沛降雨（年降雨量1500-2000毫米），为水稻全年生长创造了完美环境。\n人类驯化水稻的历史可追溯到约9000年前在珠江流域。经过数千年的选择和培育，形成了适应不同生态环境的多样化品种。越南的稻米品种以其独特的香味和口感而闻名，如”香米”就是其中的代表。\n如今，越南是世界第三大稻米出口国，在全球粮食安全中扮演着重要角色。越南大米不仅是国内主食，还为全球数亿人提供营养。得益于其三季稻作系统，越南能够保持稳定的稻米产量。然而，面对气候变化带来的挑战，越南正致力于开发更具抗性的水稻品种，以确保未来粮食供应的稳定性。\n\n\n\n\n你喜欢什么样的植物？\n植物的多样性和功能性使我对它们产生了广泛的喜爱。食用植物和蔬果是我的最爱之一，它们不仅为人类提供了必要的营养，还丰富了我们的饮食文化。想象一下没有新鲜蔬菜和香甜水果的世界，那将多么单调啊！鲜花则是大自然的艺术品，它们绚丽的色彩和优雅的姿态为我们的生活增添了无尽的美丽。无论是盛开在野外的花朵，还是精心培育的园艺品种，每一朵花都有其独特的魅力。而大树则是地球的守护者，它们不仅为我们提供了清新的氧气和舒适的阴凉，还是许多生物的家园。从某种意义上说，每一种植物都有其存在的价值和意义，因此我珍视所有的植物，欣赏它们为我们的星球带来的生机与活力。\n在你的国家最常见或最有名的职务是什么？\n在越南，大米无疑是最具代表性和最重要的植物。作为主食，大米深深植根于越南的文化和日常生活中。越南的地理条件非常适合水稻种植，特别是在红河三角洲和湄公河三角洲这两个肥沃的地区。越南大米以其独特的香味和口感而闻名于世，其中最著名的品种是“香米”。水稻种植不仅仅是一种农业活动，它还塑造了越南的乡村景观和社会结构。每年三季的稻作周期体现了越南农民的勤劳和智慧。此外，大米在越南的经济中扮演着重要角色，是主要的出口商品之一。从传统的农耕仪式到现代化的种植技术，大米见证了越南的历史变迁，并将继续影响国家的未来发展。\n在你的生活中，植物起了什么作用？\n植物在我的生活中扮演着多重而重要的角色。首先，作为粮食来源，它们是我日常饮食的基础。无论是主食的大米、小麦，还是各种蔬菜，都为我提供了必要的营养。果实则带来了额外的味觉享受和维生素补充。木材是另一个重要方面，它被用于制作家具、建筑材料，甚至是纸张，使我的生活更加舒适和便利。在一些地区，木柴仍然是重要的燃料来源，为烹饪和取暖提供能量。然而，植物最关键的贡献可能是它们产生的氧气。通过光合作用，植物不断净化空气，为地球上的生命提供赖以生存的氧气。此外，植物还美化了我的生活环境，无论是室内的盆栽还是户外的花园，都为我的生活增添了自然的活力和美感。\n\n\n\n\n朋友送我一对珍珠鸟。放在一个简易的竹条编成的笼子里，笼内还有一卷干草，那是小鸟儿舒适又温暖的巢。\n珍珠 zhen1zhu1 trân châu, 编成 bian1cheng2 tạo thành, 笼子 long2zi cage lồng， 巢 chao2 nest tổ\n有人说，这是一种怕人的鸟。我把它挂在窗前，那儿还有一大盆异常茂盛的法国吊兰。我使用吊兰长长的、串生着小绿叶的垂茎蒙盖在鸟笼上，它们就像躲进深幽的丛林一样安仝…从中传出笛儿般又细又亮的叫声，就格外轻松自在了。\n异常 yi4chang2 unsual, 茂盛 mao4sheng4 lush tươi tốt, 吊兰 diao4lan2 Chlorophytum comosum cây dây nhện, 垂 chui2 to hang, 茎 jing1 stem, 蒙 meng2 to cover, 盖 gai4 cover\n我很少扒开叶茎瞧它们，它们便渐渐敢伸出小脑袋瞅瞅我。我们就这样一点点熟悉了。\n三个月后，那一团愈发絮茂的绿茎里边，发出一种尖细又娇嫩的呜叫。我猜到，是它们有了雏儿。我呢，决不掀开叶片往里看，连添食加水时也不峥大妤奇的眼去惊动它们。过不多久，忽然有一个更小的脑袋从叶间探出未。哟，雏儿!正是这小家伙!瞧，多么像它的父母：红嘴红脚，蓝灰色的毛，只是后背还没生出珍珠似的圆圆的白点；它好肥，整个身子好像一个茎松的球儿。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html#生词",
    "href": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html#生词",
    "title": "HSK5下 | 第35课： 植物会出汗",
    "section": "",
    "text": "植物会出汗\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n炎热\nyán rè - adj. scorching, burning hot\n\n\n2\n歇\nxiē - v. to rest, to take a rest\n\n\n3\n开水\nkāi shuǐ - n. boiled water\n\n\n4\n冰激凌\nbīng jī líng - n. ice cream\n\n\n5\n肌肉\njī ròu - n. muscle\n\n\n6\n恢复\nhuī fù - v. to recover, too regain\n\n\n7\n湿润\nshī rùn - adj. moist, humid\n\n\n8\n荫凉\nyìnliáng - adj. shady and cool\n\n\n9\n指挥\nzhǐhuī - v. to command, to direct\n\n\n10\n赶快\ngǎnkuài - adv. at once, hurriedly\n\n\n11\n汗腺\nhànxiàn - n. sweat gland\n\n\n12\n毛孔\nmáo kǒng - n. pore\n\n\n13\n冒\nmào - v. to emit, to give off, to send out\n\n\n14\n片\npiàn - n. flat and thin piece\n\n\n15\n常识\ncháng shí - n. common knowledge\n\n\n16\n吸收\nxī shōu - v. to absorb, to take in\n\n\n17\n控制\nkòngzhì - v. to control, to take command of\n\n\n18\n成分\nchéng fèn - n. element, component, ingredient\n\n\n19\n梢\nshāo - n. tip, thin end of a twic, etc.\n\n\n20\n管子\nguǎn zi - n. tube, pipe\n\n\n21\n玻璃\nbō li - n. glass\n\n\n22\n根本\ngēn běn - adv. (often used in the negative) at all, simply\n\n\n23\n枝干\nzhī gàn - n. branch, limb\n\n\n24\n释放\nshì fàng - v. to release, to emit\n\n\n25\n自动\nzì dòng - adv. voluntarily, spontaneously, automatic\n\n\n26\n补充\nbǔchōng - v. to supplement, to replenish\n\n\n27\n抽\nchōu - v. to draw, to obtain by drawing\n\n\n28\n蒸腾\nzhēngténg - v. (of steam) to rise, to vaporize\n\n\n29\n特殊\ntè shū - adj. special, particular\n\n\n30\n内部\nnèibù - n. inside, interior, inner part\n\n\n31\n系统\nxì tǒng - n. system\n\n\n32\n状况\nzhuàng kuàng - n. condition, situation, state\n\n\n33\n秩序\nzhìxù - n. order, orderly state\n\n\n34\n测验\ncè yàn - v. to test"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html#注释",
    "href": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html#注释",
    "title": "HSK5下 | 第35课： 植物会出汗",
    "section": "",
    "text": "赶快\n\nphó từ, ý nghĩa là “抓紧时间、加快速度”（bắt kịp thời gian, gia tăng tốc độ）.Ví dụ:\n\n我下个月要搬家，得赶快找房子。\n这份材料下午开会要用，你赶快把它复印一下。\n·····一旦温度上升，大脑就会指挥我们的身体赶快出汗，······\n\n（1）你要赶快出发，要不该赶上堵车了。\n（2）A：我们快要到了，你赶快收拾吧。\nB：好，我马上就收拾好了\n（3）A：你怎么还在玩儿游戏？火车票预定了吗？\nB：还没呢，我现在就赶快预定。\n\n片\n\n“片”，danh từ, đồ vật có bề mặt phẳng, mỏng, thường không quá lớn. Ví dụ:\n\n瓶子里装着满满的石头、玻璃碎片和沙子。\n大树出的“汗”，通常是从叶片的气孔里冒出来的，······\n\n“片”，còn là lượng từ, dùng để chỉ những đồ vật thành phiến (成片); cũng có thể dùng với âm thanh, cảnh sắc…Ví dụ:\n\n窗外有一棵大树，秋风中，叶子一片片地掉落下来。\n同学们听了，发出一片热烈的欢呼声。\n\n（1）A：我来切菜吧，土豆怎么切？\nB：你可以把土豆切成片。\n（2）今天早上我吃了片西瓜，现在还不太饿。\n（3）A：这药怎么吃？\nB：每天吃饭前你应该吃两片药。\n\n根本\n\n“根本”, danh từ, bộ phận quan trọng nhất của sự vật. Ví dụ:\n\n教育是国家的根本。\n这个办法只能救急，不能从根本上解决问题。\n\n“根本”, còn là tính từ, ý nghĩa là “主要的、最重要的、起决定作用的”（chủ yếu, quan trọng nhất, đóng vai trò quyết định）\n\n谈判还算顺利，一些根本的问题都谈好了。\n政府工作应从人民的根本利益出发。\n\n“根本”, còn là phó từ, biểu thị từ đầu đến cuối, trước sau như một, thường dùng trong câu phủ định. Ví dụ:\n\n有时候我会梦见参加考试，可是却发现自己根本读不懂考试的题目。\n可是，经过测验计算发现，以大树输送管道的尺寸产生的毛细作用，根本无法把水分送到几十米高的地方。\n\n“根本”, khi làm phó từ, còn biểu thị triệt để, hoàn toàn. Ví dụ:\n\n事情已经根本解决了。\n他根本就是在故意找我们的麻烦。\n\n（1）A：你觉得他说的话有道理吗？\nB：他一开始说话的时候根本没有道理，我觉得你还是听听玛丽的建议吧\n（2）搬家要整理的东西太多了，我根本拿不动。你帮帮我吧。\n（3）秦国人在赵国四处散布谣言，说秦军最怕赵括，那个谣言，根本就是故意想让赵括对他自己的实力过分自信。\n\nPhân biệt 特殊 và 特别\n\n共同点：Khi làm tính từ, đều có nghĩa là không giống với bình thường.\n\n如：对我来说， 他是一个特殊/特别的人。\n\n不同点：\n\n\n\n\n\n\n\n\n\n特殊\n特别\n\n\n\n\n1\nThường dùng trong văn viết.\n如：因为跟蒸腾作用有关，这种特殊的提升力就被称为“蒸腾拉力“\nVăn viết hay khẩu ngữ đều có thể sử dụng.\n如：她穿衣服总是很特别。\n\n\n2\nKhông có cách dùng này.\nCòn có thể làm phó từ. Ý nghĩa là 格外 “ đặc biệt“.\n如：我特别喜欢学中文，尤其是汉子。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html#扩展",
    "href": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html#扩展",
    "title": "HSK5下 | 第35课： 植物会出汗",
    "section": "",
    "text": "问题：动、植物\n\n动、植物\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n老鼠\nlao3shu3 - mouse\n\n\n2\n蜜蜂\nmi4feng1 - honey bee\n\n\n3\n蛇\nshe2 - snake\n\n\n4\n狮子\nshi1zi - lion\n\n\n5\n兔子\ntu4zi - rabbit\n\n\n6\n大象\nda4xiang1 - elephant\n\n\n7\n猴子\nhou2zi - monkey\n\n\n8\n猪\nzhu1 - pig\n\n\n9\n蝴蝶\nhu2die2 - butterfly\n\n\n10\n昆虫\nkun1chong2 - insect\n\n\n11\n小麦\nxiao3mai4 - wheat\n\n\n12\n竹子\nzhu2zi - bamboo\n\n\n13\n根\ngen1 - root\n\n\n14\n果实\nguo3shi2 - fruit"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html#运用",
    "href": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html#运用",
    "title": "HSK5下 | 第35课： 植物会出汗",
    "section": "",
    "text": "你所知道的一种植物\n越南大米（Oryza sativa）是稻属一年生草本植物，属于禾本科（Poaceae）。这种植物在越南的湿热气候中茁壮成长，特别适合湄公河三角洲和红河三角洲等低地地区。越南全年高温多雨的气候为水稻生长提供了理想条件。\n越南的水稻种植独具特色，每年可以收获三季稻。这种密集的种植模式充分利用了越南的气候优势。该国气候的显著特点是高温（平均20-35°C）和充沛降雨（年降雨量1500-2000毫米），为水稻全年生长创造了完美环境。\n人类驯化水稻的历史可追溯到约9000年前在珠江流域。经过数千年的选择和培育，形成了适应不同生态环境的多样化品种。越南的稻米品种以其独特的香味和口感而闻名，如”香米”就是其中的代表。\n如今，越南是世界第三大稻米出口国，在全球粮食安全中扮演着重要角色。越南大米不仅是国内主食，还为全球数亿人提供营养。得益于其三季稻作系统，越南能够保持稳定的稻米产量。然而，面对气候变化带来的挑战，越南正致力于开发更具抗性的水稻品种，以确保未来粮食供应的稳定性。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html#口语",
    "href": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html#口语",
    "title": "HSK5下 | 第35课： 植物会出汗",
    "section": "",
    "text": "你喜欢什么样的植物？\n植物的多样性和功能性使我对它们产生了广泛的喜爱。食用植物和蔬果是我的最爱之一，它们不仅为人类提供了必要的营养，还丰富了我们的饮食文化。想象一下没有新鲜蔬菜和香甜水果的世界，那将多么单调啊！鲜花则是大自然的艺术品，它们绚丽的色彩和优雅的姿态为我们的生活增添了无尽的美丽。无论是盛开在野外的花朵，还是精心培育的园艺品种，每一朵花都有其独特的魅力。而大树则是地球的守护者，它们不仅为我们提供了清新的氧气和舒适的阴凉，还是许多生物的家园。从某种意义上说，每一种植物都有其存在的价值和意义，因此我珍视所有的植物，欣赏它们为我们的星球带来的生机与活力。\n在你的国家最常见或最有名的职务是什么？\n在越南，大米无疑是最具代表性和最重要的植物。作为主食，大米深深植根于越南的文化和日常生活中。越南的地理条件非常适合水稻种植，特别是在红河三角洲和湄公河三角洲这两个肥沃的地区。越南大米以其独特的香味和口感而闻名于世，其中最著名的品种是“香米”。水稻种植不仅仅是一种农业活动，它还塑造了越南的乡村景观和社会结构。每年三季的稻作周期体现了越南农民的勤劳和智慧。此外，大米在越南的经济中扮演着重要角色，是主要的出口商品之一。从传统的农耕仪式到现代化的种植技术，大米见证了越南的历史变迁，并将继续影响国家的未来发展。\n在你的生活中，植物起了什么作用？\n植物在我的生活中扮演着多重而重要的角色。首先，作为粮食来源，它们是我日常饮食的基础。无论是主食的大米、小麦，还是各种蔬菜，都为我提供了必要的营养。果实则带来了额外的味觉享受和维生素补充。木材是另一个重要方面，它被用于制作家具、建筑材料，甚至是纸张，使我的生活更加舒适和便利。在一些地区，木柴仍然是重要的燃料来源，为烹饪和取暖提供能量。然而，植物最关键的贡献可能是它们产生的氧气。通过光合作用，植物不断净化空气，为地球上的生命提供赖以生存的氧气。此外，植物还美化了我的生活环境，无论是室内的盆栽还是户外的花园，都为我的生活增添了自然的活力和美感。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html#朗读",
    "href": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html#朗读",
    "title": "HSK5下 | 第35课： 植物会出汗",
    "section": "",
    "text": "朋友送我一对珍珠鸟。放在一个简易的竹条编成的笼子里，笼内还有一卷干草，那是小鸟儿舒适又温暖的巢。\n珍珠 zhen1zhu1 trân châu, 编成 bian1cheng2 tạo thành, 笼子 long2zi cage lồng， 巢 chao2 nest tổ\n有人说，这是一种怕人的鸟。我把它挂在窗前，那儿还有一大盆异常茂盛的法国吊兰。我使用吊兰长长的、串生着小绿叶的垂茎蒙盖在鸟笼上，它们就像躲进深幽的丛林一样安仝…从中传出笛儿般又细又亮的叫声，就格外轻松自在了。\n异常 yi4chang2 unsual, 茂盛 mao4sheng4 lush tươi tốt, 吊兰 diao4lan2 Chlorophytum comosum cây dây nhện, 垂 chui2 to hang, 茎 jing1 stem, 蒙 meng2 to cover, 盖 gai4 cover\n我很少扒开叶茎瞧它们，它们便渐渐敢伸出小脑袋瞅瞅我。我们就这样一点点熟悉了。\n三个月后，那一团愈发絮茂的绿茎里边，发出一种尖细又娇嫩的呜叫。我猜到，是它们有了雏儿。我呢，决不掀开叶片往里看，连添食加水时也不峥大妤奇的眼去惊动它们。过不多久，忽然有一个更小的脑袋从叶间探出未。哟，雏儿!正是这小家伙!瞧，多么像它的父母：红嘴红脚，蓝灰色的毛，只是后背还没生出珍珠似的圆圆的白点；它好肥，整个身子好像一个茎松的球儿。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html#听力",
    "href": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html#听力",
    "title": "HSK5下 | 第35课： 植物会出汗",
    "section": "2.1. 听力",
    "text": "2.1. 听力\n1.  男：天气太热了，踢个十分钟就快受不了了，我得去那棵树下面歇歇。\n女：喝点儿温开水，可别吃冰激凌什么的。\n问：女的有什么建议？（C喝温水）\n2.  男：中国的植树节是哪天？\n女：3月12号，这是孙中山先生逝世（pass away）的日子，他一直提倡植树造林（trồng cây gây rừng）。\n问：中国的植树节为什么是3月12号？（D纪念伟人）\n3.  女：你昨天去看小兰了？她恢复得怎么样？\n男：挺好的，再过两天就可以出院了。\n问：从对话中可以知道什么？（B小兰住院了）\n4.  男：办公室那台新复印机（photocopier, máy photo）真不错！\n女：是啊，能自动换纸、自动换页，还能自动装订（đóng bìa）呢！\n问：他们在谈论什么？（C新设备）\n5.  女：明天的活动很重要，你们都不要淘气（nghịch ngợm）啊！\n男：老师，保证一切行动听指挥！\n问：男的是什么意思？（D我们都听您的）\n6.  男：孩子还小呢，你要控制一下自己的情绪，别吓着他。\n女：每次看到他这样，我就气不打一处来。（tức đến nỗi không chịu nổi）\n问：女的怎么了？（C为孩子生气）\n7.  女：大树是通过毛细作用来提水的吗？\n男：以前大家都认为是，但现在人们发现是蒸腾拉力在起作用。\n女：那具体是怎么运转的呢？\n男：这一点到目前还是个谜。\n问：关于蒸腾拉力，下列哪项正确？（C帮助大树提水）\n8.  男：你跟小刘谈恋爱了？\n女：胡说！我根本不认识他。\n男：你就别藏着掖着（giấu giếm）了，我都没说是哪个小刘，你就说不认识。\n女：反正不管哪个小刘，都不是我男朋友。\n问：女的是什么意思？（D他没有谈恋爱）\n9.  女：你这次测验怎么样？\n男：不太好。我觉得很多题都跟课后的补充生词有关，但我以为补充生词不会考，都没复习。\n女：之前老师还特意强调过这一点呢！\n男：其实我听见了，但复习的时间不太够，就没管。\n问：男的为什么考得不太好？（C生词没掌握好）\n10. 男：目前人流量很大，请大家自觉遵守秩序，准备好您的车票，排队进站安检。\n女：您好，我是在网上订的票，没有车票，可以凭身份证进站吗？\n男：可以，如果您需要车票，也可以到那边自动取票机上去取。\n女：好的，谢谢。\n问：男的最可能是什么人？（D车站工作人员）\n11-12.\n仙人掌是一种生命力很强的植物，它们生长在干旱的沙漠地区，为了适应缺水的气候，它们的叶子演化成短短的小刺，以减少水分的蒸发，同时也可以作为防止动物吞食的武器。它们的根非常发达，一旦下雨就会大量吸收水分，从而满足自身的生长需要。\n11．仙人掌最适应下列哪种环境？（A沙漠）\n12．仙人掌的根有什么特征？（D大量吸收水分）\n13-14.\n你见过含羞草吗？它非常奇特：名字叫作草，但它也会开花；说它是一种植物，但它就像动物一样，懂得“害羞”。其实，这是因为它对光、热很敏感，只要我们用手轻轻碰碰它，本来展开的叶子就会向下合起来，要过几分钟后才会重新放开。含羞草原产于热带美洲，现在广泛分布于世界热带和亚热带地区。含羞草的花、叶都具有较好的观赏效果，且较易成活，适宜在阳台盆栽。它还可以止血、止痛，当作药材使用。\n13．人们为什么叫这种植物“含羞草”？（B它很敏感）\n14．关于含羞草，下列哪项正确？（D有药用价值）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html#阅读",
    "href": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html#阅读",
    "title": "HSK5下 | 第35课： 植物会出汗",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n有些患者曾经出现过这样的现象：早上醒来的时候，发现自己身体不受15C控制，头脑似乎是清醒的，但被一些固定内容占据着，无法集中精力，自己不能指挥自己的大脑和身体。这种状态持续一段时间后，或者再次睡着醒来，就会16A恢复正常，只是感觉身体17D特别累。如果这样的情况不是偶尔出现，而是比较多发，可能有两个原因：支配肌肉的神经或者神经肌肉接头处出现问题，导致神经支配肌肉“不灵”；18A 或者是中枢神经系统，也就是大脑的运动神经元损伤。这个时候，应该尽快到专业的医院进行检查。\n19.B人体出汗是为了降低体温\n人体要保持相对稳定的温度，一旦温度上升，大脑就会指挥我们的身体赶快出汗，这时所有汗腺开始工作，汗水就从毛孔里冒了出来。大树也会“出汗”，这“汗”通常是从叶片的气孔里冒出来的，不过，这可不是为了降低体温，而是为了运输养分。\n20.C玻璃管太粗时水无法爬升\n所谓毛细作用，简单来说，就是水会顺着很细很细的管子向上“爬”。我们在家可以用一个比较细的玻璃管体验一下。把细玻璃管插人装有水的杯中，就能发现管内的水会慢慢上升，高于管夕卜，玻璃管越细，水爬升的高度就越高。\n21.D有很多赞美松竹梅的作品\n冬天到了，北风带来了远方的寒流，鹅毛大雪漫天飞舞，仿佛雪花仙子表演天女散花。这时候，其他植物都开始“放寒假”了，可是，还有三种植物仍然在寒风中挺拔着身姿，它们就是松、竹、梅，我们把它们称为“岁寒三友”。在中国的传统文化中，它们象征着坚强、高洁，很多人写诗作画，赞美它们。\n22.C种子存在于植物的果实中\n有很多植物需要靠蝴蝶这样的昆虫来传授花粉，然后才能结出果实。如果没有这样的昆虫，很多植物将会只开花、不结果，也就没有种子寰衍下去。除了蝴蝶外，类似的昆虫还有蜜蜂等。\n23-25.\n现代人承受着巨大的生活工作压力，通过正确的方式缓解疲劳感、保障身心健康，显得十分有必要。下面，我们就给大家介绍缓解疲倦的保健方法以及饮食妙计，希望对忙碌的上班族有所帮助。\n如果有轻微的脑疲劳现象，不必过分紧张，应该放松身心，做到劳逸结合。这时候，适量地做一些脑部运动，比如轻轻拍打头部、搓搓耳朵，就可赶走疲劳。睡觉可不是能消除疲劳的最好方法，而应该适当做一些体育运动，如打打球、做做操等强度不大的有氧运动，当大脑的氧气供应充足时，疲劳会自然消失。还有常喝荼、多晒太阳，也对缓解疲劳有好处。秋季干燥，多洗澡多揉搓身体，可以使人精神焕发。每天洗脚时用手按麾脚心处，也有解除疲劳的效果。让牙齿多活动，相互瞌一瞌，既可以保持牙齿健康，也能让人放松。经常梳理头发可以扩张皮下毛细血管，促进新陈代谢，保持头脑清醒。\n最后给大家介绍一道增强呼吸系统功能、抗疲劳的天门冬萝卜汤：将天门冬15克切成2~3毫米的片，用水约2杯，以中火煎煮15分钟，用布过滤，留汁备用；火腿150克切成长条形薄片；萝卜300克切丝。锅内放鸡汤500毫升，火腿下锅，煮沸后将萝卜丝放入，并将天门冬汁加入，盖锅煮沸后，加盐，再略煮片刻即可。食前加葱花胡椒粉等调味。\n23. A梳头发\n24. C脑疲劳是大脑缺氧的一种反应\n25. D煮\n26-28.\n树木作为现代高尔夫球场设计中必不可少的一项元索，其地位并非生而有之。事实上，在很长一段时间里，它是作为反面教材被严格禁止的_以“美国高尔夫球场设计之父”查尔斯-麦克唐纳为首的老一代设计师认为，树木在球场的存在会触发球员击球的不公平性。然而，也正是在美国这片新大陆上，树木开始被大量引用到球场设计中去，更多的美闰设计师认为，只有当环境条件元法让树木生长的时候，才能成为其不存在于球场的理由。那么，这两种观点是如何形成的?我们在从事球场设计时，又该如何运用树木这种设主十元素呢?\n在高尔夫这项运动的演变过程中，苏格兰作为其诞生地，有着得天独厚的地理条件一这里有其他地方没有的林克斯场地，即海边草原沙地。这是长年累月泥沙通过河流冲积、堆积而成的海滨滩涂地。由于当地的气候条件和常年受大风的影响，这些场地中并没有树木，当然，这里也并不适合树木的生长。\n基于苏格兰独特的地理背景，当时的设计师认为大自然就是最好的高尔夫球场，即海滩林克斯这种类型的球场才是最好的球场。著名的设计师唐纳德’罗斯因此提出了这样一种观点：“上帝创造了高尔夫球涧，设计师的任务只是发现它们。”因此，早期的高尔夫球场就是大自然的杰作，也被称为“无树球场”，设计师只是在其间指定出球场的特征区域，如发球台、果岭等，然后便开展竟技运动。\n由于欧洲传统文化与思想偏于保守，因此，虽然时过境迁，但这种传统的理念和古典的手法却一直延续至今。这既是自然地理风貌使然，也是传统保守思想在高尔夫设计领域的延伸一它虽然己成为了一种风格，但却并不代表着这样的球场设计是完美的。诞生于这种设计理念的球场，其背景显得过于单调。虽然有许多人意识到了这些弊端，但一味效仿欧洲传统的设计理念却总在束缚着球场设计的进一步发展，因此也才有了以后的设计颤币仓哑新的设计演变，才有了球场景观因树木的加人而带来的丰富变化。\n26.B反对\n27.C美国设计师\n28.C应该寻求新变化"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html#书写",
    "href": "学汉语的日记/HSK5下-第35课-植物会出汗/index.html#书写",
    "title": "HSK5下 | 第35课： 植物会出汗",
    "section": "2.3. 书写",
    "text": "2.3. 书写\n测验、特殊、秩序、歇、恢复\n上周五，我的公司正在进行年度绩效测验。办公楼里，员工们专注地坐在各自的工位上，整个楼层保持着井然有序的氛围。突然，刺耳的警报声打破了宁静，所有人都被指示立即歇下手头的工作，迅速撤离大楼。原来是一次预先安排的特殊消防演习。当警报解除后，员工们秩序地返回各自的工位，逐渐恢复正常工作节奏。这次突发的演习不仅检验了公司的应急反应能力，也考验了员工们在压力下的临场应变能力。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第33课-以堵治堵-缓解交通有妙招/index.html",
    "href": "学汉语的日记/HSK5下-第33课-以堵治堵-缓解交通有妙招/index.html",
    "title": "HSK5下 | 第33课： 以堵治堵-缓解交通有妙招",
    "section": "",
    "text": "以堵治堵缓解交通有妙招 | Treating congestion with congestion - A smart way to relieve traffic burden\n城市汽车的数量迅速增长，最初还被视为是社会发展、经济繁荣的体现。但很快人们就发现了问题。随着车流量的增加，道路变得格外拥挤，堵车在大城市中已经成了家常便饭。解决交通拥堵的问题就要减少单位面积道路内的汽车数量，新建或加宽道路被公认为最基本的方法。但事实证明这只是我们美好的主观愿望，道路扩建的速度远远跟不上车流量增加的速度，面积的增加并未使道路空出空间来，甚至还会无形之中鼓励更多司机开车上路，使得市中心的道路更加拥挤。\n那么，如何根治交通拥堵呢？这里我们不妨听听佩.詹森的故事。\n詹森一到欧洲环境保护署交通部工作，就接到了研究如何解决城市拥堵问题的任务。于是，他开始展开调查，研究收集上来的数据，归纳问题特点，并虚心咨询了有关专家。\n九月中旬的一天早晨，詹森照常提前出门赶在早高峰之前去交通部。他看到一个健身的人慢跑通过一个有过街天桥的路口时，为图省事没上天桥，而是横穿马路。结果，他被一辆车撞倒在地，虽然最后他只是受了点轻伤，而且有保险可以赔偿，但司机还是被吓得不轻。\n不过这件事倒是给了詹森启发：开车出行是为了省时省力，但如果情况相反呢？他决定要改变市民出行的观念，反其道而行之——让城市先堵起来，给司机制造麻烦，以堵治堵。经过多次努力，政府批准了他提出的改革措施，比如，增设红绿灯，让车辆不得不走走停停；在主要十字路口取消地下通道，让行人从地下重返地面；\n在购物广场、商务大厦的附近不建停车场等措施。同时，大力发展公共交通。半年过去了，虽然市民们有些抱怨，但效果非常明显，自愿放弃开私家车出门的人越来越多。这也难怪，与其堵在路上浪费时间和汽油，污染环境，倒不如改成公交出行。这样一来，道路拥堵大为缓解。\n城市本是为人而建，如今却被汽车占有，詹森的目标很明确，就是期待能够解放城市，使之更适合人类生活。\n\n\n\n\n\n\n\n\n\nPhoto credit to shutterstock\n\n\n\n\n\n\n\n\n以堵治堵-缓解交通有妙招\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n缓解\nhuǎn jiě - v. to alleviate, to ease up, to relieve\n\n\n2\n招（儿）\nzhāo (er) - n. trick, move, method\n\n\n3\n繁荣\nfán róng - adj. prosperous, thriving\n\n\n4\n体现\ntǐ xiàn - v. to manifest, to reflect\n\n\n5\n拥挤\nyōng jǐ - adj. crowded, congested\n\n\n6\n家常\njiācháng - n. daily life of a family\n\n\n7\n面积\nmiàn jī - n. area, space\n\n\n8\n宽\nkuān - adj. wide, broad\n\n\n9\n主观\nzhǔguān - adj. subjective (based on personal thoughts, feelings etc.)\n\n\n10\n扩（大）\nkuò (dà) - v. to enlarge, to expand, to broaden\n\n\n11\n根治\ngēnzhì - v. to cure once and for all\n\n\n12\n不妨\nbù fáng - adv. might as well\n\n\n13\n展开\nzhǎn kāi - v. to launch, to set off, to carry out\n\n\n14\n归纳\nguī nà - v. to infer, to sum up\n\n\n15\n虚心\nxū xīn - adj. open-minded, modest\n\n\n16\n咨询\nzī xún - v. to consult, to seek advice\n\n\n17\n中旬\nzhōngxún - n. mid ten days of a month\n\n\n18\n照常\nzhàocháng - adv. as usual\n\n\n19\n健身\njiàn shēn - v. to keep fit, to work out\n\n\n20\n图\ntú - v. to covet, to be after\n\n\n21\n受伤\nshòu shāng - v. to be hurt, to be injured\n\n\n22\n保险\nbǎoxiǎn - n. insurance\n\n\n23\n赔偿\npéi cháng - v. to compensate, to indemnify\n\n\n24\n政府\nzhèng fǔ - n. government\n\n\n25\n批准\npī zhǔn - v. to ratify, to approve\n\n\n26\n改革\ngǎi gé - v. to reform\n\n\n27\n取消\nqǔ xiāo - v. to cancel, to call off\n\n\n28\n行人\nxíng rén - n. pedestrian\n\n\n29\n广场\nguǎng chǎng - n. square, plaza\n\n\n30\n商务\nshāngwù - n. business affairs\n\n\n31\n大厦\ndàshà - n. large building, mansion\n\n\n32\n自愿\nzì yuàn - v. to volunteer (to do sth.)\n\n\n33\n难怪\nnán guài - v. to be understandable, to be reasonable\n\n\n34\n与其\nyǔ qí - conj. rather than, (与其A,不如B means “rather than A, better to do B”)\n\n\n35\n汽油\nqì yóu - n. gasoline\n\n\n36\n明确\nmíng què - adj. clear and definite, explicit\n\n\n37\n期待\nqīdài - v. to look forward to\n\n\n38\n解放\njiěfàng - v. to liberate, to free\n\n\n\n\n\n\n1. 照常\n“照常” , động từ, có nghĩa là giống như bình thường (跟平常一样). Ví dụ:\n\n虽然战争临近，但这里的日常生活，一切照常。\n大火对东区的商业活动没有造成大的影响，区内商业活动照常。\n\n“照常” ，còn có thể làm phó từ, biểu thị tình huống tiếp tục không thay đổi. Ví dụ:\n\n在东方广场的迎新活动照常举行。\n九月中旬的一天早晨，詹森照常提前出门赶在早高峰之前去交通部。\n\n（1）刘老师从北京回来后，会照常上课，他从机场直接去学校。 （2）A:王医生，请问半年后我能不能在照常运动吗？ B:一般来说，手术后三个月就可以参加一般的运动了。\n（3）A:快过年了，不知道胡同口的那家超市春节还开不开？ B:我听说那里春节还照常营业。\n2. 难怪\n“难怪”, động từ, có nghĩa là “不应当批评或抱怨” （không nên phê bình, oán trách）, mang ngữ khí thông cảm, tha thứ. Ví dụ:\n\n这也难怪，他每天那么忙，哪儿有时间操心孩子的事啊！\n这也难怪，与其堵在路上浪费时间和汽油，污染环境，倒不如改乘公交出行。\n\n“难怪”, còn là phó từ, biểu thị đã rõ nguyên nhân, không còn cảm thấy lạ lùng với tình huống nào đó nữa. Ví dụ:\n\n你的抽屉真乱，难怪总是找不到东西。\n他都18岁了，还不敢一个人走夜路，难怪大家都叫他胆小鬼。\n\n（1）这也难怪，你们第一次到这个地方还不熟悉道路，还走错。 （2）你一点儿也不理解她的想法，难怪会议中你始终强烈地反对他的意见。 （3）A:李岩和我是小葱学是小学同学，我们认识快20年了。 B:难怪这次再见他你那么高兴。\n3. 与其\nliên từ, khi so sánh 2 tình huống cần phải lựa chọn, “与其” sử dụng về mặt từ bỏ, đằng sau có thể kết hợp dùng với “不如”, “宁可”. Ví dụ:\n\n与其说是采访，不如说是向他学习。\n与其找个不认真的小时工，我宁可自己打扫。\n这也难怪，与其堵在路上浪费时间和汽油，污染环境，倒不如改乘公交出行。\n\n（1）那些赶时髦的消费者，与其说是追求合适的衣服，不如说是买牌子。 （2）与其自为我没有能力，我宁可相信是他运气好。 （3）A:我们在这儿等公交车吧，下站就是物美超市。 B:这么近，与其浪费时间等公交车，不如现在走到那边看看吧。\n\nPhân biệt 表现 và 体现\n\n共同点： Đều là động từ, đều có nghĩa là hiển thị ra\n\n如：这部电影表现/体现出鲜明的时代特点。\n\n不同点：\n\n\n\n\n\n\n\n\n\n表现\n体现\n\n\n\n\n1\nThiên về phản ánh một phong cách, tình cảm, thái độ… nào đó của người hoặc sự vật.\n如：他总是乐呵呵的，对什么事都表现得很乐观。\nNhấn mạnh hiện tượng, tính chất hoặc tư tưởng, tinh thần… nào đó thông qua người hoặc sự vật cụ thể biểu hiện ra.\n如：不同文化的差异在语言特别是词语上体现得最突出。\n\n\n2\nCòn có ý nghĩa là cố ý chứng tỏ ưu điểm, điểm mạnh của bản thân. Thường mang nghĩa xấu.\n如：为了得到领导的欣赏，他拼命地表现自己。\nKhông có ý nghĩa này.\n\n\n3\nCòn có thể làm danh từ, chỉ trạng thái của hành động lời nói.\n如：我们对你的表现很满意，你下周一能来上班吗?\nKhông có ý nghĩa này.。\n\n\n\n\n\n\n问题：交通\n\n交通\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n卡车\nka3che1 - truck\n\n\n2\n列车\nlie4che1, train, tàu hỏa\n\n\n3\n摩头车\nmo2toche1, xe máy, motocycle\n\n\n4\n行人\nxing2ren2, người đi bộ, pedestrian\n\n\n5\n车厢\nche1xiang1, khoang tàu, carriage\n\n\n6\n车库\nche1ku4, garage, gara\n\n\n7\n拐弯\nguai3wan1, turn, corner, quẹo, rẽ\n\n\n8\n绕\nrao4, to travel around an obstacle, to detour, quay xung quanh\n\n\n9\n长途\nchang2tu2, đường dài, long-distance\n\n\n10\n运输\nyun4shu1, vận tải, to transport\n\n\n11\n汽油\nqi4you2, oil, xăng dầu\n\n\n12\n罚款\nfa2kuan3, tiền phạt, fine (monetary penalty)\n\n\n\n\n\n\n绿色出行、从我做起\n绿色出行，不仅是一种生活方式，更是一份对环境的责任。每个人的选择虽小，却能汇聚成改变世界的力量。步行短途、骑行代替开车、选择公共交通工具，都是我们力所能及的行动。这不仅能减少碳排放，改善空气质量，还能锻炼身体，缓解交通压力。让我们从今天开始，从身边小事做起，为建设美丽家园贡献自己的一份力量。绿色出行，让城市更美好，让生活更健康。\n\n\n\n\n你们国家道路交通情况怎么样？\n\n越南的道路交通情况比较复杂。在胡志明市等大城市,交通拥堵是常态,特别是在早晚高峰时段。摩托车是最主要的交通工具,数量庞大,占据了大部分道路。汽车数量也在快速增长,但道路基础设施跟不上,加剧了拥堵。交通规则执行不严,很多人闯红灯、逆行等,增加了安全隐患。近年来政府在努力改善交通状况,修建新道路、地铁等,但效果还不明显。总的来说,越南的交通状况亟待改善,需要政府和民众共同努力。\n\n你平时出行一般采用何种方式？你有私家车吗？使用情况如何？\n\n作为一个典型的越南人,我平时出行主要靠摩托车。每天骑着摩托车穿梭在繁忙的街道上,虽然有些危险,但这是最方便快捷的方式。我没有私家车,主要是因为价格昂贵,而且停车也是个大问题。除了摩托车,我偶尔也会乘坐公交车或打车,但频率不高。在短距离出行时,我会选择步行,既环保又健康。总的来说,摩托车仍是我最依赖的交通工具,它灵活方便,能够轻松应对城市的交通状况。\n\n你认为造成交通拥堵现象的主要原因是什么？应该如何解决？\n\n造成交通拥堵的主要原因有几个方面。首先是私家车和摩托车数量激增,远远超过了道路承载能力。其次是公共交通系统不够发达,大多数人只能依赖私人交通工具。再者是城市规划不合理,导致交通流量分布不均。解决这些问题需要多管齐下:大力发展公共交通,如地铁和公交专用道;限制私家车使用,如征收拥堵费;改善城市规划,分散交通流量;加强交通管理,严格执法。同时,也需要提高公众意识,鼓励绿色出行。只有政府和民众共同努力,才能逐步缓解交通拥堵问题。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第33课-以堵治堵-缓解交通有妙招/index.html#生词",
    "href": "学汉语的日记/HSK5下-第33课-以堵治堵-缓解交通有妙招/index.html#生词",
    "title": "HSK5下 | 第33课： 以堵治堵-缓解交通有妙招",
    "section": "",
    "text": "以堵治堵-缓解交通有妙招\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n缓解\nhuǎn jiě - v. to alleviate, to ease up, to relieve\n\n\n2\n招（儿）\nzhāo (er) - n. trick, move, method\n\n\n3\n繁荣\nfán róng - adj. prosperous, thriving\n\n\n4\n体现\ntǐ xiàn - v. to manifest, to reflect\n\n\n5\n拥挤\nyōng jǐ - adj. crowded, congested\n\n\n6\n家常\njiācháng - n. daily life of a family\n\n\n7\n面积\nmiàn jī - n. area, space\n\n\n8\n宽\nkuān - adj. wide, broad\n\n\n9\n主观\nzhǔguān - adj. subjective (based on personal thoughts, feelings etc.)\n\n\n10\n扩（大）\nkuò (dà) - v. to enlarge, to expand, to broaden\n\n\n11\n根治\ngēnzhì - v. to cure once and for all\n\n\n12\n不妨\nbù fáng - adv. might as well\n\n\n13\n展开\nzhǎn kāi - v. to launch, to set off, to carry out\n\n\n14\n归纳\nguī nà - v. to infer, to sum up\n\n\n15\n虚心\nxū xīn - adj. open-minded, modest\n\n\n16\n咨询\nzī xún - v. to consult, to seek advice\n\n\n17\n中旬\nzhōngxún - n. mid ten days of a month\n\n\n18\n照常\nzhàocháng - adv. as usual\n\n\n19\n健身\njiàn shēn - v. to keep fit, to work out\n\n\n20\n图\ntú - v. to covet, to be after\n\n\n21\n受伤\nshòu shāng - v. to be hurt, to be injured\n\n\n22\n保险\nbǎoxiǎn - n. insurance\n\n\n23\n赔偿\npéi cháng - v. to compensate, to indemnify\n\n\n24\n政府\nzhèng fǔ - n. government\n\n\n25\n批准\npī zhǔn - v. to ratify, to approve\n\n\n26\n改革\ngǎi gé - v. to reform\n\n\n27\n取消\nqǔ xiāo - v. to cancel, to call off\n\n\n28\n行人\nxíng rén - n. pedestrian\n\n\n29\n广场\nguǎng chǎng - n. square, plaza\n\n\n30\n商务\nshāngwù - n. business affairs\n\n\n31\n大厦\ndàshà - n. large building, mansion\n\n\n32\n自愿\nzì yuàn - v. to volunteer (to do sth.)\n\n\n33\n难怪\nnán guài - v. to be understandable, to be reasonable\n\n\n34\n与其\nyǔ qí - conj. rather than, (与其A,不如B means “rather than A, better to do B”)\n\n\n35\n汽油\nqì yóu - n. gasoline\n\n\n36\n明确\nmíng què - adj. clear and definite, explicit\n\n\n37\n期待\nqīdài - v. to look forward to\n\n\n38\n解放\njiěfàng - v. to liberate, to free"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第33课-以堵治堵-缓解交通有妙招/index.html#注释",
    "href": "学汉语的日记/HSK5下-第33课-以堵治堵-缓解交通有妙招/index.html#注释",
    "title": "HSK5下 | 第33课： 以堵治堵-缓解交通有妙招",
    "section": "",
    "text": "1. 照常\n“照常” , động từ, có nghĩa là giống như bình thường (跟平常一样). Ví dụ:\n\n虽然战争临近，但这里的日常生活，一切照常。\n大火对东区的商业活动没有造成大的影响，区内商业活动照常。\n\n“照常” ，còn có thể làm phó từ, biểu thị tình huống tiếp tục không thay đổi. Ví dụ:\n\n在东方广场的迎新活动照常举行。\n九月中旬的一天早晨，詹森照常提前出门赶在早高峰之前去交通部。\n\n（1）刘老师从北京回来后，会照常上课，他从机场直接去学校。 （2）A:王医生，请问半年后我能不能在照常运动吗？ B:一般来说，手术后三个月就可以参加一般的运动了。\n（3）A:快过年了，不知道胡同口的那家超市春节还开不开？ B:我听说那里春节还照常营业。\n2. 难怪\n“难怪”, động từ, có nghĩa là “不应当批评或抱怨” （không nên phê bình, oán trách）, mang ngữ khí thông cảm, tha thứ. Ví dụ:\n\n这也难怪，他每天那么忙，哪儿有时间操心孩子的事啊！\n这也难怪，与其堵在路上浪费时间和汽油，污染环境，倒不如改乘公交出行。\n\n“难怪”, còn là phó từ, biểu thị đã rõ nguyên nhân, không còn cảm thấy lạ lùng với tình huống nào đó nữa. Ví dụ:\n\n你的抽屉真乱，难怪总是找不到东西。\n他都18岁了，还不敢一个人走夜路，难怪大家都叫他胆小鬼。\n\n（1）这也难怪，你们第一次到这个地方还不熟悉道路，还走错。 （2）你一点儿也不理解她的想法，难怪会议中你始终强烈地反对他的意见。 （3）A:李岩和我是小葱学是小学同学，我们认识快20年了。 B:难怪这次再见他你那么高兴。\n3. 与其\nliên từ, khi so sánh 2 tình huống cần phải lựa chọn, “与其” sử dụng về mặt từ bỏ, đằng sau có thể kết hợp dùng với “不如”, “宁可”. Ví dụ:\n\n与其说是采访，不如说是向他学习。\n与其找个不认真的小时工，我宁可自己打扫。\n这也难怪，与其堵在路上浪费时间和汽油，污染环境，倒不如改乘公交出行。\n\n（1）那些赶时髦的消费者，与其说是追求合适的衣服，不如说是买牌子。 （2）与其自为我没有能力，我宁可相信是他运气好。 （3）A:我们在这儿等公交车吧，下站就是物美超市。 B:这么近，与其浪费时间等公交车，不如现在走到那边看看吧。\n\nPhân biệt 表现 và 体现\n\n共同点： Đều là động từ, đều có nghĩa là hiển thị ra\n\n如：这部电影表现/体现出鲜明的时代特点。\n\n不同点：\n\n\n\n\n\n\n\n\n\n表现\n体现\n\n\n\n\n1\nThiên về phản ánh một phong cách, tình cảm, thái độ… nào đó của người hoặc sự vật.\n如：他总是乐呵呵的，对什么事都表现得很乐观。\nNhấn mạnh hiện tượng, tính chất hoặc tư tưởng, tinh thần… nào đó thông qua người hoặc sự vật cụ thể biểu hiện ra.\n如：不同文化的差异在语言特别是词语上体现得最突出。\n\n\n2\nCòn có ý nghĩa là cố ý chứng tỏ ưu điểm, điểm mạnh của bản thân. Thường mang nghĩa xấu.\n如：为了得到领导的欣赏，他拼命地表现自己。\nKhông có ý nghĩa này.\n\n\n3\nCòn có thể làm danh từ, chỉ trạng thái của hành động lời nói.\n如：我们对你的表现很满意，你下周一能来上班吗?\nKhông có ý nghĩa này.。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第33课-以堵治堵-缓解交通有妙招/index.html#扩展",
    "href": "学汉语的日记/HSK5下-第33课-以堵治堵-缓解交通有妙招/index.html#扩展",
    "title": "HSK5下 | 第33课： 以堵治堵-缓解交通有妙招",
    "section": "",
    "text": "问题：交通\n\n交通\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n卡车\nka3che1 - truck\n\n\n2\n列车\nlie4che1, train, tàu hỏa\n\n\n3\n摩头车\nmo2toche1, xe máy, motocycle\n\n\n4\n行人\nxing2ren2, người đi bộ, pedestrian\n\n\n5\n车厢\nche1xiang1, khoang tàu, carriage\n\n\n6\n车库\nche1ku4, garage, gara\n\n\n7\n拐弯\nguai3wan1, turn, corner, quẹo, rẽ\n\n\n8\n绕\nrao4, to travel around an obstacle, to detour, quay xung quanh\n\n\n9\n长途\nchang2tu2, đường dài, long-distance\n\n\n10\n运输\nyun4shu1, vận tải, to transport\n\n\n11\n汽油\nqi4you2, oil, xăng dầu\n\n\n12\n罚款\nfa2kuan3, tiền phạt, fine (monetary penalty)"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第33课-以堵治堵-缓解交通有妙招/index.html#运用",
    "href": "学汉语的日记/HSK5下-第33课-以堵治堵-缓解交通有妙招/index.html#运用",
    "title": "HSK5下 | 第33课： 以堵治堵-缓解交通有妙招",
    "section": "",
    "text": "绿色出行、从我做起\n绿色出行，不仅是一种生活方式，更是一份对环境的责任。每个人的选择虽小，却能汇聚成改变世界的力量。步行短途、骑行代替开车、选择公共交通工具，都是我们力所能及的行动。这不仅能减少碳排放，改善空气质量，还能锻炼身体，缓解交通压力。让我们从今天开始，从身边小事做起，为建设美丽家园贡献自己的一份力量。绿色出行，让城市更美好，让生活更健康。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第33课-以堵治堵-缓解交通有妙招/index.html#口语",
    "href": "学汉语的日记/HSK5下-第33课-以堵治堵-缓解交通有妙招/index.html#口语",
    "title": "HSK5下 | 第33课： 以堵治堵-缓解交通有妙招",
    "section": "",
    "text": "你们国家道路交通情况怎么样？\n\n越南的道路交通情况比较复杂。在胡志明市等大城市,交通拥堵是常态,特别是在早晚高峰时段。摩托车是最主要的交通工具,数量庞大,占据了大部分道路。汽车数量也在快速增长,但道路基础设施跟不上,加剧了拥堵。交通规则执行不严,很多人闯红灯、逆行等,增加了安全隐患。近年来政府在努力改善交通状况,修建新道路、地铁等,但效果还不明显。总的来说,越南的交通状况亟待改善,需要政府和民众共同努力。\n\n你平时出行一般采用何种方式？你有私家车吗？使用情况如何？\n\n作为一个典型的越南人,我平时出行主要靠摩托车。每天骑着摩托车穿梭在繁忙的街道上,虽然有些危险,但这是最方便快捷的方式。我没有私家车,主要是因为价格昂贵,而且停车也是个大问题。除了摩托车,我偶尔也会乘坐公交车或打车,但频率不高。在短距离出行时,我会选择步行,既环保又健康。总的来说,摩托车仍是我最依赖的交通工具,它灵活方便,能够轻松应对城市的交通状况。\n\n你认为造成交通拥堵现象的主要原因是什么？应该如何解决？\n\n造成交通拥堵的主要原因有几个方面。首先是私家车和摩托车数量激增,远远超过了道路承载能力。其次是公共交通系统不够发达,大多数人只能依赖私人交通工具。再者是城市规划不合理,导致交通流量分布不均。解决这些问题需要多管齐下:大力发展公共交通,如地铁和公交专用道;限制私家车使用,如征收拥堵费;改善城市规划,分散交通流量;加强交通管理,严格执法。同时,也需要提高公众意识,鼓励绿色出行。只有政府和民众共同努力,才能逐步缓解交通拥堵问题。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第33课-以堵治堵-缓解交通有妙招/index.html#听力",
    "href": "学汉语的日记/HSK5下-第33课-以堵治堵-缓解交通有妙招/index.html#听力",
    "title": "HSK5下 | 第33课： 以堵治堵-缓解交通有妙招",
    "section": "2.1. 听力",
    "text": "2.1. 听力\n1.  女：我们得快一点儿了，要不然赶不上2号线的末班车了。Chuyến xe cuối cùng\n男：别着急，拐过前边的路口就到地铁进站口了。guai3 - quẹo\n问：女的为什么着急？（C怕错过末班车）\n2.  男：我觉得他们配合得还不太熟练，歌词记得也不过关。Phối hợp không quá nhuần nhuyễn, thuần thục\n女：他们这一组是上个月刚组成的，时间比较短。\n问：男的认为他们怎么样？（A配合不理想）\n3.  女：先生，对不起！飞机马上就要降落了，现在卫生间暂停使用了。hạ cánh, 反义词：起飞\n男：好吧，谢谢。\n问：女的提醒男的什么事？（B不能上厕所）\n4.  女：师傅，我好像记错路了，刚才那个路口应该向左拐。\n男：好的，我到下一个路口再调头回来。\n问：男的接下来打算做什么？（B调头回去） quay đầu (đối với phương tiện giao thông)\n5.  男：你好，请问南航在哪儿办理登机牌？Chinese Southern Airline, Boarding pass\n女：您走过了，南航的服务台在H区，您往回走。\n问：说话人现在最可能在哪儿？（C机场）\n6.  女：糟糕，行李箱的钥匙怎么没了？我记得就放口袋里了呀。Tiêu rồi! kou3dai4 túi áo\n男：别着急，好好想想，你一般不是都放在随身的小背包里吗？Túi nhỏ đeo bên người\n问：女的怎么了？（C钥匙找不到了）\n7.  女：你跟卖电视的售货员咨询了吗？\n男：问过了，不过现在的新技术、新名词我也听不太懂。\n女：那你打算怎么办？\n男：依我看，功能越简单越好，没必要赶时髦。Đuổi theo xu thế, thời đại\n问：关于买电视，男的想怎么办？（A买功能简单的）\n8.  男：你坐地铁到丰联广场，在十字路口西北角有个蓝天商务大厦。\n女：然后呢？\n男：你从行人的地下通道过来，大厦楼下有个咖啡馆，我在那儿等你。\n女：好，我马上过来。\n问：女的接下来应该做什么？（A去乘地铁）\n9.  女：这两天怎么没开车？\n男：之前路上和人碰了一下，不严重，对方负全部责任。\n女：保险公司的赔偿手续都办好了吗？\n男：处理好了，车也修得差不多了。\n问：关于男的，可以知道什么？（D保险已赔偿）\n10. 男：刘老师，下周晚上的文化选修课有变动吗？\n女：怎么想起问我这个问题？\n男：下周不是要期中考试了嘛，晚上的课还上吗？Thi giữa kì, thi cuối kì là 期末考试\n女：这个没有影响，文化课照常。\n问：关于文化课，可以知道什么？（B继续上课）\n11-12.\n男：老奶奶，检票了，把您的车票给我看一下。Kiểm vé\n女：列车员同志，请问到西安要几个小时啊？Đồng chí\n男：老奶奶，我们这趟车是沈阳到上海的，不到西安。Thẩm Dương tới Thượng Hải\n女：不对，你看我的车票是北京到西安的啊。\n男：您的票是到西安的，这没错，可您上错车了。\n女：那怎么办，难道就连司机也没发现他开的方向不对吗？\n11．从对话中可以知道这趟列车是去哪儿的？（D上海）\n12．关于那位老奶奶，可以知道什么？（B做错了列车）\n13-14.\n国外一家旅游公司的调查数据显示，赴（fu4 - to go）欧洲的中国游客，46%的人仅尝过一次欧洲食物，10%的人从未吃过一次。而那唯一一次欧洲食物还是旅行社提供的，比如在法国巴黎吃海鲜、在德国吃火腿、在意大利吃意大利面，其余餐饮都是“中国式”，很多人干脆留在酒店房间吃自带的方便面。\n不过，他们分析说，这不是中国人不吃欧洲食物，只是不习惯而已，何况是在舟车劳顿（một hành trình mệt mỏi）的旅程中；另一方面，了解欧洲饮食文化并不是中国游客计划的出游项目，不能让他们兴奋，他们更关心游览著名景点。\n13．关于去欧洲旅游的中国游客，从调查数据中可以知道什么？（C大多是吃中餐）\n14．根据这段话，中国游客去欧洲最喜欢做的是下面哪项？（A游览景点）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第33课-以堵治堵-缓解交通有妙招/index.html#阅读",
    "href": "学汉语的日记/HSK5下-第33课-以堵治堵-缓解交通有妙招/index.html#阅读",
    "title": "HSK5下 | 第33课： 以堵治堵-缓解交通有妙招",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n父子俩住山上，每天都要赶牛车下山卖柴。老父亲比较有经验，负责驾车，山路不宽，弯道很多，儿子眼神比较好，总是在要转弯时提醒道:“爹，拐弯啦!”\n有一次父亲受伤了，儿子独自一人15D照常赶车下山。到了弯道，牛怎么也不肯转弯，16C儿子拥尽各种方法，下车又推又拉，用青草引它，牛就是一动不动。\n到底是怎么回事?儿子百思不得其解。最后只有一招儿了，他前后看看路上没有行人，就17B靠近牛的耳朵大声叫道：“爹，拐弯啦!” （die1 - cha, ）\n牛应声而动。\n牛用条件反射的方式活着，而人往往依靠习惯生活。一个成功的人懂得如何18A赔养好的习惯，用它们代替坏习惯，当好的习惯积累多了，自然会有一个好的人生。记住，人和人之间的差别并不是很大，优秀不过是一种习惯。\n19．A学习是为了自己\n研究学问、学习技能，应该是为充实自己，千万不能为了投他人所好，迎合别人的意愿，或随时代潮流而盲目地进行，否则达不成目的事小，白白浪费了宝贵的时光才最可惜。\n20．B孤独症与交通污染可能有关\n科研人员对300多名孤独症（austism, tự kỉ）患儿及约260名没有患孤独症的儿童进行了一项凋查，结果发现，环境因素或许是孤独症的一个致病原因，那些居住地离交通主干道大约300米的儿童患孤独症的风险较大。而离交通主干道近意味着受到噪音尾气污染的风险较大，因此研究人员认为，交通污染可能会导致孤独症。\n21．A语言交流是有技巧的\n语言真是奇妙的东西，人和人之间之所以能够沟通、交流和表达，就是因为人是能够使用语言的动物，但是，人和人之间的误解、误会和欺骗也是由语言所导致的。因此，人与人之间语言的表达、沟通和理解也就成了一门学问，特别是现在人际交往的频壑和密切已到了空前的地步。\n22．A这份名录为世人熟知\n两千多年前，许多旅行者记载下了他们旅途中的所见所闻。随着时间的推移，其中的七个建筑成为了众所周知的“古代世界七大奇迹”，之所以定为七种，是因为这份名录是希腊人制作的，而他们认为“7”是个有魔力的数字。\n23-25\n有一个人经常出差，经常买不到对号人座的车票。可是无论长途短途，无论车上多挤，他总能找到座位。\n他的办法其实很简单，就是耐心地一节车厢一节车厢找过去。这个办法听上去似乎并不高明，但却很管用。每次，他都做好了从第一节车厢走到最后一节车厢的准备，可是每次他都用不着走到最后就会发现空位。他说，这是因为像他这样镍而不舍找座位的乘客实在不多。经常是在他落座的车厢里尚余若干座位，而其他车厢的过道和车厢接头处居然人满为患。他说，大多数乘客轻易就被一两节车厢拥挤的表面现象欺骗了，不大细想在数十次停靠之中，从火车十几个车门上上下下的流动中藏着不少出现座位的机遇;即使想到了，他们也没有那一份寻找的耐心。眼前一方小小立足之地很容易让大多数人满足，为了一两个座位背负着行李挤来挤去有些人也觉得不值。他们还担心万一找不到座位，回头连个好好站着的地方也没有了。与生活中一些安于现状、不思进取害怕失败的人永远只能滞留在没有成功的起点上一样，这些不愿主动找座位的乘客大多只能在上车时最初的落脚之处一直站到下车。\n自信、执着、富有远见、勤于实践，会让你握有一张人生之旅永远的坐票。\n23. D耐心地一直找过去\n24. B站在最初上车的地方\n25. C坚持才能把握住机会\n26-28\n一天，机场的工作人员迎来了一位很特别的女乘客­­——位身体巨胖的女乘客。当这位女乘客出现在他们面前的时候，他们几乎全被惊呆了。对他们之中的大多数人来说，这或许是他们有生以来见过的最胖的女子——这位女乘客胖得几乎连走路都困难了，她是被亲人用轮椅推进候机大厅的。\n于是，问题迎面而来了。因为当大家为她办理海关、安检等手续时，猛然发现所有的门对她来说都显得实在太窄了，她根本就无法通过这一扇扇门，顺利地办好手续登机。\n工作人员不得不将这个难题交给了领导，向上级报告和请示。你一定会想，遐到这种事情，换作任何一个领导也无可奈何，他们能做的或许就是道歉、退票了。但是令大家吃惊的是，领导居然做了一个令人惊讶的决定­­­——拆门!\n领导当即决定迅速抽凋工程人员组织一个“拆门小队”，他们一定要为这名乘客拆开一扇大门，让她顺利登机。因为他们不能滞留任何一名乘客，让乘客抱憾而归。就这样，这位肥胖的女乘客一路经过拆开的大门，来到了飞机跟前。\n但是此时问题又出现了:她无法一步步顺着扶梯登机。于是机场的领导再次做出决定——用机场行李升降机把她送到舱门口。还好，这位女乘客的身体刚好能进入舱门。最后机组人员全部出动，将她安顿在了整整三个位置上。\n最终，他们把这位满怀感动的女乘客送到了她的目的地。\n无论是对那些身体有缺陷的人，还是健康的人，我们都要有一颗尊重之心。尊重别人，可能会牺牲自己眼前的一点儿私利，但却能换来长远的利益。每给予一份尊重，自我价值也会随之提升。\n26. D深爱感动\n27. C用升降机登机\n28. D每个人都应受尊重"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第33课-以堵治堵-缓解交通有妙招/index.html#书写",
    "href": "学汉语的日记/HSK5下-第33课-以堵治堵-缓解交通有妙招/index.html#书写",
    "title": "HSK5下 | 第33课： 以堵治堵-缓解交通有妙招",
    "section": "2.3. 书写",
    "text": "2.3. 书写\n交通、行人、拥挤、缓解、政府\n随着城市化进程加快，交通拥堵日益严重，行人通行困难成为普遍现象。拥挤的街道不仅影响出行效率，还增加了安全隐患。为了缓解这一问题，政府需要采取多管齐下的策略：优化公共交通网络，鼓励绿色出行；合理规划城市布局，增设人行天桥和地下通道；加强交通管理，严格执法。同时，提高市民意识也至关重要。只有政府和公众共同努力，才能创造出安全、畅通的城市环境，提升居民生活的质量。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第31课-登门槛效应/index.html",
    "href": "学汉语的日记/HSK5下-第31课-登门槛效应/index.html",
    "title": "HSK5下 | 第31课： 登门槛效应",
    "section": "",
    "text": "登门槛效应 - Foot-in-the-door effect\n一个朋友在报社当编辑，一天他去请假，他先问领导：“您今天心情好吗？”领导说：“怎么了？”朋友答：“嗯，如果您心情好，我就说件事；心情不好就改天再说。”领导有了兴趣，假，就这样轻易的请好了。不得不说，这位朋友很 会利用“登门槛效应”来处理问题。\n心理学家曾做过“登门槛技术”的现场试验他们派人到两个社区，劝人们在屋前立一块“小心驾驶”的圆形标志。在第一个社区，研究人员直接向人们提出要求，结果很多人表示拒绝，接受率仅为17%。在第二个社区，研究人员把同样的事情分成两个步骤：先向大家出示一份赞成安全驾驶的请愿书，请求他们在上面签字，几周后再提出立牌要求，这次接受者竟然达到了55%。\n第一个步骤的签字是很容易的，几乎所有人都照做了，大家可能都没意识到，这个小小的“登门槛行为”对接下来的决定产生了重要影响。日常生活中也是这样。当你想要求某人做某件较大的事情，又担心对方不愿意做时，可以先向他/她提出做一件同类型的、比较容易的事。\n比如你想与一个女孩谈恋爱，如果一开始就迫切地提出要跟她约会，女孩可能会犹豫，甚至表现得很冷淡；如果你说“饭总是要吃的吧，一起吃饭吧”，她答应了，那接下来是去看电影还是泡酒吧都无所谓了。\n你想让同事帮你值班或写报告什么的，直接说八成儿会被拒绝，把事情模糊化，“能不能帮个小忙”，“不会占用你很多时间”，台阶一铺，事情就容易多了……\n你可能会说，这些小动作会让人觉得你很狡猾。但是不得不承认，有了这些小动作的帮助，别人的确更愿意接受你的请求。有时候，当你自己认为了不起时，别人通常觉得你这人不过如此；可是当你放低身段时，会缩短与人的距离，别人并不会看不起你，反而会觉得你为人谦虚。实践证明，第二种人得到的总是比第一种人更多。\n\n\n\n\n\n\n\n\n\nPhoto credit to businessweekly\n\n\n\n\n\n\n\n\n登门槛效应\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n门槛\nménkǎn - n. threshold\n\n\n2\n报社\nbào shè - n. newspaper office, headquarters of a newspaper\n\n\n3\n编辑\nbiān jí - n. editor\n\n\n4\n嗯\nǸg - int. used to indicate positive response\n\n\n5\n轻易\nqīng yì - adj. easy, effortless\n\n\n6\n处理\nchǔ lǐ - v. to handle, to deal with\n\n\n7\n社区\nshè qū - n. community\n\n\n8\n劝\nquàn - v. to try to persuade\n\n\n9\n圆\nyuán - adj. round, circular\n\n\n10\n标志\nbiāo zhì - n. sign, mark\n\n\n11\n出示\nchūshì - v. to show, to produce\n\n\n12\n赞成\nzànchéng - v. to agree with, to approve of\n\n\n13\n请愿书\nqǐngyuàn shū - n. petition\n\n\n14\n恋爱\nliàn ài - n. (romantic) love\n\n\n15\n迫切\npò qiè - adj. urgent, pressing, eager\n\n\n16\n犹豫\nyóu yù - adj. hesitant\n\n\n17\n冷淡\nlěng dàn - adj. cold, indifferent\n\n\n18\n无所谓\nwú suǒ wèi - v. to not care, to not mind, to not take seriously\n\n\n19\n值班\nzhíbān - v. to be on duty or shift\n\n\n20\n报告\nbào gào - n. report\n\n\n21\n八成（儿）\nbāchéng (er) - adv. most probably, most likely; 80%\n\n\n22\n模糊\nmó hú - adj. blurred, indistinct, vague\n\n\n23\n狡猾\njiǎo huá - adj. crafty, cunning, sly\n\n\n24\n了不起\nliǎo bu qǐ - adj. great, amazing\n\n\n25\n身段\nshēnduàn - n. posture, manner, attitude\n\n\n26\n缩短\nsuō duǎn - v. to shorten\n\n\n27\n看不起\nkàn bu qǐ - v. to look down upon, to despise\n\n\n28\n谦虚\nqiān xū - adj. modest\n\n\n29\n实践\nshí jiàn - v. to put into practice\n\n\n\n\n\n\n\n嗯\n\n嗯 - 叹词/thán từ\n“嗯” (đọc là ńg), biểu thị nghi vấn. Ví dụ:\n\n嗯？不是28号，难道是我记错了？\n嗯？人都去哪了？\n\n“嗯” (đọc là ňg), biểu thị cảm thấy bất ngờ hoặc cho rằng không nên như vậy. Ví dụ:\n\n嗯！你的房间为什么这么冷？\n嗯！你怎么还没走啊？\n\n“嗯” (đọc là ǹg), biểu thị đáp ứng hoặc chấp thuận. Ví dụ:\n\n嗯，如果您心情好，我就说件事；心情不好就改天再说。\n嗯，没问题，我这就给他送去。\n\n\n轻易\n\n“轻易” tính từ, có nghĩa là “简单容易”(đơn giản, dễ dàng) .Thông thường làm trạng ngữ. Ví dụ:\n\n领导有了兴趣，假，就这样轻易地请好了。\n任何胜利都不是轻易得到的，背后都要付出艰苦的努力。\n\n“轻易” ,còn có thể làm phó từ, biểu thị thái độ khi xử lí công việc không cẩn thận, rất tùy tiện. Thường dùng trong câu phủ định tạo thành cách thức “轻易不·······” biểu thị ý nghĩa “ 很少（做·······）”（rất ít làm）.Ví dụ:\n\n他这个人的特点，是从不轻易决定，也不轻易转变。\n他为人好强，轻易不求人，这次向咱们借钱，一定是遇到什么难事了。\n\nBài tập:\na) 这条路改为单姓线后，路人就能惊异地过了。\nb) A: 把，天这么热，我们怎么不开空调啊？\nB: 为了节电节钱，我轻易不用空调。\nc) A: 现在骗子越来越多了，这个星期我都接到两个骗人的电话了。\nB: 你要小心点，轻易不照做陌生人的要求。\n\nPhân biệt 轻易 và 容易：\n\n共同点： Khi làm trạng ngữ, đều biểu thị làm không tốn công, khó khăn.\n不同点：\n\n\n\n\n\n\n\n\n\n轻易\n容易\n\n\n\n\n1\nThiên về hành sự thoải mái, không tốn sức lực. Thông thường làm trạng ngữ.\n如：她从小学习就好，高考时很轻易地考上了名牌大学，接着又读了研究生。\nNgoài biểu thị sự việc rất đơn giản không khó làm, còn biểu thị nội dung sự việc không phức tạp. Có thể độc lập làm vị ngữ.\n如：今天的考试特别容易，我半个小时就答完了。\n\n\n2\nKhông có ý nghĩa này.\nCòn biểu thị khả năng xảy ra sự thay đổi nào đó\n如：他最近心情不好，容易发脾气\n\n\n3\nCòn là phó từ, biểu thị tùy tiện.\n如：我爱书，无论走到哪里，我从不轻易放过书摊、书店。\nKhông có cách dùng này.\n\n\n\n\n\n\n问题：行为1\n\n行为1\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n推辞\ntui1ci4 - to decline politely - từ chối một cách lịch sự\n\n\n2\n议论\nyi4lun4 - discussion - nghị luận\n\n\n3\n转告\nzhuan3gao4 - to pass on a message\n\n\n4\n祝福\nzhu4fu2 - to bless - chúc phúc\n\n\n5\n行为\nxing2wei2 - behavior - hành vi\n\n\n6\n握手\nwo4shou3 - to shake hands - bắt tay\n\n\n7\n看望\nkan4wang4 - to visit (in hospital) - thăm\n\n\n8\n问候\nwen2hou4 - to greet - chào hỏi\n\n\n9\n处理\nchu3li3 - to handle - xử lí\n\n\n10\n恭喜\ngong1xi3 - to congratulate - cung hỉ, chúc mừng\n\n\n11\n宣布\nxuan1bu4 - to announce - thông báo\n\n\n12\n信任\nxin4ren4 - to trust - tin tưởng\n\n\n13\n配合\npei4he2 - to cooperate - phối hợp\n\n\n14\n当心\ndang1xin1 - to watch out - cẩn thận\n\n\n\n\n\n\n我该不该接受/拒绝呢\n我认为，在生活中，我们应该学会根据自身情况合理地接受或拒绝别人的要求，既要尊重他人，也要维护自己的权益。\n当别人要求我做某事时，我会顺序考虑以下因素：（1）这在我的能力范围内吗？；（2）这对我和他们的关系有什么价值，或者是否实现了我们的共同目标？；（3）这将如何影响我目前正在做的事情或占用我多长时间？。如果遇到一个“不该接受”的答案，我就立刻拒绝。当然，有时这些问题很难立即清楚地回答。但我会尽量不会影响自己，也不会因为接收后做事不到位而让别人失望。\n总之，生活中我们会面临着各种各样的请求，是否接受或拒绝这些请求需要根据具体情况进行判断。\n🚀Bao nhiêu thơi gian: 多长时间，Làm đến nơi đến chốn: 做到位。\n\n\n\n1. 你觉得帮助别人对你有影响吗\n我特别重视通过相互帮助来培养与周围人的关系。对我来说，帮助别人总是有好处的，不仅会建立他们对我的信任-这可能会在未来带来很多机会，还可以帮助我学习新东西。举个例子：当我接受为销售部门做一份综合报告时，我对他们的流程、接近客户的思维有了更多的见解，非常有趣。他们对我的信任程度也更高；当我答应帮一个朋友照顾小狗时，虽然一开始有点儿困难，但我终于学会了如何照顾宠物，并结交了一个新朋友🐶。最后，能帮助别人总让我很幸福。\n当然，我不会接受所有别人的请求。当我觉得要求超出我的能力或其他人表现出依赖的表现时，我会拒绝他们。我宁愿让别人在拒绝时轻轻地失望，也不愿在我接受但没有完成任务时感到沮丧。总之，我只会以创造积极的价值为前提来考虑是否接受别人的请求。\n2. 介绍你拒绝别人的一次经历？你觉得自己的处理怎么样\n在大学里，有一次我们不得不在几周内写一篇分析文章。一个朋友因为工作忙，怕来不及完成，就请我帮忙处理数据。我在这方面上很擅长，所以我很乐意帮助他迈出最初几步。当然，我也会向他解释如何做到这一点。 但接下来，他越来越依赖地邀请我帮助他进行后面的分析步骤。我开始感到不对劲。他似乎对自己要做的工作不感兴趣，完全依靠我。我觉得自己的工作也没有得到尊重，所以中途拒绝继续帮助他。我还对他澄清我的观点，尽量不影响我们的关系。\n那么我就不被登门槛效应牵着鼻子走。我相信我做得对，他应该对自己的结果负责，而学生们成绩的公平也应该被保持。牵着鼻子走 qian1zhebi2zi4zou3, to lead by the nose từ lóng dắt mũi 🐂，口语可能用。in formal: 指导，引导，dẫn dắt\n4. 在与人交往中，你处理“接受与拒绝”这类事情的原则是什么\n《运用相同》"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第31课-登门槛效应/index.html#生词",
    "href": "学汉语的日记/HSK5下-第31课-登门槛效应/index.html#生词",
    "title": "HSK5下 | 第31课： 登门槛效应",
    "section": "",
    "text": "登门槛效应\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n门槛\nménkǎn - n. threshold\n\n\n2\n报社\nbào shè - n. newspaper office, headquarters of a newspaper\n\n\n3\n编辑\nbiān jí - n. editor\n\n\n4\n嗯\nǸg - int. used to indicate positive response\n\n\n5\n轻易\nqīng yì - adj. easy, effortless\n\n\n6\n处理\nchǔ lǐ - v. to handle, to deal with\n\n\n7\n社区\nshè qū - n. community\n\n\n8\n劝\nquàn - v. to try to persuade\n\n\n9\n圆\nyuán - adj. round, circular\n\n\n10\n标志\nbiāo zhì - n. sign, mark\n\n\n11\n出示\nchūshì - v. to show, to produce\n\n\n12\n赞成\nzànchéng - v. to agree with, to approve of\n\n\n13\n请愿书\nqǐngyuàn shū - n. petition\n\n\n14\n恋爱\nliàn ài - n. (romantic) love\n\n\n15\n迫切\npò qiè - adj. urgent, pressing, eager\n\n\n16\n犹豫\nyóu yù - adj. hesitant\n\n\n17\n冷淡\nlěng dàn - adj. cold, indifferent\n\n\n18\n无所谓\nwú suǒ wèi - v. to not care, to not mind, to not take seriously\n\n\n19\n值班\nzhíbān - v. to be on duty or shift\n\n\n20\n报告\nbào gào - n. report\n\n\n21\n八成（儿）\nbāchéng (er) - adv. most probably, most likely; 80%\n\n\n22\n模糊\nmó hú - adj. blurred, indistinct, vague\n\n\n23\n狡猾\njiǎo huá - adj. crafty, cunning, sly\n\n\n24\n了不起\nliǎo bu qǐ - adj. great, amazing\n\n\n25\n身段\nshēnduàn - n. posture, manner, attitude\n\n\n26\n缩短\nsuō duǎn - v. to shorten\n\n\n27\n看不起\nkàn bu qǐ - v. to look down upon, to despise\n\n\n28\n谦虚\nqiān xū - adj. modest\n\n\n29\n实践\nshí jiàn - v. to put into practice"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第31课-登门槛效应/index.html#注释",
    "href": "学汉语的日记/HSK5下-第31课-登门槛效应/index.html#注释",
    "title": "HSK5下 | 第31课： 登门槛效应",
    "section": "",
    "text": "嗯\n\n嗯 - 叹词/thán từ\n“嗯” (đọc là ńg), biểu thị nghi vấn. Ví dụ:\n\n嗯？不是28号，难道是我记错了？\n嗯？人都去哪了？\n\n“嗯” (đọc là ňg), biểu thị cảm thấy bất ngờ hoặc cho rằng không nên như vậy. Ví dụ:\n\n嗯！你的房间为什么这么冷？\n嗯！你怎么还没走啊？\n\n“嗯” (đọc là ǹg), biểu thị đáp ứng hoặc chấp thuận. Ví dụ:\n\n嗯，如果您心情好，我就说件事；心情不好就改天再说。\n嗯，没问题，我这就给他送去。\n\n\n轻易\n\n“轻易” tính từ, có nghĩa là “简单容易”(đơn giản, dễ dàng) .Thông thường làm trạng ngữ. Ví dụ:\n\n领导有了兴趣，假，就这样轻易地请好了。\n任何胜利都不是轻易得到的，背后都要付出艰苦的努力。\n\n“轻易” ,còn có thể làm phó từ, biểu thị thái độ khi xử lí công việc không cẩn thận, rất tùy tiện. Thường dùng trong câu phủ định tạo thành cách thức “轻易不·······” biểu thị ý nghĩa “ 很少（做·······）”（rất ít làm）.Ví dụ:\n\n他这个人的特点，是从不轻易决定，也不轻易转变。\n他为人好强，轻易不求人，这次向咱们借钱，一定是遇到什么难事了。\n\nBài tập:\na) 这条路改为单姓线后，路人就能惊异地过了。\nb) A: 把，天这么热，我们怎么不开空调啊？\nB: 为了节电节钱，我轻易不用空调。\nc) A: 现在骗子越来越多了，这个星期我都接到两个骗人的电话了。\nB: 你要小心点，轻易不照做陌生人的要求。\n\nPhân biệt 轻易 và 容易：\n\n共同点： Khi làm trạng ngữ, đều biểu thị làm không tốn công, khó khăn.\n不同点：\n\n\n\n\n\n\n\n\n\n轻易\n容易\n\n\n\n\n1\nThiên về hành sự thoải mái, không tốn sức lực. Thông thường làm trạng ngữ.\n如：她从小学习就好，高考时很轻易地考上了名牌大学，接着又读了研究生。\nNgoài biểu thị sự việc rất đơn giản không khó làm, còn biểu thị nội dung sự việc không phức tạp. Có thể độc lập làm vị ngữ.\n如：今天的考试特别容易，我半个小时就答完了。\n\n\n2\nKhông có ý nghĩa này.\nCòn biểu thị khả năng xảy ra sự thay đổi nào đó\n如：他最近心情不好，容易发脾气\n\n\n3\nCòn là phó từ, biểu thị tùy tiện.\n如：我爱书，无论走到哪里，我从不轻易放过书摊、书店。\nKhông có cách dùng này."
  },
  {
    "objectID": "学汉语的日记/HSK5下-第31课-登门槛效应/index.html#扩展",
    "href": "学汉语的日记/HSK5下-第31课-登门槛效应/index.html#扩展",
    "title": "HSK5下 | 第31课： 登门槛效应",
    "section": "",
    "text": "问题：行为1\n\n行为1\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n推辞\ntui1ci4 - to decline politely - từ chối một cách lịch sự\n\n\n2\n议论\nyi4lun4 - discussion - nghị luận\n\n\n3\n转告\nzhuan3gao4 - to pass on a message\n\n\n4\n祝福\nzhu4fu2 - to bless - chúc phúc\n\n\n5\n行为\nxing2wei2 - behavior - hành vi\n\n\n6\n握手\nwo4shou3 - to shake hands - bắt tay\n\n\n7\n看望\nkan4wang4 - to visit (in hospital) - thăm\n\n\n8\n问候\nwen2hou4 - to greet - chào hỏi\n\n\n9\n处理\nchu3li3 - to handle - xử lí\n\n\n10\n恭喜\ngong1xi3 - to congratulate - cung hỉ, chúc mừng\n\n\n11\n宣布\nxuan1bu4 - to announce - thông báo\n\n\n12\n信任\nxin4ren4 - to trust - tin tưởng\n\n\n13\n配合\npei4he2 - to cooperate - phối hợp\n\n\n14\n当心\ndang1xin1 - to watch out - cẩn thận"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第31课-登门槛效应/index.html#运用",
    "href": "学汉语的日记/HSK5下-第31课-登门槛效应/index.html#运用",
    "title": "HSK5下 | 第31课： 登门槛效应",
    "section": "",
    "text": "我该不该接受/拒绝呢\n我认为，在生活中，我们应该学会根据自身情况合理地接受或拒绝别人的要求，既要尊重他人，也要维护自己的权益。\n当别人要求我做某事时，我会顺序考虑以下因素：（1）这在我的能力范围内吗？；（2）这对我和他们的关系有什么价值，或者是否实现了我们的共同目标？；（3）这将如何影响我目前正在做的事情或占用我多长时间？。如果遇到一个“不该接受”的答案，我就立刻拒绝。当然，有时这些问题很难立即清楚地回答。但我会尽量不会影响自己，也不会因为接收后做事不到位而让别人失望。\n总之，生活中我们会面临着各种各样的请求，是否接受或拒绝这些请求需要根据具体情况进行判断。\n🚀Bao nhiêu thơi gian: 多长时间，Làm đến nơi đến chốn: 做到位。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第31课-登门槛效应/index.html#口语",
    "href": "学汉语的日记/HSK5下-第31课-登门槛效应/index.html#口语",
    "title": "HSK5下 | 第31课： 登门槛效应",
    "section": "",
    "text": "1. 你觉得帮助别人对你有影响吗\n我特别重视通过相互帮助来培养与周围人的关系。对我来说，帮助别人总是有好处的，不仅会建立他们对我的信任-这可能会在未来带来很多机会，还可以帮助我学习新东西。举个例子：当我接受为销售部门做一份综合报告时，我对他们的流程、接近客户的思维有了更多的见解，非常有趣。他们对我的信任程度也更高；当我答应帮一个朋友照顾小狗时，虽然一开始有点儿困难，但我终于学会了如何照顾宠物，并结交了一个新朋友🐶。最后，能帮助别人总让我很幸福。\n当然，我不会接受所有别人的请求。当我觉得要求超出我的能力或其他人表现出依赖的表现时，我会拒绝他们。我宁愿让别人在拒绝时轻轻地失望，也不愿在我接受但没有完成任务时感到沮丧。总之，我只会以创造积极的价值为前提来考虑是否接受别人的请求。\n2. 介绍你拒绝别人的一次经历？你觉得自己的处理怎么样\n在大学里，有一次我们不得不在几周内写一篇分析文章。一个朋友因为工作忙，怕来不及完成，就请我帮忙处理数据。我在这方面上很擅长，所以我很乐意帮助他迈出最初几步。当然，我也会向他解释如何做到这一点。 但接下来，他越来越依赖地邀请我帮助他进行后面的分析步骤。我开始感到不对劲。他似乎对自己要做的工作不感兴趣，完全依靠我。我觉得自己的工作也没有得到尊重，所以中途拒绝继续帮助他。我还对他澄清我的观点，尽量不影响我们的关系。\n那么我就不被登门槛效应牵着鼻子走。我相信我做得对，他应该对自己的结果负责，而学生们成绩的公平也应该被保持。牵着鼻子走 qian1zhebi2zi4zou3, to lead by the nose từ lóng dắt mũi 🐂，口语可能用。in formal: 指导，引导，dẫn dắt\n4. 在与人交往中，你处理“接受与拒绝”这类事情的原则是什么\n《运用相同》"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第31课-登门槛效应/index.html#听力",
    "href": "学汉语的日记/HSK5下-第31课-登门槛效应/index.html#听力",
    "title": "HSK5下 | 第31课： 登门槛效应",
    "section": "2.1. 听力",
    "text": "2.1. 听力\n1.  女：刘先生说周三来换饮水机（yin3shui3ji1 - máy lọc nước），问你想要哪个牌子（pai2zi4 - thương hiệu）的？\n男：没关系，好用就成。成 - làm được, 成交 - đạt được thỏa thuận, dealed\n问：男的对这件事的态度怎么样？（B无所谓）\n2.  男：中午碰见（peng4jian4 - bắt gặp）林教授了，他说找你有事。\n女：本来说好了，上午给他送实验报告，结果他临时外出，就没送成。\n问：女的为什么没送成报告？（A林教授不在）\n3.  男：小李，宣传册（ce4 - tờ）是在哪儿印的？有些地方印得比较模糊。\n女：那我马上跟对方联系一下，让他们重新印。\n问：女的打算让对方怎么做？（C重新印制）\n4.  女：别犹豫了，既然房东同意续租（xu4zu1 - gia hạn cho thuê），你又没别的合适的，就再住半年吧。\n男：也是啊，房租涨（zhang3 - tăng）得还能接受，也省得再搬了。cũng đỡ tốn công chuyển đi\n问：在租房问题上，男的是怎么想的？（B不想再搬家）\n5.  男：昨晚你电话一直占线 ，你在给谁打电话呢？占线 - zhan4xian4 - bận (đường dây điện thoại)\n女：我朋友小梅，两口子吵着要离，我在电话里一直在劝（quan4 - khuyên）她要冷静。两口子， hai vợ chồng — cãi nhau vài lời đã đòi ly hôn\n问：小梅怎么了？（D要离婚）\n6.  男：你做的方案会上通过了吗？\n女：我把计划跟他们一说，结果，没一个人赞成。\n问：大家认为这个计划怎么样？（A行不通 không hiệu quả）\n7.  男：是小丽来的电话？怎么打这么久\n女：是你妹妹，又失恋了。你这当哥哥的也不管。\n男：这事我怎么管？你是她嫂子，替我多安慰安慰她。嫂子 sao3zi4, chị dâu; 姐夫 jie3fu1, anh rể; 弟妹 em dâu; 妹夫 em rể\n女：就知道你会这么说。\n问：说话的两个人是什么关系？（B夫妻）\n8.  女：上次拜访（bai4fang3）的那家公司有结果了吗?\n男：联系好几次了，每次约他们刘经理，对我都特冷淡。\n女：开始时都这样，光看资料了解还是不够的。\n男：是的，我争取能再去给他演示一下。演示 - yǎnshì - trình bày, demo\n问：男的接下来想要做什么？（D去演示产品）\n9.  女：谢谢你送我回家，你对这里不太熟吧\n男：你是奇怪这么近的路我怎么开了这么久吗？\n女：是啊，你好像不太认识路似的。\n男：如果我对这儿不熟悉，我怎么能开一个多小时的车，而一次也没经过你家的门口呢？半斤八两 Bànjīnbāliǎng, kẻ tám lạng người nửa cân\n问：从对话中可以知道什么？(B男的故意多绕路) Người Nam cố ý đi lòng vòng\n\n男：你也认识刘京\n\n女：是啊，我们是高中同学，他学文科，我在理科班。\n男：他武术（wu3shu4 - martial）特别棒（bang4）。\n女：没错，上学的时候，他就是武术队的，还得过太极拳比赛的冠军呢。\n问：关于刘京，可以知道什么？（C比赛得过冠军）\n🚀Ngoài lề: 爆发力 bao4fa1li4 sức bùng phát, 耐力 nai4li4 sức bền\n11-12.\n女：先生，您的车票呢？\n男：车票？我没有。\n女：没有？那您要去哪儿？\n男：我哪儿也不去。\n女：那您为什么上这列火车？\n男：我在车站遇到这列火车，听到车上的广播大声叫喊：“请大家赶快上车坐好！于是我不得不走进车厢。” (che1xiang1 - )\n11．关于那位先生，可以知道什么？（D有点儿糊涂）\n12．说话时他们最可能在哪儿？（A车厢里面）\n13-14.\n我已经暗恋（an4lian4 - yêu thầm lặng）她两年了，可是始终没有勇气向她表白（biao3bai2 - express/profess/thú nhận）。在朋友的鼓励下，我终于写了一份充满爱意的字条（zi4tiao2 - brief note）。可是，几次见到她，那只紧握（jin3wo4 - to hold firmly, giữ chặt）字条的手总是无法从口袋里拿出来。就这样，浪费了好几次机会，字条已变得皱皱巴巴。 皱皱巴巴 - zhòu zhou bā bā - Wrinkled and wrinkled, nhăn nheo\n终于有一天，不知是哪儿来的勇气，我一见到她，便把那张皱巴巴的字条塞进她手里，然后慌忙（huang1mang2 - panicky, flurry）逃（tao2）走了。\n第二天，她打来电话，说要跟我见面。我的心情是既兴奋又紧张，昏暗（hun1an4 - lờ mờ）的路灯下我们见面了。她看着紧张不安的我，迫切地问道：“昨天你塞给我一百块钱干吗？”\n13．关于这段话中的“我”，下列哪项正确？（C给女孩儿写了求爱字条）\n14．从这段话中可以知道什么？（B“我”总是没勇气表白）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第31课-登门槛效应/index.html#阅读",
    "href": "学汉语的日记/HSK5下-第31课-登门槛效应/index.html#阅读",
    "title": "HSK5下 | 第31课： 登门槛效应",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n好多人都会犯（fan4 - phạm）这样的15C毛病，心里很想要成功，但是又缺乏勇气而迟迟不敢采取行动。\n小黄性格内向胆小（dan3xiao3 - nhát gan），因此年近39岁仍然单身一人。上个月公司新来了一位女职员，小黄常常找16A借口接近这个姑娘，但每次都面红耳赤，结结巴巴（jie1ba1 - stammeringly, lắp bắp）地聊上几句便走开了。他也曾经想过约这个姑娘看电影或逛街，但是每次都因为没有勇气开口，使约会遥遥无期。面红耳赤 - mian4hong2er3chi4 - flushed with anger, mặt đỏ tới mang tai遥遥无期 - yao2yao2wu2qi4 - far in the indefine future, xa xôi không hẹn\n这一天是“惰（duo4 - lazy）人节”，小黄在妹妹的再三鼓励下，终于害羞（hai4xiu1 - shy, embarassed, thẹn thùng）地关上房门，然后战战兢兢  地给这位女同事打电话，希望能约她共度佳节（jia1jie2 - festival）。战战兢兢 - zhan4jing1 - nơp nớp lo sợ\n妹妹在房外等了好一阵。不久小黄兴奋地从房里冲了出来又跳又叫，妹妹17B迫切地问道：“怎么样？18B她答应啦？”小黄如释重负  地说：“哇（wa1），我好幸运，幸好她不在家。”如释重负 ru2shi4zhong4fu4 - as if relieved from a burden, như trút được gánh nặng\n\n\n\n人际交往，是一个古老而年轻的概念（gai4nian4）。说它古老，是因为自人类产生之日起，人们就开始感知它的存在。一个古代的阿拉伯哲人就曾经形象地描述（Miáoshù - miêu tả）过交往的重要性，他说一个不会交往的人，犹如（you2ru2 - similar, giống như）陆地（lu4di4 - dry land, đất liền）上的船，永远不会漂流（piao1liu2 - trôi）到人生的大海中去。（B交往对人类非常重要）阿拉伯 ala1bo2 - Ả Rập\n\n\n\n成就感 ，不是生命中“额外”的享受，而是保持生命力的“根本”因素。管理者只有在工作上充分满足员工的成就感，才能真正激发（qi1fa1 - to arouse kích thích）并延续（yan2xu4 - to go on kéo dài）员工的工作干劲儿，从而将个体生命力与公司竟争力紧密结合，提高工作效率。（D成就感可提升工作干劲儿）成就感 cheng1jjiu4gan3 - sense of archivement干劲 gan4jin4 enthusiasm for doing something紧密 jin3mi4 chặt chẽ, mật thiết\n\n\n\n荷兰猪其实是老鼠（lao3shu3 - rate）的亲戚（qin1qi - a relative）。也许是因为它们的身体矮矮胖胖（ai3pang4 - short & fat）、圆圆滚滚（yuan2 - circle, gun3 - roll）的，所以，当欧洲人第一次看见这种南美“小毛球”时，就用他们熟悉的农场动物一猪来为其命名。不管荷兰猪的名字究竞因何而来，人们对这种小巧可爱的宠物（chung3wu4 - pet）的喜爱并没有因为它奇怪的名字而受到任何影响。（A荷兰猪原产于南美）荷兰猪 he2lan2zhu1 Guinea Pig因何而来 yin1he2e2lai2 - why did it come from, từ đâu mà đến\n\n\n\n大街小巷和人一样，各有各的名字。每条胡同一形成，人们自然会给它起个名。这个名称一旦被大多数人所接受，叫开了，就成为人们交往、逼信等活动中不可缺少的标志（biao1zhi4 - mark）。在中国从胡同开始形成起，它的名称一直都是靠人们口头相传，至于用文字写在标牌（biao1pai2 - label, tag）上挂在胡同口上，是二十世纪初才开始有的。（B胡同名都是经大众认可的）大街小巷 - da4jie1xiao3xiang4 ngõ ngách lớn nhỏ, everywhere in the city叫开了, ai cũng gọi như thế, quen rồi\n23-25.\n第一次登月的宇航员其实有两位，除了大家熟知的阿姆斯特朗外，还有一位是奥尔德林，当时阿姆斯特朗说过的一句话“我个人的一小步，是全人类的一大步”，早已成为全世界家喻户晓的名言。登月 - deng1yue4, go up to the moon宇航员 yu3hang2yuan2 - astronaut, phi hành gia阿姆斯特朗 a1mu3si1te4lang3 - Armstrong奥尔德林 ao2er3de2lin2 - Aldrin家喻户晓 Jiāyùhùxiǎo understood by everyone, nổi tiếng\n在庆祝（qing4zhu4 - celebrate）登月成功的记者招待会上，一个记者突然问了奥尔德林一个特别的问题：“阿姆斯特朗先下去，成为登上月球的第一人，你会不会觉得有点儿遗憾？”在人们有点儿尴尬（gan1ga4 - awkward）的注视（zhu4shi4 - to gaze at）下，奥尔德林很有风度地回答：“各位，千万别忘了，回到地面时，我可是最先出太空舱（tai4kong4cang1）的。”他环顾四周笑着说：“所以我是由别的星球来到地球的第一人。”大家在笑声中给予他（ji3yu3 - give）最热烈的掌声（zhang3sheng1 - applause）。记者招待会 ji4zhe3 zhao4dai4hui4环顾四周 - huan2gu4si4zhou4 - to look around\n步出太空舱有先后，但相互的支持配合是不分先后的，像这祥的伟大工程，个人的力量有限，必须要依靠群体的力量。成功不必在“我”，团队的成功就是“我”的成功。\n\nA全人类的伟大进步\nC轻松幽默\nC团队配合更重要\n\n26-28.\n某户人家养着一只小狗。有一天，小狗忽然走失了，这户人家马上报了菩，盼望能找回小狗。几天后，小狗被好心人士找到了，并且将它送到警察局，警察立即通知了这家人。在等待主人到来的时候，警察突然发现这只小狗不但没有欢喜的神情，反而悲伤地流下了眼泪。警察相当好奇，低头问小狗：“你走丢了，现在好不容易可以回家，应该高高兴兴的，怎么还流泪呢？”\n小狗回答：“警察先生啊，你有所不知，我是离家出走的!”\n警察吃惊地问道：“你家主人对你不好吗？为什么要离家出走呢？”\n小狗悲伤地说：“我在主人家已经待了好多年，从一开始就负责家人的安全，平时看门，偶尔四处走走，看看有没有陌生人闯入，一直尽忠职守，当然主人也感觉到了，平时见到我会漠摸（mo4 - vuốt ve）我、拍拍（pai1 - vỗ）我，一有假日就会带我出去散散步。那种保卫一家人的成就感，那种受重视、疼爱（teng2ai4）的感觉，让我更加提醒自己要好好照顾这一家人。直到有一天……”闯入 chuang3ru4 - đột nhập尽忠 jin4zhong1 loyal to the end, 职守 zhi2shou3 duty保卫 bao3wei4 guard\n“怎么样？”警察关心地问道。\n“有一天家里请来几个工人，在门口装了，从此我失业了，看门不再是我的职责，家人也不需我保护了，我整天无所事事，对家庭一点儿用都没有。虽然主人还是一样地喂养我，但是我实在受不了这种受冷落的感觉，所以才会离家出走，宁可去过流浪的曰子。”防盗器 fang2dao4 qi4 - Anti-theft device无所事事 wu2suo3shi4shi4 - have nothing to do冷落 leng3luo4 - desolate流浪 liu2lang4 lưu lạc, to wander\n\nA不愿意回家\nC不在重用它了\nC成就感的重要性"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第31课-登门槛效应/index.html#书写",
    "href": "学汉语的日记/HSK5下-第31课-登门槛效应/index.html#书写",
    "title": "HSK5下 | 第31课： 登门槛效应",
    "section": "2.3. 书写",
    "text": "2.3. 书写\n态度、冷淡、赞成、无所谓、犹豫\n刚听到10岁儿子请求允许周末参加志愿清扫街道时，虽然心里已经很赞成儿子的意愿，但妈妈还态度冷淡地问道：“拿着扫帚不稳的你连卧室还没打扫好，更不用说打扫街道了”。儿子毫不犹豫地回答妈妈：“那如果现在我能把卧室打扫干净，你会同意吗？”。妈妈笑着说：“行，星期天你回家自己洗衣服就无所谓了”。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第29课-培养对手/index.html",
    "href": "学汉语的日记/HSK5下-第29课-培养对手/index.html",
    "title": "HSK5下 | 第29课： 培养对手",
    "section": "",
    "text": "培养对手 | Training your rivals\n建伟在大学的一座公寓楼里开了一家书店，顺便卖点儿文具、电池、小日用品等。一年多来，虽然每件商品的利润都并不高，但他诚信经营，薄利多销，使书店生意越来越红火，甚至成为了媒体的采访对象。这个大学里另外还有三家书店，由于受到了建伟书店的影响，这三家书店的经营空间越来越小，三家的营业额加起来还不如他一家高。建伟成了这里的“书店老大”。\n这时，许多亲朋好友便建议他干脆把另三家书店挤垮，垄断这个市场。可建伟不但没有去挤垮对手，反而还经常帮助三家书店搞一些营销活动，对于一家快要倒闭的书店，他还主动热心地借给其资金，想办法让他继续经营下去。\n有人问他：“你怎么这么傻？ 就让他们倒霉，不好吗？！”\n建伟说，我是在保持这一地区图书市场的“生态平衡”。商业领域其实和自然界一样，自然界中的生物，适当有一些“敌人”，会促使它们生长得更好；同样的，对手并不会妨碍我的发展，反而会促进经营，让我获得更多利益。 一个原因是这样能创造让客户有所比较和优中选优的购物环境，通过比较，学生们才知道我的书店服务好、品种优、价格合理。如果只有我一家书店了，学生们没有了比较，价格定得再低也会认为我的书价高，万一他们自己跑到其他图书市场去“货比三家”，那我的生意就完了。 还有一个很重要的原因，就是维持这种书店饱和的“生态”，避免更多、更强的对手来“插足”。我把其他三家都挤垮了，不见得是件好事，因为别人一看这么大的地方只有我一家书店，新的书店可能就会出现，弄不好来一个比我更强的对手。所以，为了保持目前这种经营的“生态平衡”，我要继续把对手培养好。\n改编自《小故事大道理》，作者：宗先哲。\n\n\n\n培养对手\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n培养\npéi yǎng - v. to foster, to train\n\n\n2\n对手\nduì shǒu - n. opponent, rival\n\n\n3\n公寓\ngōngyù - n. apartment, flat\n\n\n4\n文具\nwén jù - n. stationery\n\n\n5\n电池\ndiàn chí - n. battery, cell\n\n\n6\n日用品\nrì yòng pǐn - n. articles of everyday use, daily necessities\n\n\n7\n利润\nlì rùn - n. profit\n\n\n8\n诚信\nchéngxìn - adj. honest, in good faith\n\n\n9\n媒体\nméitǐ - n. media, mass media\n\n\n10\n对象\nduì xiàng - n. target, object\n\n\n11\n营业\nyíng yè - v. to do business, to operate\n\n\n12\n额\né - n. specific number, sum, volume or amount\n\n\n13\n不如\nbùrú - v. to be not as good as, to be inferior to\n\n\n14\n干脆\ngān cuì - adv. simply, just\n\n\n15\n挤\njǐ - v. to squeeze, to push out\n\n\n16\n垮\nkuǎ - v. to collapse, to break down\n\n\n17\n垄断\nlǒng duàn - v. to monopolize\n\n\n18\n倒闭\ndǎo bì - v. to close down, to go bankrupt\n\n\n19\n热心\nrèxīn - adj enthusiastic, earnest\n\n\n20\n资金\nzī jīn - n. capital, fund\n\n\n21\n傻\nshǎ - adj. stupid, foolish\n\n\n22\n倒霉\ndǎoméi - adj. having bad luck, unlucky\n\n\n23\n生态\nshēng tài - n. ecology\n\n\n24\n商业\nshāng yè - n. business, commerce\n\n\n25\n领域\nlǐng yù - n. field, domain, realm\n\n\n26\n适当\nshì dàng - adj. proper, adequate\n\n\n27\n促使\ncù shǐ - v. to urge, to spur, to prompt\n\n\n28\n生长\nshēng zhǎng - v. to grow\n\n\n29\n妨碍\nfáng ài - v. to hinder, to impede\n\n\n30\n促进\ncù jìn - v. to promote, to accelerate\n\n\n31\n利益\nlìyì - n. benefit, interest\n\n\n32\n合理\nhé lǐ - adj. reasonable\n\n\n33\n万一\nwàn yī - conj. in case, if by any chance\n\n\n34\n维持\nwéi chí - v. to keep, to maintain\n\n\n35\n饱和\nbǎo hé - v. to be saturated, to be filled to capacity\n\n\n36\n不见得\nbú jiàn dé - adv. not necessarily, may not\n\n\n\n\n\n\n\n不如\n\n(không bằng) động từ, biểu thị không thể sánh bằng. Ví dụ:\n\n求人不如求己。\n·····由于受到了建伟书店的影响，这三家书店的经营空间越来越小，三家的营业额加起来还不如他一家高。\n如果找一个棋艺不如你或者和你差不多的人下棋，虽然你可能会轻易地战胜对手，但并不能使你的棋艺得到提高。\n\n\n干脆\n\n“干脆，tính từ, hình dung khi nói chuyện, làm việc thẳng thắn, sòng phẳng, không do dự. Ví dụ:\n\n他这人很干脆，说行就行，说不行就不行。\n我求他帮忙，他答应得很干脆。\n\n“干脆” (dứt khoát/ cứ)，cũng có thể làm phó từ, biểu thị đơn giản, quyết đoán. Ví dụ:\n\n我已经试过六次了，还是不行，我看我干脆放弃好了。\n这时，许多新朋好友建议他干脆把另三家书店挤垮，垄断这个市场。\n\n\n万一\n\n“万一” ,liên từ, biểu thị khả năng rất nhỏ,thông thường dùng với trường hợp không ngờ đến hoặc bất lợi. Ví dụ:\n\n·····万一他们自己跑到其他图书市场去“货比三家”，那我的生意就完了。\n不要将所有的鸡蛋都放在一个篮子里，因为万一不小心，鸡蛋就有可能全部打碎。\n\n“万一”, có thể làm danh từ, biểu thị tình huống bất ngờ có khả năng rất nhỏ. Thường dùng trong cách thức cố định “就怕万一”“以防万一”. Ví dụ:\n\n不怕一万，就怕万一。\n她总是带着一把枪，以防万一。\n\n\nPhân biệt 挤 và 拥挤\n\n共同点：Đều có thể làm động từ và tính từ, nghĩa về mặt tính từ đều là địa điểm đấy quá nhỏ so với người và vật.\n\n如：这么小的教室里放三四张桌子，太挤/拥挤了！\n\n不同点：\n\n不同点\n\n\n\n\n\n\n\n\n挤\n拥挤\n\n\n\n\n1\nĐộng từ, nhấn mạnh dùng sức lực của mình để lách ra từ đám đông.\n如：坐车的人太多了，我挤了半天才挤上车。\nĐộng từ, nhấn mạnh chen chúc cùng một chỗ.\n如：请先下后上，不要拥挤\n\n\n2\nThông thường làm vị ngữ.\n如：为了买到票，我挤得满头大汗。\nCó thể làm chủ ngữ hoặc tân ngữ.\n如：交通拥挤是个大问题。\n\n\n3\nĐộng từ ( vặn, bóp, nắn), chỉ dùng sức làm cho đồ vật từ trong lỗ nhỏ hoặc khe nhỏ ra ngoài.\n如：牙膏用完了，已经挤不出来了。\nKhông có ý nghĩa này.\n\n\n4\nĐộng từ, chỉ người hoặc vật chen chúc , dồn lại sát một chỗ, hoặc sự việc tập trung/dồn lại trong cùng một lúc.\n如果你的生活先被不重要的事挤满了，那你就无法再装进更大、更重要的事了。\nKhông có ý nghĩa này.\n\n\n5\nCó ý nghĩa gạt bỏ, lật đổ, loại trừ.\n如：许多新朋好友建议他干脆把另三家书店挤垮，垄断这个市场。\nKhông có ý nghĩa này.\n\n\n\n\n\n\n\n经济1\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n发票\nfa1piao4 - hóa đơn, invoice\n\n\n2\n收据\nshou1ju4 - receipt, biên nhận\n\n\n3\n支票\nzhi1piao4 - check, cheque, séc\n\n\n4\n欠\nqian4 - owe, nợ ai đó\n\n\n5\n税\nshui4 - tax, thuế\n\n\n6\n市场\nshi4chang3 - thị trường, market\n\n\n7\n执照\nzhi2zhao4 - giấy phép, license\n\n\n8\n柜台\ngui4tai2 - counter\n\n\n9\n商品\nshang1pin3 - thương phẩm, commodity\n\n\n10\n优惠\nyou1hui4 - to discount\n\n\n11\n讨价还价\ntao3jia4-huan2jia4, to bargain, mặc cả\n\n\n12\n兑换\ndui4huan4 - to exchange, trao đổi\n\n\n13\n投资\ntou2zi1 - đầu tư, to invest\n\n\n14\n分配\nfei1pei4 - to distribute, phân bổ\n\n\n\n\n\n\n如果我是建伟。\n如果我是建伟，我也跟他一样做——在采取行动时始终考虑长期利益。把那三家书店挤垮虽然会带来暂时的好处。但从长远来看，这行动会造成不利的后果。首先，我书店的生意在于薄利多销，比其他书店有价格合理的优势。如果在这市场内没有对手的话，客户便没有的比较，那竞争优势就无用了。其次，一旦市场的平衡被打破，就会吸引新的竞争对手进入这所学校，说不定会比我强。那时我后悔就晚了。最后，竞争对手是我努力更好地服务客户的动力，我为什么要挤垮他们？\n\n\n\n1.  课文中建伟面对不如自己的竞争对手和亲朋好友的建议，她是怎么做的？\n当许多亲朋好友建议她干脆把另三家书店对手挤垮，垄断者学校里的市场，建伟可不但没有去挤垮对手，反而还经常帮助三家书店搞一些营销活动。对于一家要快倒闭的书店，建伟还主动热心地借给其资金，想办法让他继续经营下去。\n2.  你同意他的做法吗？你认为他的说法有没有道理？\n我完全同意他的做法。我也认为商业中的竞争关系就像自然界中的生存竞争。自然界中的生物，适当有一些“敌人”，会促使它们生长的更好；同样的对手并不会妨碍建伟的生意发展，反而会促进经营，让他获得更多利益。主要的原因就是对手们能保持让客户有所比较和优中选优的购物环境。通过比较，学生们才知道建伟的书店服务号、品种优、价格合理。另一个也很重要的原因就是建伟要保持这市场得“生态平衡”，避免更多、更强的对手来参加市场。他对培养对手的说法对我来说很有道理。\n3.  如果你是建伟，你会怎么做？\n课本运用"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第29课-培养对手/index.html#生词",
    "href": "学汉语的日记/HSK5下-第29课-培养对手/index.html#生词",
    "title": "HSK5下 | 第29课： 培养对手",
    "section": "",
    "text": "培养对手\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n培养\npéi yǎng - v. to foster, to train\n\n\n2\n对手\nduì shǒu - n. opponent, rival\n\n\n3\n公寓\ngōngyù - n. apartment, flat\n\n\n4\n文具\nwén jù - n. stationery\n\n\n5\n电池\ndiàn chí - n. battery, cell\n\n\n6\n日用品\nrì yòng pǐn - n. articles of everyday use, daily necessities\n\n\n7\n利润\nlì rùn - n. profit\n\n\n8\n诚信\nchéngxìn - adj. honest, in good faith\n\n\n9\n媒体\nméitǐ - n. media, mass media\n\n\n10\n对象\nduì xiàng - n. target, object\n\n\n11\n营业\nyíng yè - v. to do business, to operate\n\n\n12\n额\né - n. specific number, sum, volume or amount\n\n\n13\n不如\nbùrú - v. to be not as good as, to be inferior to\n\n\n14\n干脆\ngān cuì - adv. simply, just\n\n\n15\n挤\njǐ - v. to squeeze, to push out\n\n\n16\n垮\nkuǎ - v. to collapse, to break down\n\n\n17\n垄断\nlǒng duàn - v. to monopolize\n\n\n18\n倒闭\ndǎo bì - v. to close down, to go bankrupt\n\n\n19\n热心\nrèxīn - adj enthusiastic, earnest\n\n\n20\n资金\nzī jīn - n. capital, fund\n\n\n21\n傻\nshǎ - adj. stupid, foolish\n\n\n22\n倒霉\ndǎoméi - adj. having bad luck, unlucky\n\n\n23\n生态\nshēng tài - n. ecology\n\n\n24\n商业\nshāng yè - n. business, commerce\n\n\n25\n领域\nlǐng yù - n. field, domain, realm\n\n\n26\n适当\nshì dàng - adj. proper, adequate\n\n\n27\n促使\ncù shǐ - v. to urge, to spur, to prompt\n\n\n28\n生长\nshēng zhǎng - v. to grow\n\n\n29\n妨碍\nfáng ài - v. to hinder, to impede\n\n\n30\n促进\ncù jìn - v. to promote, to accelerate\n\n\n31\n利益\nlìyì - n. benefit, interest\n\n\n32\n合理\nhé lǐ - adj. reasonable\n\n\n33\n万一\nwàn yī - conj. in case, if by any chance\n\n\n34\n维持\nwéi chí - v. to keep, to maintain\n\n\n35\n饱和\nbǎo hé - v. to be saturated, to be filled to capacity\n\n\n36\n不见得\nbú jiàn dé - adv. not necessarily, may not"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第29课-培养对手/index.html#注释",
    "href": "学汉语的日记/HSK5下-第29课-培养对手/index.html#注释",
    "title": "HSK5下 | 第29课： 培养对手",
    "section": "",
    "text": "不如\n\n(không bằng) động từ, biểu thị không thể sánh bằng. Ví dụ:\n\n求人不如求己。\n·····由于受到了建伟书店的影响，这三家书店的经营空间越来越小，三家的营业额加起来还不如他一家高。\n如果找一个棋艺不如你或者和你差不多的人下棋，虽然你可能会轻易地战胜对手，但并不能使你的棋艺得到提高。\n\n\n干脆\n\n“干脆，tính từ, hình dung khi nói chuyện, làm việc thẳng thắn, sòng phẳng, không do dự. Ví dụ:\n\n他这人很干脆，说行就行，说不行就不行。\n我求他帮忙，他答应得很干脆。\n\n“干脆” (dứt khoát/ cứ)，cũng có thể làm phó từ, biểu thị đơn giản, quyết đoán. Ví dụ:\n\n我已经试过六次了，还是不行，我看我干脆放弃好了。\n这时，许多新朋好友建议他干脆把另三家书店挤垮，垄断这个市场。\n\n\n万一\n\n“万一” ,liên từ, biểu thị khả năng rất nhỏ,thông thường dùng với trường hợp không ngờ đến hoặc bất lợi. Ví dụ:\n\n·····万一他们自己跑到其他图书市场去“货比三家”，那我的生意就完了。\n不要将所有的鸡蛋都放在一个篮子里，因为万一不小心，鸡蛋就有可能全部打碎。\n\n“万一”, có thể làm danh từ, biểu thị tình huống bất ngờ có khả năng rất nhỏ. Thường dùng trong cách thức cố định “就怕万一”“以防万一”. Ví dụ:\n\n不怕一万，就怕万一。\n她总是带着一把枪，以防万一。\n\n\nPhân biệt 挤 và 拥挤\n\n共同点：Đều có thể làm động từ và tính từ, nghĩa về mặt tính từ đều là địa điểm đấy quá nhỏ so với người và vật.\n\n如：这么小的教室里放三四张桌子，太挤/拥挤了！\n\n不同点：\n\n不同点\n\n\n\n\n\n\n\n\n挤\n拥挤\n\n\n\n\n1\nĐộng từ, nhấn mạnh dùng sức lực của mình để lách ra từ đám đông.\n如：坐车的人太多了，我挤了半天才挤上车。\nĐộng từ, nhấn mạnh chen chúc cùng một chỗ.\n如：请先下后上，不要拥挤\n\n\n2\nThông thường làm vị ngữ.\n如：为了买到票，我挤得满头大汗。\nCó thể làm chủ ngữ hoặc tân ngữ.\n如：交通拥挤是个大问题。\n\n\n3\nĐộng từ ( vặn, bóp, nắn), chỉ dùng sức làm cho đồ vật từ trong lỗ nhỏ hoặc khe nhỏ ra ngoài.\n如：牙膏用完了，已经挤不出来了。\nKhông có ý nghĩa này.\n\n\n4\nĐộng từ, chỉ người hoặc vật chen chúc , dồn lại sát một chỗ, hoặc sự việc tập trung/dồn lại trong cùng một lúc.\n如果你的生活先被不重要的事挤满了，那你就无法再装进更大、更重要的事了。\nKhông có ý nghĩa này.\n\n\n5\nCó ý nghĩa gạt bỏ, lật đổ, loại trừ.\n如：许多新朋好友建议他干脆把另三家书店挤垮，垄断这个市场。\nKhông có ý nghĩa này."
  },
  {
    "objectID": "学汉语的日记/HSK5下-第29课-培养对手/index.html#扩展",
    "href": "学汉语的日记/HSK5下-第29课-培养对手/index.html#扩展",
    "title": "HSK5下 | 第29课： 培养对手",
    "section": "",
    "text": "经济1\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n发票\nfa1piao4 - hóa đơn, invoice\n\n\n2\n收据\nshou1ju4 - receipt, biên nhận\n\n\n3\n支票\nzhi1piao4 - check, cheque, séc\n\n\n4\n欠\nqian4 - owe, nợ ai đó\n\n\n5\n税\nshui4 - tax, thuế\n\n\n6\n市场\nshi4chang3 - thị trường, market\n\n\n7\n执照\nzhi2zhao4 - giấy phép, license\n\n\n8\n柜台\ngui4tai2 - counter\n\n\n9\n商品\nshang1pin3 - thương phẩm, commodity\n\n\n10\n优惠\nyou1hui4 - to discount\n\n\n11\n讨价还价\ntao3jia4-huan2jia4, to bargain, mặc cả\n\n\n12\n兑换\ndui4huan4 - to exchange, trao đổi\n\n\n13\n投资\ntou2zi1 - đầu tư, to invest\n\n\n14\n分配\nfei1pei4 - to distribute, phân bổ"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第29课-培养对手/index.html#运用",
    "href": "学汉语的日记/HSK5下-第29课-培养对手/index.html#运用",
    "title": "HSK5下 | 第29课： 培养对手",
    "section": "",
    "text": "如果我是建伟。\n如果我是建伟，我也跟他一样做——在采取行动时始终考虑长期利益。把那三家书店挤垮虽然会带来暂时的好处。但从长远来看，这行动会造成不利的后果。首先，我书店的生意在于薄利多销，比其他书店有价格合理的优势。如果在这市场内没有对手的话，客户便没有的比较，那竞争优势就无用了。其次，一旦市场的平衡被打破，就会吸引新的竞争对手进入这所学校，说不定会比我强。那时我后悔就晚了。最后，竞争对手是我努力更好地服务客户的动力，我为什么要挤垮他们？"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第29课-培养对手/index.html#口语",
    "href": "学汉语的日记/HSK5下-第29课-培养对手/index.html#口语",
    "title": "HSK5下 | 第29课： 培养对手",
    "section": "",
    "text": "1.  课文中建伟面对不如自己的竞争对手和亲朋好友的建议，她是怎么做的？\n当许多亲朋好友建议她干脆把另三家书店对手挤垮，垄断者学校里的市场，建伟可不但没有去挤垮对手，反而还经常帮助三家书店搞一些营销活动。对于一家要快倒闭的书店，建伟还主动热心地借给其资金，想办法让他继续经营下去。\n2.  你同意他的做法吗？你认为他的说法有没有道理？\n我完全同意他的做法。我也认为商业中的竞争关系就像自然界中的生存竞争。自然界中的生物，适当有一些“敌人”，会促使它们生长的更好；同样的对手并不会妨碍建伟的生意发展，反而会促进经营，让他获得更多利益。主要的原因就是对手们能保持让客户有所比较和优中选优的购物环境。通过比较，学生们才知道建伟的书店服务号、品种优、价格合理。另一个也很重要的原因就是建伟要保持这市场得“生态平衡”，避免更多、更强的对手来参加市场。他对培养对手的说法对我来说很有道理。\n3.  如果你是建伟，你会怎么做？\n课本运用"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第29课-培养对手/index.html#听力",
    "href": "学汉语的日记/HSK5下-第29课-培养对手/index.html#听力",
    "title": "HSK5下 | 第29课： 培养对手",
    "section": "2.1. 听力",
    "text": "2.1. 听力\n1.  女：学校里有好几家小超市，你们怎么都喜欢去那家？\n男：那家服务好，品种齐全，价格合理。\n问：下列哪项是那家超市的优点？（B品种多）\n2.  男：那两家店生意都那么差，你干吗不挤垮他们算了？\n女：我把其他两家都挤垮了，不见得是件好事，因为别人一看这么大的地方只有我一家，新店就可能会出现。\n问：女的为什么不挤垮对手？（C不愿出现新对手）\n3.  女：真不知道怎么才能把这些孩子教好！\n男：你要先跟他们培养感情，然后再培养他们对钢琴的兴趣。\n问：男的是什么意思？（B要先跟孩子相处好）\n4.  男：我觉得这套公寓不错，大小合适，价钱便宜，下面还有个小商店，买东西很方便。\n女：嗯，房东看起来也很热心。\n问：他们在谈论什么？（D租房）\n5.  女：这产品挺好的，就是价格高了点儿。\n男：我们的质量肯定没话说。您要多少？如果量大的话，可以打八折。\n问：男的是什么意思？（C价格可以商量）\n6.  男：合同里对付款方式是怎么约定的？\n女：货到后银行转账付款，对方三天内开具发票。\n问：双方约定如何付款？（B货到后付款）\n7. 女：今天来上班，真是累死我了！\n男：太阳从西边出来了，你不是从不锻炼身体吗？\n女：没办法，路上太堵，地铁又挤。\n男：你也该动动了。\n问：女的今天最可能是怎么来的？（B走路）\n8.  男：妈，我出去买块电池。\n女：拿把伞吧。\n男：天气这么好，要什么伞啊？\n女：万一下雨呢？天气预报说今天有雨的。\n问：女的让男的干什么？（A带伞）\n9. 女：这孩子真乐观。\n男：乐观多好啊！\n女：我就是觉得他乐观得有点儿过分了，整天笑得跟傻子似的。\n男：再怎么说，开心也比发愁好。\n问：男的是什么意思？（A乐观是优点）\n10.男：小青，这次项目你做得非常好，给你半个月假期休息休息吧。\n女：老板，能换一个奖励吗？\n男：怎么，你不想休假？你不是一直说想跟男朋友去旅行吗？\n女：对，我是想去旅行，但我觉得现在时间不合适。\n问：老板为什么要给小青放假？（C小青工作完成得好）\n女：建伟，好久不见了！\n男：是啊，有大半年了吧？\n女：听说现在你那家书店开得挺红火的，最近都有媒体来采访你了。\n男：嗯，还不错。除了书，我还顺便卖点儿文具、电池、小日用品。虽然每件商品的利润都并不高，但诚信经营、薄利多销，现在生意越做越好了。\n女：大学里原来还有三家书店吧？\n男：对，那三家的经营空间越来越小了，三家的营业额加起来还不如我一家高。\n11.建伟现在主要做什么？（A卖书）\n12．关于另外三家书店，下列哪项正确？(D卖出的书不多)\n体育运动对性格发展有什么影响？是促进还是妨碍？这是一个学术问题，也是一个让很多人困惑的现实问题。本文以针对300人进行的调查为基础，采用文献分析的方法，梳理了体育运动对性格发展促进和妨碍的两方面证据。然后,从三个角度分析了产生分歧的原因。最后,在此基础上对未来研究做出几点展望。\n13．这段话可能来自于什么？（C论文）\n14．这段话主要谈论什么？（D体育运动对性格发展的影响）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第29课-培养对手/index.html#阅读",
    "href": "学汉语的日记/HSK5下-第29课-培养对手/index.html#阅读",
    "title": "HSK5下 | 第29课： 培养对手",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n心理学上有一种问题叫“自我妨碍”，意思是人的意识与行为不一致。比如一个人明明很喜欢自己的工作，但却忍不住要破坏工作环境或者工作15C对象，最后造成无法正常工作的结果。16D导致这一问题的原因可能在于,以往的成功和别人的关注为这个人设置了很高的标准，而他害怕无法继续保持这样的成绩，就17A干脆不再好好工作。有这种问题的人，以往越是表现好，就越逼自己，越害怕下次不能做到这个程度，形成一个恶性循环。如果发现自己有这样的问题，那就18B尽快去看心理医生吧。\n\nD对手之间可能会相互促进\n\n商业领域其实和自然界一样，自然界中的生物，适当有一些“敌人”，会促使它们生长得更好；同样，对手并不会妨碍我的发展，反而会促进经营，让我获得更多利益。所以，我不但不想挤垮对手，在需要的时候反而还会帮助他们。\n\nB梅瑞公司会把顾客介绍给竞争对手\n\n梅瑞是美国一家著名的大型百货公司。该公司的购物大厅里有一个咨询服务台，其主要职责是，如果消费者在梅瑞公司没买到自己想要的商品，咨询台的工作人员会介绍他到另一家有这种商品的商店。梅瑞公司这一违反常规的做法，不仅获得了顾客的好感，也赢得了对手的信任与尊重。\n\nD男的买东西一般比较干脆\n\n女人逛超市从来不怕累，为了找到喜欢的商品，跑断腿都不怕；找到之后，还会根据价格反复比较，精挑细选。有时候，就算没有找到自己喜欢的商品，女人也会大包小包，买很多其他东西回来。而男人去超市时普遍都目标明确，他们清楚自己到底要买什么，直接把购物车推到该商品的货架前，不管贵不贵，拿起来就往购物车里装。他们不愿意比较价格，也不会去寻找打折商品。\n\nB所有的投资都可能是风险\n\n有句话叫风险与收益并存。你想追求高收益高利润，就一定会有相对较大的风险，没有什么投资是稳赚不赔的。要投资，首先要做好承担亏损的思想准备。尤其是初期投资，最高目标就是保值不亏，当然，这个“不亏”的意思是说，不管外面的市场怎么变化，我们用来投资的钱，还值原来的那么多。\n23-25.\n因为工作需要，我利用业余时间报了个口译班。班里有一位老师，大约30岁，口译功夫了得。后来才知道，她大学学的是历史，本职工作是一家公司的公关部经理，跟英语没什么关系，儿子已经5岁，她每天要上班、做家务、带孩子。\n打开她的博客，已经更新了500多页，全部都是每天她自己做口译练习的文章，平均每天两篇长的一篇短的，她坚持做这件事已经快10年了，非专业出身的她因为爱好英语而一直努力。我对她表示钦佩，她说，10年前，她曾经看到一份调查报告，一个人如果要掌握一项技能并成为专家，需要不间断地练习1万个小时。当时她算了一笔账，如果每天练习5个小时，每年300天的话，那么需要近7年的时间。\n披头士乐队在成名前已举办过1200场音乐会，比尔ㅤ盖茨在发家前已做了7年的程序员。\n为什么你工作了10年还只是一名小职员？为什么你在家做了7年的饭，也没变成特级大厨？那是因为，你没有投入精力和热情来练习一项技能。每天上班只是看报纸、上网、应付琐碎的任务，每天做饭只是为了让家庭正常运转，并没用专业的眼光看待这件事。\n生命中的下一个7年，下一个1万个小时，你打算怎样度过？\n\nB她做口译老师是兼职\nA掌握技能需要坚持练习\nC怎样才可以成为专家\n\n26-28.\n春节将至，京城各大商场又开始忙着发广告，准备打折促销、送积分送礼品、延长 营业时间等，希望在春节”黄金周”里大赚一笔。这就是人们常说的”假日经济”。\n其实，所谓的“假日经济”并不存在。\n去年12月31日至今年1月2 日的元旦假期中，各大商场的确人山人海，营业额创纪录。元旦假期一过，商场客流就下降，营业额甚至不到元旦期间的四分之一。此情此景给人们一种错觉：“假日经济”就像强心针，可以给 现在不景气的商场带来繁荣，促进销售。\n但大部分人没有注意到，在商场假日销售额保 持增长的同时，利润并没有明显增加，因为商场的 热闹基本都是由各种促销活动带来的。另外，顾客 通常有固定的消费预算，在一定时期内，用于消费 的金额实际上是有一定限度的，假期买得多了，其他时间自然就会减少购物行为。所以说，假期打折等促成了人们特定消费心理的形成， 但更多的只是影响了顾客购物的时间调整，而并不能从根本上改变商业销售的旺淡规律。 从某种意义上说，一年两三次的“假日经济”对商家长期稳定的发展并不利。商家应该 多想想，节日之后该怎么办。\n\nC购物返现\nB给商场带来表面的繁荣\nC反对的"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第29课-培养对手/index.html#书写",
    "href": "学汉语的日记/HSK5下-第29课-培养对手/index.html#书写",
    "title": "HSK5下 | 第29课： 培养对手",
    "section": "2.3. 书写",
    "text": "2.3. 书写\n对手、倒霉、热心、促进、不如。\n面对强大的对手，气馁不如将他们视为激励自己的目标，不断努力提升自身实力。同时，也要保持热心，乐于助人，在帮助他人的过程中促进友谊和成长。即使遇到倒霉的事情，也不要怨天尤人，而是积极面对，从中吸取教训，让自己变得更加强大。相信只要坚持不懈，努力进取，最终一定能够超越对手，取得成功。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第27课-下棋/index.html",
    "href": "学汉语的日记/HSK5下-第27课-下棋/index.html",
    "title": "HSK5下|第27课：下棋",
    "section": "",
    "text": "下棋 | PlayingChess\n我父亲是一位象棋教练。那一年，我大学放假回家，父亲要跟我下棋，我高兴地答应了。\n父亲让我先走三步。不到三分钟，我的棋子损失大半，棋盘上空空的，只剩下几个子了。没办法，眼睁睁看着父亲“将军”，我输了。\n我不服气，说：“这次运气不好，再来！”第二局又输了，“这次没发挥好，我们再来”！几局下来，基本上都是不到10分钟我就败下阵来。我有些灰心。父亲看看我说：“你初学棋，输是正常的。但是你要知道输在什么地方，要吸取教训。否则，你就再下上10年，也未必能赢。”\n“我知道，我技术没你好，经验也不足。”\n“这只是次要因素，不是最重要的。”\n“那最重要的是什么？”我奇怪地问。\n“最重要的问题在于你心态不对。你不够珍惜你的棋子。”\n“怎么不珍惜呀？我每走一步，都想半天。”我否认说。\n“那是后来。开始你是这样吗？我仔细观察过，你三分之二的棋子是在前三分之一的时间失去的。这期间你好像很有把握，下棋时不假思索，拿起来就走，失去了也不觉得可惜。因为你觉得棋子很多，失一两个不算什么。后三分之二的时间，你又犯了相反的错误：对棋子过于珍惜，每走一步都过于谨慎，一个棋子也不想失，反而一个一个都失去了。”\n说到这，父亲停下来，把棋子重新在棋盘上摆好，抬起头，看着我，问：“这是一盘待下的棋。我问你，下棋的基本原则是什么？”\n我想也没想，脱口而出：“赢啊！”\n“那是目的。”父亲用责备的眼光看了我一眼，“至于原则，是要考虑得失。有得必然有失，有失才会有得。每走一步，你事先都应该想清楚：为了赢得什么，你愿意失去什么，这样才可能赢。可惜，大部分人都像你这样，开始不考虑得失，等到后来失去得多了，又开始舍不得，后果就是屡下屡败。其实不仅是下棋，人生也是如此啊！”\n改编自《今日中学生》，作者：林夕\n\n\n\n下棋\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n象棋\nxiàng qí - n. Chinese chess\n\n\n2\n教练\njiào liàn - n. coach, instructor\n\n\n3\n答应\ndāying - v. to agree, to promise\n\n\n4\n损失\nsǔn shī - v. to lose\n\n\n5\n睁\nzhēng - v. to open (one’s eyes)\n\n\n6\n眼睁睁\nyǎn zhēng zhēng - adj. (looking on) helplessly or indifferently\n\n\n7\n服气\nfú qì - v. to be convinced, to be won over\n\n\n8\n运气\nyùn qì - n. luck, fortune\n\n\n9\n局\njú - m. game, set\n\n\n10\n发挥\nfā huī - v. to bring into play, to give rein to\n\n\n11\n灰心\nhuī xīn - adj. discouraged\n\n\n12\n吸取\nxī qǔ - v. to absorb, to draw\n\n\n13\n教训\njiào xùn - n. lesson, moral\n\n\n14\n未必\nwèi bì - adv. may not, not necessarily\n\n\n15\n次要\ncì yào - adj. less important, secondary\n\n\n16\n因素\nyīnsù - n. factor\n\n\n17\n心态\nxīntài - n. psychology, mental attitude\n\n\n18\n珍惜\nzhēn xī - v. to cherish, to treasure\n\n\n19\n否认\nfǒu rèn - v. to deny, to disavow\n\n\n20\n观察\nguān chá - v. to observe, to watch\n\n\n21\n失去\nshī qù - v. to lose\n\n\n22\n期间\nqī jiān - n. time, period\n\n\n23\n把握\nbǎ wò - n. assurance, confidence\n\n\n24\n不假思索\nbùjiǎ-sīsuǒ - without thinking or hesitation\n\n\n25\n犯\nfàn - v. to commit (an error, a crime, etc.)\n\n\n26\n过于\nguò yú - adv. too, excessively\n\n\n27\n原则\nyuán zé - n. principle, tenet (belief)\n\n\n28\n责备\nzé bèi - v. to blame, to reproach\n\n\n29\n必然\nbì rán - adj. inevitable, certain\n\n\n30\n事先\nshì xiān - n. beforehand, in advance\n\n\n31\n舍不得\nshě bu de - v. to be unwilling to part with or give up, to grudge\n\n\n32\n后果\nhòu guǒ - n. consequence, aftermath\n\n\n33\n屡\nlǚ - adv. repeatedly, time and again\n\n\n\n\n\n\n\n动词+下来\n\nbiểu thị hoàn thành,có lúc bao gồm cả ý nghĩa thoát ly hoặc cố định. Ví dụ:\n\n你的论文大概什么时候发表？定下来了吗？\n你看，那张纸是从这本书里撕下来的。\n几局下来，基本上都是不到10分钟我就败下阵来。\n\n\n舍不得\n\nđộng từ, biểu thị không muốn từ bỏ, tiêu phí hoặc sử dụng. Hình thức khẳng định “舍得” dùng trong câu hỏi hoặc trả lời, so sánh. Ví dụ:\n\n把你最喜欢的玩具送给小朋友，你舍得吗？\n有些人对于把钱花在为家庭和自己的生活增加乐趣的事情上，总是有些舍不得。\n可惜，大部分人都像你这样，开始不考虑得失，等到后来失去得多了，又开始舍不得，后果就是屡下屡败。\n\n\nPhân biệt 损失 và 失去\n\n共同点：Đều có thể làm động từ, đều có nghĩa là ban đầu có còn sau này thì khôngcó.\n如：每走一步，你事先都应该想清楚：为了赢得什么，你愿意损失/失去什么，这样才可能赢。\n不同点：\n\n不同点\n\n\n\n\n\n\n\nNo.\n损失\n失去\n\n\n\n\n1\nBiểu thị giảm đi.\n如：不到三分钟，我的棋子损失大半。\nThông thường chỉ không còn gì.\n如：战争让他失去了家庭。\n\n\n2\nCó thể làm danh từ.\n公司会赔偿我们的损失\nKhông thể làm danh từ.\n\n\n\n\n\n\n\n家居\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n夹子\njia1zi-clip, cái kẹp\n\n\n2\n梳子\nshu1zi-comb, cái lược\n\n\n3\n肥皂\nfei2zao4-soap, xà bông\n\n\n4\n扇子\nshan4zi-fan, quạt tay\n\n\n5\n剪刀\njian3dao1-scissors, kéo\n\n\n6\n绳子\nsheng2zi-rope, dây thừng\n\n\n7\n锁\nsuo3-lock, khóa\n\n\n8\n叉子\ncha1zi-fork, nĩa\n\n\n9\n锅\nguo1-pot, nồi\n\n\n10\n壶\nhu2-(tea)pot,kettle, ấm\n\n\n11\n盆\npen2-basin,chậu, chảo\n\n\n12\n火柴\nhuo3chai2-match, diêm\n\n\n\n\n\n\n得与失\n在生活中，我们总是面临许多选择，每一个选择都会把我们带到不同的十字路口，决定我们是否实现目标。比如选择大学，选择第一份工作，选择留在一家公司，或者转到另一家公司。在每一个选择中，我们同时有得和有失。得的就是关于我们用自己的选择来体验生活，从中受益。失的是时间、金钱、精力，最重要的是其他机会。这就是为什么选择对我们来说总是很困难。“人生如棋”，我们要有全局观念，考虑长期利益而做出决定。有时，为了实现长期目标，我们需要牺牲短期利益。当然，没有人能预测生活，有时我们会牺牲却一无所获。但乐观地看，我们已经“得”了一番教训。只要我们继续这样，考虑得全面，照顾到全局，一定会走到最后的成功。（292字）\n\n\n\n1. 课文中的父亲是怎样教育孩子的？\n2. 你同意父亲的观点吗？\n3. 你对于“得”“失”有什么看法？"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第27课-下棋/index.html#生词",
    "href": "学汉语的日记/HSK5下-第27课-下棋/index.html#生词",
    "title": "HSK5下|第27课：下棋",
    "section": "",
    "text": "下棋\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n象棋\nxiàng qí - n. Chinese chess\n\n\n2\n教练\njiào liàn - n. coach, instructor\n\n\n3\n答应\ndāying - v. to agree, to promise\n\n\n4\n损失\nsǔn shī - v. to lose\n\n\n5\n睁\nzhēng - v. to open (one’s eyes)\n\n\n6\n眼睁睁\nyǎn zhēng zhēng - adj. (looking on) helplessly or indifferently\n\n\n7\n服气\nfú qì - v. to be convinced, to be won over\n\n\n8\n运气\nyùn qì - n. luck, fortune\n\n\n9\n局\njú - m. game, set\n\n\n10\n发挥\nfā huī - v. to bring into play, to give rein to\n\n\n11\n灰心\nhuī xīn - adj. discouraged\n\n\n12\n吸取\nxī qǔ - v. to absorb, to draw\n\n\n13\n教训\njiào xùn - n. lesson, moral\n\n\n14\n未必\nwèi bì - adv. may not, not necessarily\n\n\n15\n次要\ncì yào - adj. less important, secondary\n\n\n16\n因素\nyīnsù - n. factor\n\n\n17\n心态\nxīntài - n. psychology, mental attitude\n\n\n18\n珍惜\nzhēn xī - v. to cherish, to treasure\n\n\n19\n否认\nfǒu rèn - v. to deny, to disavow\n\n\n20\n观察\nguān chá - v. to observe, to watch\n\n\n21\n失去\nshī qù - v. to lose\n\n\n22\n期间\nqī jiān - n. time, period\n\n\n23\n把握\nbǎ wò - n. assurance, confidence\n\n\n24\n不假思索\nbùjiǎ-sīsuǒ - without thinking or hesitation\n\n\n25\n犯\nfàn - v. to commit (an error, a crime, etc.)\n\n\n26\n过于\nguò yú - adv. too, excessively\n\n\n27\n原则\nyuán zé - n. principle, tenet (belief)\n\n\n28\n责备\nzé bèi - v. to blame, to reproach\n\n\n29\n必然\nbì rán - adj. inevitable, certain\n\n\n30\n事先\nshì xiān - n. beforehand, in advance\n\n\n31\n舍不得\nshě bu de - v. to be unwilling to part with or give up, to grudge\n\n\n32\n后果\nhòu guǒ - n. consequence, aftermath\n\n\n33\n屡\nlǚ - adv. repeatedly, time and again"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第27课-下棋/index.html#注释",
    "href": "学汉语的日记/HSK5下-第27课-下棋/index.html#注释",
    "title": "HSK5下|第27课：下棋",
    "section": "",
    "text": "动词+下来\n\nbiểu thị hoàn thành,có lúc bao gồm cả ý nghĩa thoát ly hoặc cố định. Ví dụ:\n\n你的论文大概什么时候发表？定下来了吗？\n你看，那张纸是从这本书里撕下来的。\n几局下来，基本上都是不到10分钟我就败下阵来。\n\n\n舍不得\n\nđộng từ, biểu thị không muốn từ bỏ, tiêu phí hoặc sử dụng. Hình thức khẳng định “舍得” dùng trong câu hỏi hoặc trả lời, so sánh. Ví dụ:\n\n把你最喜欢的玩具送给小朋友，你舍得吗？\n有些人对于把钱花在为家庭和自己的生活增加乐趣的事情上，总是有些舍不得。\n可惜，大部分人都像你这样，开始不考虑得失，等到后来失去得多了，又开始舍不得，后果就是屡下屡败。\n\n\nPhân biệt 损失 và 失去\n\n共同点：Đều có thể làm động từ, đều có nghĩa là ban đầu có còn sau này thì khôngcó.\n如：每走一步，你事先都应该想清楚：为了赢得什么，你愿意损失/失去什么，这样才可能赢。\n不同点：\n\n不同点\n\n\n\n\n\n\n\nNo.\n损失\n失去\n\n\n\n\n1\nBiểu thị giảm đi.\n如：不到三分钟，我的棋子损失大半。\nThông thường chỉ không còn gì.\n如：战争让他失去了家庭。\n\n\n2\nCó thể làm danh từ.\n公司会赔偿我们的损失\nKhông thể làm danh từ."
  },
  {
    "objectID": "学汉语的日记/HSK5下-第27课-下棋/index.html#扩展",
    "href": "学汉语的日记/HSK5下-第27课-下棋/index.html#扩展",
    "title": "HSK5下|第27课：下棋",
    "section": "",
    "text": "家居\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n夹子\njia1zi-clip, cái kẹp\n\n\n2\n梳子\nshu1zi-comb, cái lược\n\n\n3\n肥皂\nfei2zao4-soap, xà bông\n\n\n4\n扇子\nshan4zi-fan, quạt tay\n\n\n5\n剪刀\njian3dao1-scissors, kéo\n\n\n6\n绳子\nsheng2zi-rope, dây thừng\n\n\n7\n锁\nsuo3-lock, khóa\n\n\n8\n叉子\ncha1zi-fork, nĩa\n\n\n9\n锅\nguo1-pot, nồi\n\n\n10\n壶\nhu2-(tea)pot,kettle, ấm\n\n\n11\n盆\npen2-basin,chậu, chảo\n\n\n12\n火柴\nhuo3chai2-match, diêm"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第27课-下棋/index.html#运用",
    "href": "学汉语的日记/HSK5下-第27课-下棋/index.html#运用",
    "title": "HSK5下|第27课：下棋",
    "section": "",
    "text": "得与失\n在生活中，我们总是面临许多选择，每一个选择都会把我们带到不同的十字路口，决定我们是否实现目标。比如选择大学，选择第一份工作，选择留在一家公司，或者转到另一家公司。在每一个选择中，我们同时有得和有失。得的就是关于我们用自己的选择来体验生活，从中受益。失的是时间、金钱、精力，最重要的是其他机会。这就是为什么选择对我们来说总是很困难。“人生如棋”，我们要有全局观念，考虑长期利益而做出决定。有时，为了实现长期目标，我们需要牺牲短期利益。当然，没有人能预测生活，有时我们会牺牲却一无所获。但乐观地看，我们已经“得”了一番教训。只要我们继续这样，考虑得全面，照顾到全局，一定会走到最后的成功。（292字）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第27课-下棋/index.html#口语",
    "href": "学汉语的日记/HSK5下-第27课-下棋/index.html#口语",
    "title": "HSK5下|第27课：下棋",
    "section": "",
    "text": "1. 课文中的父亲是怎样教育孩子的？\n2. 你同意父亲的观点吗？\n3. 你对于“得”“失”有什么看法？"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第27课-下棋/index.html#听力",
    "href": "学汉语的日记/HSK5下-第27课-下棋/index.html#听力",
    "title": "HSK5下|第27课：下棋",
    "section": "2.1.听力",
    "text": "2.1.听力\n1. 男：到了新公司，你一定要吸取教训，争取留用。\n女：上次的机会我没有珍惜，这次一定好好表现。\n问：关于女的，可以知道什么？（D得到了试用机会）\n2. 女：参加这个项目的选手一个比一个厉害，刘云能进决赛真不容易。\n男：今年他的状态很好，可以说是超水平发挥了。\n问：男的觉得刘云怎么样？（B发挥很出色）\n3. 男：寒假你不打算回家了吗？\n女：我在一家旅行社实习，公司让我设计一条新线路，年前要出差做调研。\n问：女的寒假为什么不回家？（A出差去做调研）\n4. 女：小刘运气真不错，刚来一年就成了主力队员。\n男：你也别灰心，论实力你未必比她差，下个月的比赛好好把握。\n问：男的对女的说这些话是想怎样？（C鼓励她）\n5. 女：我对小林是有些看法cóthànhkiếnvớiaiđó，这一点儿我不否认。\n男：我希望你找机会和他沟通一下，把问题谈开nóirõ，别影响工作。\n问：女的和小林最可能是什么关系？(B同事)\n6. 女：你不知道吗？比赛期间，任何队员都不能随便外出。\n男：我事先跟教练打过招呼的，他同意了。\n问：关于男的，可以知道什么？（D教训难了假）\n7. 女：昨天我去隔壁莉莉家还东西，他们家的窗帘我特喜欢。\n男：怎么？你又想干什么？\n女：咱家客厅的窗帘用了好多年了，花样họatiết也不流行了。\n男：我看就是有些脏了，洗洗还能用。\n问：关于窗帘，男的是什么意思？（C不想换新的）\n8. 男：上次李阳去德国讲学，本来领导也问过我。\n女：那你怎么不去呀？\n男：当时孩子小，我有点儿犹豫。现在觉得有点儿后悔了。\n女：多好的机会失去了，我的经验就是，有机会一定要好好把握。\n问：男的为什么没去德国讲学？（A爱照顾孩子）\n9. 女：刘京报的价格好像不对，你怎么没检查出来？\n男：真是的！上次开会讨论得很清楚了，他怎么还弄错了。\n女：你遇到问题总爱责备别人，就不想从自己身上找找原因。\n男：我主要是太信任他了。\n问：在这件事上，女的觉得男的怎么样？（D爱抱怨别人）\n10.女：小刘请假看球赛是你同意的吗？\n男：他最近修改设计方案，搞得很辛苦，我想让他放松放松。\n女：工作辛苦可以理解，但不能因为他跟你关系好就不按规定办事。\n男：我知道了，下次一定不再自作主张。\n问：女的批评男的什么？（B不坚持原则）\n11-12.\n刘明遇见了从体育馆出来的一位朋友。他问这个朋友：“玩儿得怎么样？”\n“玩儿得很好！”朋友回答，“我打了网球，下了象棋。既赢了象棋冠军，又赢了网球冠军。”\n“你对打网球、下象棋都很在行chuyênnghiệp/ngườitrongngành吗？”刘明问。\n“我和网球冠军一起下象棋，赢了他，后来，我又和象棋冠军一起打网球，我也赢了他。”\n比赛中要做到知己知彼，有利地选择对手，这也是取胜的好办法。\n11．关于刘明的朋友，可以知道什么？（D下棋打球都赢了）\n12．这段话想告诉我们什么道理？（D了解对手更易取胜）\n13-14.\n在14日的足球比赛中，上海队主场迎战北京队。这天，上海整个下午都在下雨。上半时，赛场条件尚可，一开始主队就控制了场上局势，可惜浪费了大量得分机会。他们在第24分钟时才首先破门，可4分钟后客队队长就为北京队追平比分。\n下半时，赛场局部区域积水明显，双方队长都对主裁判表示愿意继续比赛，但在比赛进行了20分钟后，赛场条件变得非常糟糕，大有变成水球比赛的可能。裁判只好暂停比赛，球员回到了休息室。15分钟后，主裁判见赛场积水更加严重，于是只好宣布比赛中止。\n13．从上半场的比赛可以知道什么？（D上海队丢掉很多机会）\n14．下半场比赛实际进行了多长时间？（C20分钟）\n##2.2.阅读\n15-18.\n有一个韩国家庭，收到别人送的两箱苹果。其中一箱苹果已有些15C过分成熟了,如果不马上吃掉，很快就会烂掉；另一箱比较新鲜，还可以16B存在长一点儿的时间。父亲把二个儿子找来，17A商量苹果的吃法。大儿子说，趁还没有完全坏，先吃那成熟的一箱。父亲说，不过等这一箱吃完，那一箱也就快坏了。二儿子说，先吃那好的一箱，这样能尽可能多吃一点儿好苹果。父亲说，可是这样一来，过熟的那一箱肯定全部浪费了。三儿子说，我们把两箱苹果混合起来，分一半给邻居，所有的苹果都不会浪费。这位二儿子，名叫潘基文，就是前任联合国秘书长。可以看出，潘基文的办法，于己无损，于人有益，更重要的是节约了社会资源，使每一个苹果都能18B发挥作用。这样的人，一定会朋友多，一定会在人生的道路上得到更多的支持和帮助，也会取得更多的成就。\n\nC询问需要有技巧\n\n语言是一种艺术，询问是一种技巧。能否最快地得到想要的答案，是判别一个人设计问题高下方法。这也是为什么有些人能当首席记者，采访世界名人，而有些人只能替人校稿。\n\nB木板大鼓的伴奏乐器为三弦\n\n“京韵大鼓”形成于北京和天津一带。20世纪初期著名鼓书艺人刘宝全等人在河北“木板大鼓的基础上，吸收京剧唱腔和北京地方民间小调，同时使用北京语音进行演唱，并在原有伴奏乐器三弦外，增加了四胡和琵琶，创造出“京韵大鼓”这门曲艺艺术。\n\nA郎中是对医生的一种尊称\n\n南方人尊称医生为郎中。为什么会有“郎中”这样的称呼呢？原来，郎中本来是一种官名，他的职责就是保护、陪同帝王，并随时提出建议。自战国时期就有此官，以后各朝各代都把侍郎、郎中作为各部门的重要职务。唐代以后因国家设立的官职太多太滥了，社会上就有了把医生叫作郎中的风俗。\n\nB“我”只为孩子的事操心\n\n人到中年，除了每日处理繁忙的工作外，身边总是围绕着或大或小、或急或缓的事儿。比如孩子的教育、双方父母的身体、对亲朋好友或精神或物质的照顾，等等。这些事情接二连三，频繁不断，不管你是否有所准备，它们都不期而至，时常搞得人心烦意乱、疲惫不堪。我的生活就是这个状态。\n23-25.\n好日子和坏日子，是有一定比例的。就是说，你的一生，不可能都是好日子——天天蜜里调油，也不可能都是坏日子-每时每刻黄连拌苦胆。必是好坏日子交叉着来，仿佛一块花格子布。如果算下来，你的好日子多，就如同布面上的红黄色多，亮堂鲜艳；如果你的坏日子多，那就是黑灰色多，阴云密布。\n什么是好坏日子的判断标准呢？钱吗？好像不是。有钱的人不一定承认他过的是好日子。钱少的人或没钱的人，也不一定感觉他过的就是坏日子。健康吗？好像也不是。无痛无灾的人不一定觉得他过的是好日子，生病或残疾的人也不一定承认他过的就是坏日子。美丽和能力吗？似乎，更不像了。看看周围，有多少漂亮能干的男人女人，锁着眉苦着脸，抱怨着岁月的难熬……说了若干的标准，都不是。那么，什么是好日子和坏日子的界限呢？\n不知他人的答案如何，我猜，是爱吧？\n有爱的日子，也许我们很穷，但每一分钱都能带给我们双倍快乐；也许我们的身体坏了，但我们牵着相爱的人的手，慢慢老去，旅途就不再孤独；也许我们是平凡和渺小的，但我们用尽全力做着喜欢的事，心中便充满温暖安宁。\n因此有爱就是好日子，你的那块花格子布上，就绽开了鲜花。\n\nD幸福与痛苦交织\nA爱\nC心中有爱才会幸福\n\n26-28.\n每个家长都“望子成龙”，家长们常将这种期望与孩子的学业成绩直接挂钩，希望孩子的成绩在班里能排前几名，甚至第一名，如果孩子的成绩处于中等水平，就不能接受,如果成绩差的话，就更不能容忍了。那么，孩子的学业成绩究竟应该有多好呢？\n孩子的成绩处于中等水平是普遍现象，无论学校有多好，教师的水平有多高，学生如何聪明，在班里，绝大多数学生的成绩都处于中等水平。中等生是班级里的大多数,这些孩子的心理压力和冲突相对来说最小：他们既不会因为“木秀于林，风必摧之”而受人嫉妒，整日为如何保持成绩、争夺名次而承受巨大的心理压力，也不必因为学习跟不上而焦虑不安。身处这个大多数人的群体，他们可以和大多数同学平等交往，享受同学之间纯洁的友谊，会因为太普通而承受相对较少的干扰和压力，享有较为宽松的人际环境。在学业上，他们基本上不存在吃不饱和消化不了的问题，学校的课程就是针对他们的情况而设置的。所以，从理论上讲，他们是校园中身心发育最充分、满足程度最高的一类人群，他们通常开朗、随和、宽容、不敏感，能容许别人比自己好,也能体谅比自己差的同学。这些“优势”为他们成年后适应社会奠定了良好的基础。因此，家长应该接受孩子的成绩处于中等水平。\n\nA很正常\nB与同学能较好相处\nD家长该怎样看特孩子的成绩"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第27课-下棋/index.html#书写",
    "href": "学汉语的日记/HSK5下-第27课-下棋/index.html#书写",
    "title": "HSK5下|第27课：下棋",
    "section": "2.3.书写",
    "text": "2.3.书写\n教训、把握、吸取、原则、后果\n在人生的旅途中，我们难免会遇到挫折和失败。面对这些挑战，我们要学会从教训中吸取经验，坚持住前进的方向，并遵循正确的原则，以避免造成不可挽回的后果。只有不断地反思和学习，才能在未来的道路上把握所有的机会，走得更加稳健和成功。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第25课-给自己加满水/index.html",
    "href": "学汉语的日记/HSK5下-第25课-给自己加满水/index.html",
    "title": "HSK5下 | 第25课： 给自己加满水",
    "section": "",
    "text": "给自己加满水 | Adding up the load on yourself\n有一位经验丰富的老船长，一次返航中，天气恶劣，他们的船遇到了可怕的巨大风浪。正当水手们慌张得不知如何是好时，老船长命令水手们立刻打开货舱，使劲儿朝里面放水。\n“船长简直是疯了，这样做只会增加船的压力，船就会下沉得更快，这不是找死吗？”一个年轻的水手骂道。\n看着船长严肃的表情，水手们还是照做了。随着货舱里的水位越升越高，船一点一点地下沉，狂风巨浪依然猛烈，对船的威胁却减小了，船也渐渐取得了平衡。\n船长望着松了一口气的水手们说：“几万吨的钢铁巨轮很少有被打翻的，被打翻的常常是根基很轻的小船。船在有一定重量的时候是最安全的．在空的时候则是最危险的。\n另一个相似的故事发生在某一著名风景区，那里有一段被当地人称为“鬼谷”的最危险的路段，山路非常窄，两边是万丈深渊。每当导游们带队来这里游览时，一定要让游客们背点或者拿点什么东西。\n“这么危险的地方，我不拿东西两腿都发抖，再负重前行，那不是更容易摔倒吗？”一位妇女不解地问。\n导游小姐解释道：“这里以前发生过好几起意外，都是迷路的游客在丝毫没有感觉到压力的情况下，一不小心滚下去的。当地人每天都从这条路上背着东西来来往往，却从来没人出事。假如你感觉到了有风险，谨慎地负重前行，反而会更安全。\n这就是“压力效应”。那些胸怀理想、肩上有责任感的人，才能承受住压力，从历史的风雨中走过“鬼谷”；而那些没有理想，没有一点压力，做一天和尚撞一天钟的人，就像一艘风暴中的空船，往往一场人生的狂风巨浪便会把他们彻底地打翻在地。\n改编自 《小故事大道理》，作者：汪胜战\n\n\n\n给自己加满水\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n返航\nfǎnháng - v. to be on the homeward voyage\n\n\n2\n恶劣\nè liè - adj. bad, vile\n\n\n3\n可怕\nkě pà - adj. terrible, dreadful\n\n\n4\n风浪\nfēng làng - n. storm, stormy waves\n\n\n5\n舱\ncāng - n. cabin\n\n\n6\n使劲（儿）\nshǐjìn (er) - v. to exert oneself, to make efforts\n\n\n7\n朝\ncháo - prep. towards\n\n\n8\n简直\njiǎn zhí - adv. simply, virtually\n\n\n9\n沉\nchén - v. to sink\n\n\n10\n严肃\nyán sù - adj. serious, solemn\n\n\n11\n猛烈\nměngliè - adj. strong, violent, fierce\n\n\n12\n狂\nkuáng - adj. wildly, unrestrained\n\n\n13\n威胁\nwēi xié - v. to threaten\n\n\n14\n平衡\npíng héng - adj. balanced\n\n\n15\n吨\ndūn - m. metric ton\n\n\n16\n钢铁\ngāng tiě - n. steel\n\n\n17\n根基\ngēn jī - n. basis, foundation\n\n\n18\n重量\nzhòngliàng - n. weight\n\n\n19\n相似\nxiāng sì - adj. similar\n\n\n20\n风景\nfēng jǐng - n. scenery, view\n\n\n21\n窄\nzhǎi - adj. narrow\n\n\n22\n万丈\nwànzhàng - num.-m. lofty, bottomless\n\n\n23\n深渊\nshēnyuān - n. abyss, bottomless pit\n\n\n24\n游览\nyóu lǎn - v. to visit, to tour\n\n\n25\n发抖\nfā dǒu - v. to tremble, to shiver\n\n\n26\n负重\nfùzhòng - v. to bear a load or weight\n\n\n27\n摔倒\nshuāi dǎo - to fall down, to tumble\n\n\n28\n妇女\nfù nǚ - n. woman\n\n\n29\n起\nqǐ - m. case, instance\n\n\n30\n丝毫\nsī háo - adj. slightest, at all\n\n\n31\n滚\ngǔn - v. to roll, to tumble\n\n\n32\n风险\nfēng xiǎn - n. risk\n\n\n33\n谨慎\njǐn shèn - adj. cautious, prudent\n\n\n34\n效应\nxiào yìnɡ - n. effect\n\n\n35\n胸\nxiōng - n. chest, bosom\n\n\n36\n承受\nchéng shòu - v. to bear, to endure\n\n\n37\n和尚\nhéshang - n. (Buddhist) monk\n\n\n38\n钟\nzhōng - n. bell\n\n\n39\n彻底\nchè dǐ - adj. thorough, complete\n\n\n\n\n\n\n\n朝\n\n“朝” động từ, biểu thị hướng về, đối mặt. Ví dụ:\n\n我们学校的正门坐西朝东。\n我进去时，他正脸朝里和李主任商量着什么，没注意到我的到来。\n\n“朝” cũng có thể làm giới từ, biểu thị phương hướng của hành vi, động tác đã chỉ. Không giống với “向”, không thể làm bổ ngữ.\n\n·····老船长命今水手们立刻打开货舱，使劲儿朝里面放水。\n我仿佛看到胜利正朝我们走来。\n\n\n简直 jian3zhi2\n\n简直 (quả là/thật là/tưởng như là) , phó từ, biểu thị gần như là vậy nhưng cũng không hẳn hoàn toàn là như vậy. Mang ngữ khí khoa trương, nhấn mạnh. Ví dụ:\n\n听到刘方离婚的消息时，我简直不敢相信自己的耳朵。\n这次张小姐变得格外可气、礼貌，与从前相比，简直像换了个人。\n“船长简直是疯了，这样做只会增加船的压力，船就会下沉得更快，这不是找死吗？”\n\n\nPhân biệt 严肃 và 严格\n\n共同点：Đều là tính từ, biểu thị sự nghiêm túc,nghiêm khắc, không lơ là nhưng phạm vi sử dụng của cả hai lại khác nhau rất lớn, không thể thay thế dùng cho nhau.\n不同点：\n\n不同点\n\n\n\n\n\n\n\nNo.\n严肃\n严格\n\n\n\n\n1\nNhấn mạnh tác phong, thái độ, và các phương diện khác rất nghiêm túc.\n如：小林这件事影响恶劣，我们对他一定要严肃批评。\nBiểu thị khi tuân thủ chế độ, hoặc nắm vững tiêu chuẩn nghiêm khắc, không lơ là.\n如：小华妈妈，平时对孩子教育很严格。\n\n\n2\nBiểu thị thần sắc, bầu không khí, làm cho người khác vừa tôn trọng vừa sợ hãi.\n如：一句幽默的笑话可以让紧张严肃的气氛变得轻松愉快。\nKhông có ý nghĩa như thế.\n\n\n\n\n\n\n\n度量单位和学习用具\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n厘米\nli2mi3 - centimeter\n\n\n2\n克\nke4 - gram, gam\n\n\n3\n平方\nping2fang1 - squared, bình phương\n\n\n4\n吨\ndun1 - ton, tấn\n\n\n5\n尺子\nchi3zi - ruler, thước\n\n\n6\n胶水\njiao1shui3 - glue, keo dán\n\n\n7\n文具\nwen2ju4 - stationery, văn phòng phẩm\n\n\n\n\n\n\n你喜欢/不喜欢压力\n在现代社会中，压力已成为一种普遍的疾病，每个人都有。压力可能来自很多方面，如家庭、朋友、学业、事业，甚至有人在思考太多时给自己施加压力。对我来说，压力给我带来了一些麻烦，如焦虑、胃痛和失眠，所以我一点也不喜欢它。但公平地说，我真的需要压力来发展和进步。以工作为例，无论那天的工作量是多少，我都会用同样的时间来完成他们。我认为少工作会让我懒惰，如果那天的任务越多，我会感觉越有效。压力是我专注和努力工作的动力。（204字）\n\n\n\n1.  目前，你遇到的最大的压力来自哪方面？说说你的情况？\n我现在是在越南大城市的一名26岁的上班族。学习基本上都完成了，还没被父母催婚，所以对我来说，我现在面临最大的压力就是来自工作方面。我本来学的是会计专业，但现在从事数据科学工作，这份工作需要高度专业化的知识，并随时准备更新新技术。我一直觉得我的知识和技能需要提高才能胜任这项工作。此外，我在一家初创公司工作，独自承担许多工作，总是很忙。除了面对工作中的期限以外，压力还来自家人的期望。父母一直希望我有稳定的工作、高收入、为未来积累更多的财富。所以我总是一面要完成工作，一面要思考自己的长远路径，把自己的进步和同龄人做比较。\n2.  你觉得压力给你带来了什么影响？有什么好处和坏处？\n压力对我来说又有好处，又有坏处。压力是我工作、学习和进步的动力。压力来自需要完成的工作、需要解决的问题，它指导和迫使我在工作上集中全部精力。当我处于压力环境中时，我总是感觉更好。相反，太多的空闲时间会让我失去注意力，没有成就感。\n然而，压力也给我带来了一些麻烦，如过度焦虑，导致失眠和胃痛。\n–》 怎么平衡这两个方面。想法和框架\n3.  如果你觉得压力过大时，你有什么好的减轻压力的办法呢？\n如果我觉得压力过大时，我会尽量冷静下来，先自己思考我正在面临的压力来自哪个方面。我会（搞清楚）弄清楚那些问题有多严重、是不是暂时的、我是否能自己解决、谁能帮助我。然后我会找一个人分享，并寻求建议。他可能是经理、朋友或者父母。如果问题超出了我的控制能力或短期内无法解决，我就不用发愁。如果有什么办法能解决问题，根据别人的建议，我可以一步一步地处理它们。实际上，每次感到工作超量，我会跟经理讨论一下，搞清楚哪项任务要马上完成、哪项还不太紧。积极与同事交谈是我克服工作压力的方式。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第25课-给自己加满水/index.html#生词",
    "href": "学汉语的日记/HSK5下-第25课-给自己加满水/index.html#生词",
    "title": "HSK5下 | 第25课： 给自己加满水",
    "section": "",
    "text": "给自己加满水\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n返航\nfǎnháng - v. to be on the homeward voyage\n\n\n2\n恶劣\nè liè - adj. bad, vile\n\n\n3\n可怕\nkě pà - adj. terrible, dreadful\n\n\n4\n风浪\nfēng làng - n. storm, stormy waves\n\n\n5\n舱\ncāng - n. cabin\n\n\n6\n使劲（儿）\nshǐjìn (er) - v. to exert oneself, to make efforts\n\n\n7\n朝\ncháo - prep. towards\n\n\n8\n简直\njiǎn zhí - adv. simply, virtually\n\n\n9\n沉\nchén - v. to sink\n\n\n10\n严肃\nyán sù - adj. serious, solemn\n\n\n11\n猛烈\nměngliè - adj. strong, violent, fierce\n\n\n12\n狂\nkuáng - adj. wildly, unrestrained\n\n\n13\n威胁\nwēi xié - v. to threaten\n\n\n14\n平衡\npíng héng - adj. balanced\n\n\n15\n吨\ndūn - m. metric ton\n\n\n16\n钢铁\ngāng tiě - n. steel\n\n\n17\n根基\ngēn jī - n. basis, foundation\n\n\n18\n重量\nzhòngliàng - n. weight\n\n\n19\n相似\nxiāng sì - adj. similar\n\n\n20\n风景\nfēng jǐng - n. scenery, view\n\n\n21\n窄\nzhǎi - adj. narrow\n\n\n22\n万丈\nwànzhàng - num.-m. lofty, bottomless\n\n\n23\n深渊\nshēnyuān - n. abyss, bottomless pit\n\n\n24\n游览\nyóu lǎn - v. to visit, to tour\n\n\n25\n发抖\nfā dǒu - v. to tremble, to shiver\n\n\n26\n负重\nfùzhòng - v. to bear a load or weight\n\n\n27\n摔倒\nshuāi dǎo - to fall down, to tumble\n\n\n28\n妇女\nfù nǚ - n. woman\n\n\n29\n起\nqǐ - m. case, instance\n\n\n30\n丝毫\nsī háo - adj. slightest, at all\n\n\n31\n滚\ngǔn - v. to roll, to tumble\n\n\n32\n风险\nfēng xiǎn - n. risk\n\n\n33\n谨慎\njǐn shèn - adj. cautious, prudent\n\n\n34\n效应\nxiào yìnɡ - n. effect\n\n\n35\n胸\nxiōng - n. chest, bosom\n\n\n36\n承受\nchéng shòu - v. to bear, to endure\n\n\n37\n和尚\nhéshang - n. (Buddhist) monk\n\n\n38\n钟\nzhōng - n. bell\n\n\n39\n彻底\nchè dǐ - adj. thorough, complete"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第25课-给自己加满水/index.html#注释",
    "href": "学汉语的日记/HSK5下-第25课-给自己加满水/index.html#注释",
    "title": "HSK5下 | 第25课： 给自己加满水",
    "section": "",
    "text": "朝\n\n“朝” động từ, biểu thị hướng về, đối mặt. Ví dụ:\n\n我们学校的正门坐西朝东。\n我进去时，他正脸朝里和李主任商量着什么，没注意到我的到来。\n\n“朝” cũng có thể làm giới từ, biểu thị phương hướng của hành vi, động tác đã chỉ. Không giống với “向”, không thể làm bổ ngữ.\n\n·····老船长命今水手们立刻打开货舱，使劲儿朝里面放水。\n我仿佛看到胜利正朝我们走来。\n\n\n简直 jian3zhi2\n\n简直 (quả là/thật là/tưởng như là) , phó từ, biểu thị gần như là vậy nhưng cũng không hẳn hoàn toàn là như vậy. Mang ngữ khí khoa trương, nhấn mạnh. Ví dụ:\n\n听到刘方离婚的消息时，我简直不敢相信自己的耳朵。\n这次张小姐变得格外可气、礼貌，与从前相比，简直像换了个人。\n“船长简直是疯了，这样做只会增加船的压力，船就会下沉得更快，这不是找死吗？”\n\n\nPhân biệt 严肃 và 严格\n\n共同点：Đều là tính từ, biểu thị sự nghiêm túc,nghiêm khắc, không lơ là nhưng phạm vi sử dụng của cả hai lại khác nhau rất lớn, không thể thay thế dùng cho nhau.\n不同点：\n\n不同点\n\n\n\n\n\n\n\nNo.\n严肃\n严格\n\n\n\n\n1\nNhấn mạnh tác phong, thái độ, và các phương diện khác rất nghiêm túc.\n如：小林这件事影响恶劣，我们对他一定要严肃批评。\nBiểu thị khi tuân thủ chế độ, hoặc nắm vững tiêu chuẩn nghiêm khắc, không lơ là.\n如：小华妈妈，平时对孩子教育很严格。\n\n\n2\nBiểu thị thần sắc, bầu không khí, làm cho người khác vừa tôn trọng vừa sợ hãi.\n如：一句幽默的笑话可以让紧张严肃的气氛变得轻松愉快。\nKhông có ý nghĩa như thế."
  },
  {
    "objectID": "学汉语的日记/HSK5下-第25课-给自己加满水/index.html#扩展",
    "href": "学汉语的日记/HSK5下-第25课-给自己加满水/index.html#扩展",
    "title": "HSK5下 | 第25课： 给自己加满水",
    "section": "",
    "text": "度量单位和学习用具\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n厘米\nli2mi3 - centimeter\n\n\n2\n克\nke4 - gram, gam\n\n\n3\n平方\nping2fang1 - squared, bình phương\n\n\n4\n吨\ndun1 - ton, tấn\n\n\n5\n尺子\nchi3zi - ruler, thước\n\n\n6\n胶水\njiao1shui3 - glue, keo dán\n\n\n7\n文具\nwen2ju4 - stationery, văn phòng phẩm"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第25课-给自己加满水/index.html#运用",
    "href": "学汉语的日记/HSK5下-第25课-给自己加满水/index.html#运用",
    "title": "HSK5下 | 第25课： 给自己加满水",
    "section": "",
    "text": "你喜欢/不喜欢压力\n在现代社会中，压力已成为一种普遍的疾病，每个人都有。压力可能来自很多方面，如家庭、朋友、学业、事业，甚至有人在思考太多时给自己施加压力。对我来说，压力给我带来了一些麻烦，如焦虑、胃痛和失眠，所以我一点也不喜欢它。但公平地说，我真的需要压力来发展和进步。以工作为例，无论那天的工作量是多少，我都会用同样的时间来完成他们。我认为少工作会让我懒惰，如果那天的任务越多，我会感觉越有效。压力是我专注和努力工作的动力。（204字）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第25课-给自己加满水/index.html#口语",
    "href": "学汉语的日记/HSK5下-第25课-给自己加满水/index.html#口语",
    "title": "HSK5下 | 第25课： 给自己加满水",
    "section": "",
    "text": "1.  目前，你遇到的最大的压力来自哪方面？说说你的情况？\n我现在是在越南大城市的一名26岁的上班族。学习基本上都完成了，还没被父母催婚，所以对我来说，我现在面临最大的压力就是来自工作方面。我本来学的是会计专业，但现在从事数据科学工作，这份工作需要高度专业化的知识，并随时准备更新新技术。我一直觉得我的知识和技能需要提高才能胜任这项工作。此外，我在一家初创公司工作，独自承担许多工作，总是很忙。除了面对工作中的期限以外，压力还来自家人的期望。父母一直希望我有稳定的工作、高收入、为未来积累更多的财富。所以我总是一面要完成工作，一面要思考自己的长远路径，把自己的进步和同龄人做比较。\n2.  你觉得压力给你带来了什么影响？有什么好处和坏处？\n压力对我来说又有好处，又有坏处。压力是我工作、学习和进步的动力。压力来自需要完成的工作、需要解决的问题，它指导和迫使我在工作上集中全部精力。当我处于压力环境中时，我总是感觉更好。相反，太多的空闲时间会让我失去注意力，没有成就感。\n然而，压力也给我带来了一些麻烦，如过度焦虑，导致失眠和胃痛。\n–》 怎么平衡这两个方面。想法和框架\n3.  如果你觉得压力过大时，你有什么好的减轻压力的办法呢？\n如果我觉得压力过大时，我会尽量冷静下来，先自己思考我正在面临的压力来自哪个方面。我会（搞清楚）弄清楚那些问题有多严重、是不是暂时的、我是否能自己解决、谁能帮助我。然后我会找一个人分享，并寻求建议。他可能是经理、朋友或者父母。如果问题超出了我的控制能力或短期内无法解决，我就不用发愁。如果有什么办法能解决问题，根据别人的建议，我可以一步一步地处理它们。实际上，每次感到工作超量，我会跟经理讨论一下，搞清楚哪项任务要马上完成、哪项还不太紧。积极与同事交谈是我克服工作压力的方式。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第25课-给自己加满水/index.html#听力",
    "href": "学汉语的日记/HSK5下-第25课-给自己加满水/index.html#听力",
    "title": "HSK5下 | 第25课： 给自己加满水",
    "section": "2.1. 听力",
    "text": "2.1. 听力\n1.   男：听说你去爬华山了？够刺激吧？\n女：没错，有些地方山路非常窄，吓得我腿直发抖。\n问：女的爬山时感觉怎么样？(A 有些慌张)\n2.   女：刘教授的讲座是明天下午几点？\n男：本来定的是两点半，临时又通知说改在三点了。\n问：关于那个讲座，下列哪项正确？(B 推迟了)\n3.   男：我们运气真好，海面风平浪静，风景多美啊！\n女：是啊，临出门时，我还担心会晕船呢。\n问：关于女的，可以知道什么？(C 没有晕船)\n4.   男：早上起来时，突然感觉脑子特别晕yūn，身体一下子失去平衡就摔倒了。\n女：那后来怎么样了？您这个毛病可得好好查查了。\n问：男的为什么摔倒了？(A 头晕)\n5.   女：老师让你把作文再检查一下，上面还有标点错误。\n男：好的，我现在就看。\n问：老师觉得作文还有什么问题？(D标点有误)\n6.   女：听说你选刘宏Liu2hong2 – Lưu Hồng 老师做你的导师了？\n男：是的，刘教授很有学问，分析问题也很透彻tòuchè，就是有点儿严肃。\n问：男的觉得刘教授怎么样？(B看着很严肃)\n7.   男：您选的这两套房子户型很相似，只是面积相差了二十多平米。\n女：房屋质量怎么样？\n男：这个绝对有保证，我建议您买大一点儿的那套，住着更舒服。\n女：大的是好，就是这个价格我有点儿承受不了。\n问：女的觉得男的推荐的房子怎么样？ (D 价钱有点儿贵)\n8.   男：昨晚的风实在太大了，顶着风都走不动路了。\n女：我们家对面楼上的广告牌都让风给刮下来了。\n男：太可怕了，没砸着zázhe人吧？\n女：幸亏没有人，不过，警察、消防xiāofáng都来了。\n问：关于昨晚的风，从对话中可以知道什么？ (C 刮坏了广告牌)\n9.   男：擦地时在电视柜下面发现了你的口红。\n女：我说哪儿去了，怎么滚到那里去了。 `\n男：去问你的宝贝儿子吧，昨天幼儿园老师叫他们画一幅妈妈的画儿。\n女：哦，我说呢，画上红嘴唇的颜色怎么那么像我口红的颜色。\n问：女的的口红是在哪儿找到的？ (B 电视柜下)\n10. 女：这个瓶子每次你盖上后，谁都打不开。\n男：我来帮你。\n女：以后你盖上时，别太使劲儿了。\n男：我没有使多大的劲儿，是瓶口沾上了蜂蜜。\n问：女的认为打不开瓶子的原因是什么？(B男的盖得太紧)\n11-12.\n古时候，有个士兵在一次战斗中腿部中了一箭，疼得要命。军官请了一位外科医生来给他治伤。医生看后说：“这个不难！”就拿出一把剪子，把露在外边的箭杆剪掉了，然后要了手术费就要走。士兵着急地说：“剪掉箭杆儿谁不会呀？我要你拔出射进肉里的箭头！”医生摇摇头说：“外科的事我已做完了，挖掉肉里的箭头，那是内科的事。”\n11. 关于那个士兵，可以知道什么？(B腿受伤了)\n12. 关于那个医生，下列哪项正确？(C没解决问题)\n13-14.\n产品销售会上，销售情况极其令人失望，经理对负责销售的职员大声责备道：“我已经看够了你们恶劣的工作表现，听够了你们的那些理由。如果你们无法胜任这项工作，会有人代替你们，卖出这些有价值的产品。”然后，他转身朝一名新招聘来的职员——一个刚刚从足球队退下来的球员——说道：“如果一支球队赢不了球，会怎么样呢？队员们都得被换掉，对吗？”沉默了几秒钟后，这名前足球队员回答道：“实际上，先生，如果整个队伍都有麻烦的话，我们通常只是换个新教练。”\n13. 经理觉得销售人员的业绩怎么样？ (A今天失望)\n14. 经理问前足球队员的目的是什么？(D想吓吓那些销售人员)"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第25课-给自己加满水/index.html#阅读",
    "href": "学汉语的日记/HSK5下-第25课-给自己加满水/index.html#阅读",
    "title": "HSK5下 | 第25课： 给自己加满水",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n26岁的荷兰残疾姑娘莫妮克收到了最棒的圣诞礼物：从轮椅上站起来了。原来，莫妮克曾在一次15B可怕的交通事故中瘫痪，但坚强的莫妮克并没有放弃生活，经过16A刻苦训练，她成为一名残疾运动员，并曾在2008年北京残奥会公路自行车项目中获得亚军。或许是莫妮克这种17D不向命运认输的精神感动了上天，奇迹发生了。在西班牙集训时，一辆全速行驶的自行车将莫妮克撞倒在地。她说，当时自己毫无知觉的双腿感觉像通了电一样，之后奇迹发生了。那18C起事故仿佛打通了莫妮克双腿的“行走按钮”，酥麻感过去后，莫妮克惊奇地发现自己可以从轮椅上站起来了，她虽然走得很艰难，但这却预示着一个全新的未来。\n\nD宋应星的成就和影响与狄德罗相当\n\n宋应星的《天工开物》是一部系统地记载中国古代农业和手工业成就的伟大著作。这本书受到了世界各国的重视，先后被翻译成日、法、德、俄等多种文字。狄德罗是因编写《百科全书》而具有世界影响的法国学者，编写《中国科技史》的英国学者李约瑟称宋应星为“中国的狄德罗”。\n\nC天气阴沉易影响心情\n\n冬季天气阴沉，容易让人感觉精神不愉快。如何赶走坏心情呢？一项最新研究表明,每天只要花上5分钟倾听清晨的鸟叫，就能有效地帮助人们击退负面情绪。如果无法听见真的鸟叫，听鸟叫的录音也能够达到相似的效果。\n21. A信心有助于人发挥才智\n在做一件事前，你是否常在心中对自己说“可能不行吧”“万一怎么样怎么样”，结果可能还没去做，你就没有信心了，事情十有八九就会朝着你设想的不利方向发展。所以，你要相信自己是最优秀的，有了信心，你的能力和智慧才能发挥到最好。\n22. C放松性练习可以帮助消除疲劳\n运动后，可以花5〜10分钟做一些放松性练习，如慢跑、柔软体操、放松按摩等,也可以做一些静态的伸展运动。这些运动被称作积极性休息，有助于促使疲劳的消除和机体的恢复，因此，运动结束阶段也是运动过程中不可忽视的重要阶段。\n23-25.\n一对老夫妇住进了公寓，与我成了对门邻居。\n两位老人都70多岁，入住后极少外出，一些食物和日用品，大都由住在附近的女儿、女婿送来。偶尔与老两口在楼道碰面，他们会对我微笑点头，但很少开口说话。\n那天，我出门上班，刚好看到老头儿在取报纸。彼此点点头后，我无意中看见他家挂在楼道上的报箱并未用锁，而是用铜线弯成圆圈将箱门连接上。取报时，主人稍转动一下铜圈，箱门便打开了。我对他说：“您应买把锁把箱门锁上，以防报纸丢失。”老人听后，点头答应，可事后，我看到他的报箱依然如故。\n在我又一次提示后，老头儿终于开口说话，他指着楼道墙上那成排的报箱说：“锁是用来防小偷的。现在楼里住户大都订了报刊，都有书报阅读，就不会出现家贼；小区里还有保安，丢报纸的可能性很小；再说，邻里之间相处是缘分.大家应该生活在真挚、友爱、信任的环境里。多信任、少戒心，尽量不要人为设置障碍。”\n我听了老头儿一番话，也认真效仿起来。果然，半年多的时间过去了，不愉快的事情从未发生，而我也惊喜地发现，楼道里报箱不用锁的越来越多了。\n23. C搬来时间不久\n24. B信任邻居\n25. C人与人应多些信任\n26-28.\n有一个博士到一家研究所应聘，成为该研究所学历最高的一个人。\n有一天他到单位后面的小池塘去钓鱼，正好正副所长在他的一左一右，也在钓鱼。\n他只是微微点了点头，这两个本科生，有啥好聊的呢？\n不一会儿，正所长放下钓竿，伸伸懒腰，噌噌噌从水面上走到对面上厕所。\n博士眼睛睁得都快掉出来了。水上漂？不会吧！这可是一个池塘啊。\n正所长上完厕所回来的时候，同样也是噌噌噌地从水上“漂”回来的。\n怎么回事？博士生又不好去问，自己是博士生啊！\n过了一阵，副所长也站起来，走几步，同样噌噌噌地漂过水面上厕所。这下子博士更是差点儿晕倒，心想：不会吧，到了一个江湖高手集中的地方？\n博士生紧张得也想上厕所了。这个池塘两边有围墙，要到对面厕所非得绕十分钟的路，而回单位上又太远，怎么办？\n博士生又不愿意去问两位所长，忍了半天后，也起身往水里跨，暗想：我就不信本科生能过的水面，我博士生不能过。\n只听咚的一声，博士生栽到了水里。\n两位所长将他拉了出来，问他为什么要下水，他问：“为什么你们可以走过去呢？”\n两所长相视一笑：“这池塘里有两排木桩子，由于这两天下雨涨水正好没在水面下。我们都知道这木桩的位置，所以可以踩着桩子过去。你怎么不问一声呢？”\n26. A瞧不起正副所长\n27. C从水面上走到对岸\n28. B去对面上厕所"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第25课-给自己加满水/index.html#书写",
    "href": "学汉语的日记/HSK5下-第25课-给自己加满水/index.html#书写",
    "title": "HSK5下 | 第25课： 给自己加满水",
    "section": "2.3. 书写",
    "text": "2.3. 书写\n风险、威胁、慌张、承认、谨慎。\n攀登充满了风险与威胁。面对险峻的山势，慌张是无济于事的，只有承认目前的困难情况，并谨慎地采取行动，才能克服困难，最终攀上峰顶。攀登不仅是挑战自我，更是磨练意志的过程。在克服风险和威胁的过程中，我们学会了冷静和明快，也收获了战胜困难的勇气和信心。(122字)\n险峻（xian3jun4 – steep）；无济于事（not help matters）\n29一个偶然的机会彻底改变了他的命运。\n30 陈工程师也有与老人相似的经历。\n31 李老师的书房简直就是一个小图书馆。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第23课-放手/index.html",
    "href": "学汉语的日记/HSK5下-第23课-放手/index.html",
    "title": "HSK5下 | 第23课： 放手",
    "section": "",
    "text": "放手 | Letting go\n文文从小是个乖乖女，学习刻苦，遵守纪律。大事小事，尽管妈妈表示也要征求她的意见，但上哪所学校、念什么专业，甚至跟什么人交朋友，基本上都是妈妈说了算。\n可是到了大学阶段，亲爱的女儿竟然违反了乖乖女的各种规矩，越来越有自己的主见，越来越能干、独立了。尽管她的成绩仍然是第一名，但她不再甘于当“好学生”：她逃课去听各种讲座，出席欧盟商会的鸡尾酒会，做志愿者，拍电影，学摄影，泡酒吧，参加了学生会并担任了学生会主席，还组织各种社会活动。妈妈以前要她当外交官的计划，在她眼里“实在没什么意思”，她觉得经商才是自己的目标。\n她放弃了本系保送研究生、放弃了各种工作的面试，坚持要去国外留学，结果12所世界名牌大学录取了她。当面临是否选择牛津大学，她们全家开会，爸爸妈妈认为应该去，但文文跟他们的意见不致，她坚持要去美国。\n这一次妈妈让步了。她隐隐约约觉得：自己该完全放手了。没想到，正是妈妈的放手，让风筝越飞越高。\n几年后，文文在美国工作，妈妈去洛杉矶看她。在这个陌生的地方，妈妈感到她们好像交换了某种身份：自己倒像女儿，而文文倒像妈妈。她们建立了一种新的关系。\n最初，妈妈哪儿也不敢去，不能单独出门，不能与人沟通，什么都要靠女儿。后来文文工作忙，就给她地图、车钥匙、机票，鼓励她自己出去。从家门口的超市开始，妈妈越走越远，最后竟然独自把美国横穿了一遍。她说，是女儿的“放手”，让她走得更远。做妈妈的，这才算是真正明白了“放手”的重要。\n2010年3月的一个夜晚，母女俩躺在夏威夷的沙滩上谈心。妈妈第一次为以前对女儿的“不放手”而道歉。文文沉默了很久，最后吻了妈妈一下，轻轻地说：“妈妈，我真的很喜欢现在的你。”妈妈忍𣎴住流下了眼泪。她说：“幸亏那晚天色很暗。\n改编自《中国青年报》，作者：从玉华\n\n\n\n放手\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n乖\nguāi - adj. obedient, well-behaved\n\n\n2\n刻苦\nkè kǔ - adj. hardworking, assiduous\n\n\n3\n遵守\nzūn shǒu - v. to abide by, to observe\n\n\n4\n纪律\njì lǜ - n. discipline, rule\n\n\n5\n征求\nzhēng qiú - v. to seek, to ask for\n\n\n6\n念\nniàn - v. to study, to read outloud\n\n\n7\n基本\njīběn - adv. basically, on the whole\n\n\n8\n阶段\njiē duàn - n. stage, phase\n\n\n9\n亲爱\nqīn ài - adj. dear, beloved\n\n\n10\n违反\nwéi fǎn - v. to violate, to go against\n\n\n11\n规矩\nguī ju - n. rule, established practice\n\n\n12\n能干\nnéng gàn - adj. capable\n\n\n13\n讲座\njiǎng zuò - n. lecture\n\n\n14\n出席\nchūxí - v. to attend, to be present\n\n\n15\n酒吧\njiǔ bā - n. bar (for drinks)\n\n\n16\n担任\ndān rèn - v. to serve as, to hold the post of\n\n\n17\n主席\nzhǔ xí - n. chairperson\n\n\n18\n组织\nzǔ zhī - v. to organize\n\n\n19\n外交\nwàijiāo - n. diplomacy\n\n\n20\n经商\njīng shāng - v. to do business, to engage in trade\n\n\n21\n目标\nmù biāo - n. goal, objective\n\n\n22\n系\nxì - n. department (of a university)\n\n\n23\n名牌\nmíng pái - n. famous brand\n\n\n24\n录取\nlù qǔ - v. to enroll, to admit\n\n\n25\n面临\nmiàn lín - v. to face, to confront\n\n\n26\n一致\nyīzhì - adj. identical, unanimous\n\n\n27\n让步\nràng bù - v. to concede, to give in\n\n\n28\n隐约\nyǐn yuē - adj. indistinct, vague\n\n\n29\n陌生\nmò shēng - adj. strange, unfamiliar\n\n\n30\n某\nmǒu - pron. some, certain\n\n\n31\n建立\njiànlì - v. to build, to establish\n\n\n32\n单独\ndān dú - adv. alone, by oneself\n\n\n33\n沟通\ngōu tōng - v. to communicate\n\n\n34\n横\nhéng - adj. across, horizontal\n\n\n35\n沙滩\nshā tān - n. sand beach\n\n\n36\n沉默\nchén mò - v. to be silent\n\n\n37\n吻\nwěn - v. to kiss\n\n\n38\n忍不住\nrěn bu zhù - cannot help (doing sth.)\n\n\n39\n幸亏\nxìng kuī - adv. fortunately\n\n\n40\n暗\nàn - adj. dark, dim\n\n\n\n\n\n\n\n一致\n\n“一致” Tính từ, biểu thị không có sự chia rẽ, bất đồng. Ví dụ:\n\n·····但文文跟他们的意见不一致，她坚持要去美国。\n长期共同生活的夫妻在兴趣爱好，心理情绪上也趋于一致。\n\n“一致” Cũng có thể làm phó từ, biểu thị cùng nhau. Ví dụ:\n\n双方一致表示将进一步发展友好合作关系。\n专家们一致认为这是一种成功的产品，可以放心使用。\n\n\n某\n\n“某” Đại từ chỉ thị , thường chỉ một người hoặc một vật nhất định, bình thường dùng sau họ (họ tên) , biểu thị biết tên nhưng không nói ra，có lúc mang nghĩa xúc phạm. Ví dụ:\n\n公司业员季某闻之大喜，以为自己碰到了一个大买主。\n在公园的墙上写“某某到此一游”之类的行为是极不文明的。\n\n“某” Cũng có thể chỉ người hoặc vật không xác định. Ví dụ:\n\n人们如果长期进行某一方面的训练，就可以使大脑在某一方面的反应能力提高。\n在这个陌生的地方，妈妈感到她们好像交换了某种身份：自己倒像女儿，而文文倒像妈妈。\n\n\n幸亏\n\nPhó từ, biểu thị do một vài nguyên nhân nào đó mà tránh được một số chuyện không hy vọng sẽ phát sinh. Ví dụ:\n\n幸亏你提醒了我，我今天就去报名。\n医生说这个病人是心脏问题，幸亏送来得及时。\n妈妈忍不住流下了眼泪。她说：“幸亏那晚天色很暗”。\n\n\nPhân biệt 单独 và 独自\n\n共同点：Đều có thể làm phó từ, có nghĩa là một mình.\n\n如：你太年轻了，恐怕不能单独/独自一人完成这个任务。\n\n不同点：\n\n不同点\n\n\n\n\n\n\n\nNo.\n单独\n独自\n\n\n\n\n1\nThiên về nghĩa là không cùng với người khác.\n如：你下午有时间吗？我想和你单独谈谈。\nThiên về nghĩa tự mình làm việc gì đó.\n如：孩子饿得等不及爸爸了，就独自先吃了起来。\n\n\n2\nCó thể dùng với sự vật.\n如：做这个菜时，鸡蛋要先单独炒好备用。\nKhông thể dùng với sự vật\n\n\n3\nCó thể làm tính từ, trong câu thì làm định ngữ.\n如：本科生上课有单独的教室。\nKhông thể làm tính từ.\n\n\n\n\n\n\n\n教学和学术\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n教材\njiao4cai2 - learning material, tài liệu học tập\n\n\n2\n课程\nke4cheng2 - curriculum, chương trình dạy\n\n\n3\n实习\nshi2xi2 - internship, thực tập\n\n\n4\n学历\nxue2li4 - education, học lực, background\n\n\n5\n本科\nben3ke4 - undergraduate, đại học\n\n\n6\n系\nxi4 - falcuty, bộ môn, khoa\n\n\n7\n讲座\njiang3zuo4 - lecture, bài giảng\n\n\n8\n学术\nxue2shu4 - academic, học thuật, học tập, learning\n\n\n9\n学问\nxue2wen4 - learning, knowledge, học vấn\n\n\n10\n理论\nli3lun4 - lý luận, theory\n\n\n11\n资料\nzi1liao4 - tư liệu, data\n\n\n12\n修改\nxiu1gai3 - modify, điều chỉnh, thay đổi\n\n\n13\n发表\nfa1biao3 - phát biểu\n\n\n\n\n\n\n我想对父母说的是。。。\n“谢谢”是我一直想对我父母说的话。我在农村地区出生和长大，学习条件无法与城市相比，但是父母总是给我最好的学习条件。我父母都是严格的人，但绝对不是老式的父母。所有与学习、生活有关的问题，如选择大学专业或计划将来的工作，父母总是以贡献者的资格参与，从不强迫孩子。因此，我为出色教育这般的父母感到幸运。\n\n\n\n\n在学习问题上，你和父母有过争吵吗?\n\n在学习问题上，我和父母很少有争吵。我从小就是一个比较喜欢学习而不是玩耍的人，总把学习放在第一位。我的父母在这方面对我始终放心。\n\n你和父母交流时，你感觉你们是平等的吗?\n\n我和父母交流时，我感觉父母总是尊重我的观点，我也是这样的。在我生命中几个重要的时刻，我和父母也有争论。比如，当我选择大学和专业时，父母希望我成为士兵或菩察，觉得那样我会有稳定的生活。但我一直喜欢自由，想要学习经济学。当我选择行业时，他们想让我我当公务员。我两次都坚持说服他们，最后，父母都尊重我的决定。父母在我做出的每一个决定中都扮演着提供建议的角色，但从来没有强迫我服从他们的意愿。\n\n当你遐到问题或犯了错误时，父母是怎么帮助你的?举例说明。\n\n每次我犯了错误时，我的父母都会试图讲解我做错了什么、有多严重、我应该怎么做才是对的。然后，他们会再提醒我，让我记住那个错误，以未来不会重演。举个例子，我小时候写得很好，我父母总是为此感到自豪。有一次，当我上小学三年级的时候，我因为贪玩儿写得太草率了，结果只得了3分。我爸爸虽然很生气，但还冷清地向我说明，如果小事做得粗心，以后就不能有尊严做大事了。然后他把我写的那张纸剪出来，贴在墙上。这样每夭我都看到它，提醒我做事要用心。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第23课-放手/index.html#生词",
    "href": "学汉语的日记/HSK5下-第23课-放手/index.html#生词",
    "title": "HSK5下 | 第23课： 放手",
    "section": "",
    "text": "放手\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n乖\nguāi - adj. obedient, well-behaved\n\n\n2\n刻苦\nkè kǔ - adj. hardworking, assiduous\n\n\n3\n遵守\nzūn shǒu - v. to abide by, to observe\n\n\n4\n纪律\njì lǜ - n. discipline, rule\n\n\n5\n征求\nzhēng qiú - v. to seek, to ask for\n\n\n6\n念\nniàn - v. to study, to read outloud\n\n\n7\n基本\njīběn - adv. basically, on the whole\n\n\n8\n阶段\njiē duàn - n. stage, phase\n\n\n9\n亲爱\nqīn ài - adj. dear, beloved\n\n\n10\n违反\nwéi fǎn - v. to violate, to go against\n\n\n11\n规矩\nguī ju - n. rule, established practice\n\n\n12\n能干\nnéng gàn - adj. capable\n\n\n13\n讲座\njiǎng zuò - n. lecture\n\n\n14\n出席\nchūxí - v. to attend, to be present\n\n\n15\n酒吧\njiǔ bā - n. bar (for drinks)\n\n\n16\n担任\ndān rèn - v. to serve as, to hold the post of\n\n\n17\n主席\nzhǔ xí - n. chairperson\n\n\n18\n组织\nzǔ zhī - v. to organize\n\n\n19\n外交\nwàijiāo - n. diplomacy\n\n\n20\n经商\njīng shāng - v. to do business, to engage in trade\n\n\n21\n目标\nmù biāo - n. goal, objective\n\n\n22\n系\nxì - n. department (of a university)\n\n\n23\n名牌\nmíng pái - n. famous brand\n\n\n24\n录取\nlù qǔ - v. to enroll, to admit\n\n\n25\n面临\nmiàn lín - v. to face, to confront\n\n\n26\n一致\nyīzhì - adj. identical, unanimous\n\n\n27\n让步\nràng bù - v. to concede, to give in\n\n\n28\n隐约\nyǐn yuē - adj. indistinct, vague\n\n\n29\n陌生\nmò shēng - adj. strange, unfamiliar\n\n\n30\n某\nmǒu - pron. some, certain\n\n\n31\n建立\njiànlì - v. to build, to establish\n\n\n32\n单独\ndān dú - adv. alone, by oneself\n\n\n33\n沟通\ngōu tōng - v. to communicate\n\n\n34\n横\nhéng - adj. across, horizontal\n\n\n35\n沙滩\nshā tān - n. sand beach\n\n\n36\n沉默\nchén mò - v. to be silent\n\n\n37\n吻\nwěn - v. to kiss\n\n\n38\n忍不住\nrěn bu zhù - cannot help (doing sth.)\n\n\n39\n幸亏\nxìng kuī - adv. fortunately\n\n\n40\n暗\nàn - adj. dark, dim"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第23课-放手/index.html#注释",
    "href": "学汉语的日记/HSK5下-第23课-放手/index.html#注释",
    "title": "HSK5下 | 第23课： 放手",
    "section": "",
    "text": "一致\n\n“一致” Tính từ, biểu thị không có sự chia rẽ, bất đồng. Ví dụ:\n\n·····但文文跟他们的意见不一致，她坚持要去美国。\n长期共同生活的夫妻在兴趣爱好，心理情绪上也趋于一致。\n\n“一致” Cũng có thể làm phó từ, biểu thị cùng nhau. Ví dụ:\n\n双方一致表示将进一步发展友好合作关系。\n专家们一致认为这是一种成功的产品，可以放心使用。\n\n\n某\n\n“某” Đại từ chỉ thị , thường chỉ một người hoặc một vật nhất định, bình thường dùng sau họ (họ tên) , biểu thị biết tên nhưng không nói ra，có lúc mang nghĩa xúc phạm. Ví dụ:\n\n公司业员季某闻之大喜，以为自己碰到了一个大买主。\n在公园的墙上写“某某到此一游”之类的行为是极不文明的。\n\n“某” Cũng có thể chỉ người hoặc vật không xác định. Ví dụ:\n\n人们如果长期进行某一方面的训练，就可以使大脑在某一方面的反应能力提高。\n在这个陌生的地方，妈妈感到她们好像交换了某种身份：自己倒像女儿，而文文倒像妈妈。\n\n\n幸亏\n\nPhó từ, biểu thị do một vài nguyên nhân nào đó mà tránh được một số chuyện không hy vọng sẽ phát sinh. Ví dụ:\n\n幸亏你提醒了我，我今天就去报名。\n医生说这个病人是心脏问题，幸亏送来得及时。\n妈妈忍不住流下了眼泪。她说：“幸亏那晚天色很暗”。\n\n\nPhân biệt 单独 và 独自\n\n共同点：Đều có thể làm phó từ, có nghĩa là một mình.\n\n如：你太年轻了，恐怕不能单独/独自一人完成这个任务。\n\n不同点：\n\n不同点\n\n\n\n\n\n\n\nNo.\n单独\n独自\n\n\n\n\n1\nThiên về nghĩa là không cùng với người khác.\n如：你下午有时间吗？我想和你单独谈谈。\nThiên về nghĩa tự mình làm việc gì đó.\n如：孩子饿得等不及爸爸了，就独自先吃了起来。\n\n\n2\nCó thể dùng với sự vật.\n如：做这个菜时，鸡蛋要先单独炒好备用。\nKhông thể dùng với sự vật\n\n\n3\nCó thể làm tính từ, trong câu thì làm định ngữ.\n如：本科生上课有单独的教室。\nKhông thể làm tính từ."
  },
  {
    "objectID": "学汉语的日记/HSK5下-第23课-放手/index.html#扩展",
    "href": "学汉语的日记/HSK5下-第23课-放手/index.html#扩展",
    "title": "HSK5下 | 第23课： 放手",
    "section": "",
    "text": "教学和学术\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n教材\njiao4cai2 - learning material, tài liệu học tập\n\n\n2\n课程\nke4cheng2 - curriculum, chương trình dạy\n\n\n3\n实习\nshi2xi2 - internship, thực tập\n\n\n4\n学历\nxue2li4 - education, học lực, background\n\n\n5\n本科\nben3ke4 - undergraduate, đại học\n\n\n6\n系\nxi4 - falcuty, bộ môn, khoa\n\n\n7\n讲座\njiang3zuo4 - lecture, bài giảng\n\n\n8\n学术\nxue2shu4 - academic, học thuật, học tập, learning\n\n\n9\n学问\nxue2wen4 - learning, knowledge, học vấn\n\n\n10\n理论\nli3lun4 - lý luận, theory\n\n\n11\n资料\nzi1liao4 - tư liệu, data\n\n\n12\n修改\nxiu1gai3 - modify, điều chỉnh, thay đổi\n\n\n13\n发表\nfa1biao3 - phát biểu"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第23课-放手/index.html#运用",
    "href": "学汉语的日记/HSK5下-第23课-放手/index.html#运用",
    "title": "HSK5下 | 第23课： 放手",
    "section": "",
    "text": "我想对父母说的是。。。\n“谢谢”是我一直想对我父母说的话。我在农村地区出生和长大，学习条件无法与城市相比，但是父母总是给我最好的学习条件。我父母都是严格的人，但绝对不是老式的父母。所有与学习、生活有关的问题，如选择大学专业或计划将来的工作，父母总是以贡献者的资格参与，从不强迫孩子。因此，我为出色教育这般的父母感到幸运。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第23课-放手/index.html#口语",
    "href": "学汉语的日记/HSK5下-第23课-放手/index.html#口语",
    "title": "HSK5下 | 第23课： 放手",
    "section": "",
    "text": "在学习问题上，你和父母有过争吵吗?\n\n在学习问题上，我和父母很少有争吵。我从小就是一个比较喜欢学习而不是玩耍的人，总把学习放在第一位。我的父母在这方面对我始终放心。\n\n你和父母交流时，你感觉你们是平等的吗?\n\n我和父母交流时，我感觉父母总是尊重我的观点，我也是这样的。在我生命中几个重要的时刻，我和父母也有争论。比如，当我选择大学和专业时，父母希望我成为士兵或菩察，觉得那样我会有稳定的生活。但我一直喜欢自由，想要学习经济学。当我选择行业时，他们想让我我当公务员。我两次都坚持说服他们，最后，父母都尊重我的决定。父母在我做出的每一个决定中都扮演着提供建议的角色，但从来没有强迫我服从他们的意愿。\n\n当你遐到问题或犯了错误时，父母是怎么帮助你的?举例说明。\n\n每次我犯了错误时，我的父母都会试图讲解我做错了什么、有多严重、我应该怎么做才是对的。然后，他们会再提醒我，让我记住那个错误，以未来不会重演。举个例子，我小时候写得很好，我父母总是为此感到自豪。有一次，当我上小学三年级的时候，我因为贪玩儿写得太草率了，结果只得了3分。我爸爸虽然很生气，但还冷清地向我说明，如果小事做得粗心，以后就不能有尊严做大事了。然后他把我写的那张纸剪出来，贴在墙上。这样每夭我都看到它，提醒我做事要用心。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第23课-放手/index.html#听力",
    "href": "学汉语的日记/HSK5下-第23课-放手/index.html#听力",
    "title": "HSK5下 | 第23课： 放手",
    "section": "2.1. 听力",
    "text": "2.1. 听力\n1.  女：这件事情，我建议你先回去征求一下父母的意见。\n男：老师，我相信父母会支持我的。\n问：对这件事情，老师的看法是什么？（D应该问父母的意见）\n2.  男：能被牛津大学这样的世界名校录取多好啊！你怎么能放弃呢？\n女：牛津大学当然很好，但是我想去美国。\n问：女的是什么意思？（C他不想去牛津大学）\n3.  女：我们终于取得一致了，真是太不容易了。\n男：大家的目标从来都是一致的，都是为了公司好。\n问：男的是什么意思？（C大家的目的是一样的）\n4.  男：小王，你怎么不说说你的看法？\n女：你不允许我们说不同意，我只好沉默。\n问：女的是什么态度？（C不满）\n5.  女：你跟文文现在发展到什么程度了？\n男：昨天我第一次吻了她。\n问：男的跟文文最可能是什么关系？（B恋人）\n6.  男：你听说了吗，昨天34路车出事了！\n女：是啊，我回家经常坐那趟车呢！幸亏昨天加班走得晚，打车回去的。\n问：女的昨天为什么没坐34路车？（D他昨天加班了）\n7.  女：你都这么大年纪了，还这么辛苦干什么？把生意交给儿子吧。\n男：他要是靠得住，我早就交给他了。\n女：儿子有什么不好？你总是不信任他！\n男：你还说，都是你把他惯坏了！我说往东，他就偏要（）往西！\n问：说话人最可能是什么关系？（C夫妻）\n8.  男：妈妈，我去上班的时候，您自己也可以出去走走，别整天关在家里。\n女：我一句外语也不会说，怎么出门啊？\n男：放心吧，您先去附近的超市看看。我给您准备好地图，写上我们家的地址，如果找不到您就拿给别人看。\n女：那我明天试试吧。\n问：女的为什么不愿意出门？（B语言不通）\n9.  女：这次能够获奖，您有什么想说的吗？\n男：能够得到这个奖，全靠大家的肯定与帮助，还有领导的关心，甚至还有善意的批评。\n女：那您最想感谢的是谁？\n男：最想感谢的是我的家人。如果没有他们的鼓励与支持，我是不可能坚持下来的。\n问：男的最想感谢什么？（C家人的支持）\n10.男：上课了，小明怎么站在教室外面？\n女：一定是他又淘气，被李老师罚站了。\n男：就算孩子违反了纪律，也不能体罚！\n女：嗯（ng4 - ưn），下课后我跟李老师谈谈。\n问：男的是什么意思？（C老师不该体罚小明）\n11-12.\n孩子的教育是全社会特别关心的一个问题。很多家长在教育孩子的问题上，都学了很多理论，有自己的主张，听起来都很有道理。但是在实际生活中，父母们往往又不按照这些理论去做。他们说：道理很简单，我工作太忙了，实在没有那么多时间按照那些理论去教育孩子，我希望学校、老师能去完成这些事情。\n11.全社会都特别关心什么问题？（A孩子的教育问题）\n12.家长们为什么不按照教育理论去做？（D他们没有时间这么做）\n13-14.\n我自己是个数学老师，但我女儿小时候特别不喜欢数学，成绩很差。有一天，我给女儿出了10道题，女儿竟然做错了9道。我并没有生气，相反，对她大加称赞：“这么难的题目，你竟然也能做对！我小时候可是一道都做不出啊！”第二天晚上，我又准备了10道难度降低了的题目，再让女儿做，结果这次做对了5道。我鼓励女儿说：“天哪，你真是太了不起了！一天之内，你可以进步这么大！”第三天，女儿自己主动要求：“爸爸，今晚我们还做数学题吧！”半年之后，女儿成了班里的数学课代表。\n13.女儿第二天做题时为什么对得比较多？（C题目容易了）\n14.这段话主要谈什么？（B孩子需要鼓励）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第23课-放手/index.html#阅读",
    "href": "学汉语的日记/HSK5下-第23课-放手/index.html#阅读",
    "title": "HSK5下 | 第23课： 放手",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n有人哭，是因为伤心；有人哭，是因为激动。而我15B最难忘的那一次哭，是因为同学们带给我的感动。\n那天我要代表我们16A系参加全校的书法比赛，没想到，就在比赛的那天早上,我突然发起了高烧。宿舍的同屋都劝我不要勉强，不过我还是坚持走进了赛场。但是,身体的不适的确影响了我的状态，最后写出来的字，完全不像练了十几年书法的人写的。我知道，这次我输了。可是同学们都没有笑我批评我，反而安慰我说：“没关系，以后还有机会，我们都相信你的17C能力。”听了大家的话，我18D忍不住哭了。\n\nB女儿积极参加社会活动\n\n到了大学阶段，亲爱的女儿竟然违反了”乖乖女”的各种规矩，越来越有自己的主见，越来越能干、独立了。尽管她的成绩仍然是第一名，但她不再甘于当“好学生”：她逃课去听讲座，参加欧盟商会的鸡尾酒会，做志愿者，拍电影，学摄影，泡酒吧，竞选并担任了学生会主席，组织各种社会活动。\n\nA单亲妈妈不应该让孩子觉得爸爸很坏\n\n谈到单亲家庭的教育问题，专家说，有些离了婚的妈妈一个人带孩子很辛苦，于是她们就把对孩子爸爸的不满灌输给孩子，让孩子觉得爸爸很坏，这样会严重地影响孩子的心理健康。因此，母亲自身的文化修养很重要。\n\nC父母应该努力让孩子理解自己的要求\n\n今晚我们的《父与子》栏目，让四对父子面对面说出了自己的心里话。希望所有的父子都能够像他们一样，多交流，多沟通，加强双方的理解。父母可以不同意孩子的观点，但要给他们说出来的机会，让双方取得共识，而不应强迫孩子顺从自己的意愿。\n\nD师范教育的水平会影响国家的经济文化建设\n\n未来社会将是一个更加重视知识的”学习化社会”，人人都需要受教育，因此，教育将成为最大的行业之一。其中，师范教育又尤为重要，因为师范教育的质量关系到教师的质量，教师的质量又影响着学生的质量。从这个意义上说，办好师范教育，对提高一个国家整体的国民素质，进而促进国家经济的发展、推动政治文化的建设，都起着举足轻重的作用。\n23-25.\n大三下学期，就读于西安交通大学的徐达士得到了去芬兰交换学习一年的机会。\n芬兰有无边无际的森林、星罗棋布的湖泊。徐达士说：“在芬兰，感受到更多的是平静以及人与自然的相互依赖，我很喜欢那种与自然亲近的生活状态。”\n由于是交换生，选课相对自由。基于自己的兴趣爱好，他学习了文化人类学、跨文化交流、芬兰语等课程。开朗的性格使他很快融入了当地人的生活，暑假他还和朋友们一起去了俄罗斯、德国、荷兰等地。\n“那一年的交换生经历让我结识到了很多来自世界各地的学生、学者，算是第一次体验到了跨文化交流，交到了几个好朋友。他们来自荷兰、希腊、匈牙利，当然还有芬兰。”\n当谈到选择到美国读研究生时，徐达士说：“美国机会更多，研究生教育在世界范围内更知名，而且美国文化更加多样化。在芬兰的学生食堂，可能95%都是芬兰人，而在美国的食堂，你可以看到更多不同国家、不同文化背景的人。”\n然而，“交换生和研究生是两个完全不同的角色，交换时过得很潇洒，压力小，到处旅行。去美国读研要忙很多，过得更紧凑。”去年暑假，徐达士在国际救援委员会做了三个月的志愿实习。在平时，他也会去做些其他工作，如给老师做助研、教汉语、组织汉语角、帮图书馆录数据等。\n现在就读于美国约翰・霍普金斯大学公共政策专业的他，对未来充满了希望，期望自己能够领略不同的风景，体验不一样的人生。\n\nC研究生\nB在芬兰学习压力比较小\nD他想经历不一样的生活\n\n26-28.\n有二个孩子在树林里玩儿，都不小心被树枝划破了裤子。面对裤腿上的破洞和孩子不安的脸，三位母亲用不同的态度处理了这件事情。\n第一位母亲大声教训了孩子之后，用一根线绳像系麻袋一样把那个破洞扎紧，整条裤腿因此显得皱皱巴巴。破洞是没有了，取而代之的那个结却像孩子撅起的小嘴。孩子也因此受到严厉的警告：“今后再也不准到树林里去玩儿！”\n第二位母亲不打也不骂，默默地把那个破洞一针一线缝补好,裤子上留下了针线的痕迹。\n第三位母亲面对孩子裤腿上的破洞，安慰孩子：“不要紧，哪个小孩子不贪玩儿，你奶奶说你爸爸小时候比你还调皮呢。”她把孩子的裤子脱下来，用彩线在破洞上绣了朵漂亮的小花，好像原本那里就有一朵花。孩子笑得好开心。\n同样的问题，因为用了三种不同的解决办法，就导致了不同的结果：第一位母亲让孩子感到恐惧和失望，那皱巴巴的裤腿就如同母亲脸上写满的愤怒，孩子不得不活在母亲强制的意愿中；第二位母亲平平常常，孩子得到的是一个顺其自然的生活环境；第二位母亲是最优秀的教育家，她用裤子上的花朵启发了孩子美好的想象，她脸上灿若朝阳的微笑给了孩子更多的宽容，让孩子在成长的路上充满自信并富有创造力。\n\nC第三位\nC批评警告了孩子\nB自信心"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第23课-放手/index.html#书写",
    "href": "学汉语的日记/HSK5下-第23课-放手/index.html#书写",
    "title": "HSK5下 | 第23课： 放手",
    "section": "2.3. 书写",
    "text": "2.3. 书写\n目标，单独，基本，面临，幸亏。\n我认为想要在人生中不断发展，我们需要不断设定目标。在面临挑战时，目标就是我们尽力克服困难的动力。在大多数情况下，我们需要独立面对挑战。但如果有别人的帮助，事情会容易得多。幸亏我们不总要单独面对困难，总是有家人和朋友在我们身边。所以在生活中我们感到迷失方向的某个阶段，应该为自己设定一个目标，当然要有实际基本，然后努力的实现它。生活便会好起来。（171字）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第21课-汉字叔叔 - 一个美国人的汉字情缘/index.html",
    "href": "学汉语的日记/HSK5下-第21课-汉字叔叔 - 一个美国人的汉字情缘/index.html",
    "title": "HSK5下 | 第21课： 汉字叔叔：一个美国人的汉字情缘",
    "section": "",
    "text": "汉字叔叔：一个美国人的汉字情缘 | Uncle Hanzi: The predestined love of an American for Chinese characters\n1972年，22岁的理查德·希尔斯爱上了中文，但是他感觉汉字很复杂，汉字的一笔一画没有任何逻辑，只能死记硬背。一个偶然的机会，他发现如果了解汉字的来源和演变过程，再学习它就变得轻松、容易。但是他遗憾地发现，几乎没有一本英文书能充分解释汉字的字源。\n1994年，理查德得了心脏病，当时医生说他剩下的时间可能不多了。那时，他开始思考自己的人生，“我该怎么办？我该做什么？”“如果只能活24小时，我会打电话和朋友们说再见；如果我还能活一年，我要抓紧时间尽快把《说文解字》电脑化。就这样，一部部古汉字经典进入他的资料库，仅仅复印、整理和把这些资料输入电脑就用了8年。2002年元旦，战胜疾病的他决定把自己创办的网站公开，让更多喜欢中文的人在学习汉字时，不再像他最初那样学得那么痛苦。\n2011年，有人把他的故事放到微博上，引起了广泛关注，他也因此被网友亲切地称呼为“汉字叔叔”\n打开汉字叔叔克服种种困难、花光全部积蓄创办的网站，可以看到他收集整理的近10万个汉字，包含了它们演变的全部字形，当然也包括繁体宇形和简体字形，还有普通话和部分方言读音、英文释义等内容，被网友称赞为“有图有真相”。更让人佩服的是，汉宇叔权将网站上\n的内容全部开放给网友免费下载。\n现在，有很多单位向理查德发出了工作邀请，而理查德选择了去北京师范大学教书，因为那里也有人在做汉字识别查询的研究。在北师大，他除了教物理，还有充分的时间继续研究他的汉字，完善他的网站。\n在中国，60多岁已经是退休的年纪了。但汉字叔叔每天的日程却安排得很满。他说：“我不会退休，我还要继续追求我的梦想，我要‘活到老，学到老’。\n改编自《中国电视报》，作者：孙莲莲 Tôn Liên Liên\n\n\n\n汉字叔叔\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n情缘\nqíngyuán - n. predestined love, sentimental bond\n\n\n2\n逻辑\nluó ji - n. logic\n\n\n3\n死记硬背\nsǐ jì yìnɡ bèi - to memorize mechanically without understanding\n\n\n4\n偶然\nǒurán - adj./adv. accidental; by chance\n\n\n5\n演变\nyǎnbiàn - v. to change, to evolve\n\n\n6\n遗憾\nyí hàn - adj./n. regretful; deep regret\n\n\n7\n心脏\nxīnzàng - n. heart\n\n\n8\n思考\nsī kǎo - v. to think deeply, to ponder\n\n\n9\n抓紧\nzhuā jǐn - v. to firmly grasp\n\n\n10\n尽快\njǐn kuài - adv. as soon as possible\n\n\n11\n经典\njīng diǎn - n./adj. classics; classical\n\n\n12\n库\nkù - n. storehouse, bank\n\n\n13\n输入\nshū rù - v. to input\n\n\n14\n元旦\nyuándàn - n. New Year’s Day\n\n\n15\n疾病\njíbìng - n. disease, illness\n\n\n16\n创办\nchuàngbàn - v. to establish, to set up\n\n\n17\n公开\ngōng kāi - v./adj. to make known to the public; open\n\n\n18\n最初\nzuìchū - n. first, earliest\n\n\n19\n痛苦\ntòng kǔ - adj. painful, suffering\n\n\n20\n微博\nwēi bó - n. microblog\n\n\n21\n称呼\nchēng hu - v./n. to call, to address; form of address\n\n\n22\n克服\nkè fú - v. to overcome, to conquer\n\n\n23\n收集\nshōují - v. to collect, to gather\n\n\n24\n包含\nbāo hán - v. to contain, to include\n\n\n25\n繁体字\nfán tǐ zì - n. complex form, traditional Chinese characters\n\n\n26\n简体字\njiǎn tǐ zì - n. simplified form, simplified Chinese characters\n\n\n27\n方言\nfāng yán - n. dialect\n\n\n28\n称赞\nchēng zàn - v. to praise, to commend\n\n\n29\n真相\nzhēn xiàng - n. truth, fact\n\n\n30\n佩服\npèi fú - v. to admire\n\n\n31\n开放\nkāi fàng - v. to open to the public\n\n\n32\n下载\nxiàzài - v. to download\n\n\n33\n单位\ndānwèi - n. company, employer\n\n\n34\n识别\nshíbié - v. to recognize, to identify\n\n\n35\n查询\ncháxún - v. to search, to retrieve\n\n\n36\n物理\nwù lǐ - n. physics\n\n\n37\n完善\nwán shàn - v./adj. to make perfect, to improve; perfect\n\n\n38\n退休\ntuì xiū - v. to retire\n\n\n39\n日程\nrìchéng - n. schedule\n\n\n40\n追求\nzhuī qiú - v. to pursue, to go after\n\n\n41\n梦想\nmèng xiǎng - n./v. dream; to dream\n\n\n\nBonus\n\n《说文解字》：thuyết văn giải tự\n师范（shi1fan4）n.: pedagogical, sư phạm\n预料（yu4liao4）n.：epxectation, dự liệu\n\n\n\n\n\n硬 ying4\n\n“硬” Phó từ, biểu thị kiên quyết hoặc cố chấp làm một việc gì đó. Ví dụ:\n\n在中国历史故事“指鹿为马”中，赵高把鹿硬说成马。\n·····但是他感觉汉子很复杂，汉字的一笔一画没有任何逻辑只能死记硬背。\n\n“硬” Khi làm phó từ còn biểu thị không đủ năng lực nhưng vẫn tiếp tục tận lực để làm một việc gì đó. Ví dụ:\n\n你不知道这一年我是怎么硬挺过来的。\n虽然中药汤有点儿苦，但为了治病，他还是硬把它喝下去了。\n\n\n偶然\n\n“偶然” (ngẫu nhiên/ bất ngờ) tính từ, biểu thị sự việc phát sinh ngoài dự đoán hoặc dựa vào quy luật thông thường thì không có khả năng phát sinh. Ví dụ:\n\n一个偶然的机会，他发现如果了解汉字的来源和演变过程，再学习它就变得轻松、容易。\n虽然桂花偶然也能长成18米高的大树，但是绝大多数情况下它们都很矮。\n\n“偶然” cũng có thể làm phó từ, có nghĩa là “偶尔，有时候”(thỉnh thoảng/ có lúc). Ví dụ:\n\n她专心地织着毛衣，偶然也会抬眼看一下墙上的挂钟。\n那些我生活过的地方，偶然也会在我梦中出现，但都不是我的“家”！\n\n\n尽快\n\n“尽快” Phó từ, có nghĩa là nhanh hết mức có thể. Ví dụ:\n\n·····我要抓紧时间尽快把《说文解字》电脑化。\n新产品出了点儿问题，你和严经理尽快商量一下这事。\n趁这两天天气好，您尽快把过季的衣服洗一洗，收起来。\n\n\nPhân biệt 偶然 và 偶尔\n\n共同点：Đều có thể là phó từ, đều có nghĩa là không thường xuyên, có lúc có thể dùng thay thế cho nhau, nhưng ý nghĩa có chút không giống.\n\n如：在校园里，我偶然/偶尔也会碰到李艳。\n\n不同点：\n\n不同点\n\n\n\n\n\n\n\n\n偶然\n偶尔\n\n\n\n\n1\nThiên về biểu thị có chút đột ngột, không nghĩ đến, trái ngược với 必然.\n如：这本书是她一次逛书市时偶然发现的。\nThiên về nhấn mạnh số lần ít, trái ngược với 经常.\n如：我平时加班不多，月底偶尔有一两天。\n\n\n2\nCòn có thể biểu thị sự việc phát sinh ngoài ý muốn hoặc không thể xảy ra theo quy luật thông thường. Có thể làm định ngữ, vị ngữ, đằng trước có thể kết hợp với phó từ chỉ mức độ.\n如：李阳的父亲是一位画家，所以，李阳喜欢画画儿并非偶然。\nCòn có thể là từ thuộc tính, chỉ làm định ngữ, đằng trước không thể thêm phó từ chỉ mức độ, cũng không thể làm vị ngữ. Cách dùng này không thường được sử dụng.\n如：他在农村的生活很单调，偶尔的聚会还是在村里的老房子里举行，很无聊。\n\n\n\n\n\n\n\n学科和电脑\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n哲学\nzhe2xue2 - phisolophy, triết học\n\n\n2\n化学\nhua4xue2 - chemistry, hóa học\n\n\n3\n物理\nwu4li3 - physics, vật lí\n\n\n4\n政治\nzheng4zhi4 - politicals, chính trị\n\n\n5\n粘贴\nzhan1tie1 - paste, dán\n\n\n6\n复制\nfu4zhi4 - copy\n\n\n7\n浏览\nliu2lan3 - browse, duyệt\n\n\n8\n删除\nshan1chu2 - delete, xóa\n\n\n9\n搜索\nsou1suo3 - search, tìm kiếm\n\n\n10\n文件\nwen2jian4 - file\n\n\n11\n软件\nruan3jian4 - software, phần mềm\n\n\n12\n操作\ncao1zuo4 - operation, hoạt động\n\n\n\n\n\n\n我为什么学汉语\n对我来说，学汉语产生了深远的影响。首先，汉语作为全球最普遍的语言之一，掌握它对未来的职业发展打开了新的大门。中国经济的崛起使汉语在国际商务中变得尤为重要。其次，学习汉语是一项认知挑战，拓展了我的语言能力和逻辑思维。通过深入了解中国文化和历史，我培养了更广泛的跨文化了解，这对于全球化时代的交流至关重要。这门语言的学习不仅是一种技能，更是一次丰富个人见识和提升综合素质的过程。\n崛起（jue2qi3 – rise abruptly）；拓展（tuo4zhan3）；至关重要（zhi4… – extremely important）\n\n\n\n1.  学汉语的原因？\n我是一个对历史和文化充满热情的越南人。在阅读或参观古代建筑的时候，我经常遇到《漢喃》文字，而现在越南人使用的书写系统是《拉汀》的。我一直想了解越南古人使用的文字，相信会提高我对越南历史和文化的知识。我知道越南和中国在古中代有密切的关系，越南古代文字主要基于古代汉字。此外，汉语现在是世界上最流行的语言之一，与中国的经济关系对越南的GDP也有很大贡献。学到中文对我的职业生涯一定有帮助。所以在去年的一个偶然的机会，我就开始学了中文了。\nGDP（国内生产总值）\n2.  困难？怎么克服？\n在学习韩语的过程中，我遇到了一些困难。首先，虽然中文的发音很简单，开始时我还有一些发音辅音和音调问题。每次发音错误，老师都会纠正，我会重新发音，平时我也会自己注意，就这样问题现在大概减掉了。其次，因为我母语写的字是拉丁，而汉字是象形的，所以汉字对我来说，又难写的、又难记得。我只能多写汉字，每次遇到生词我都会查古典，然后记下它怎么写，词类和用法。最后，法语虽然很有逻辑，很系统，但我还应用不了大部分学过的语法。因为平时我没有太多机会说汉语。。。\n3.  影响？得到了哪些收获？\n学汉语对我的生活产生了很大的影响。当开始学汉语的同时，我还了解了更多关于中国历史、文化和生活的信息。我意识到许多年轻的越南人正在学习汉语，通过学中文结识了更多的朋友。它也丰富了我原来只有工作的生活，减少了工作压力的一部分。我每周花大约12个小时学习，所以我学会了如何管理时间。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第21课-汉字叔叔 - 一个美国人的汉字情缘/index.html#生词",
    "href": "学汉语的日记/HSK5下-第21课-汉字叔叔 - 一个美国人的汉字情缘/index.html#生词",
    "title": "HSK5下 | 第21课： 汉字叔叔：一个美国人的汉字情缘",
    "section": "",
    "text": "汉字叔叔\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n情缘\nqíngyuán - n. predestined love, sentimental bond\n\n\n2\n逻辑\nluó ji - n. logic\n\n\n3\n死记硬背\nsǐ jì yìnɡ bèi - to memorize mechanically without understanding\n\n\n4\n偶然\nǒurán - adj./adv. accidental; by chance\n\n\n5\n演变\nyǎnbiàn - v. to change, to evolve\n\n\n6\n遗憾\nyí hàn - adj./n. regretful; deep regret\n\n\n7\n心脏\nxīnzàng - n. heart\n\n\n8\n思考\nsī kǎo - v. to think deeply, to ponder\n\n\n9\n抓紧\nzhuā jǐn - v. to firmly grasp\n\n\n10\n尽快\njǐn kuài - adv. as soon as possible\n\n\n11\n经典\njīng diǎn - n./adj. classics; classical\n\n\n12\n库\nkù - n. storehouse, bank\n\n\n13\n输入\nshū rù - v. to input\n\n\n14\n元旦\nyuándàn - n. New Year’s Day\n\n\n15\n疾病\njíbìng - n. disease, illness\n\n\n16\n创办\nchuàngbàn - v. to establish, to set up\n\n\n17\n公开\ngōng kāi - v./adj. to make known to the public; open\n\n\n18\n最初\nzuìchū - n. first, earliest\n\n\n19\n痛苦\ntòng kǔ - adj. painful, suffering\n\n\n20\n微博\nwēi bó - n. microblog\n\n\n21\n称呼\nchēng hu - v./n. to call, to address; form of address\n\n\n22\n克服\nkè fú - v. to overcome, to conquer\n\n\n23\n收集\nshōují - v. to collect, to gather\n\n\n24\n包含\nbāo hán - v. to contain, to include\n\n\n25\n繁体字\nfán tǐ zì - n. complex form, traditional Chinese characters\n\n\n26\n简体字\njiǎn tǐ zì - n. simplified form, simplified Chinese characters\n\n\n27\n方言\nfāng yán - n. dialect\n\n\n28\n称赞\nchēng zàn - v. to praise, to commend\n\n\n29\n真相\nzhēn xiàng - n. truth, fact\n\n\n30\n佩服\npèi fú - v. to admire\n\n\n31\n开放\nkāi fàng - v. to open to the public\n\n\n32\n下载\nxiàzài - v. to download\n\n\n33\n单位\ndānwèi - n. company, employer\n\n\n34\n识别\nshíbié - v. to recognize, to identify\n\n\n35\n查询\ncháxún - v. to search, to retrieve\n\n\n36\n物理\nwù lǐ - n. physics\n\n\n37\n完善\nwán shàn - v./adj. to make perfect, to improve; perfect\n\n\n38\n退休\ntuì xiū - v. to retire\n\n\n39\n日程\nrìchéng - n. schedule\n\n\n40\n追求\nzhuī qiú - v. to pursue, to go after\n\n\n41\n梦想\nmèng xiǎng - n./v. dream; to dream\n\n\n\nBonus\n\n《说文解字》：thuyết văn giải tự\n师范（shi1fan4）n.: pedagogical, sư phạm\n预料（yu4liao4）n.：epxectation, dự liệu"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第21课-汉字叔叔 - 一个美国人的汉字情缘/index.html#注释",
    "href": "学汉语的日记/HSK5下-第21课-汉字叔叔 - 一个美国人的汉字情缘/index.html#注释",
    "title": "HSK5下 | 第21课： 汉字叔叔：一个美国人的汉字情缘",
    "section": "",
    "text": "硬 ying4\n\n“硬” Phó từ, biểu thị kiên quyết hoặc cố chấp làm một việc gì đó. Ví dụ:\n\n在中国历史故事“指鹿为马”中，赵高把鹿硬说成马。\n·····但是他感觉汉子很复杂，汉字的一笔一画没有任何逻辑只能死记硬背。\n\n“硬” Khi làm phó từ còn biểu thị không đủ năng lực nhưng vẫn tiếp tục tận lực để làm một việc gì đó. Ví dụ:\n\n你不知道这一年我是怎么硬挺过来的。\n虽然中药汤有点儿苦，但为了治病，他还是硬把它喝下去了。\n\n\n偶然\n\n“偶然” (ngẫu nhiên/ bất ngờ) tính từ, biểu thị sự việc phát sinh ngoài dự đoán hoặc dựa vào quy luật thông thường thì không có khả năng phát sinh. Ví dụ:\n\n一个偶然的机会，他发现如果了解汉字的来源和演变过程，再学习它就变得轻松、容易。\n虽然桂花偶然也能长成18米高的大树，但是绝大多数情况下它们都很矮。\n\n“偶然” cũng có thể làm phó từ, có nghĩa là “偶尔，有时候”(thỉnh thoảng/ có lúc). Ví dụ:\n\n她专心地织着毛衣，偶然也会抬眼看一下墙上的挂钟。\n那些我生活过的地方，偶然也会在我梦中出现，但都不是我的“家”！\n\n\n尽快\n\n“尽快” Phó từ, có nghĩa là nhanh hết mức có thể. Ví dụ:\n\n·····我要抓紧时间尽快把《说文解字》电脑化。\n新产品出了点儿问题，你和严经理尽快商量一下这事。\n趁这两天天气好，您尽快把过季的衣服洗一洗，收起来。\n\n\nPhân biệt 偶然 và 偶尔\n\n共同点：Đều có thể là phó từ, đều có nghĩa là không thường xuyên, có lúc có thể dùng thay thế cho nhau, nhưng ý nghĩa có chút không giống.\n\n如：在校园里，我偶然/偶尔也会碰到李艳。\n\n不同点：\n\n不同点\n\n\n\n\n\n\n\n\n偶然\n偶尔\n\n\n\n\n1\nThiên về biểu thị có chút đột ngột, không nghĩ đến, trái ngược với 必然.\n如：这本书是她一次逛书市时偶然发现的。\nThiên về nhấn mạnh số lần ít, trái ngược với 经常.\n如：我平时加班不多，月底偶尔有一两天。\n\n\n2\nCòn có thể biểu thị sự việc phát sinh ngoài ý muốn hoặc không thể xảy ra theo quy luật thông thường. Có thể làm định ngữ, vị ngữ, đằng trước có thể kết hợp với phó từ chỉ mức độ.\n如：李阳的父亲是一位画家，所以，李阳喜欢画画儿并非偶然。\nCòn có thể là từ thuộc tính, chỉ làm định ngữ, đằng trước không thể thêm phó từ chỉ mức độ, cũng không thể làm vị ngữ. Cách dùng này không thường được sử dụng.\n如：他在农村的生活很单调，偶尔的聚会还是在村里的老房子里举行，很无聊。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第21课-汉字叔叔 - 一个美国人的汉字情缘/index.html#扩展",
    "href": "学汉语的日记/HSK5下-第21课-汉字叔叔 - 一个美国人的汉字情缘/index.html#扩展",
    "title": "HSK5下 | 第21课： 汉字叔叔：一个美国人的汉字情缘",
    "section": "",
    "text": "学科和电脑\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n哲学\nzhe2xue2 - phisolophy, triết học\n\n\n2\n化学\nhua4xue2 - chemistry, hóa học\n\n\n3\n物理\nwu4li3 - physics, vật lí\n\n\n4\n政治\nzheng4zhi4 - politicals, chính trị\n\n\n5\n粘贴\nzhan1tie1 - paste, dán\n\n\n6\n复制\nfu4zhi4 - copy\n\n\n7\n浏览\nliu2lan3 - browse, duyệt\n\n\n8\n删除\nshan1chu2 - delete, xóa\n\n\n9\n搜索\nsou1suo3 - search, tìm kiếm\n\n\n10\n文件\nwen2jian4 - file\n\n\n11\n软件\nruan3jian4 - software, phần mềm\n\n\n12\n操作\ncao1zuo4 - operation, hoạt động"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第21课-汉字叔叔 - 一个美国人的汉字情缘/index.html#运用",
    "href": "学汉语的日记/HSK5下-第21课-汉字叔叔 - 一个美国人的汉字情缘/index.html#运用",
    "title": "HSK5下 | 第21课： 汉字叔叔：一个美国人的汉字情缘",
    "section": "",
    "text": "我为什么学汉语\n对我来说，学汉语产生了深远的影响。首先，汉语作为全球最普遍的语言之一，掌握它对未来的职业发展打开了新的大门。中国经济的崛起使汉语在国际商务中变得尤为重要。其次，学习汉语是一项认知挑战，拓展了我的语言能力和逻辑思维。通过深入了解中国文化和历史，我培养了更广泛的跨文化了解，这对于全球化时代的交流至关重要。这门语言的学习不仅是一种技能，更是一次丰富个人见识和提升综合素质的过程。\n崛起（jue2qi3 – rise abruptly）；拓展（tuo4zhan3）；至关重要（zhi4… – extremely important）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第21课-汉字叔叔 - 一个美国人的汉字情缘/index.html#口语",
    "href": "学汉语的日记/HSK5下-第21课-汉字叔叔 - 一个美国人的汉字情缘/index.html#口语",
    "title": "HSK5下 | 第21课： 汉字叔叔：一个美国人的汉字情缘",
    "section": "",
    "text": "1.  学汉语的原因？\n我是一个对历史和文化充满热情的越南人。在阅读或参观古代建筑的时候，我经常遇到《漢喃》文字，而现在越南人使用的书写系统是《拉汀》的。我一直想了解越南古人使用的文字，相信会提高我对越南历史和文化的知识。我知道越南和中国在古中代有密切的关系，越南古代文字主要基于古代汉字。此外，汉语现在是世界上最流行的语言之一，与中国的经济关系对越南的GDP也有很大贡献。学到中文对我的职业生涯一定有帮助。所以在去年的一个偶然的机会，我就开始学了中文了。\nGDP（国内生产总值）\n2.  困难？怎么克服？\n在学习韩语的过程中，我遇到了一些困难。首先，虽然中文的发音很简单，开始时我还有一些发音辅音和音调问题。每次发音错误，老师都会纠正，我会重新发音，平时我也会自己注意，就这样问题现在大概减掉了。其次，因为我母语写的字是拉丁，而汉字是象形的，所以汉字对我来说，又难写的、又难记得。我只能多写汉字，每次遇到生词我都会查古典，然后记下它怎么写，词类和用法。最后，法语虽然很有逻辑，很系统，但我还应用不了大部分学过的语法。因为平时我没有太多机会说汉语。。。\n3.  影响？得到了哪些收获？\n学汉语对我的生活产生了很大的影响。当开始学汉语的同时，我还了解了更多关于中国历史、文化和生活的信息。我意识到许多年轻的越南人正在学习汉语，通过学中文结识了更多的朋友。它也丰富了我原来只有工作的生活，减少了工作压力的一部分。我每周花大约12个小时学习，所以我学会了如何管理时间。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第21课-汉字叔叔 - 一个美国人的汉字情缘/index.html#听力",
    "href": "学汉语的日记/HSK5下-第21课-汉字叔叔 - 一个美国人的汉字情缘/index.html#听力",
    "title": "HSK5下 | 第21课： 汉字叔叔：一个美国人的汉字情缘",
    "section": "2.1. 听力",
    "text": "2.1. 听力\n1.   男：您好！我在网上查询话费，它要我输入密码，可我没设置过密码呀！\n女：您可以输入手机号码，选择获取随机密码就可以登录了。\n问：男的想要做什么？（C查询话费）\n2.   男：你最近怎么不跟李阳打羽毛球了？\n女：他参加了学校的演讲比赛，已经进了复赛，这段时间都在忙着准备呢。\n问：李阳最近在忙什么？（B准备演讲比赛）\n3.   男：这次修改后的论文您觉得怎么样？\n女：结论部分我认为有必要再完善一下，别留遗憾，你抓紧点儿时间吧。\n问：女的觉得这篇论文怎么样？（A需要在完善）\n4.   男：这么晚了，你怎么还在加班？\n女：马主任说名单又有调整，确定后就给我发邮件，我再等会儿。\n问：女的现在在做什么？（C在单位加班）\n5.   女：我爸爸抽了二十多年的烟，现在说戒就戒了。\n男：真让人佩服。 要想戒烟关键就看有没有决心。\n问：男的认为戒烟怎么样？（B要有决心）\n6.   女：这几天的日程怎么安排得这么满？你要注意身体。\n男：放心吧，等我把这个合同谈下来，咱们就去海边玩儿几天。\n问：男的最近怎么样？（C日程很紧张）\n7.   男：你怎么一大早就打哈欠，昨晚没睡好？\n女：我最好的朋友回国来看我，就住在我家了。\n男：多年没见了，这下可有的聊了。\n女：可不是，我们俩硬是聊了一夜都没睡。\n问：关于女的，从对话中可以知道什么？（C和朋友聊了一夜）\n8.   男：你这么快就把论文写好了？真是佩服啊！\n女：我这还不是最快的，刘京都准备答辩了。\n男：我的调查问卷还没收齐呢，看着你们，真让人着急。\n女：大可不必，早晚没关系，通过最重要。\n问：关于男的的论文，从对话中可以知道什么？（A在收集资料）\n9.   女：这段时间怎么一直没见到你？去实习啦？\n男：最近跑了好几家单位，递了很多简历。\n女：面试通过了吗？有单位录取吗？\n男：有是有，不过，我还没有想好要不要去。\n问：关于男的，从对话中可以知道什么？（C已接到录用）\n10. 男：出访的日程表我看过了，安排得很满，有些地方不够合理。\n女：是的，我正和有关部门商量，一些细节要再完善。\n男：好的，一有消息及时通知我。\n女：您放心，估计周末就能给您。\n问：关于出访的日程，男的觉得怎么样？（D还要调整完善）\n11-12.\n有个笑话是这样的。 两个工人安装灯泡， 一个踩在另一个人的肩膀上。过了老半天，下面的人也不见上面有什么动静，就问上面的人：“喂，装好了吗？”上面的人说：“你不知道这是螺丝口的灯泡吗？你不转，我怎么装得上啊？”\n在工作中，你不得不承认，个人的知识和能力总是有限的，依靠和利用朋友的知识、经验和能力共同完成项目是明智的选择。但是，你却不能因此完全放弃了自己的努力。\n11. 关于那两个工人，可以知道什么？（B上面的人很懒）\n12. 这段话主要想告诉我们什么？（D合作需要双方共同努力）\n从前有个人非常自私，对别人的事从不关心，还常说：“别人的事，天大的也不要管。”因此别人送了他一个外号叫“天不管”。\n一天，“天不管”买了一袋大米背回家。路上，袋子破了，米不断漏出来。同伴看见了，问他：“别人的事要不要管？”他不加思考地说：“天大的事也不管。”一会儿，米漏掉了不少。同伴又问：“对人家有好处的事难道也不管吗？”\n他还说：“只要对自己没有好处，一百个也不管！”\n快到家时，“天不管”觉得肩上轻多了，这才发现一袋米已漏掉了半袋。他又气又急，责备他的同伴说：“你为什么不早点儿跟我说？”同伴学着他的腔调说：“不管，不管，一百个不管！”\n13. 根据这段话，别人为什么叫那个人“天不管”？（A他做人很自私）\n14. 关于“天不管”，下列哪项正确？（B背着大米回家）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第21课-汉字叔叔 - 一个美国人的汉字情缘/index.html#阅读",
    "href": "学汉语的日记/HSK5下-第21课-汉字叔叔 - 一个美国人的汉字情缘/index.html#阅读",
    "title": "HSK5下 | 第21课： 汉字叔叔：一个美国人的汉字情缘",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n不可否认，传统纸质出版有着不可避免的局限。随着技术的发展和电子书自身的15B完善,从长远来看，很难说人们两千多年的阅读习惯不会改变。正如许多作家从最初不愿放弃稿纸到现在习惯了在电脑上16C输入文字一样，阅读的习惯在数字化浪潮下也在默默改变。多年以后，纸质书或许将会作为一种有限范围内的阅读方式，被小众化地予以保留。就像现在我们中的一小部分人，仍然对经典的线装书十分着迷一样。我个人认为，如果书本真的远离了我们的生活，那将是一件十分17A遗憾的事情。\n在科技高速发展、人们生活的各方面都受到极大冲击的时代，传统的阅读方式是否会被颠覆？电子书能为我们的文化产业带来什么？对“无纸化阅读”该保持怎样的态度？传统的图书出版业又该何去何从？这都是我们需要深入18D思考的问题。\n\nC目的一致是合作的基础\n\n两个搬运工人在门道里搬运一个大木箱。他们又推又拉，用尽了力气，却始终无法将箱子搬动。最后那个在门外的人说：“算了吧，这么大的箱子，我们怎么也搬不进去。”“谁说要把它搬进来？”在里面的那个人说，“我还以为是要把它搬出去呢。”\n\nB儿童绘画能反映其心态\n\n绘画是一种语言，是一种不规范的表达手段，它反映一个人的心态，儿童也如此。儿童绘画的寓意很丰富，它是儿童同外部世界联系、向他人诉说和表明他的存在的一种方式。不管画中有无色彩，不管画的是圆还是方，不管画得夸张还是细腻，儿童的每一幅画都有其内在的含义。\n\nB不发达国家女性识字率较低\n\n全球15岁以上的人中，大约有84%具备识字能力。识字率因国家和地区而异。一个人是否具备识字能力受到许多因素的影响，比如财富、性别、受教育机会和所在的地区等。全球目前还有7亿7400万不具备读写能力的人，其中二分之二为女性。这主要是因为在一些欠发达国家，女性往往缺少接受教育的机会。\n\nA各国的官方语言数量不等\n\n目前地球上共有70多亿人，190多个独立的国家，但人类所说的语言却超过5000种。有些国家只有一种官方语言，比如日本，而有些则不然。在印度，现在除了全国性的法定官方语言英语和印地语外，还有20种地方性的联邦官方语言。专家们认为，人类的语言种类最多时曾达到10000种，目前数量已经减少了近一半，并且仍在不断减少中。\n23-25.\n1993年，一个14岁的孩子在上海青年篮球队里打球。当时，球队里的队员都穿着帆布面的篮球鞋。有一次，他在观看一场国外球队的比赛时发现，那些国外的球员竟然都穿着皮制的篮球鞋。这种球鞋不但美观，而且穿着舒适。于是，这个孩子就梦想能穿上一双皮制的篮球鞋。\n一天，当他把这个梦想告诉教练后，教练笑着说：“努力吧，孩子。如果你能进入国家青年队，你就能穿这样的鞋。”\n从这一刻起，这个孩子就把进入国家青年队作为自己奋斗的目标。终于在17岁那年，他凭借自己超凡的球技,被选人国家青年队。穿上了向往已久的皮制篮球鞋，他倍加珍惜。一位队友发现此事后告诉他：“不用在意一双球鞋。如果你能进入国家队，这样的篮球鞋你想有多少就有多少！”这句极具诱惑力的话深深触动了他。于是，他又有了新的奋斗目标—中国国家篮球队。\n功夫不负有心人，经过一年的苦练，他真的穿上了国家队的队服。\n2001年，亚洲篮球锦标赛上，他为中国国家队夺得冠军做出了突出贡献。2002年6月26日，美国NBA人的选秀大会上，休斯敦火箭队选择了他。几年后，他成为中国篮球运动的标志性人物。他叫姚明。\n23. C想得到一双皮制球鞋\n24. B对中国篮球贡献很大\n25. C目标未必要很远大\n26-28.\n减肥是令许多人望而却步的难事，是许多胖子的大难题。但有一家减肥健美俱乐部 却效果显著，久负盛名。\n一天，一位胖男子慕名而来，他已有过多次失败的经历了。他抱着最后一试的态度 问教练他该怎么办。教练记下了他的地址，然后告诉他：“回家等候通知。明天会有人告 诉你怎么做。”\n第二天一早，门铃响了，一位漂亮性感的女郎站 在门口，对胖子说：“教练吩咐，你要是能追上我，我 就做你的女朋友。”\n胖子大喜，从此每天早晨都在女郎后边狂追。如 此数月下来，胖子已逐渐身手矫健起来，他早就忘了 这是减肥，只是想一定要把那位姑娘追到手。\n直到有一天，胖子心想：今天我一定能追到她了。他早早起来在门口等着，那位姑娘没来，来的是一位同他以前一样胖的女士。\n胖女士对他说：“教练吩咐，我要能追到你，你就做我的男朋友。”\n这只是一个笑话，但它却告诉我们，不妨“偷换概念目标”，把一些艰苦的过程变得 轻松有趣起来，这样你就能更好地坚持下去。\n26. B名气很大\n27. A晨跑坚持了几个月\n28. B善于发现乐趣"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第21课-汉字叔叔 - 一个美国人的汉字情缘/index.html#书写",
    "href": "学汉语的日记/HSK5下-第21课-汉字叔叔 - 一个美国人的汉字情缘/index.html#书写",
    "title": "HSK5下 | 第21课： 汉字叔叔：一个美国人的汉字情缘",
    "section": "2.3. 书写",
    "text": "2.3. 书写\n梦想，抓紧，思考，遗憾，追求\n在体育比赛中，取得冠军都是每个运动员的梦想。因此，每个运动员每天都在努力训练，抓紧每一个机会参加比赛来学习和发展。然而，在追求这个目标的同时，不要因为太过重视结果而忽视过程，得了可能影响整个职业生涯的创伤，留下了遗憾。运动训练是一个漫长的旅程，需要仔细思考以确定科学的路线和有效方法。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第19课-家乡的萝卜饼/index.html",
    "href": "学汉语的日记/HSK5下-第19课-家乡的萝卜饼/index.html",
    "title": "HSK5下 | 第19课： 家乡的萝卜饼",
    "section": "",
    "text": "家乡的萝卜饼 | Turnip pancakes in my hometown\n家乡的众多美食中，萝卜饼是最让我怀念的。它那丰富的色彩、微甜的口感，至今仍让我十分想念。\n家乡的萝卜有青、红、紫三种。三种萝卜看起来赏心悦目，吃起来，青的甜中带点儿辣，红的辣中带着甜，紫的像山泉般清淡可口。父老乡亲们夸它说：“橘子、葡萄、梨，比不上咱的萝卜皮。”而萝卜饼就是用这三种颜色的萝卜做成的。\n萝卜饼的做法极其简单，既不必炒或煮，也不用油炸。先把三色萝卜洗净切丝，放入油、盐等，用筷子搅拌均匀，萝卜饼的原料便做成了。最关键的功夫是擀面。高手往往把面擀得薄如白纸，拌好的萝卜丝儿铺到饼上后，得再折叠两三次，要求饼熟之后表皮是透明的，能透过表皮看见萝卜丝儿。最后用刀切成块状，饼便做好了。\n接下来，拿一个平底锅，先在锅里淋一圈油，待油锅烫手时，将切好的萝卜饼一块一块地放进锅里。盖锅前须放进一些温水，预防糊底。火最好用文火，等能闹到香味时，便可开锅了。萝卜饼要趁热吃，喜欢口味重的，还可以加少许酱油和醋。刚出锅的萝下饼，香味扑鼻，外焦里嫩，吃上一口，便让人永远忘不了。\n如今，美食家们对吃提出了更高的要求。他们不仅要观色、闸香、尝味、赏形，而且还要求食物具有养生方面的特色。我想，家乡的萝卜饼完全具备这几个方面的条件，人们不是常说吗——“鱼生火，肉生痰，青菜萝卜保平安”，养生的功能，让我更加喜爱它了。\n改编自《中国电视报》，作者：李星涛 tao1 - Lý Tinh Đào\n\n\n\n家乡的萝卜饼\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n家乡\njiāxiāng - n. hometown, native place, quê nhà\n\n\n2\n萝卜\nluóbo - n. turnip, radish, củ cải\n\n\n3\n怀念\nhuái niàn - v. to miss, to feel nostalgic, hoài niệm\n\n\n4\n色彩\nsècǎi - n. color, characteristic, màu sắc, tính chất\n\n\n5\n想念\nxiǎng niàn - v. to recall with longing, to miss\n\n\n6\n青\nqīng - adj. greenish blue\n\n\n7\n紫\nzǐ - adj. purple\n\n\n8\n赏心悦目\nshǎngxīnyuèmù - pleasing to both the eye and the mind\n\n\n9\n般\nbān - part. sort, kind (means 一样／似的)\n\n\n10\n清淡\nqīngdàn - adj. lightly flavored and not greasy, thanh đạm\n\n\n11\n可口\nkěkǒu - adj. tasty, delicious\n\n\n12\n夸\nkuā - v. to praise\n\n\n13\n橘子\njúzi - n. mandarin, tangerine\n\n\n14\n梨\nlí - n. pear\n\n\n15\n炒\nchǎo - v. to stir-fry\n\n\n16\n煮\nzhǔ - v. to boil, to stew\n\n\n17\n油炸\nyóu zhá - v. to deep fry\n\n\n18\n切\nqiē - v. to cut, to chop, to slice\n\n\n19\n丝\nsī - n. shred, anything threadlike\n\n\n20\n搅拌\njiǎo bàn - v. to stir, to mix\n\n\n21\n均匀\njūn yún - adj. even, well-distributed\n\n\n22\n原料\nyuánliào - n. raw material\n\n\n23\n擀\ngǎn - v. to roll (dough, etc. with a rolling pin)\n\n\n24\n薄\nbáo - adj. thin\n\n\n25\n折叠\nzhédié - v. to fold\n\n\n26\n透明\ntòu míng - adj. transparent\n\n\n27\n淋\nlín - v. to splatter, to sprinkle\n\n\n28\n圈\nquān - n. circle, ring\n\n\n29\n烫\ntàng - v./adj. to scald, to burn; very hot, scalding\n\n\n30\n盖\ngài - v./n. to cover; lid, cover\n\n\n31\n预防\nyùfáng - v. to guard against, to prevent\n\n\n32\n糊\nhú - v. to be burnt\n\n\n33\n文火\nwénhuǒ - n. slow fire, gentle heat\n\n\n34\n闻\nwén - v. to smell\n\n\n35\n趁\nchèn - prep. to take advantage of, (to do…) at the time when\n\n\n36\n口味\nkǒu wèi - n. taste, flavor\n\n\n37\n少许\nshǎoxǔ - adj. a little, some\n\n\n38\n酱油\njiàng yóu - n. soy sauce\n\n\n39\n醋\ncù - n. vinegar\n\n\n40\n焦\njiāo - adj. burnt, scorched\n\n\n41\n嫩\nnèn - adj. soft, tender\n\n\n42\n特色\ntèsè - n. characteristic, distinctive feature\n\n\n43\n痰\ntán - n. phlegm, sputum (mucus)\n\n\n44\n平安\npíng ān - adj. safe, well, safe and sound\n\n\n\n\n\n\n\n般 ban1\n\n“般” trợ từ, có nghĩa là “一样” “似的”(tựa như) , thường đứng sau danh từ, tạo thành một cụm từ làm định ngữ hoặc trạng ngữ. Ví dụ:\n\n·····紫的像山泉般清淡可口。\n说起那段往事，她的脸上露出阳光般的笑容。\n望着爸爸远去的背景，我的眼泪雨点般不停地往下掉。\n\n\n闻 men2\n\n“闻” khi làm ngữ tố, có nghĩa là “nghe thấy hoặc tin tức, sự việc được nghe thấy”.Ví dụ:\n\n你们到各地去旅游，一定会增加对中国的了解，老话说：百闻不如一见。\n时隔多年再来这里，所到之处、所见所闻，无不给人一种新鲜感。\n邻居们闻声赶来，纷纷跳入水中救起了落水的小孩。\n假期里，她唱带孩子们到世界各地旅行，增长他们的见闻。\n\n“闻” khi dùng làm động từ, biểu thị dùng mũi để cảm nhận, nhận biết mùi vị. Ví dụ:\n\n火最好用文火，等能闻到香味时，便可开锅了。\n他把壶盖儿打开，闻了闻，原来是酒。\n\n\n趁 chen4\n\n“趁” giới từ, có nghĩa là lợi dụng (thời gian, cơ hội) , đằng sau có thể là cụm động từ, danh từ, tính từ và câu đơn. Ví dụ:\n\n趁着这几天休息，我们去看看房子吧。\n趁电影还没开始，我去买两瓶矿泉水。\n萝卜饼要趁热吃，喜欢口味重的，还可以加少许酱油和醋。\n\n\nPhân biệt 怀念 và 想念\n\n共同点：Đều là động từ, đều có ý nghĩa biểu thị nhớ nhung, không thể quên đối với người hoặc hoàn cảnh nào đó.\n\n如：每当回忆起小学时代的学习生活，我最想念/怀念的人是刘老师。\n\n不同点：\n\n不同点\n\n\n\n\n\n\n\n\n怀念\n想念\n\n\n\n\n1\nThường dùng trong văn viết, về ngữ nghĩa nhấn mạnh thường xuyên nhớ đến, không thể quên đi.\n如：刘教授非常怀念年轻时在国外留学的那段生活。\nThường dùng trong khẩu ngữ, về mặt ngữ nghĩa nhấn mạnh hy vọng được gặp người nào đó.\n如：女儿告诉我，她很想念出差的爸爸。\n\n\n2\nThường dùng với những người đã khuất hoặc những hoàn cảnh không thể nào gặp lại được.\n如：从文章中我们读到了先生对去世的母亲的怀念。\nThường dùng với những người đang sống hoặc những hoàn cảnh còn có thể được lặp lại.\n如：每到春节，我就格外想念家乡的一草一木。\n\n\n\n\n\n\n\n饮食\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n零食\nling2shi2 - snack, đồ ăn vặt\n\n\n2\n冰激凌\nbing1ji1ling2 - ice cream, kem\n\n\n3\n酱油\njiang4you2 - soy sauce, dầu\n\n\n4\n醋\ncu4 - giấm, vinegar\n\n\n5\n开水\nkai2shui3 - boiled water, nước đun sôi (để uống)\n\n\n6\n点心\ndian3xin1 - điểm tâm, snack, pastry\n\n\n7\n营养\nying2yang3 - nutrition, dinh dưỡng\n\n\n8\n口味\nkou3wei4 - flavor, hương vị\n\n\n9\n胃口\nwei4kou3 - appetite, khẩu vị\n\n\n10\n淡\ndan4 - bland, nhạt, thanh (đạm)\n\n\n11\n臭\nchou4 - stinky, hôi\n\n\n12\n软\nruan3 - mềm, soft\n\n\n13\n嫩\nnen4 - mềm, nục, tender\n\n\n14\n过期\nguo4qi1 - (food) expired, (loan, deadline) overdue, quá kỳ\n\n\n\n\n\n\n我喜欢/会做的中国菜\n我喜欢做的中国菜是红烧肉，是一道具有浓厚中式风味的传统佳肴。选用五花肉为主料，切成块状，搭配生姜、香叶、八角等香料，再用适量料酒提鲜。在炖煮的过程中，酱油、冰糖的加入增加肉块深沉的色彩和独特的口感。慢慢炖煮，肉质逐渐入味，汤汁香浓。炖肉的独特之处在于火候的把握，足够的时间让肉质酥烂，入口即化。这道菜肴既有浓郁的家常味道，也使我想起了妈妈做的越南炖肉，所以让我更喜欢它了。\n佳肴（jia1yao2 - delicacies）; 五花肉; 生姜（sheng1jiang1 – ginger）；香叶（xiang1ye4 – bay leaf）；八角（ba1jiao3 - anise）；冰糖（bing1tang2 – crystal sugar）；赋予（fu4yu3）；深沉（shen1chen2 - deep）\n\n\n\n1.  你喜欢什么口味的菜？不喜欢什么的？你比较喜欢那些中国菜？\n我喜欢口味浓郁而鲜美的菜肴，尤其喜欢微辣的口味。对于不喜欢的菜，我对腥味重或过于油腻的食物较为挑剔。我在越南吃过一些著名的中国菜，如烤鸭、红烧肉、饺子，很适合我的口味，但没有令我印象深刻，尤其是与越南食物相比。每次我想换口味或与朋友一起吃饭时，中国菜都会让我的选择多样化。\n腥（xing1/fishy smell）\n我较为挑剔（tiao1ti/nitpick）\n2.  你学过做中国菜吗？学的是什么菜？\n我很少做菜，但学过了一个中国菜,就是红烧肉。主要是因为它有点像越南的炖肉，让我每次吃到都想念我妈妈每年过年专做的菜。红烧肉在越南南方很常见，可能是因为这里的华人社区非常拥挤。\n3.  你能介绍一下这个菜是什么做的吗？\n红烧肉，那是一份充满家乡味道的美味回忆。五花肉块在锅中煎至微黄，散发着诱人的香气。姜片在热油中翻滚，伴随着料酒，使得这道菜肴充满层次感。随后，酱油、冰糖的加入激发了红烧的色泽和味道。经过耐心炖煮，红烧肉的肉质入味鲜嫩，红亮的色泽令人垂涎欲滴。一口红烧肉，勾起了对故乡的浓浓思念，让每一次烹饪都成为一次体验温馨家的感觉。\n垂涎欲滴（chui2xian2yu4di）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第19课-家乡的萝卜饼/index.html#生词",
    "href": "学汉语的日记/HSK5下-第19课-家乡的萝卜饼/index.html#生词",
    "title": "HSK5下 | 第19课： 家乡的萝卜饼",
    "section": "",
    "text": "家乡的萝卜饼\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n家乡\njiāxiāng - n. hometown, native place, quê nhà\n\n\n2\n萝卜\nluóbo - n. turnip, radish, củ cải\n\n\n3\n怀念\nhuái niàn - v. to miss, to feel nostalgic, hoài niệm\n\n\n4\n色彩\nsècǎi - n. color, characteristic, màu sắc, tính chất\n\n\n5\n想念\nxiǎng niàn - v. to recall with longing, to miss\n\n\n6\n青\nqīng - adj. greenish blue\n\n\n7\n紫\nzǐ - adj. purple\n\n\n8\n赏心悦目\nshǎngxīnyuèmù - pleasing to both the eye and the mind\n\n\n9\n般\nbān - part. sort, kind (means 一样／似的)\n\n\n10\n清淡\nqīngdàn - adj. lightly flavored and not greasy, thanh đạm\n\n\n11\n可口\nkěkǒu - adj. tasty, delicious\n\n\n12\n夸\nkuā - v. to praise\n\n\n13\n橘子\njúzi - n. mandarin, tangerine\n\n\n14\n梨\nlí - n. pear\n\n\n15\n炒\nchǎo - v. to stir-fry\n\n\n16\n煮\nzhǔ - v. to boil, to stew\n\n\n17\n油炸\nyóu zhá - v. to deep fry\n\n\n18\n切\nqiē - v. to cut, to chop, to slice\n\n\n19\n丝\nsī - n. shred, anything threadlike\n\n\n20\n搅拌\njiǎo bàn - v. to stir, to mix\n\n\n21\n均匀\njūn yún - adj. even, well-distributed\n\n\n22\n原料\nyuánliào - n. raw material\n\n\n23\n擀\ngǎn - v. to roll (dough, etc. with a rolling pin)\n\n\n24\n薄\nbáo - adj. thin\n\n\n25\n折叠\nzhédié - v. to fold\n\n\n26\n透明\ntòu míng - adj. transparent\n\n\n27\n淋\nlín - v. to splatter, to sprinkle\n\n\n28\n圈\nquān - n. circle, ring\n\n\n29\n烫\ntàng - v./adj. to scald, to burn; very hot, scalding\n\n\n30\n盖\ngài - v./n. to cover; lid, cover\n\n\n31\n预防\nyùfáng - v. to guard against, to prevent\n\n\n32\n糊\nhú - v. to be burnt\n\n\n33\n文火\nwénhuǒ - n. slow fire, gentle heat\n\n\n34\n闻\nwén - v. to smell\n\n\n35\n趁\nchèn - prep. to take advantage of, (to do…) at the time when\n\n\n36\n口味\nkǒu wèi - n. taste, flavor\n\n\n37\n少许\nshǎoxǔ - adj. a little, some\n\n\n38\n酱油\njiàng yóu - n. soy sauce\n\n\n39\n醋\ncù - n. vinegar\n\n\n40\n焦\njiāo - adj. burnt, scorched\n\n\n41\n嫩\nnèn - adj. soft, tender\n\n\n42\n特色\ntèsè - n. characteristic, distinctive feature\n\n\n43\n痰\ntán - n. phlegm, sputum (mucus)\n\n\n44\n平安\npíng ān - adj. safe, well, safe and sound"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第19课-家乡的萝卜饼/index.html#注释",
    "href": "学汉语的日记/HSK5下-第19课-家乡的萝卜饼/index.html#注释",
    "title": "HSK5下 | 第19课： 家乡的萝卜饼",
    "section": "",
    "text": "般 ban1\n\n“般” trợ từ, có nghĩa là “一样” “似的”(tựa như) , thường đứng sau danh từ, tạo thành một cụm từ làm định ngữ hoặc trạng ngữ. Ví dụ:\n\n·····紫的像山泉般清淡可口。\n说起那段往事，她的脸上露出阳光般的笑容。\n望着爸爸远去的背景，我的眼泪雨点般不停地往下掉。\n\n\n闻 men2\n\n“闻” khi làm ngữ tố, có nghĩa là “nghe thấy hoặc tin tức, sự việc được nghe thấy”.Ví dụ:\n\n你们到各地去旅游，一定会增加对中国的了解，老话说：百闻不如一见。\n时隔多年再来这里，所到之处、所见所闻，无不给人一种新鲜感。\n邻居们闻声赶来，纷纷跳入水中救起了落水的小孩。\n假期里，她唱带孩子们到世界各地旅行，增长他们的见闻。\n\n“闻” khi dùng làm động từ, biểu thị dùng mũi để cảm nhận, nhận biết mùi vị. Ví dụ:\n\n火最好用文火，等能闻到香味时，便可开锅了。\n他把壶盖儿打开，闻了闻，原来是酒。\n\n\n趁 chen4\n\n“趁” giới từ, có nghĩa là lợi dụng (thời gian, cơ hội) , đằng sau có thể là cụm động từ, danh từ, tính từ và câu đơn. Ví dụ:\n\n趁着这几天休息，我们去看看房子吧。\n趁电影还没开始，我去买两瓶矿泉水。\n萝卜饼要趁热吃，喜欢口味重的，还可以加少许酱油和醋。\n\n\nPhân biệt 怀念 và 想念\n\n共同点：Đều là động từ, đều có ý nghĩa biểu thị nhớ nhung, không thể quên đối với người hoặc hoàn cảnh nào đó.\n\n如：每当回忆起小学时代的学习生活，我最想念/怀念的人是刘老师。\n\n不同点：\n\n不同点\n\n\n\n\n\n\n\n\n怀念\n想念\n\n\n\n\n1\nThường dùng trong văn viết, về ngữ nghĩa nhấn mạnh thường xuyên nhớ đến, không thể quên đi.\n如：刘教授非常怀念年轻时在国外留学的那段生活。\nThường dùng trong khẩu ngữ, về mặt ngữ nghĩa nhấn mạnh hy vọng được gặp người nào đó.\n如：女儿告诉我，她很想念出差的爸爸。\n\n\n2\nThường dùng với những người đã khuất hoặc những hoàn cảnh không thể nào gặp lại được.\n如：从文章中我们读到了先生对去世的母亲的怀念。\nThường dùng với những người đang sống hoặc những hoàn cảnh còn có thể được lặp lại.\n如：每到春节，我就格外想念家乡的一草一木。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第19课-家乡的萝卜饼/index.html#扩展",
    "href": "学汉语的日记/HSK5下-第19课-家乡的萝卜饼/index.html#扩展",
    "title": "HSK5下 | 第19课： 家乡的萝卜饼",
    "section": "",
    "text": "饮食\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n零食\nling2shi2 - snack, đồ ăn vặt\n\n\n2\n冰激凌\nbing1ji1ling2 - ice cream, kem\n\n\n3\n酱油\njiang4you2 - soy sauce, dầu\n\n\n4\n醋\ncu4 - giấm, vinegar\n\n\n5\n开水\nkai2shui3 - boiled water, nước đun sôi (để uống)\n\n\n6\n点心\ndian3xin1 - điểm tâm, snack, pastry\n\n\n7\n营养\nying2yang3 - nutrition, dinh dưỡng\n\n\n8\n口味\nkou3wei4 - flavor, hương vị\n\n\n9\n胃口\nwei4kou3 - appetite, khẩu vị\n\n\n10\n淡\ndan4 - bland, nhạt, thanh (đạm)\n\n\n11\n臭\nchou4 - stinky, hôi\n\n\n12\n软\nruan3 - mềm, soft\n\n\n13\n嫩\nnen4 - mềm, nục, tender\n\n\n14\n过期\nguo4qi1 - (food) expired, (loan, deadline) overdue, quá kỳ"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第19课-家乡的萝卜饼/index.html#运用",
    "href": "学汉语的日记/HSK5下-第19课-家乡的萝卜饼/index.html#运用",
    "title": "HSK5下 | 第19课： 家乡的萝卜饼",
    "section": "",
    "text": "我喜欢/会做的中国菜\n我喜欢做的中国菜是红烧肉，是一道具有浓厚中式风味的传统佳肴。选用五花肉为主料，切成块状，搭配生姜、香叶、八角等香料，再用适量料酒提鲜。在炖煮的过程中，酱油、冰糖的加入增加肉块深沉的色彩和独特的口感。慢慢炖煮，肉质逐渐入味，汤汁香浓。炖肉的独特之处在于火候的把握，足够的时间让肉质酥烂，入口即化。这道菜肴既有浓郁的家常味道，也使我想起了妈妈做的越南炖肉，所以让我更喜欢它了。\n佳肴（jia1yao2 - delicacies）; 五花肉; 生姜（sheng1jiang1 – ginger）；香叶（xiang1ye4 – bay leaf）；八角（ba1jiao3 - anise）；冰糖（bing1tang2 – crystal sugar）；赋予（fu4yu3）；深沉（shen1chen2 - deep）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第19课-家乡的萝卜饼/index.html#口语",
    "href": "学汉语的日记/HSK5下-第19课-家乡的萝卜饼/index.html#口语",
    "title": "HSK5下 | 第19课： 家乡的萝卜饼",
    "section": "",
    "text": "1.  你喜欢什么口味的菜？不喜欢什么的？你比较喜欢那些中国菜？\n我喜欢口味浓郁而鲜美的菜肴，尤其喜欢微辣的口味。对于不喜欢的菜，我对腥味重或过于油腻的食物较为挑剔。我在越南吃过一些著名的中国菜，如烤鸭、红烧肉、饺子，很适合我的口味，但没有令我印象深刻，尤其是与越南食物相比。每次我想换口味或与朋友一起吃饭时，中国菜都会让我的选择多样化。\n腥（xing1/fishy smell）\n我较为挑剔（tiao1ti/nitpick）\n2.  你学过做中国菜吗？学的是什么菜？\n我很少做菜，但学过了一个中国菜,就是红烧肉。主要是因为它有点像越南的炖肉，让我每次吃到都想念我妈妈每年过年专做的菜。红烧肉在越南南方很常见，可能是因为这里的华人社区非常拥挤。\n3.  你能介绍一下这个菜是什么做的吗？\n红烧肉，那是一份充满家乡味道的美味回忆。五花肉块在锅中煎至微黄，散发着诱人的香气。姜片在热油中翻滚，伴随着料酒，使得这道菜肴充满层次感。随后，酱油、冰糖的加入激发了红烧的色泽和味道。经过耐心炖煮，红烧肉的肉质入味鲜嫩，红亮的色泽令人垂涎欲滴。一口红烧肉，勾起了对故乡的浓浓思念，让每一次烹饪都成为一次体验温馨家的感觉。\n垂涎欲滴（chui2xian2yu4di）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第19课-家乡的萝卜饼/index.html#听力",
    "href": "学汉语的日记/HSK5下-第19课-家乡的萝卜饼/index.html#听力",
    "title": "HSK5下 | 第19课： 家乡的萝卜饼",
    "section": "2.1. 听力",
    "text": "2.1. 听力\n1.   女：怎么你今天吃得这么少？是不是这些菜不合你的口味？ \n男：这两天有点儿不舒服，没什么胃口。\n问：男的是什么意思？ （C不想吃东西）\n2.   女：这香肠的颜色怎么这样？是不是有点儿问题？ \n男：放在冰箱里就忘了，你赶紧看看生产日期吧。\n问：男的觉得香肠怎么了？ （B可能过期了）\n3.   女：欢迎光临！对不起，现在没位子了，您在这儿坐下等一会儿可以吗？这儿准备了点心和茶水，您先用一点儿。\n男： 哦，谢谢！我下午跟你们订过一个包间，你查一下李先生的预订有没有？\n问：男的现在最可能在哪儿？ （D饭馆）\n4.   女：这萝卜饼的饼皮薄得跟纸似的，萝卜颜色红红绿绿的，真好看。\n男：怎么样，好吃吧？这饼可是我们家乡的特色美食。\n问：关于萝卜饼，可以知道什么？ （C男的对他很熟悉）\n5.   男：半年不见，你比以前苗条多了。怎么，最近减肥呢？\n女：我比以前瘦了吗？我这几个月一直在健身，看来真有效果。\n问：关于女的，下列哪项正确？（B经常健身）\n6.   女：星期六下午有个聚会，给丽丽过生日，你一定要去啊。\n男：我晚点儿过去，行吗？下午正好有培训课。你们几点开始？\n问：星期六女的希望男的做什么？（B据参加聚会）\n7.   女：我看见林林又在吃零食了，你少给他买点儿这些东西吧。\n男：我不是答应他考试成绩好可以满足他一个要求吗？\n女：这算什么要求？你也不能不讲原则啊。\n男：油炸食品不健康，这个我懂，少吃一点儿就行了。\n问：他们主要在谈什么问题？(B孩子该不该吃零食)\n8.   男：服务员，给我们推荐几个你们这儿的特色菜吧。\n女：我们这儿是川菜馆，麻婆豆腐来的客人基本上都会点。\n男：好，来一个。\n女：您喜欢海鲜吗？今天的干烧黄鱼是特价。\n问：根据对话，下列哪项正确？（D女的推荐干烧黄鱼）\n9.   男：奶奶，超市里的有机蔬菜是无污染的绿色食品，卖得可好了。\n女：就是价钱比肉还贵。\n男：现在人们都追求健康，不是每天大鱼大肉才是吃得好了。\n女：那倒是，现在很多人得了富贵病，还不是吃得太好了？\n问：根据对话，女的同意哪种看法？（C大鱼大肉不健康）\n10. 女：我做了些点心，你尝尝看。\n男：味道真不错，很好吃。 跟谁学的？\n女：这是我们家乡的特产，过年回家时奶奶教我做的。\n男：怪不得我从来都没吃过呢。\n问：关于这种点心，可以知道什么？（B女的自己做的）\n11-12.\n男：我教你啊，把肉切丝，辣椒、土豆也都一样，炒的时候，少放酱油。\n女：是想清淡点儿吗？\n男：咸淡自己根据口味，主要是做出来颜色漂亮。\n女：那加点儿胡萝卜丝不就更好看了？\n男：那是当然。不过，有人不喜欢那味道。\n女：我没问题，营养丰富更重要。\n11. 男的让女的怎么做这个菜？（A肉和菜切成丝）\n12. 做菜时，女的更看重什么？（D营养）\n13-14.\n提起新疆的吐鲁番，就不得不说葡萄。 这里为什么会生长出这么好吃的葡萄呢？原来这与当地的气候有很大的关系。这里气候干燥，晴天多，阴天少，再加上纬度比较高，夏季日照时间长，日照强度大，可以让葡萄进行长时间的光合作用，以制造出大量的有机物质和糖分；而到了晚上，气温降得很低，植物的呼吸作用减弱，这样就减少了养分的损失。所以果实中能够积累大量的营养物质，不但个儿长得大，而且十分甜美。\n13. 根据这段话，吐鲁番的葡萄好吃跟什么有很大关系？（A气候）\n14. 根据这段话，晚上气温低可以使葡萄怎么样？（C减少养分损失）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第19课-家乡的萝卜饼/index.html#阅读",
    "href": "学汉语的日记/HSK5下-第19课-家乡的萝卜饼/index.html#阅读",
    "title": "HSK5下 | 第19课： 家乡的萝卜饼",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n白菜是十字花科的蔬菜，原产于中国北方，后引种南方，直到19世纪才传入日本和欧美。\n白菜在中国的栽种历史非常15C悠久,早在二国时期的《吴录》中，就有“陆逊催人种豆、林”这样的内容，“崧”指的就是白菜。不过，到了隋唐以后，白菜的种植才得到大面积16D推广。\n白菜耐保存,中国人特别是北方人对白菜有特殊的感情。在经济困难时期，白菜是他们整个冬季的主要蔬菜，一个家庭往往需要购买数百斤白菜17C以度过漫长的冬季,因此白菜在中国就有了炖、炒、腌、拌等各种做法。冬季，最低气温18B为零下5工左右时,白菜仍然可以在室外存放过冬，外部叶子干燥后可以为内部保温。\n白菜不仅含有丰富的维生素、蛋白质等营养成分，还具有一定的药用价值，俗话说：“鱼生火，肉生痰，白菜豆腐保平安。”\n\nD大火快炒能保留蔬菜更多营养\n\n蔬菜中的维生素^和维生素8都怕热、怕煮，在炒蔬菜的时候，如果用小火炒，维生素会损失很多，如果用大火快炒，维生素损失只有1/5。所以，炒蔬菜一定要用旺火。\n\nD水煮牛肉的味道香浓\n\n水煮牛肉是中国传统的菜品，通过大火用煮的方式，让牛肉中那些人体难以消化的粗纤维更好地分解，让牛肉更香浓。但是，炖煮的过程中，一些营养成分会流入汤料中，造成极大的浪费，人体也无法完全吸收。\n\nC用盆接水洗菜有害健康\n\n无论是蔬菜还是海鲜，或是肉类，在烹饪前都需要用水清洗。很多人为了节约用水,往往用盆把水接住，然后反复洗不同的食材，这种做法不仅不能将食材洗干净，反而会造成二次污染，危害我们的健康。因此，清洗食材的时候，用流动的清水是最好的。\n\nA西式快餐也有健康的食物\n\n孩子是西式快餐厅的主角，有的家长为了不让孩子吃垃圾食品，可谓想尽办法。其实，西式快餐中并不都是垃圾食品，也有一些有益食品，如土豆泥、蔬菜沙拉等。而且，偶尔吃一两次并不会给健康带来多么大的影响。我们应该理性和冷静地看待西式快餐，在味道和营养中获得平衡。\n23-25.\n臭豆腐是经过发酵后有特殊气味的小块豆腐，闻着臭，吃着香。\n说起臭豆腐的由来，还有这么一段传说。据说清朝康熙八年，安徽有个名叫王致和的读书人来京考试，不幸没有考中。身上带的钱已花完无法返乡，王家原以做豆腐为生,王致和也会这门手艺，于是，为挣够回家路费，他就在北京住了下来，做起了卖豆腐的小买卖。他每天早早就起来制作豆腐，然后挑着豆腐走街串巷叫卖，但生意总是很清淡,仅仅能够吃饱肚子。\n有一次，他做的豆腐剩了不少，他舍不得倒掉，又怕豆腐放久了会变坏，就在豆腐上撒了一些盐，放在罐子里存放起来。过了一段时间，他取出一看，大吃一惊。原来豆腐变成了灰绿色，而且发出一股奇特的味道。他大胆地尝了尝，味道鲜美而可口，这可真使他意想不到。于是.他按照这种方法制作豆腐，并给这种豆腐起名为”臭豆腐”。这一新产品一进入市场就深受顾客欢迎，供不应求。王致和的臭豆腐生意越做越好。从此,臭豆腐的“臭名”便传开了。\n\nB参加考试\nD要赚钱回家\nB意外发明\n\n26-28.\n中国饮食重视“味”，而西方是一种理性饮食观念，不论食物的色、香、味、形如 何，营养一定要得到保证。西方人更关心自己一天要获取多少热量、维生素、蛋白质等问题，即使口味千篇一律，也一定要吃下去—因为有营养。在宴席上，可以讲究餐具，讲究用料，讲究服务，讲究食材形、色方面的搭配，可不管怎么豪华高档，从洛杉矶到纽约，牛排都只有一种味道。作 为菜肴，鸡就是鸡，牛排就是牛排，就是有搭配，也是在盘 中进行的。一盘“法式羊排”，一边放土豆泥，旁边摆块羊 排，另一边配煮青豆，加几片西红柿就可以了。色彩上对比 鲜明，但在味道上各种原料互不相干、互不调和，各是各的 味，简单明了。\n中国是很重视“吃”的民族，“民以食为天”，从这句俗 语就可以看出中国人对吃的重视程度。如果一种文化把吃看成第一位的事，那么就会出现两种现象：一方面会把这种吃的功能发挥到顶峰，不仅维 持生存，还利用它保持健康；另一方面，对吃的过分重视，会促使人不断地追求美味。 中国人对美味的追求达到了极高的程度，就连中国人到国外做生意，也常有人以开餐馆 为业。但我们的很多传统食品都要经过油炸或长时间的文火炖煮，菜肴的营养成分受到 了破坏。当我们对美味过分追求，把它作为第一要求时，我们往往会忽视对食物最根本 的营养要求。\n\nD味道\nB食物的营养\nC中外不同的饮食观念"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第19课-家乡的萝卜饼/index.html#书写",
    "href": "学汉语的日记/HSK5下-第19课-家乡的萝卜饼/index.html#书写",
    "title": "HSK5下 | 第19课： 家乡的萝卜饼",
    "section": "2.3. 书写",
    "text": "2.3. 书写\n口味，特色，色彩，想念，趁\n越南美食以独一无二的口味和丰富的特色而著称。每道菜肴都展现着独特的色彩和味道，令人忍不住想念起家乡的气息。趁着闲暇时间，我经常尝试在家做一些特色小吃，以填补思乡的心情。那些美味的菜肴，不仅带给我视觉和味觉的享受，更是一种对家的深情回应，仿佛在异国他乡的城市，我能够感受到家的温暖。\n著称（zhu4cheng1）"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第17课-在最美好的时刻离开/index.html",
    "href": "学汉语的日记/HSK5上-第17课-在最美好的时刻离开/index.html",
    "title": "HSK5上 | 第17课：在最美好的时刻离开",
    "section": "",
    "text": "在最美好的时刻离开\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n事物\nshì wù - n. thing, object\n\n\n2\n高峰\ngāofēng - n. peak, summit\n\n\n3\n终点\nzhōng diǎn - n. end, destination\n\n\n4\n迅速\nxùn sù - adj. quick, rapid\n\n\n5\n深刻\nshēn kè - adj. deep, profound\n\n\n6\n戏剧\nxì jù - n. drama, play\n\n\n7\n投入\ntóurù - v./n./adj. to put into, to spend on; input; devoted\n\n\n8\n服装\nfúzhuāng - n. clothing, costume\n\n\n9\n化妆\nhuà zhuāng - v. to put on make-up\n\n\n10\n道具\ndàojù - n. stage property, prop\n\n\n11\n美术\nměi shù - n. fine art\n\n\n12\n良好\nliánghǎo - adj. good, fine\n\n\n13\n争取\nzhēng qǔ - v. to strive for, to endeavor to\n\n\n14\n忽视\nhū shì - v. to ignore, to overlook\n\n\n15\n魅力\nmèilì - n. charm\n\n\n16\n糟糕\nzāogāo - adj. bad awful\n\n\n17\n婚礼\nhūnlǐ - n. wedding\n\n\n18\n等于\nděngyú - v. to be equal to\n\n\n19\n度过\ndù guò - v. to pass, to spend\n\n\n20\n告别\ngào bié - v. to say goodbye\n\n\n21\n平常\npíngcháng - adj./n. common, ordinary; mediocrity\n\n\n22\n依然\nyī rán - adv. still nonetheless\n\n\n23\n推荐\ntuī jiàn - v. to recommend\n\n\n24\n淋漓尽致\nlín lí jìn zhì - fully, thoroughly\n\n\n25\n评价\npíng jià - v./n. to evaluate; evaluation, comment\n\n\n26\n烂\nlàn - adj. bad, lame\n\n\n27\n主持\nzhǔ chí - v./n. to host, to preside over; host/hostess\n\n\n28\n运用\nyùn yòng - v. to put into practice, to apply\n\n\n29\n开幕式\nkāi mù shì - n. opening ceremony\n\n\n30\n宁可\nnìng kě - adv. would rather\n\n\n31\n集中\njí zhōng - v./adj. to concentrate, to focus; concentrated\n\n\n32\n体会\ntǐ huì - v./n. to learn from experience, to realize; feeling"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第17课-在最美好的时刻离开/index.html#生词",
    "href": "学汉语的日记/HSK5上-第17课-在最美好的时刻离开/index.html#生词",
    "title": "HSK5上 | 第17课：在最美好的时刻离开",
    "section": "",
    "text": "在最美好的时刻离开\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n事物\nshì wù - n. thing, object\n\n\n2\n高峰\ngāofēng - n. peak, summit\n\n\n3\n终点\nzhōng diǎn - n. end, destination\n\n\n4\n迅速\nxùn sù - adj. quick, rapid\n\n\n5\n深刻\nshēn kè - adj. deep, profound\n\n\n6\n戏剧\nxì jù - n. drama, play\n\n\n7\n投入\ntóurù - v./n./adj. to put into, to spend on; input; devoted\n\n\n8\n服装\nfúzhuāng - n. clothing, costume\n\n\n9\n化妆\nhuà zhuāng - v. to put on make-up\n\n\n10\n道具\ndàojù - n. stage property, prop\n\n\n11\n美术\nměi shù - n. fine art\n\n\n12\n良好\nliánghǎo - adj. good, fine\n\n\n13\n争取\nzhēng qǔ - v. to strive for, to endeavor to\n\n\n14\n忽视\nhū shì - v. to ignore, to overlook\n\n\n15\n魅力\nmèilì - n. charm\n\n\n16\n糟糕\nzāogāo - adj. bad awful\n\n\n17\n婚礼\nhūnlǐ - n. wedding\n\n\n18\n等于\nděngyú - v. to be equal to\n\n\n19\n度过\ndù guò - v. to pass, to spend\n\n\n20\n告别\ngào bié - v. to say goodbye\n\n\n21\n平常\npíngcháng - adj./n. common, ordinary; mediocrity\n\n\n22\n依然\nyī rán - adv. still nonetheless\n\n\n23\n推荐\ntuī jiàn - v. to recommend\n\n\n24\n淋漓尽致\nlín lí jìn zhì - fully, thoroughly\n\n\n25\n评价\npíng jià - v./n. to evaluate; evaluation, comment\n\n\n26\n烂\nlàn - adj. bad, lame\n\n\n27\n主持\nzhǔ chí - v./n. to host, to preside over; host/hostess\n\n\n28\n运用\nyùn yòng - v. to put into practice, to apply\n\n\n29\n开幕式\nkāi mù shì - n. opening ceremony\n\n\n30\n宁可\nnìng kě - adv. would rather\n\n\n31\n集中\njí zhōng - v./adj. to concentrate, to focus; concentrated\n\n\n32\n体会\ntǐ huì - v./n. to learn from experience, to realize; feeling"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第17课-在最美好的时刻离开/index.html#听力",
    "href": "学汉语的日记/HSK5上-第17课-在最美好的时刻离开/index.html#听力",
    "title": "HSK5上 | 第17课：在最美好的时刻离开",
    "section": "2.1. 听力",
    "text": "2.1. 听力"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第17课-在最美好的时刻离开/index.html#阅读",
    "href": "学汉语的日记/HSK5上-第17课-在最美好的时刻离开/index.html#阅读",
    "title": "HSK5上 | 第17课：在最美好的时刻离开",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n19.\n20.\n21.\n22.\n23-25.\n23.\n24.\n25.\n26-28.\n26.\n27.\n28."
  },
  {
    "objectID": "学汉语的日记/HSK5上-第17课-在最美好的时刻离开/index.html#书写",
    "href": "学汉语的日记/HSK5上-第17课-在最美好的时刻离开/index.html#书写",
    "title": "HSK5上 | 第17课：在最美好的时刻离开",
    "section": "2.3. 书写",
    "text": "2.3. 书写"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第15课-纸上谈兵/index.html",
    "href": "学汉语的日记/HSK5上-第15课-纸上谈兵/index.html",
    "title": "HSK5上 | 第15课：纸上谈兵",
    "section": "",
    "text": "纸上谈兵\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n纸上谈兵\nzhǐ shàng tán bīng － to be an armchair strategist\n\n\n2\n军事\njūn shì - n. military affairs\n\n\n3\n敌（人）\ndí (rén) - n. enemy\n\n\n4\n理论\nlǐ lùn - n. theory\n\n\n5\n作战\nzuò zhàn - v. to fight a battle\n\n\n6\n毛病\nmáo bìng - n. shortcoming, weakness\n\n\n7\n道理\ndàolǐ - n. principle, theory\n\n\n8\n迟早\nchí zǎo - adv. sooner or alter\n\n\n9\n军队\njūn duì - n. army\n\n\n10\n派\npài - v. to send, to assign, to appoint\n\n\n11\n弱\nruò - adj. weak\n\n\n12\n形势\nxíng shì - n. situation, state of affairs\n\n\n13\n命令\nmìnglìng - v./n. to command; order\n\n\n14\n守\nshǒu - v. to guard, to defend\n\n\n15\n阵地\nzhèndì - n. position, front\n\n\n16\n绝对\njué duì - adv./adj. absolutely, definitely; absolute\n\n\n17\n主动\nzhǔ dòng - adj. on one’s own initiative\n\n\n18\n挑战\ntiǎo zhàn - v./n. to challenge, to battle; challenge\n\n\n19\n骂\nmà - v. to curse, to call names\n\n\n20\n胆小鬼\ndǎn xiǎo guǐ - n. coward\n\n\n21\n胜利\nshèng lì - v. to win a victory\n\n\n22\n调（动）\ndiào (dòng) - v. to transfer, to shift\n\n\n23\n散布\nsàn bù - v. to spread, to disseminate\n\n\n24\n谣言\nyáo yán - n. rumor\n\n\n25\n上当\nshàng dàng - v. to be taken in, to be deceived\n\n\n26\n再三\nzàisān - adv. again and again\n\n\n27\n阻止\nzǔ zhǐ - v. to stop, to prevent\n\n\n28\n任命\nrènmìng - v. to appoint\n\n\n29\n独立\ndúlì - v. to be on one’s own\n\n\n30\n资格\nzī gé - n. qualification\n\n\n31\n糊涂\nhú tu - adj. muddleheaded\n\n\n32\n公元\ngōng yuán - n. Christian era\n\n\n33\n盲目\nmángmù - adj. blind, ignorant\n\n\n34\n轻视\nqīng shì - v. to look down upon, to belittle\n\n\n35\n方案\nfāng àn - n. plan, scheme\n\n\n36\n进攻\njìn gōng - v. to attack, to assault\n\n\n37\n宝贵\nbǎo guì - adj. precious, valuable\n\n\n38\n讽刺\nfěng cì - v. to satirize, to mock, irony, sarcasm\n\n\n39\n灵活\nlíng huó - adj. flexible, elastic"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第15课-纸上谈兵/index.html#生词",
    "href": "学汉语的日记/HSK5上-第15课-纸上谈兵/index.html#生词",
    "title": "HSK5上 | 第15课：纸上谈兵",
    "section": "",
    "text": "纸上谈兵\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n纸上谈兵\nzhǐ shàng tán bīng － to be an armchair strategist\n\n\n2\n军事\njūn shì - n. military affairs\n\n\n3\n敌（人）\ndí (rén) - n. enemy\n\n\n4\n理论\nlǐ lùn - n. theory\n\n\n5\n作战\nzuò zhàn - v. to fight a battle\n\n\n6\n毛病\nmáo bìng - n. shortcoming, weakness\n\n\n7\n道理\ndàolǐ - n. principle, theory\n\n\n8\n迟早\nchí zǎo - adv. sooner or alter\n\n\n9\n军队\njūn duì - n. army\n\n\n10\n派\npài - v. to send, to assign, to appoint\n\n\n11\n弱\nruò - adj. weak\n\n\n12\n形势\nxíng shì - n. situation, state of affairs\n\n\n13\n命令\nmìnglìng - v./n. to command; order\n\n\n14\n守\nshǒu - v. to guard, to defend\n\n\n15\n阵地\nzhèndì - n. position, front\n\n\n16\n绝对\njué duì - adv./adj. absolutely, definitely; absolute\n\n\n17\n主动\nzhǔ dòng - adj. on one’s own initiative\n\n\n18\n挑战\ntiǎo zhàn - v./n. to challenge, to battle; challenge\n\n\n19\n骂\nmà - v. to curse, to call names\n\n\n20\n胆小鬼\ndǎn xiǎo guǐ - n. coward\n\n\n21\n胜利\nshèng lì - v. to win a victory\n\n\n22\n调（动）\ndiào (dòng) - v. to transfer, to shift\n\n\n23\n散布\nsàn bù - v. to spread, to disseminate\n\n\n24\n谣言\nyáo yán - n. rumor\n\n\n25\n上当\nshàng dàng - v. to be taken in, to be deceived\n\n\n26\n再三\nzàisān - adv. again and again\n\n\n27\n阻止\nzǔ zhǐ - v. to stop, to prevent\n\n\n28\n任命\nrènmìng - v. to appoint\n\n\n29\n独立\ndúlì - v. to be on one’s own\n\n\n30\n资格\nzī gé - n. qualification\n\n\n31\n糊涂\nhú tu - adj. muddleheaded\n\n\n32\n公元\ngōng yuán - n. Christian era\n\n\n33\n盲目\nmángmù - adj. blind, ignorant\n\n\n34\n轻视\nqīng shì - v. to look down upon, to belittle\n\n\n35\n方案\nfāng àn - n. plan, scheme\n\n\n36\n进攻\njìn gōng - v. to attack, to assault\n\n\n37\n宝贵\nbǎo guì - adj. precious, valuable\n\n\n38\n讽刺\nfěng cì - v. to satirize, to mock, irony, sarcasm\n\n\n39\n灵活\nlíng huó - adj. flexible, elastic"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第15课-纸上谈兵/index.html#听力",
    "href": "学汉语的日记/HSK5上-第15课-纸上谈兵/index.html#听力",
    "title": "HSK5上 | 第15课：纸上谈兵",
    "section": "2.1. 听力",
    "text": "2.1. 听力"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第15课-纸上谈兵/index.html#阅读",
    "href": "学汉语的日记/HSK5上-第15课-纸上谈兵/index.html#阅读",
    "title": "HSK5上 | 第15课：纸上谈兵",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n19.\n20.\n21.\n22.\n23-25.\n23.\n24.\n25.\n26-28.\n26.\n27.\n28."
  },
  {
    "objectID": "学汉语的日记/HSK5上-第15课-纸上谈兵/index.html#书写",
    "href": "学汉语的日记/HSK5上-第15课-纸上谈兵/index.html#书写",
    "title": "HSK5上 | 第15课：纸上谈兵",
    "section": "2.3. 书写",
    "text": "2.3. 书写"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第13课-据掉生活的筐底/index.html",
    "href": "学汉语的日记/HSK5上-第13课-据掉生活的筐底/index.html",
    "title": "HSK5上 | 第13课：据掉生活的“筐底”",
    "section": "",
    "text": "据掉生活的“筐底”\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n锯（子）\njù (zi) - v./n. to cut with a saw; saw\n\n\n2\n筐\nkuāng - n. basket\n\n\n3\n训练\nxùnliàn - v. to train\n\n\n4\n缺乏\nquē fá - v. to lack, to be short of\n\n\n5\n项目\nxiàngmù - n. item, project\n\n\n6\n桃\ntáo - n. peach\n\n\n7\n装\nzhuāng - v. to load, to hold\n\n\n8\n启发\nqǐ fā - v. to enlighten, to inspire\n\n\n9\n安装\nān zhuāng - v. to install, to fix\n\n\n10\n栏杆\nlán gān - n. railing, balustrade\n\n\n11\n甲\njiǎ - n. first\n\n\n12\n乙\nyǐ - n. second\n\n\n13\n工具\ngōng jù - n. tool, instrument\n\n\n14\n投篮\ntóu lán - v. to shoot (a basket)\n\n\n15\n踩\ncǎi - v. to step on, to tread on\n\n\n16\n一再\nyī zài - adv. over and over again\n\n\n17\n重复\nchóng fù - v. to repeat\n\n\n18\n断断续续\nduàn duàn xù xù - adj. off and on, intermittent\n\n\n19\n激烈\njī liè - adj. intense, fierce\n\n\n20\n气氛\nqì fēn - n. atmosphere\n\n\n21\n何况\nhé kuàng - conj. let alone\n\n\n22\n球迷\nqiú mí - n. (ball game) fan\n\n\n23\n工程师\ngōng chéng shī - n. engineer\n\n\n24\n机器\njī qì - n. machine\n\n\n25\n顺畅\nshùnchàng - adj. smooth, unhindered\n\n\n26\n幼儿园\nyòu’éryuán - n. kindergarten\n\n\n27\n好奇\nhàoqí - adj. curious\n\n\n28\n何必\nhé bì - adv. why (indicating that there is no need for sth.)\n\n\n29\n多亏\nduō kuī - v. thanks to, luckily\n\n\n30\n连忙\nlián máng - adv. promptly, at once\n\n\n31\n瞧\nqiáo - v. to look, to see\n\n\n32\n困扰\nkùn rǎo - v. to trouble, to haunt\n\n\n33\n思维\nsī wéi - n./v. thinking; to think\n\n\n34\n呆\ndāi - adj./v. dull, dumb; to stagnate\n\n\n35\n造成\nzào chéng - v. to cause, to give rise to\n\n\n36\n仿佛\nfǎng fú - adv./v. as if; to be like, to be similar to\n\n\n37\n阻碍\nzǔài - v. to hinder, to impede"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第13课-据掉生活的筐底/index.html#生词",
    "href": "学汉语的日记/HSK5上-第13课-据掉生活的筐底/index.html#生词",
    "title": "HSK5上 | 第13课：据掉生活的“筐底”",
    "section": "",
    "text": "据掉生活的“筐底”\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n锯（子）\njù (zi) - v./n. to cut with a saw; saw\n\n\n2\n筐\nkuāng - n. basket\n\n\n3\n训练\nxùnliàn - v. to train\n\n\n4\n缺乏\nquē fá - v. to lack, to be short of\n\n\n5\n项目\nxiàngmù - n. item, project\n\n\n6\n桃\ntáo - n. peach\n\n\n7\n装\nzhuāng - v. to load, to hold\n\n\n8\n启发\nqǐ fā - v. to enlighten, to inspire\n\n\n9\n安装\nān zhuāng - v. to install, to fix\n\n\n10\n栏杆\nlán gān - n. railing, balustrade\n\n\n11\n甲\njiǎ - n. first\n\n\n12\n乙\nyǐ - n. second\n\n\n13\n工具\ngōng jù - n. tool, instrument\n\n\n14\n投篮\ntóu lán - v. to shoot (a basket)\n\n\n15\n踩\ncǎi - v. to step on, to tread on\n\n\n16\n一再\nyī zài - adv. over and over again\n\n\n17\n重复\nchóng fù - v. to repeat\n\n\n18\n断断续续\nduàn duàn xù xù - adj. off and on, intermittent\n\n\n19\n激烈\njī liè - adj. intense, fierce\n\n\n20\n气氛\nqì fēn - n. atmosphere\n\n\n21\n何况\nhé kuàng - conj. let alone\n\n\n22\n球迷\nqiú mí - n. (ball game) fan\n\n\n23\n工程师\ngōng chéng shī - n. engineer\n\n\n24\n机器\njī qì - n. machine\n\n\n25\n顺畅\nshùnchàng - adj. smooth, unhindered\n\n\n26\n幼儿园\nyòu’éryuán - n. kindergarten\n\n\n27\n好奇\nhàoqí - adj. curious\n\n\n28\n何必\nhé bì - adv. why (indicating that there is no need for sth.)\n\n\n29\n多亏\nduō kuī - v. thanks to, luckily\n\n\n30\n连忙\nlián máng - adv. promptly, at once\n\n\n31\n瞧\nqiáo - v. to look, to see\n\n\n32\n困扰\nkùn rǎo - v. to trouble, to haunt\n\n\n33\n思维\nsī wéi - n./v. thinking; to think\n\n\n34\n呆\ndāi - adj./v. dull, dumb; to stagnate\n\n\n35\n造成\nzào chéng - v. to cause, to give rise to\n\n\n36\n仿佛\nfǎng fú - adv./v. as if; to be like, to be similar to\n\n\n37\n阻碍\nzǔài - v. to hinder, to impede"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第13课-据掉生活的筐底/index.html#听力",
    "href": "学汉语的日记/HSK5上-第13课-据掉生活的筐底/index.html#听力",
    "title": "HSK5上 | 第13课：据掉生活的“筐底”",
    "section": "2.1. 听力",
    "text": "2.1. 听力"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第13课-据掉生活的筐底/index.html#阅读",
    "href": "学汉语的日记/HSK5上-第13课-据掉生活的筐底/index.html#阅读",
    "title": "HSK5上 | 第13课：据掉生活的“筐底”",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n19.\n20.\n21.\n22.\n23-25.\n23.\n24.\n25.\n26-28.\n26.\n27.\n28."
  },
  {
    "objectID": "学汉语的日记/HSK5上-第13课-据掉生活的筐底/index.html#书写",
    "href": "学汉语的日记/HSK5上-第13课-据掉生活的筐底/index.html#书写",
    "title": "HSK5上 | 第13课：据掉生活的“筐底”",
    "section": "2.3. 书写",
    "text": "2.3. 书写"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第11课-闹钟的危害/index.html",
    "href": "学汉语的日记/HSK5上-第11课-闹钟的危害/index.html",
    "title": "HSK5上 | 第11课：闹钟的危害",
    "section": "",
    "text": "闹钟的危害\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n危害\nwēi hài - to harm, to jeopardize\n\n\n2\n人类\nrén lèi - humankind\n\n\n3\n机制\njīzhì - mechanism\n\n\n4\n生物\nshēng wù - living things\n\n\n5\n规律\nguī lǜ - law, regular pattern; regular\n\n\n6\n光线\nguāngxiàn - light; ray\n\n\n7\n必要\nbìyào - necessary, essential\n\n\n8\n过度\nguò dù - to transit\n\n\n9\n浅\nqiǎn - shallow, light\n\n\n10\n现代\nxiàn dài - modern times; modern\n\n\n11\n享受\nxiǎng shòu - to enjoy\n\n\n12\n用途\nyòngtú - n. use, purpose\n\n\n13\n实验\nshí yàn - experiment; to make an experiment\n\n\n14\n铃\nlíng - bell\n\n\n15\n所\nsuǒ - part. use before verb followed by noun which is the receiver of the action\n\n\n16\n状态\nzhuàngtài - state, status\n\n\n17\n清醒\nqīng xǐng - sober, to regain consciousness\n\n\n18\n呼吸\nhūxī - to breathe\n\n\n19\n心理\nxīn lǐ - mentality, psychology\n\n\n20\n慌张\nhuāng zhāng - flurried, flustered\n\n\n21\n情绪\nqíng xù - emotion, mood\n\n\n22\n低落\ndīluò - down, depressed\n\n\n23\n记忆\njì yì - to remember; memory\n\n\n24\n计算\njì suàn - to calculate, to compute\n\n\n25\n相当\nxiāng dāng - to be equal to\n\n\n26\n持续\nchí xù - to continue, to last\n\n\n27\n数\nshù - reveral\n\n\n28\n导致\ndǎo zhì - to cause, to lead to\n\n\n29\n失眠\nshī mián - to suffer from insomnia\n\n\n30\n专家\nzhuān jiā - expert\n\n\n31\n采用\ncǎi yòng - to use, to employ\n\n\n32\n柔和\nróuhé - gentle, soft\n\n\n33\n愿望\nyuàn wàng - n. wish, hope\n\n\n34\n窗帘\nchuāng lián - curtain\n\n\n35\n市场\nshì chǎng - market\n\n\n36\n产品\nchǎn pǐn - product, produce\n\n\n37\n模仿\nmó fǎng - to imitate, to model on\n\n\n38\n避免\nbì miǎn - to prevent, to avoid\n\n\n39\n传统\nchuán tǒng - tradition; traditional"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第11课-闹钟的危害/index.html#生词",
    "href": "学汉语的日记/HSK5上-第11课-闹钟的危害/index.html#生词",
    "title": "HSK5上 | 第11课：闹钟的危害",
    "section": "",
    "text": "闹钟的危害\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n危害\nwēi hài - to harm, to jeopardize\n\n\n2\n人类\nrén lèi - humankind\n\n\n3\n机制\njīzhì - mechanism\n\n\n4\n生物\nshēng wù - living things\n\n\n5\n规律\nguī lǜ - law, regular pattern; regular\n\n\n6\n光线\nguāngxiàn - light; ray\n\n\n7\n必要\nbìyào - necessary, essential\n\n\n8\n过度\nguò dù - to transit\n\n\n9\n浅\nqiǎn - shallow, light\n\n\n10\n现代\nxiàn dài - modern times; modern\n\n\n11\n享受\nxiǎng shòu - to enjoy\n\n\n12\n用途\nyòngtú - n. use, purpose\n\n\n13\n实验\nshí yàn - experiment; to make an experiment\n\n\n14\n铃\nlíng - bell\n\n\n15\n所\nsuǒ - part. use before verb followed by noun which is the receiver of the action\n\n\n16\n状态\nzhuàngtài - state, status\n\n\n17\n清醒\nqīng xǐng - sober, to regain consciousness\n\n\n18\n呼吸\nhūxī - to breathe\n\n\n19\n心理\nxīn lǐ - mentality, psychology\n\n\n20\n慌张\nhuāng zhāng - flurried, flustered\n\n\n21\n情绪\nqíng xù - emotion, mood\n\n\n22\n低落\ndīluò - down, depressed\n\n\n23\n记忆\njì yì - to remember; memory\n\n\n24\n计算\njì suàn - to calculate, to compute\n\n\n25\n相当\nxiāng dāng - to be equal to\n\n\n26\n持续\nchí xù - to continue, to last\n\n\n27\n数\nshù - reveral\n\n\n28\n导致\ndǎo zhì - to cause, to lead to\n\n\n29\n失眠\nshī mián - to suffer from insomnia\n\n\n30\n专家\nzhuān jiā - expert\n\n\n31\n采用\ncǎi yòng - to use, to employ\n\n\n32\n柔和\nróuhé - gentle, soft\n\n\n33\n愿望\nyuàn wàng - n. wish, hope\n\n\n34\n窗帘\nchuāng lián - curtain\n\n\n35\n市场\nshì chǎng - market\n\n\n36\n产品\nchǎn pǐn - product, produce\n\n\n37\n模仿\nmó fǎng - to imitate, to model on\n\n\n38\n避免\nbì miǎn - to prevent, to avoid\n\n\n39\n传统\nchuán tǒng - tradition; traditional"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第11课-闹钟的危害/index.html#听力",
    "href": "学汉语的日记/HSK5上-第11课-闹钟的危害/index.html#听力",
    "title": "HSK5上 | 第11课：闹钟的危害",
    "section": "2.1. 听力",
    "text": "2.1. 听力"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第11课-闹钟的危害/index.html#阅读",
    "href": "学汉语的日记/HSK5上-第11课-闹钟的危害/index.html#阅读",
    "title": "HSK5上 | 第11课：闹钟的危害",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n19.\n20.\n21.\n22.\n23-25.\n23.\n24.\n25.\n26-28.\n26.\n27.\n28."
  },
  {
    "objectID": "学汉语的日记/HSK5上-第11课-闹钟的危害/index.html#书写",
    "href": "学汉语的日记/HSK5上-第11课-闹钟的危害/index.html#书写",
    "title": "HSK5上 | 第11课：闹钟的危害",
    "section": "2.3. 书写",
    "text": "2.3. 书写"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第09课-别样鲁迅/index.html",
    "href": "学汉语的日记/HSK5上-第09课-别样鲁迅/index.html",
    "title": "HSK5上 | 第09课：别样鲁迅",
    "section": "",
    "text": "别样鲁迅\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n表现\nbiǎo xiàn - to show, to display; manifestation\n\n\n2\n突出\ntū chū - prominent, salient (most noticable)\n\n\n3\n文学家\nwén xué jiā - writer / man of letters\n\n\n4\n算\nsuàn - to regard as; to count as; to calculate\n\n\n5\n地道\ndì dào - true, genuine, authentic\n\n\n6\n行家\nháng jiā - expert\n\n\n7\n亲自\nqīn zì - personally, in person\n\n\n8\n见解\njiàn jiě - understanding, opinion\n\n\n9\n近代\njìndài - modern times (mid 19th century to 1919)\n\n\n10\n时尚\nshí shàng - fashion\n\n\n11\n写作\nxiě zuò - to write\n\n\n12\n点心\ndiǎn xin - dessert, dimsum\n\n\n13\n作为\nzuò wéi - to be, as, being\n\n\n14\n学问\nxué wèn - knowledge, learning\n\n\n15\n讲究\njiǎng jiu - to be particular about, to stress\n\n\n16\n平均\npíngjūn - average\n\n\n17\n胡同\nhú tòng - alley, lane\n\n\n18\n位于\nwèi yú - to be located at (in, on, etc.)\n\n\n19\n首\nshǒu - first, head, m. for songs and poems\n\n\n20\n豪华\nháo huá - luxurious, lavish\n\n\n21\n光临\nguāng lín - to visit, to frequent\n\n\n22\n交际\njiāo - social contact, communication\n\n\n23\n大方\ndà fāng - generous\n\n\n24\n好客\nhàokè - to be hospitable\n\n\n25\n招待\nzhāodài - to receive, to entertain\n\n\n26\n高档\ngāo dàng - high grade, top-grade\n\n\n27\n胃口\nwèi kǒu - appetite\n\n\n28\n明明\nmíngmíng - evidently, undoubtedly\n\n\n29\n胃\nwèi - stomach\n\n\n30\n戒\njiè - to give up, to quit, to abstain\n\n\n31\n保存\nbǎocún - to keep, to save\n\n\n32\n资料\nzīliào - data, material\n\n\n33\n曾经\ncéng jīng - once, in the past\n\n\n34\n形容\nxíng róng - to describe, to depict\n\n\n35\n蒙眬\nméng lóng - drowsy, half-asleep\n\n\n36\n悠悠\nyōuyōu - leisurely, unhurried\n\n\n37\n形象\nxíng xiàng - vivid"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第09课-别样鲁迅/index.html#生词",
    "href": "学汉语的日记/HSK5上-第09课-别样鲁迅/index.html#生词",
    "title": "HSK5上 | 第09课：别样鲁迅",
    "section": "",
    "text": "别样鲁迅\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n表现\nbiǎo xiàn - to show, to display; manifestation\n\n\n2\n突出\ntū chū - prominent, salient (most noticable)\n\n\n3\n文学家\nwén xué jiā - writer / man of letters\n\n\n4\n算\nsuàn - to regard as; to count as; to calculate\n\n\n5\n地道\ndì dào - true, genuine, authentic\n\n\n6\n行家\nháng jiā - expert\n\n\n7\n亲自\nqīn zì - personally, in person\n\n\n8\n见解\njiàn jiě - understanding, opinion\n\n\n9\n近代\njìndài - modern times (mid 19th century to 1919)\n\n\n10\n时尚\nshí shàng - fashion\n\n\n11\n写作\nxiě zuò - to write\n\n\n12\n点心\ndiǎn xin - dessert, dimsum\n\n\n13\n作为\nzuò wéi - to be, as, being\n\n\n14\n学问\nxué wèn - knowledge, learning\n\n\n15\n讲究\njiǎng jiu - to be particular about, to stress\n\n\n16\n平均\npíngjūn - average\n\n\n17\n胡同\nhú tòng - alley, lane\n\n\n18\n位于\nwèi yú - to be located at (in, on, etc.)\n\n\n19\n首\nshǒu - first, head, m. for songs and poems\n\n\n20\n豪华\nháo huá - luxurious, lavish\n\n\n21\n光临\nguāng lín - to visit, to frequent\n\n\n22\n交际\njiāo - social contact, communication\n\n\n23\n大方\ndà fāng - generous\n\n\n24\n好客\nhàokè - to be hospitable\n\n\n25\n招待\nzhāodài - to receive, to entertain\n\n\n26\n高档\ngāo dàng - high grade, top-grade\n\n\n27\n胃口\nwèi kǒu - appetite\n\n\n28\n明明\nmíngmíng - evidently, undoubtedly\n\n\n29\n胃\nwèi - stomach\n\n\n30\n戒\njiè - to give up, to quit, to abstain\n\n\n31\n保存\nbǎocún - to keep, to save\n\n\n32\n资料\nzīliào - data, material\n\n\n33\n曾经\ncéng jīng - once, in the past\n\n\n34\n形容\nxíng róng - to describe, to depict\n\n\n35\n蒙眬\nméng lóng - drowsy, half-asleep\n\n\n36\n悠悠\nyōuyōu - leisurely, unhurried\n\n\n37\n形象\nxíng xiàng - vivid"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第09课-别样鲁迅/index.html#听力",
    "href": "学汉语的日记/HSK5上-第09课-别样鲁迅/index.html#听力",
    "title": "HSK5上 | 第09课：别样鲁迅",
    "section": "2.1. 听力",
    "text": "2.1. 听力"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第09课-别样鲁迅/index.html#阅读",
    "href": "学汉语的日记/HSK5上-第09课-别样鲁迅/index.html#阅读",
    "title": "HSK5上 | 第09课：别样鲁迅",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n19.\n20.\n21.\n22.\n23-25.\n23.\n24.\n25.\n26-28.\n26.\n27.\n28."
  },
  {
    "objectID": "学汉语的日记/HSK5上-第09课-别样鲁迅/index.html#书写",
    "href": "学汉语的日记/HSK5上-第09课-别样鲁迅/index.html#书写",
    "title": "HSK5上 | 第09课：别样鲁迅",
    "section": "2.3. 书写",
    "text": "2.3. 书写"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第07课-成语故事两则/index.html",
    "href": "学汉语的日记/HSK5上-第07课-成语故事两则/index.html",
    "title": "HSK5上 | 第07课：成语故事两则",
    "section": "",
    "text": "成语故事两则\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n成语\nchéng yǔ - idiom, set phrase\n\n\n2\n则\nzé - measure word for news, writings\n\n\n3\n盲人\nmáng rén - blind person\n\n\n4\n摸\nmō - to feel, to touch, to stroke\n\n\n5\n大象\ndà xiàng - elephant\n\n\n6\n智慧\nzhì huì - wisdom\n\n\n7\n士兵\nshì bīng - soldier\n\n\n8\n瞎\nxiā - to be blind; blindly; foolishly\n\n\n9\n分别\nfēnbié - seperately; respectively; difference; differentiate\n\n\n10\n寻找\nxún zhǎo - to look for, to seek\n\n\n11\n牙齿\nyá chǐ - tooth\n\n\n12\n胡说\nhú shuō - to talk nonsense\n\n\n13\n尾巴\nwěi ba - tail\n\n\n14\n绳子\nshéng zi - rope\n\n\n15\n平\npíng - flat, level\n\n\n16\n墙\nqiáng - wall\n\n\n17\n扇子\nshàn zi - fan\n\n\n18\n片面\npiàn miàn - one-sided\n\n\n19\n结论\njié lùn - conclusion\n\n\n20\n将军\njiāngjūn - general (military rank)\n\n\n21\n善于\nshàn yú - be good at\n\n\n22\n称\nchēng - to call, to give a particular name\n\n\n23\n打猎\ndǎ liè - to go hunting\n\n\n24\n忽然\nhū rán - suddenly\n\n\n25\n蹲\ndūn - to squat, to crouch\n\n\n26\n摇\nyáo - to wave, to shake\n\n\n27\n不要紧\nbú yào jǐn- to doesn’t matter\n\n\n28\n支\nzhī - measure word for long, thin, inflexible objects\n\n\n29\n摆\nbǎi - to put; to place; to set in order\n\n\n30\n姿势\nzī shì - pose, posture\n\n\n31\n全神贯注\nquán shén guàn zhù - to concentrate on, to be absorbed in\n\n\n32\n尽力\njìn lì - to try one’s best\n\n\n33\n反应\nfǎn yìng - response, to react\n\n\n34\n确定\nquè dìng - to confirm, to make sure\n\n\n35\n石头\nshí tou - stone, rock\n\n\n36\n连续\nlián xù - to be continuous, to be in succession\n\n\n37\n根\ngēn - root (plant), foundation, measure for long and thin objects\n\n\n38\n碎\nsuì - to break into pieces; broken, fragmentary\n\n\n39\n杆\ngān - pole, shaft\n\n\n40\n哎\nāi - used to express surprise or dissatisfaction\n\n\n41\n唉\nāi - sighing sound indicating sadness or regret\n\n\n42\n金属\njīnshǔ - metal\n\n\n43\n硬\nyìng - hard, tough\n\n\n44\n便\nbiàn - (like 就 but more formal) used to indicate something comes naturally under certain conditions or circumstances"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第07课-成语故事两则/index.html#生词",
    "href": "学汉语的日记/HSK5上-第07课-成语故事两则/index.html#生词",
    "title": "HSK5上 | 第07课：成语故事两则",
    "section": "",
    "text": "成语故事两则\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n成语\nchéng yǔ - idiom, set phrase\n\n\n2\n则\nzé - measure word for news, writings\n\n\n3\n盲人\nmáng rén - blind person\n\n\n4\n摸\nmō - to feel, to touch, to stroke\n\n\n5\n大象\ndà xiàng - elephant\n\n\n6\n智慧\nzhì huì - wisdom\n\n\n7\n士兵\nshì bīng - soldier\n\n\n8\n瞎\nxiā - to be blind; blindly; foolishly\n\n\n9\n分别\nfēnbié - seperately; respectively; difference; differentiate\n\n\n10\n寻找\nxún zhǎo - to look for, to seek\n\n\n11\n牙齿\nyá chǐ - tooth\n\n\n12\n胡说\nhú shuō - to talk nonsense\n\n\n13\n尾巴\nwěi ba - tail\n\n\n14\n绳子\nshéng zi - rope\n\n\n15\n平\npíng - flat, level\n\n\n16\n墙\nqiáng - wall\n\n\n17\n扇子\nshàn zi - fan\n\n\n18\n片面\npiàn miàn - one-sided\n\n\n19\n结论\njié lùn - conclusion\n\n\n20\n将军\njiāngjūn - general (military rank)\n\n\n21\n善于\nshàn yú - be good at\n\n\n22\n称\nchēng - to call, to give a particular name\n\n\n23\n打猎\ndǎ liè - to go hunting\n\n\n24\n忽然\nhū rán - suddenly\n\n\n25\n蹲\ndūn - to squat, to crouch\n\n\n26\n摇\nyáo - to wave, to shake\n\n\n27\n不要紧\nbú yào jǐn- to doesn’t matter\n\n\n28\n支\nzhī - measure word for long, thin, inflexible objects\n\n\n29\n摆\nbǎi - to put; to place; to set in order\n\n\n30\n姿势\nzī shì - pose, posture\n\n\n31\n全神贯注\nquán shén guàn zhù - to concentrate on, to be absorbed in\n\n\n32\n尽力\njìn lì - to try one’s best\n\n\n33\n反应\nfǎn yìng - response, to react\n\n\n34\n确定\nquè dìng - to confirm, to make sure\n\n\n35\n石头\nshí tou - stone, rock\n\n\n36\n连续\nlián xù - to be continuous, to be in succession\n\n\n37\n根\ngēn - root (plant), foundation, measure for long and thin objects\n\n\n38\n碎\nsuì - to break into pieces; broken, fragmentary\n\n\n39\n杆\ngān - pole, shaft\n\n\n40\n哎\nāi - used to express surprise or dissatisfaction\n\n\n41\n唉\nāi - sighing sound indicating sadness or regret\n\n\n42\n金属\njīnshǔ - metal\n\n\n43\n硬\nyìng - hard, tough\n\n\n44\n便\nbiàn - (like 就 but more formal) used to indicate something comes naturally under certain conditions or circumstances"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第07课-成语故事两则/index.html#听力",
    "href": "学汉语的日记/HSK5上-第07课-成语故事两则/index.html#听力",
    "title": "HSK5上 | 第07课：成语故事两则",
    "section": "2.1. 听力",
    "text": "2.1. 听力"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第07课-成语故事两则/index.html#阅读",
    "href": "学汉语的日记/HSK5上-第07课-成语故事两则/index.html#阅读",
    "title": "HSK5上 | 第07课：成语故事两则",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n19.\n20.\n21.\n22.\n23-25.\n23.\n24.\n25.\n26-28.\n26.\n27.\n28."
  },
  {
    "objectID": "学汉语的日记/HSK5上-第07课-成语故事两则/index.html#书写",
    "href": "学汉语的日记/HSK5上-第07课-成语故事两则/index.html#书写",
    "title": "HSK5上 | 第07课：成语故事两则",
    "section": "2.3. 书写",
    "text": "2.3. 书写"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第05课-济南的泉水/index.html",
    "href": "学汉语的日记/HSK5上-第05课-济南的泉水/index.html",
    "title": "HSK5上 | 第05课：济南的泉水",
    "section": "",
    "text": "济南的泉水\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n悠久\nyōu jiǔ - long-standing, age-old\n\n\n2\n文字\nwénzì - written language\n\n\n3\n记载\njìzǎi - to record, to put down in writing\n\n\n4\n形状\nxíngzhuàng - shape\n\n\n5\n描写\nmiáo xiě - to describe, to depict\n\n\n6\n赞美\nzàn měi - to praise, to extol\n\n\n7\n诗\nshī - poem\n\n\n8\n老百姓\nLǎobǎixìng - ordinary people, civilians\n\n\n9\n充满\nchōng mǎn - to be full of\n\n\n10\n感激\ngǎn jī - to feel grateful\n\n\n11\n从而\ncóng ér - thus, consequently\n\n\n12\n产生\nchǎn shēng - to emerge, to arise\n\n\n13\n传说\nchuán shuō - legend\n\n\n14\n善良\nshàn liáng - kind-hearted\n\n\n15\n救\njiù - to save, to rescue\n\n\n16\n晕\nyūn - to faint, to pass out\n\n\n17\n龙\nlóng - dragon\n\n\n18\n治疗\nzhì liáo - to treat, to cure\n\n\n19\n玉\nyù - jade\n\n\n20\n壶\nhú - pot, kettle\n\n\n21\n抢\nqiǎng - to rob, to snatch\n\n\n22\n埋\nmái - to bury\n\n\n23\n躲藏\nduǒ cáng - to hide\n\n\n24\n如今\nrú jīn - nowadays, present\n\n\n25\n分布\nfēnbù - to be distributed (over an area), to be scattered\n\n\n26\n天然\ntiānrán - natural\n\n\n27\n优美\nyōu měi - graceful, beautiful\n\n\n28\n独特\ndú tè - unique; distinct\n\n\n29\n反映\nfǎn yìng - to reflect, to mirror\n\n\n30\n珍珠\nzhēnzhū - pearl\n\n\n31\n形成\nxíng chéng - to form; to take shape\n\n\n32\n于\nyú - from, out of, in, at, to, by, than\n\n\n33\n广大\nguǎng dà - vast, extensive\n\n\n34\n岩石\nyánshí - rock\n\n\n35\n亿\nyì - one hundred million\n\n\n36\n石灰岩\nshíhuīyán - limestone\n\n\n37\n地区\ndì qū - area / region\n\n\n38\n表面\nbiǎomiàn - surface\n\n\n39\n角度\njiǎodù - angle, degree of angle\n\n\n40\n斜\nxié - oblique, slanting\n\n\n41\n为\nwéi - to become\n\n\n42\n火成岩\nhuǒchéngyán - igneous rock\n\n\n43\n碰\npèng - to touch, to meet, to come across\n\n\n44\n挡\ndǎng - to block, to get in the way of\n\n\n45\n地势\ndì shì - terrain; topography\n\n\n46\n冲\nchōng - to rush, to dash"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第05课-济南的泉水/index.html#生词",
    "href": "学汉语的日记/HSK5上-第05课-济南的泉水/index.html#生词",
    "title": "HSK5上 | 第05课：济南的泉水",
    "section": "",
    "text": "济南的泉水\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n悠久\nyōu jiǔ - long-standing, age-old\n\n\n2\n文字\nwénzì - written language\n\n\n3\n记载\njìzǎi - to record, to put down in writing\n\n\n4\n形状\nxíngzhuàng - shape\n\n\n5\n描写\nmiáo xiě - to describe, to depict\n\n\n6\n赞美\nzàn měi - to praise, to extol\n\n\n7\n诗\nshī - poem\n\n\n8\n老百姓\nLǎobǎixìng - ordinary people, civilians\n\n\n9\n充满\nchōng mǎn - to be full of\n\n\n10\n感激\ngǎn jī - to feel grateful\n\n\n11\n从而\ncóng ér - thus, consequently\n\n\n12\n产生\nchǎn shēng - to emerge, to arise\n\n\n13\n传说\nchuán shuō - legend\n\n\n14\n善良\nshàn liáng - kind-hearted\n\n\n15\n救\njiù - to save, to rescue\n\n\n16\n晕\nyūn - to faint, to pass out\n\n\n17\n龙\nlóng - dragon\n\n\n18\n治疗\nzhì liáo - to treat, to cure\n\n\n19\n玉\nyù - jade\n\n\n20\n壶\nhú - pot, kettle\n\n\n21\n抢\nqiǎng - to rob, to snatch\n\n\n22\n埋\nmái - to bury\n\n\n23\n躲藏\nduǒ cáng - to hide\n\n\n24\n如今\nrú jīn - nowadays, present\n\n\n25\n分布\nfēnbù - to be distributed (over an area), to be scattered\n\n\n26\n天然\ntiānrán - natural\n\n\n27\n优美\nyōu měi - graceful, beautiful\n\n\n28\n独特\ndú tè - unique; distinct\n\n\n29\n反映\nfǎn yìng - to reflect, to mirror\n\n\n30\n珍珠\nzhēnzhū - pearl\n\n\n31\n形成\nxíng chéng - to form; to take shape\n\n\n32\n于\nyú - from, out of, in, at, to, by, than\n\n\n33\n广大\nguǎng dà - vast, extensive\n\n\n34\n岩石\nyánshí - rock\n\n\n35\n亿\nyì - one hundred million\n\n\n36\n石灰岩\nshíhuīyán - limestone\n\n\n37\n地区\ndì qū - area / region\n\n\n38\n表面\nbiǎomiàn - surface\n\n\n39\n角度\njiǎodù - angle, degree of angle\n\n\n40\n斜\nxié - oblique, slanting\n\n\n41\n为\nwéi - to become\n\n\n42\n火成岩\nhuǒchéngyán - igneous rock\n\n\n43\n碰\npèng - to touch, to meet, to come across\n\n\n44\n挡\ndǎng - to block, to get in the way of\n\n\n45\n地势\ndì shì - terrain; topography\n\n\n46\n冲\nchōng - to rush, to dash"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第05课-济南的泉水/index.html#听力",
    "href": "学汉语的日记/HSK5上-第05课-济南的泉水/index.html#听力",
    "title": "HSK5上 | 第05课：济南的泉水",
    "section": "2.1. 听力",
    "text": "2.1. 听力"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第05课-济南的泉水/index.html#阅读",
    "href": "学汉语的日记/HSK5上-第05课-济南的泉水/index.html#阅读",
    "title": "HSK5上 | 第05课：济南的泉水",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n19.\n20.\n21.\n22.\n23-25.\n23.\n24.\n25.\n26-28.\n26.\n27.\n28."
  },
  {
    "objectID": "学汉语的日记/HSK5上-第05课-济南的泉水/index.html#书写",
    "href": "学汉语的日记/HSK5上-第05课-济南的泉水/index.html#书写",
    "title": "HSK5上 | 第05课：济南的泉水",
    "section": "2.3. 书写",
    "text": "2.3. 书写"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第03课-一生有选择，一切可改变/index.html",
    "href": "学汉语的日记/HSK5上-第03课-一生有选择，一切可改变/index.html",
    "title": "HSK5上 | 第03课：一生有选择，一切可改变",
    "section": "",
    "text": "一生有选择，一切可改变\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n人生\nrénshēng - n. life\n\n\n2\n工人\ngōngrén - n. worker\n\n\n3\n稳定\nwěndìng - adj. stable\n\n\n4\n待遇\ndàiyù - n. pay and perks\n\n\n5\n发愁\nfā chóu - v. to worry\n\n\n6\n平静\npíngjìng - adj. quiet, peaceful\n\n\n7\n帆船\nfānchuán - n. sailing boat/ship\n\n\n8\n撞\nzhuàng - v. to bump against\n\n\n9\n艘\nsōu - m used for boats/ ships\n\n\n10\n航行\nhángxíng - v. to sail, to navigate by air or water\n\n\n11\n积蓄\njīxù - n./v. savings; to save\n\n\n12\n二手\nèrshǒu - adj. second-hand\n\n\n13\n彩虹\ncǎihóng - n. rainbow\n\n\n14\n包括\nbāokuò - v. to include\n\n\n15\n疯\nfēng - v. to be crazy, to go mad\n\n\n16\n辞职\ncí zhí - v. to quit a job\n\n\n17\n驾驶\njiàshǐ - v. to drive, to pilot\n\n\n18\n轮流\nlúnliú - v. to take turns\n\n\n19\n钓\ndiào - v. to fish with a hook and line\n\n\n20\n顿\ndùn - m. used for meals\n\n\n21\n海鲜\nhǎixiān - n. seafood\n\n\n22\n傍晚\nbàngwǎn - n. towards evening, at dusk\n\n\n23\n舒适\nshūshì - adj. comfortable, cozy\n\n\n24\n干活儿\ngàn huór - v. to work\n\n\n25\n盼望\npànwàng - v. to look forward to\n\n\n26\n陆地\nlùdì - n. land\n\n\n27\n各自\ngèzì - pron. each, respective\n\n\n28\n勿\nwù - adv. (used in imperative sentences) don’t\n\n\n29\n时刻\nshíkè - n. moment\n\n\n30\n着火\nzháo huǒ - v. to catch fire\n\n\n31\n漏\nlòu - v. (of a container) to leak\n\n\n32\n雷\nléi - n. thunder\n\n\n33\n随时\nsuíshí - adv. at any time\n\n\n34\n闪电\nshǎndiàn - n. lightning\n\n\n35\n击\njī - v. to hit, to strike\n\n\n36\n拥抱\nyōngbào - v. to hug, to embrace\n\n\n37\n海里\nhǎilǐ - m. sea mile\n\n\n38\n台阶\ntáijiē - n. flight of steps\n\n\n39\n未来\nwèilái - n. future\n\n\n40\n太太\ntàitai - n. wife\n\n\n41\n时代\nshídài - n. era, age, epoch"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第03课-一生有选择，一切可改变/index.html#生词",
    "href": "学汉语的日记/HSK5上-第03课-一生有选择，一切可改变/index.html#生词",
    "title": "HSK5上 | 第03课：一生有选择，一切可改变",
    "section": "",
    "text": "一生有选择，一切可改变\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n人生\nrénshēng - n. life\n\n\n2\n工人\ngōngrén - n. worker\n\n\n3\n稳定\nwěndìng - adj. stable\n\n\n4\n待遇\ndàiyù - n. pay and perks\n\n\n5\n发愁\nfā chóu - v. to worry\n\n\n6\n平静\npíngjìng - adj. quiet, peaceful\n\n\n7\n帆船\nfānchuán - n. sailing boat/ship\n\n\n8\n撞\nzhuàng - v. to bump against\n\n\n9\n艘\nsōu - m used for boats/ ships\n\n\n10\n航行\nhángxíng - v. to sail, to navigate by air or water\n\n\n11\n积蓄\njīxù - n./v. savings; to save\n\n\n12\n二手\nèrshǒu - adj. second-hand\n\n\n13\n彩虹\ncǎihóng - n. rainbow\n\n\n14\n包括\nbāokuò - v. to include\n\n\n15\n疯\nfēng - v. to be crazy, to go mad\n\n\n16\n辞职\ncí zhí - v. to quit a job\n\n\n17\n驾驶\njiàshǐ - v. to drive, to pilot\n\n\n18\n轮流\nlúnliú - v. to take turns\n\n\n19\n钓\ndiào - v. to fish with a hook and line\n\n\n20\n顿\ndùn - m. used for meals\n\n\n21\n海鲜\nhǎixiān - n. seafood\n\n\n22\n傍晚\nbàngwǎn - n. towards evening, at dusk\n\n\n23\n舒适\nshūshì - adj. comfortable, cozy\n\n\n24\n干活儿\ngàn huór - v. to work\n\n\n25\n盼望\npànwàng - v. to look forward to\n\n\n26\n陆地\nlùdì - n. land\n\n\n27\n各自\ngèzì - pron. each, respective\n\n\n28\n勿\nwù - adv. (used in imperative sentences) don’t\n\n\n29\n时刻\nshíkè - n. moment\n\n\n30\n着火\nzháo huǒ - v. to catch fire\n\n\n31\n漏\nlòu - v. (of a container) to leak\n\n\n32\n雷\nléi - n. thunder\n\n\n33\n随时\nsuíshí - adv. at any time\n\n\n34\n闪电\nshǎndiàn - n. lightning\n\n\n35\n击\njī - v. to hit, to strike\n\n\n36\n拥抱\nyōngbào - v. to hug, to embrace\n\n\n37\n海里\nhǎilǐ - m. sea mile\n\n\n38\n台阶\ntáijiē - n. flight of steps\n\n\n39\n未来\nwèilái - n. future\n\n\n40\n太太\ntàitai - n. wife\n\n\n41\n时代\nshídài - n. era, age, epoch"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第03课-一生有选择，一切可改变/index.html#听力",
    "href": "学汉语的日记/HSK5上-第03课-一生有选择，一切可改变/index.html#听力",
    "title": "HSK5上 | 第03课：一生有选择，一切可改变",
    "section": "2.1. 听力",
    "text": "2.1. 听力"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第03课-一生有选择，一切可改变/index.html#阅读",
    "href": "学汉语的日记/HSK5上-第03课-一生有选择，一切可改变/index.html#阅读",
    "title": "HSK5上 | 第03课：一生有选择，一切可改变",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n19.\n20.\n21.\n22.\n23-25.\n23.\n24.\n25.\n26-28.\n26.\n27.\n28."
  },
  {
    "objectID": "学汉语的日记/HSK5上-第03课-一生有选择，一切可改变/index.html#书写",
    "href": "学汉语的日记/HSK5上-第03课-一生有选择，一切可改变/index.html#书写",
    "title": "HSK5上 | 第03课：一生有选择，一切可改变",
    "section": "2.3. 书写",
    "text": "2.3. 书写"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第01课-爱的细节/index.html",
    "href": "学汉语的日记/HSK5上-第01课-爱的细节/index.html",
    "title": "HSK5上 | 第01课： 爱的细节",
    "section": "",
    "text": "爱的细节\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n细节\nxì jié - detail\n\n\n2\n电台\ndiàn tái - radio station\n\n\n3\n恩爱\nēn ài - loving (of husband and wife)\n\n\n4\n对比\nduì bǐ -to compare, to contrast\n\n\n5\n入围\nrù wéi - to be shortlisted\n\n\n6\n评委\npíng wěi - Judge，member of judging panel\n\n\n7\n如何\nrúhé - how\n\n\n8\n瘫痪\ntān huàn - to be paralyzed\n\n\n9\n自杀\nzìshā - to commit suicide\n\n\n10\n抱怨\nbàoyuàn - to complain\n\n\n11\n爱护\nài hù - to take good care of\n\n\n12\n婚姻\nhūn yīn - marriage\n\n\n13\n吵架\nchǎo jià - to quarrel\n\n\n14\n相敬如宾\nxiāngjìngrúbīn - husband & wife respect each other like guests\n\n\n15\n暗暗\nàn àn - secretly, to oneself\n\n\n16\n轮\nlún - to take turns\n\n\n17\n不耐烦\nbú nài fán - impatient\n\n\n18\n靠\nkào - to lean against\n\n\n19\n肩膀\njiān bǎng - shoulder\n\n\n20\n喊\nhǎn - to shout, to call\n\n\n21\n伸\nshēn - to stretch, to extend\n\n\n22\n手指\nshǒu zhǐ - finger\n\n\n23\n歪歪扭扭\nwāiwāiniǔniǔ - crooked, askew\n\n\n24\n递\ndì - to hand over, to pass\n\n\n25\n脑袋\nnǎo dai - head\n\n\n26\n女士\nnǚ shì - lady / madam\n\n\n27\n叙述\nxù shù - to narrate\n\n\n28\n居然\njū rán - indicate unexpectness\n\n\n29\n催\ncuī - to urge, to push\n\n\n30\n等待\nděng dài - to wait\n\n\n31\n蚊子\nwén zi - mosquito\n\n\n32\n叮\ndīng - to sting, to bite\n\n\n33\n老婆\nlǎopó - wife\n\n\n34\n项\nxiàng - measure word for itemized things\n\n\n35\n患难与共\nhuànnàn yǔgòng - to share weal and woe"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第01课-爱的细节/index.html#生词",
    "href": "学汉语的日记/HSK5上-第01课-爱的细节/index.html#生词",
    "title": "HSK5上 | 第01课： 爱的细节",
    "section": "",
    "text": "爱的细节\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n细节\nxì jié - detail\n\n\n2\n电台\ndiàn tái - radio station\n\n\n3\n恩爱\nēn ài - loving (of husband and wife)\n\n\n4\n对比\nduì bǐ -to compare, to contrast\n\n\n5\n入围\nrù wéi - to be shortlisted\n\n\n6\n评委\npíng wěi - Judge，member of judging panel\n\n\n7\n如何\nrúhé - how\n\n\n8\n瘫痪\ntān huàn - to be paralyzed\n\n\n9\n自杀\nzìshā - to commit suicide\n\n\n10\n抱怨\nbàoyuàn - to complain\n\n\n11\n爱护\nài hù - to take good care of\n\n\n12\n婚姻\nhūn yīn - marriage\n\n\n13\n吵架\nchǎo jià - to quarrel\n\n\n14\n相敬如宾\nxiāngjìngrúbīn - husband & wife respect each other like guests\n\n\n15\n暗暗\nàn àn - secretly, to oneself\n\n\n16\n轮\nlún - to take turns\n\n\n17\n不耐烦\nbú nài fán - impatient\n\n\n18\n靠\nkào - to lean against\n\n\n19\n肩膀\njiān bǎng - shoulder\n\n\n20\n喊\nhǎn - to shout, to call\n\n\n21\n伸\nshēn - to stretch, to extend\n\n\n22\n手指\nshǒu zhǐ - finger\n\n\n23\n歪歪扭扭\nwāiwāiniǔniǔ - crooked, askew\n\n\n24\n递\ndì - to hand over, to pass\n\n\n25\n脑袋\nnǎo dai - head\n\n\n26\n女士\nnǚ shì - lady / madam\n\n\n27\n叙述\nxù shù - to narrate\n\n\n28\n居然\njū rán - indicate unexpectness\n\n\n29\n催\ncuī - to urge, to push\n\n\n30\n等待\nděng dài - to wait\n\n\n31\n蚊子\nwén zi - mosquito\n\n\n32\n叮\ndīng - to sting, to bite\n\n\n33\n老婆\nlǎopó - wife\n\n\n34\n项\nxiàng - measure word for itemized things\n\n\n35\n患难与共\nhuànnàn yǔgòng - to share weal and woe"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第01课-爱的细节/index.html#听力",
    "href": "学汉语的日记/HSK5上-第01课-爱的细节/index.html#听力",
    "title": "HSK5上 | 第01课： 爱的细节",
    "section": "2.1. 听力",
    "text": "2.1. 听力"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第01课-爱的细节/index.html#阅读",
    "href": "学汉语的日记/HSK5上-第01课-爱的细节/index.html#阅读",
    "title": "HSK5上 | 第01课： 爱的细节",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n19.\n20.\n21.\n22.\n23-25.\n23.\n24.\n25.\n26-28.\n26.\n27.\n28."
  },
  {
    "objectID": "学汉语的日记/HSK5上-第01课-爱的细节/index.html#书写",
    "href": "学汉语的日记/HSK5上-第01课-爱的细节/index.html#书写",
    "title": "HSK5上 | 第01课： 爱的细节",
    "section": "2.3. 书写",
    "text": "2.3. 书写"
  },
  {
    "objectID": "学汉语的日记/2024-06-20_hskk-langdu-hujiahuwei/index.html",
    "href": "学汉语的日记/2024-06-20_hskk-langdu-hujiahuwei/index.html",
    "title": "HSKK, 朗读：狐假虎威",
    "section": "",
    "text": "Photo credit to aiyuipta11\n\n\n\n\n\n\n\n假 - jia3, fake, to borrow, giả；威 - wei1, power, prestige, oai, uy；爪子 - zhoa3zi, móng vuốt， claw；绝望 - jue2wang4, tuyệt vọng, to give up all hope；勉强 mian3qiang3 - miễn cưỡng, to force sb to do smt, to do with diff；天帝 - tian1di4, thiên hoàng, God of Heaven, Celestial Emperor；百兽 - bai3shou4, muôn loài, all creatures, every kind of animal；岂能 - qi3neng2, sao có thể, how can?；吞 tun4, to take, to swallow, ăn；违抗 - wei2kang4, bất tuân, disobey；惩罚 - cheng2fa2, punishment, trừng phạt\n\n狐假虎威\n\n\n打量 - da3liang, to look sb up & down, nhìn một vòng；见状 - jian4zhuang4, upon seeing this, thấy thế；瞧瞧 - qiao2, to look at, see, xem xem, nhìn xem；露面 - lou4mian4, to show one’s face, lộ diện, lộ mặt；敬畏 - jing4wei4, to revere, tôn kính\nThe fox borrows the tiger’s fierceness\nIt means “Cáo mượn oai hùm” in Vietnamese and “an Ass in a Lion’s skin” in English.\n\n\n\n昂首阔步 - Ángshǒukuòbù, đi vênh vang, striding forward with head high；毕恭毕敬 - Bìgōngbìjìng, reverent & respectful, tôn kính；狼狈 - lang2bei4, diff situation；缘故 - yuan2gu4, reason, cause, lý do；不由得 - can’t help, can not but, không thể làm gì hơn；诚惶诚恐 - chénghuángchéngkǒng， in fear and trepidation, hết mức cẩn trọng đến mức sợ hãi và bất an；溜走 - liu1zou3, slip away, leave secretly, lẻn đi\n有一天，一只狐狸落在老虎的爪子下，眼看着就要成为老虎的美餐了。狐狸绝望地勉强抗争，对老虎说它是天帝派来做百兽之王的，岂能允许一只小小的老虎把它吞掉!“你要是胆敢这样做，”它对老虎说，“就是违抗天意，将会受到严厉的惩罚。”\n老虎用眼睛打量了一番狐狸的小个头儿，犹犹豫豫地表示它不十分相信。“好吧，”狐狸见状，变得犬胆起耒，它装出一副不耐烦的神气说，“要是你的怀疑还没有消失的话，就让我来证实一下我的话吧。跟上我，瞧瞧当我露面的时候，百兽对我是何等的敬畏!”\n于是狐狸昂首阔步在前面走，老虎毕恭毕敬地在后面跟着。它们走过森林，遇到不少动物，这些动物一见到老虎，都慌忙逃命。狐狸得意地说:“怎么样?你看到它们的那副狼狈相了吧？”而老虎呢，还真以为那些野兽逃跑是由于害怕狐狸的缘故，不由得更加诚惶诚恐了。最后，它找个机会悄悄溜走了。\n\n\n参考资料\n\n新 HSK 速成强化教程 口试 (高级), 作者：金舒年 编著， 第85页\nWiki"
  },
  {
    "objectID": "学汉语的日记/2024-06-02_hskk-langdu-beijing/index.html",
    "href": "学汉语的日记/2024-06-02_hskk-langdu-beijing/index.html",
    "title": "HSKK, 朗读：北京",
    "section": "",
    "text": "北京故宫，photo credit to blog travel4u\n\n\n\n\n\n\n🚢北京\n北京是世界著名的历史文化名域。 3000年的建城史， 近千年的帝都， 给这里留下了数以百计的具有极大历史、 文化、 艺术价值的文物古迹。 有学者把北京称为 “人类最伟大的个体工程” 并非过誉， 因为北京城本身就是一个伟大而丰富的历史博物馆：巍峨盘旋于群山之间的古长城， 雄伟壮丽的故宫， 庄严肃穆的天坛， 风景如画的皇家园林颐和园、圆明园和北海， 还有那些超凡脱俗的宗教建筑， 无一不让人深深领略到古老东方文化的魅力。\n北京的文学艺术传统源远流长， 并且因不同时代多民族文化的交汇碰撞而显现出多样化的趋势; 但在多样化的背后， 又有着浓浓的地域和乡土特质， 最具代表性的莫过于堪称中国国粹的京剧艺术了。 在这座逐渐现代化的域市里， 民俗风情仍然体现出这座古都传承东方文明的精细态度。 在琉璃厂文化街， 在老四合院里， 在庙会上， 你可以看到流动的现实和悠久历史遗存的痕迹奇妙地交融。\n\n\n✍越南语翻译 Dịch nghĩa\nBắc Kinh là một thành phố với lịch sử và văn hóa nổi tiếng thế giới. Lịch sử 3.000 năm xây dựng thành phố và gần một nghìn năm làm kinh đô đã để lại cho nơi đây hàng trăm di tích văn hóa có giá trị lịch sử, văn hóa, nghệ thuật to lớn. Một số học giả gọi Bắc Kinh là “công trình đơn lẻ vĩ đại nhất của nhân loại”, điều này không hề cường điệu, bởi bản thân thành phố đã là một bảo tàng lịch sử vĩ đại và phong phú: Vạn Lý Trường Thành cổ kính cao chót vót sừng sững giữa núi non, Cố Cung (Tử Cấm Thành) hùng vĩ tráng lệ, Thiên Đàn trang nghiêm, Di Hòa Viên, Viên Minh Viên và Bắc Hải phong cảnh như tranh vẽ, còn có những kiến trúc tôn giáo siêu phàm thoát tục, không gì là không khiến người ta cảm nhận sâu sắc sức quyến rũ của văn hóa phương Đông cổ.\nTruyền thống văn học nghệ thuật của Bắc Kinh có lịch sử lâu đời, thể hiện xu hướng đa dạng do sự giao thoa, va chạm của các nền văn hóa đa sắc tộc ở các thời đại khác nhau; Nhưng đằng sau sự đa dạng hóa, lại có đặc tính địa vực và quê hương nồng đậm, tiêu biểu nhất chính là nghệ thuật kinh kịch có thể nói là quốc túy Trung Quốc. Ở thành phố đang dần hiện đại hóa này, phong tục tập quán dân tộc vẫn thể hiện thái độ tinh tế của cố đô này trong việc kế thừa nền văn minh phương Đông. Tại phố văn hóa Lưu Ly Xưởng, trong tứ hợp viện cũ, tại lễ hội, bạn có thể thấy sự giao thoa kỳ diệu giữa hiện thực lưu động và dấu vết lịch sử lâu đời.\n\n\n📚生词\n帝都：di4du1 - imperial capital / đế đô\n数以百计：shu1yi3bai3ji4 - hundreds of / lên đến hàng trăm\n古迹：gu3ji4 - historic site / Cổ tích ~ di tích lịch sử\n过誉：guo4yu4 - overpraise / nói quá, nói ngoa\n巍峨：wei1e2 - towering，lofty，majestic，imposingtall， and rugged / nguy nga, to lớn đồ sộ\n盘旋：pan2xuan2 - hover，spiral，wheel，stop，circle around，stay，pace up and down / bay lượn, nhấp nhô\n雄伟壮丽：xiong2wei3zhuang4li4 - grand，sublime，magnificent / hùng vĩ tráng lệ\n庄严肃穆：zhuang1yan2su4mu4 - in a solemn atmosphere，An atmosphere of solemnity and reverence prevailed，in a solemn and awe-inspiring / cả hai từ đều có nghĩa là trang nghiêm\n天坛：tian1tan2 - Temple of Heaven (in Beijing) / Thiên Đàn\n风景如画：feng1jing3ru2hua4 - The scenery is as beautiful as a painting / cảnh đẹp như tranh\n皇家园林：huang2jia1yuan2lin2 - royal garden，the imperial garden / Hoàng Gia Viên Lâm - cung điện hoàng gia\n颐和园：yi2he2yuan2 - Summer Palace  / Di Hòa Viên\n圆明园：yuan2ming2yuan2 - Old Summer Palace，imperial garden and palace burnt by British/French troops in 1860  / Viên Minh Viên\n超凡脱俗：chāo fán tuō sú - Extraordinary and refined / phi thường, siêu phàm, thoát tục\n宗教：zōng jiào - religion / tôn giáo\n深深：shen1shen1 - deeply，deep，profoundly，far，keenly，keen / sâu sắc\n领略：ling3lue4 - appreciate，realize，have a taste of / cảm nhận, lĩnh ngộ\n源远流长：yuán yuǎn liú cháng - Having a long and rich history / lâu đời\n交汇：jiao1hui4 - intersection / giao hội\n碰撞：peng4zhuang4 - collision / va chạm\n趋势：qu1shi4 - trend / xu hướng\n乡土：xiang1tu3 - local，native soil，home village，of one’s native land / hương thổ\n地域：di4yu4 - region / địa vực\n莫过于：mo4guo4yu2 - nothing is better than，nothing is more … than，nothing is more than / không gì hơn\n堪称：kan1cheng1 - Can be called, can be rated as / được xem là, được coi là\n国粹：guo2cui4 - the quintessence of Chinese culturenational legacy / quốc túy\n传承：chuan2cheng2 - impart and inherit / truyền nhận\n精细：jing1xi4 - fine，meticulous，careful，delicacy / tinh tế\n遗存：yi2cun2 - remain / còn lại\n痕迹: hén jī - mark / dấu vết\n奇妙：qi2miao4 - wonderful / tuyệt vời\n交融：jiao1rong2 - blend / pha trộn\n\n\n参考资料\n新 HSK 速成强化教程 口试 (高级), 作者：金舒年 编著， 第85页"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my Blog!",
    "section": "",
    "text": "I am blogging\n\n\n\n\n\n\n\n\n\n\n\nLet’s build GPT, in code, spelled out!\n\n\nBuild a Generatively Pretrained Transformer (GPT), following the paper ‘Attention is All You Need’ and OpenAI’s GPT-2 / GPT-3\n\n\n\nTuan Le Khac\n\n\nApr 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI completed 25 days of DAX (Friday) challenge, in 2 days!\n\n\nby Curbal AB, Edition 3, on Northwind dataset\n\n\n\nTuan Le Khac\n\n\nFeb 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Large Language Models: A Summary of Andrej Karpathy’s Talk\n\n\nSummary of Andrej Karpathy’s “Intro to Large Language Models” talk, deeply diving into the core concepts, current state, future directions, and security challenges…\n\n\n\nTuan Le Khac\n\n\nDec 12, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n\nGo to all posts →\n\n\nI am learning Mandarin\n\n\n\n\n\n\n\n\n\n\n\nHSK5上 | 第01课： 爱的细节\n\n\n。。。有人认为夫妻之间最重要的是恩爱，有人说是诚实，也有人说是关爱和理解。。。\n\n\n\nTuan Le Khac\n\n\nDec 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第19课： 家乡的萝卜饼\n\n\n家乡的众多美食中，萝卜饼是最让我怀念的。它那丰富的色彩、微甜的口感，至今仍让我十分想念。\n\n\n\nTuan Le Khac\n\n\nDec 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHSK5下 | 第20课： 小人书摊\n\n\n。。。一些印刷精美、有特色的作品则身价大涨，成了收藏品，甚至进了博物馆。小人书和小人书摊已成为历史的记忆。\n\n\n\nTuan Le Khac\n\n\nJan 6, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n\nGo to all lessons →\n\n\nI am training Jiu-jitsu\n\n\n\n\n\n\n\n\n\n\n\nMột số khuôn mẫu tư duy trong Brazilian Jiu-jitsu\n\n\na quick and short note on “Mechanic Models of BJJ: A Crash Course” by Steve Kwan\n\n\n\nTuan Le Khac\n\n\nAug 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBJJ Beginners Guide\n\n\nThumbnail image credit to this post\n\n\n\nTuan Le Khac\n\n\nJul 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI am starting training Brazilian Jiu-jitsu\n\n\nWhen you do Jiu-jitsu, you won’t look at a bigger guy and say: Oh No!, you’ll look and say: How Interesting! - Rener Gracie.\n\n\n\nTuan Le Khac\n\n\nFeb 28, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "curriculum/index.html",
    "href": "curriculum/index.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Summary\nResults-driven data analyst with extensive experience in risk management and SME lending, backed by a foundation in external audit and corporate finance. Proven expertise in developing credit risk models, portfolio analytics, and data-driven solutions. Seeking to leverage advanced analytics capabilities and banking domain knowledge to drive strategic risk management decisions as well as new challenges in others domain: e-commerce, insurance, manufacturing.\nInterested topics:\n\nDeep credit risk analytics (Basel, CECL, IFRS 9);\nBayesian Modeling;\nCausal Inference;\nAdvanced visualization (grammar of graphics, ICBS standard).\n\nThis is my curriculum vitae, for a one-pager profile please refer to my résumé.\n\n\nCareer\n\n\nVGrowth Development Co.,Ltd\nRisk Management (Mar, 23 - present)\nI worked as a Senior Data Analyst:\n\nLead end-to-end credit analytics initiatives covering underwriting, portfolio management, and collections, resulting in optimized credit policies and risk controls;\nDevelop and implement an application scorecard for MSMEs/Households lending products, collaborating closely with CRO/CDO throughout the model lifecycle;\nManage data curation, warehousing, and reporting workflows to support operation and reporting system;\nStreamline cross-functional processes across Sales, Product, Underwriting, Collection, Finance, and Operations teams through data-driven solutions,implementing KPIs and customer segmentation frameworks to enhance automation and operational efficiency.\n\n\n\nVietCredit Finance Company\nRisk Management (Feb, 22 - Mar, 23)\nI worked as a Data Analyst:\n\nConducted comprehensive portfolio quality analyses and product reporting, delivering actionable recommendations for product policy and risk control;\nManaged data curation and reporting workflows for critical daily/monthly deliverables while contributing to enterprise data projects;\nParticipated in company’s digital transformations projects, monitored UAT for new applications, assisted others departments on daily operation.\n\n\n\nTechtronic Industries Vietnam Mfg. Co., Ltd\nOperation Finance (Jan, 21 - Feb, 22)\nI worked as a Financial Analyst:\n\nPartnered with Production teams to ensure data accuracy and generate efficiency reports for Vietnam factories;\nPrepared monthly functional P&L, directly handled accrual, analysis, forecast, & budget preparation for Direct Labor;\nPerformed cost control in the belt-tightening period due to the Covid pandemic, co-operated effectively with other functional departments for daily operation process.\n\n\n\nErnst & Young Vietnam Ltd\nCore Assurance (Dec, 19 - Jan, 21)\nI worked as an Audit Assistant:\n\nInterviewed client, evaluated control environment from the critical understanding of the business process, defined possibility of accounting material misstatements;\nExecuted risk-based audit procedures following EY methodology to verify financial statement accuracy;\nSupported senior management in financial statement preparation and issuance.\n\n\n\n\nEducation\n\n\nForeign Trade University, HCMC campus\nBachelor of Accounting & Auditing (Dec, 16 - Apr, 20)\n\nGPA: 8.4/10 (3.41/4)\nGraduation thesis: 8.9/10 (“Accounting information quality, information asymmetry, and the cost of capital” - used R & Stata for data crawling, manipulation, and modeling).\n\n\n\nPhan Boi Chau High School for the Gifted\nMath Specilized Class (Dec, 13 - May, 16)\n\n\n\nTechnical Skills\n\n\nDomain Expertise: Credit risk analytics • Customer segmentation • Product pricing • Financial analysis\nData Processing: Salesforce • Excel & Power Query • Google Sheets & Apps Script • Pandas • dplyr\nVisualization/Presentation: Excel & Powerpoint • Power BI • Ggplot2 • R Markdown & Quarto\nModeling: Power Pivot & Power BI • scikit-learn • statsmodels\nProgramming: Python • R • SQL • PowerShell/Bash • Git\n\n\n\nLanguages\n\nVietnamese (native language) • English (fluent in communication) • Chinese Mandarin (HSK 5 equivalent).\nPlease also visit my GitHub for the projects I did and visit my Linked in for more details."
  },
  {
    "objectID": "blog/2024-11-26-nn-z2h-p4/index.html",
    "href": "blog/2024-11-26-nn-z2h-p4/index.html",
    "title": "NN-Z2H Lesson 4: Building makemore part 3 - Activations & Gradients, BatchNorm",
    "section": "",
    "text": "This is not orginal content!\n\n\n\nThis is my study notes / codes along with Andrej Karpathy’s “Neural Networks: Zero to Hero” series.\nWe want to stay a bit longer with the MLPs, to have more concrete intuitive of the activations in the neural nets and gradients that flowing backwards. It’s good to learn about the development history of these architectures. Since Recurrent Neural Network (RNN), they are although very expressive but not easily optimizable with current gradient techniques we have so far. Let’s get started!"
  },
  {
    "objectID": "blog/2024-11-26-nn-z2h-p4/index.html#starter-code",
    "href": "blog/2024-11-26-nn-z2h-p4/index.html#starter-code",
    "title": "NN-Z2H Lesson 4: Building makemore part 3 - Activations & Gradients, BatchNorm",
    "section": "starter code",
    "text": "starter code\n\n\nShow the code\nimport torch\nimport torch.nn.functional as F\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n\n\nShow the code\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt\"\nwords = pd.read_csv(url, header=None).iloc[:, 0].tolist()\nwords[:8]\n\n\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n\n\n\n\nShow the code\nlen(words)\n\n\n32033\n\n\n\n\nShow the code\n# build the vocabulary of characters and mapping to/from integer\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i, s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i: s for s, i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n\n\n{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n\n\n\n\nShow the code\nblock_size = 3\n# build the dataset\ndef buid_dataset(words):\n    X, Y = [], []\n\n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix]\n\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\nXtr, Ytr = buid_dataset(words[:n1])        # 80#\nXdev, Ydev = buid_dataset(words[n1:n2])    # 10%\nXte, Yte = buid_dataset(words[n2:])        # 10%\n\n\ntorch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n\n\n\n\nShow the code\n# MLP revisited\nn_emb = 10 # no of dimensions of the embedding space.\nn_hidden = 200 # size of the hidden - tanh layer\n\n# Lookup table - 10 dimensional space\ng = torch.Generator().manual_seed(2147483647) # for reproductivity\nC = torch.randn((vocab_size, n_emb),                  generator=g)\n\n# Layer 1 - tanh - 300 neurons\nW1 = torch.randn((block_size * n_emb, n_hidden),      generator=g)\nb1 = torch.randn(n_hidden,                            generator=g)\n\n# Layer 2 - softmax\nW2 = torch.randn((n_hidden, vocab_size),              generator=g)\nb2 = torch.randn(vocab_size,                          generator=g)\n\n# All params\nparameters = [C, W1, b1, W2, b2]\nprint(\"No of params: \", sum(p.nelement() for p in parameters))\n\n# Pre-training\nfor p in parameters:\n    p.requires_grad = True\n\n\nNo of params:  11897\n\n\n\n\nShow the code\n# Optimization\nmax_steps = 50_000 #200_000\nbatch_size = 32\n\n# Stats holders\nlossi = []\n\n# Training on Xtr, Ytr\nfor i in range(max_steps):\n\n    # minibatch construct      \n    ix = torch.randint(0, Xtr.shape[0], (batch_size,)) \n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X, Y\n\n    # forward pass:\n    emb = C[Xb] # embed the characters into vectors   \n    emb_cat = emb.view(emb.shape[0], -1) # concatenate the vectors\n    h_pre_act = emb_cat @ W1 + b1 # hidden layer pre-activation\n    h = torch.tanh(h_pre_act) # hidden layer\n    logits = h @ W2 + b2 # output layer\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass:\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt;= max_steps / 2 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += - lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print once every while\n      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n\n\n      0/  50000: 22.5552\n  10000/  50000: 2.3148\n  20000/  50000: 2.1559\n  30000/  50000: 2.3941\n  40000/  50000: 2.2245\n\n\n\n\nShow the code\nplt.plot(lossi)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n@torch.no_grad() # disables gradient tracking\ndef split_loss(split: str):\n  x, y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte)\n  }[split]\n  emb = C[x] # (N, block_size, n_emb)\n  emb_cat = emb.view(emb.shape[0], -1) # concatenate into (N, block_size * n_emb)\n  h = torch.tanh(emb_cat @ W1 + b1) # (N, n_hidden)\n  logits = h @ W2 + b2 # (N, vocab_size)\n  loss = F.cross_entropy(logits, y) # loss function\n  print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\n\ntrain 2.22798752784729\nval 2.250197410583496\n\n\n\n\nShow the code\n# sample from the model\ng = torch.Generator().manual_seed(2147483647 + 10)\n\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n      # forward pass the neural net\n      emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n      h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n      logits = h @ W2 + b2\n      probs = F.softmax(logits, dim=1)\n      # sample from the distribution\n      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n      # shift the context window and track the samples\n      context = context[1:] + [ix]\n      out.append(ix)\n      # if we sample the special '.' token, break\n      if ix == 0:\n        break\n    \n    print(''.join(itos[i] for i in out)) # decode and print the generated word\n\n\nmoraagmyaz.\nseel.\nnpyn.\nalarethastendrarg.\naderedieliighlynnelle.\nelieananaraelyn.\nmalara.\nnoshabergihimies.\nkindreelle.\njeberorius.\nkynd.\nriyah.\nfaeha.\nkaysh.\nsamyah.\nhil.\nsalynnsti.\nzakel.\njuren.\ncresti.\n\n\nOkay so now our network has multiple things wrong at the initialization, let’s list down below. The final code will be presented in the end of part 1, with # 👈 for lines that had been added / modified. The right code cell below re-initializes states at the beginning of network’s parameter (in my notebook, it’s rendered linearly!).\n\n\nShow the code\nn_emb = 10 # no of dimensions of the embedding space.\nn_hidden = 200 # size of the hidden - tanh layer\n# Lookup table - 10 dimensional space\ng = torch.Generator().manual_seed(2147483647) # for reproductivity\nC = torch.randn((vocab_size, n_emb),                  generator=g)\n# Layer 1 - tanh - 300 neurons\nW1 = torch.randn((block_size * n_emb, n_hidden),      generator=g)\nb1 = torch.randn(n_hidden,                            generator=g)\n# Layer 2 - softmax\nW2 = torch.randn((n_hidden, vocab_size),              generator=g)\nb2 = torch.randn(vocab_size,                          generator=g)\n# All params\nparameters = [C, W1, b1, W2, b2]\n# Pre-training\nfor p in parameters:\n    p.requires_grad = True\n# Optimization\nmax_steps = 50_000 #200_000\nbatch_size = 32\n# Training on Xtr, Ytr\nfor i in range(max_steps):\n    # minibatch construct      \n    ix = torch.randint(0, Xtr.shape[0], (batch_size,)) \n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X, Y\n    # forward pass:\n    emb = C[Xb] # embed the characters into vectors   \n    emb_cat = emb.view(emb.shape[0], -1) # concatenate the vectors\n    h_pre_act = emb_cat @ W1 + b1 # hidden layer pre-activation\n    h = torch.tanh(h_pre_act) # hidden layer\n    logits = h @ W2 + b2 # output layer\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    break"
  },
  {
    "objectID": "blog/2024-11-26-nn-z2h-p4/index.html#fixing-the-initial-loss",
    "href": "blog/2024-11-26-nn-z2h-p4/index.html#fixing-the-initial-loss",
    "title": "NN-Z2H Lesson 4: Building makemore part 3 - Activations & Gradients, BatchNorm",
    "section": "fixing the initial loss",
    "text": "fixing the initial loss\nWe can see at the step = 0, the loss was 27 and after some ks training loops it decreased to 1 or 2. It extremely high at the begining. In practice, we should give the network somehow the expectation we want when generating a character after some characters (3).\n\n\nShow the code\nloss.item()\n\n\n24.27707862854004\n\n\nIn this case, without training yet, we expect all 27 characters’ possibilities to be equal (1 / 27.0) ~ uniform distribution, so the loss ~ negative log likelihood would be:\n\n\nShow the code\n- torch.tensor(1 / 27.0).log()\n\n\ntensor(3.2958)\n\n\nIt’s far lower than 27, we say that the network is confidently wrong. Andrej demonstrated by another simple 5 elements tensor and showed that the loss is lowest when all elements are equal.\nWe want the logits to be low entropy as possible (but not equal to 0, which will be showed later), we added multipliers 0.01 to W2, and 0 to b2. We got the loss to be 3.xx at the beginning.\n\n\nShow the code\n# Layer 2 - softmax\nW2 = torch.randn((n_hidden, vocab_size),              generator=g) * 0.01\nb2 = torch.randn(vocab_size,                          generator=g) * 0\n\n\nNow re-train the model and we will notice the the lossi will not look like the hookey stick anymore! Morever the final loss on train set and dev set is better!"
  },
  {
    "objectID": "blog/2024-11-26-nn-z2h-p4/index.html#fixing-the-saturated-tanh",
    "href": "blog/2024-11-26-nn-z2h-p4/index.html#fixing-the-saturated-tanh",
    "title": "NN-Z2H Lesson 4: Building makemore part 3 - Activations & Gradients, BatchNorm",
    "section": "fixing the saturated tanh",
    "text": "fixing the saturated tanh\nThe logits are now okay, the next problem is about the h - the activations of the hidden states! It’s hard to see but in the output of code cell below, there are too many values of 1 and -1 in this tensor.\n\n\nShow the code\nh\n\n\ntensor([[-0.9999, -0.7462, -0.9995,  ...,  0.6402, -1.0000, -0.9974],\n        [-0.9983, -1.0000, -1.0000,  ...,  1.0000, -1.0000, -1.0000],\n        [-0.9793, -0.9999, -1.0000,  ...,  0.7836, -0.7058,  0.2913],\n        ...,\n        [-1.0000, -0.9995, -0.9891,  ...,  0.9995, -0.8793,  0.9375],\n        [-0.9997, -0.9674, -1.0000,  ..., -0.8636, -0.0804,  0.7250],\n        [ 0.9988,  1.0000,  0.9998,  ..., -1.0000,  0.9901,  0.9985]],\n       grad_fn=&lt;TanhBackward0&gt;)\n\n\nRecall that tanh is activation function that squashing arbitrary numbers to the range [-1:1]. Let’s visualize the distribution of h.\n\n\nShow the code\nplt.hist(h.view(-1).tolist(), 50); # the \";\" removes the presenting of data in code-block's output\n\n\n\n\n\n\n\n\n\nMost of them were distributed to the extreme values -1 and 1. Now come to the h_pre_act, we can see a flat-tails distribution from -15 to 15.\n\n\nShow the code\nplt.hist(h_pre_act.view(-1).tolist(), 50);\n\n\n\n\n\n\n\n\n\nLooking back to how we implemented tanh in micrograd (which is mathematically the same with PyTorch), we’re multiplying the forward node’s gradient with (1 - t**2), which t is local tanh. When tanh is near -1 or 1, this is close to 0, we are killing the gradients. We are stopping the backpropagation through this tanh unit.\n\n\nShow the code\n...\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n\n        def _backward():\n            self.grad += (1 - t**2) * out.grad\n        out._backward = _backward\n        return out\n...\n\n\nWhen the gradients become zero, the previous nodes’ gradients will be vanishing. We call this saturated tanh, this leads to dead neurons ~ always off and because the gradient is zero then they will never be turned on, and happens for other activations as well: sigmoid, ReLU, etc (but less significant on Leaky ReLU or ELU). The network is not learning!\nThe same with logits, now we want h to be more near zero, we add multipliers to the W1 and b1:\n\n\nShow the code\n# Layer 1 - tanh - 300 neurons\nW1 = torch.randn((block_size * n_emb, n_hidden),      generator=g)  * 0.2\nb1 = torch.randn(n_hidden,                            generator=g) * 0.01 # keep a little bit entropy, \n# It's okay to initialize the b1 to zero but AK found emperically this will enhance the optimiaztion\n\n\nWe can see now less peak distribution of h:\n\n\n\n\n\ntanh\n\n\n\n\n\n\npre-activation tanh"
  },
  {
    "objectID": "blog/2024-11-26-nn-z2h-p4/index.html#calculating-the-init-scale-kaiming-init",
    "href": "blog/2024-11-26-nn-z2h-p4/index.html#calculating-the-init-scale-kaiming-init",
    "title": "NN-Z2H Lesson 4: Building makemore part 3 - Activations & Gradients, BatchNorm",
    "section": "calculating the init scale: “Kaiming init”",
    "text": "calculating the init scale: “Kaiming init”\nNow let’s look to the number 0.02, in practice no one will set it manually. Let’s look into the example below to see how parameters of Gaussian Distribution of y differ from x when multiplying by W.\nThe question is how we set the W to preserve the Gaussian Distribution of X. Emperical researches found out that the multiplier to W should be square root of the “fan in”, in this case is 10^0.5.\n\n\nShow the code\nx = torch.randn(1000, 10)\nW = torch.randn(10, 200)\ny = x @ W\n\nW1 = torch.randn(10, 200) / 10**0.5\ny1 = x @ W1\nprint(x.mean(), x.std())\nprint(y.mean(), y.std())\nprint(y1.mean(), y1.std())\nplt.figure(figsize=(20,5))\nplt.subplot(131).set_title(\"Input X\")\nplt.hist(x.view(-1).tolist(), 50, density=True);\nplt.subplot(132).set_title(\"Initial output y, expanded by W\")\nplt.hist(y.view(-1).tolist(), 50, density=True);\nplt.subplot(133).set_title(\"y1, preserve the X's Gaussian Dist\")\nplt.hist(y1.view(-1).tolist(), 50, density=True);\n\n\ntensor(-0.0039) tensor(1.0023)\ntensor(-0.0044) tensor(3.1220)\ntensor(-0.0002) tensor(1.0091)\n\n\n\n\n\n\n\n\n\nPlease investigate more here:\n\nKaiming et al. paper: https://arxiv.org/abs/1502.01852\nImplementation in Pytorch: https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_\n\nIt’s recommended in Kaiming paper to use a gain multiplier base on nonlinearity/activation function (here), for tanh it’s 5/3. We endup modified the initialization of W1 with:\n\n\nShow the code\nW1 = torch.randn((block_size * n_emb, n_hidden),      generator=g)  * (5/3) / ((block_size * n_emb)**0.5) # * 0.2\n\n\nIn this case is roughly 0.3, re-train and although the loss only improved so insignificant (because previously we set it to be 0.2 - very close), but we’ve parameterized this hyper-constant."
  },
  {
    "objectID": "blog/2024-11-26-nn-z2h-p4/index.html#batch-normalization",
    "href": "blog/2024-11-26-nn-z2h-p4/index.html#batch-normalization",
    "title": "NN-Z2H Lesson 4: Building makemore part 3 - Activations & Gradients, BatchNorm",
    "section": "batch normalization",
    "text": "batch normalization\nAs discussed before, we dont want the h_pre_act to be way too small (~is not doing anything) or too large (~saturated), we want it to just roughly follow the standardized Gaussian Distribution (ie. mean equal to 0, std equal to 1).\nWe’ve done it at the initialization, why don’t we just normalize the hidden states to be unit Gaussian? in batch normalization, this can be achieved by 4 steps, demonstrated with our case:\n\n\nShow the code\n# 1. mini-batch mean\nhpa_mean = h_pre_act.mean(0, keepdim=True)\n# 2. mini-batch variance / standard deviation\nhpa_std = h_pre_act.std(0, keepdim=True)\n# 3. normalize\nh_pre_act = (h_pre_act - hpa_mean) / hpa_std\n# 4. scale and shift\n# multiply by a \"gain\" then \"shift\" it with a bias\nbngain = torch.ones((1, n_hidden))\nbnbias = torch.zeros((1, n_hidden))\nh_pre_act = bngain * h_pre_act + bnbias\n\n\nWe modified our code accordingly and re-run the code, actually this time the model did not improve much. Because actually this is very simple and shallow neural network. We also notice that the training loop now is slower than before, because the calculation volumn is bigger. Batch Normalization also unexpectedly comes up with a side effect, the forward and backward pass of any input now also depend on the mini-batch, not just itself (because of mean()/std()). This effect is suprisingly a good thing and acts as a regularizer.\nThere are also non-coupling regularizers such as: Linear Normalization, Layer Normalization, Group Normalization.\nOne othering to consider is in the deployment/testing phase, we dont want to use the batch norm calculated by a mini-batch. Instead we want to use the mean and standard deviation from the whole training data set:\n\n\nShow the code\n# calibrate the batch norm after training\n\nwith torch.no_grad():\n    # pass the training set through\n    emb = C[x_train]\n    embcat = emb.view(-1, emb.shape[1] * emb.shape[2])\n    hpreact = embcat @ W1 + b1\n    # measure the mean/std over the entire training set\n    bnmean = hpreact.mean(0, keepdim=True)\n    bnstd = hpreact.std(0, keepdim=True)\n\n\nRather, we can also use the running mean and standard deviation as implemented below which will give close estimates. Remaining 2 notes on the BN are:\n\nDividing zeros: we add a \\(\\epsilon\\) value to the variance to avoid. We do not include this here as it likely not to happen with out example;\nThe bias b1 will be subtracting in BN calculation, we will notice the b1.grad will be zeros as it does not impact any other calculation. Thus when using the BN, for layer before like weight, we should remove the bias. The bnbias now will be incharge for biasing the distributions."
  },
  {
    "objectID": "blog/2024-11-26-nn-z2h-p4/index.html#real-example-resnet50-walkthrough",
    "href": "blog/2024-11-26-nn-z2h-p4/index.html#real-example-resnet50-walkthrough",
    "title": "NN-Z2H Lesson 4: Building makemore part 3 - Activations & Gradients, BatchNorm",
    "section": "real example: resnet50 walkthrough",
    "text": "real example: resnet50 walkthrough\nThe code AK presented here: https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L108\n\n\n\nThe architecture of ResNet-50 model."
  },
  {
    "objectID": "blog/2024-11-26-nn-z2h-p4/index.html#summary-of-the-lecture",
    "href": "blog/2024-11-26-nn-z2h-p4/index.html#summary-of-the-lecture",
    "title": "NN-Z2H Lesson 4: Building makemore part 3 - Activations & Gradients, BatchNorm",
    "section": "summary of the lecture",
    "text": "summary of the lecture\nUnderstand the activations (non-linearity) and gradients is crucial when training deep / large neural networks, in part 1 we have observed some issue and come up with many solutions:\n\nConfidently wrong of network at init leads to hookey stick for loss in training loop: adding multipliers to logits’s weights and biases;\nFlat-tails distribution or saturated tanh: Kaiming init;\nNormalization of the hidden states: introduction to BN.\n\nOur final code in part 1 (un-fold to see), # 👈 indicates a change:\n\n\nShow the code\nblock_size = 3\n\n# MLP revisited\nn_emb = 10 # no of dimensions of the embedding space.\nn_hidden = 200 # size of the hidden - tanh layer\n\n# Lookup table - 10 dimensional space\ng = torch.Generator().manual_seed(2147483647) # for reproductivity\nC = torch.randn((vocab_size, n_emb),                  generator=g)\n\n# Layer 1 - tanh - 300 neurons\nW1 = torch.randn((block_size * n_emb, n_hidden),      generator=g) * (5/3) / ((block_size * n_emb)**0.5) # * 0.2       # 👈\n# b1 = torch.randn(n_hidden,                            generator=g) * 0.01       # 👈\n\n# Layer 2 - softmax\nW2 = torch.randn((n_hidden, vocab_size),              generator=g) * 0.01       # 👈\nb2 = torch.randn(vocab_size,                          generator=g) * 0          # 👈\n\n# Batch Normalization gain and bias\nbngain = torch.ones((1, n_hidden))                                              # 👈\nbnbias = torch.zeros((1, n_hidden))                                             # 👈\n\n# Add running mean/std\nbnmean_running = torch.zeros((1, n_hidden))                             # 👈\nbnstd_running = torch.ones((1, n_hidden))                               # 👈\n\n# All params (deleted b1)\nparameters = [C, W1, W2, b2, bngain, bnbias]                                # 👈\nprint(\"No of params: \", sum(p.nelement() for p in parameters))\n\n# Pre-training\nfor p in parameters:\n    p.requires_grad = True\n\n# Optimization\nmax_steps = 50_000 #200_000\nbatch_size = 32\n\n# Stats holders\nlossi = []\n\n# Training on Xtr, Ytr\nfor i in range(max_steps):\n\n    # minibatch construct      \n    ix = torch.randint(0, Xtr.shape[0], (batch_size,)) \n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X, Y\n\n    # forward pass:\n    emb = C[Xb] # embed the characters into vectors   \n    emb_cat = emb.view(emb.shape[0], -1) # concatenate the vectors\n    # Linear layer\n    h_pre_act = emb_cat @ W1 # + b1 # hidden layer pre-activation                               # 👈\n    # BatchNorm layer\n    bnmeani = h_pre_act.mean(0, keepdim=True)                                                   # 👈\n    bnstdi = h_pre_act.std(0, keepdim=True)                                                     # 👈\n    h_pre_act = bngain * ((h_pre_act - bnmeani) / bnstdi) + bnbias                              # 👈\n    # Updating running mean and std (this runs outside the training loop)\n    with torch.no_grad():                                                                       # 👈\n        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani                               # 👈\n        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi                                  # 👈\n    # Non-linearity\n    h = torch.tanh(h_pre_act) # hidden layer\n    logits = h @ W2 + b2 # output layer\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass:\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt;= max_steps / 2 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += - lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print once every while\n      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n\n\nNo of params:  12097\n      0/  50000: 3.3045\n  10000/  50000: 2.2005\n  20000/  50000: 2.1628\n  30000/  50000: 2.0014\n  40000/  50000: 2.1175\n\n\n\n\nShow the code\nplt.plot(lossi)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n@torch.no_grad() # disables gradient tracking\ndef split_loss(split: str):\n  x, y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte)\n  }[split]\n  emb = C[x]\n  emb_cat = emb.view(emb.shape[0], -1) \n  h_pre_act = emb_cat @ W1 + b1                                                                                         # 👈 \n  # h_pre_act = bngain * ((h_pre_act - h_pre_act.mean(0, keepdim=True)) / h_pre_act.std(0, keepdim=True)) + bnbias      # 👈\n  # h_pre_act = bngain * ((h_pre_act - bnmean) / bnstd) + bnbias                                                        # 👈\n  h_pre_act = bngain * ((h_pre_act - bnmean_running) / bnstd_running) + bnbias                                          # 👈\n  h = torch.tanh(h_pre_act) \n  logits = h @ W2 + b2\n  loss = F.cross_entropy(logits, y)\n  print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\n\ntrain 2.4766831398010254\nval 2.490326404571533\n\n\n\nloss logs\nThe numbers somehow are approximate, I don’t know why my Thinkpad-E14 gave different results when running codes multiple times 😂.\n\nLoss logs\n\n\n\n\n\n\n\nStep\nWhat we did\nLoss we got (accum)\n\n\n\n\n1\noriginal\ntrain 2.1169614791870117\nval 2.1623435020446777\n\n\n2\nfixed softmax confidently wrong\ntrain 2.0666463375091553\nval 2.1468191146850586\n\n\n3\nfixed tanh layer too saturated at init\ntrain 2.033477544784546\nval 2.115907907485962\n\n\n4\nused semi principle “kaiming init” instead of hacking init\ntrain 2.038902997970581\nval 2.1138899326324463\n\n\n5\nadded batch norm layer\ntrain 2.0662825107574463\nval 2.1201331615448"
  },
  {
    "objectID": "blog/2024-11-26-nn-z2h-p4/index.html#viz-1-forward-pass-activations-statistics",
    "href": "blog/2024-11-26-nn-z2h-p4/index.html#viz-1-forward-pass-activations-statistics",
    "title": "NN-Z2H Lesson 4: Building makemore part 3 - Activations & Gradients, BatchNorm",
    "section": "viz #1: forward pass activations statistics",
    "text": "viz #1: forward pass activations statistics\n\n\nShow the code\n# visualize histograms\nplt.figure(figsize=(11, 3)) # width and height of the plot\nlegends = []\nfor i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n  if isinstance(layer, Tanh):\n    t = layer.out\n    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() &gt; 0.97).float().mean()*100))\n    hy, hx = torch.histogram(t, density=True)\n    plt.plot(hx[:-1].detach(), hy.detach())\n    legends.append(f'layer {i} ({layer.__class__.__name__}')\nplt.legend(legends);\nplt.title('activation distribution')\n\n\nlayer 1 (      Tanh): mean -0.02, std 0.75, saturated: 20.25%\nlayer 3 (      Tanh): mean -0.00, std 0.69, saturated: 8.38%\nlayer 5 (      Tanh): mean +0.00, std 0.67, saturated: 6.62%\nlayer 7 (      Tanh): mean -0.01, std 0.66, saturated: 5.47%\nlayer 9 (      Tanh): mean -0.02, std 0.66, saturated: 6.12%\n\n\nText(0.5, 1.0, 'activation distribution')\n\n\n\n\n\n\n\n\n\nIf we set the gain to 1, the std is shrinking, and the saturation is coming to zeros, due to the first layer is pretty decent, but the next ones are shrinking to zero because of the tanh() - a squashing function.\nlayer 1 (      Tanh): mean -0.02, std 0.62, saturated: 3.50%\nlayer 3 (      Tanh): mean -0.00, std 0.48, saturated: 0.03%\nlayer 5 (      Tanh): mean +0.00, std 0.41, saturated: 0.06%\nlayer 7 (      Tanh): mean +0.00, std 0.35, saturated: 0.00%\nlayer 9 (      Tanh): mean -0.02, std 0.32, saturated: 0.00%\nText(0.5, 1.0, 'activation distribution')\n\n\n\nIf the gain is 1\n\n\nBut if we set the gain is far too high, let’s say 3, we can see the saturation is too high.\nlayer 1 (      Tanh): mean -0.03, std 0.85, saturated: 47.66%\nlayer 3 (      Tanh): mean +0.00, std 0.84, saturated: 40.47%\nlayer 5 (      Tanh): mean -0.01, std 0.84, saturated: 42.38%\nlayer 7 (      Tanh): mean -0.01, std 0.84, saturated: 42.00%\nlayer 9 (      Tanh): mean -0.03, std 0.84, saturated: 42.41%\nText(0.5, 1.0, 'activation distribution')\n\n\n\nIf the gain is 3\n\n\nSo 5/3 is a nice one, balancing the std and saturation.\n\n\n\n\n\n\nWhy 5/3?\n\n\n\nA comment in his video explains why 5/3 is recommended, it comes from the avg of \\([\\tanh(x)]^2\\) where \\(x\\) is distributed as a Gaussian:\n\\(\\int_{-\\infty}^{\\infty} \\frac{[\\tanh(x)]^2 \\exp(-\\frac{x^2}{2})}{\\sqrt{2\\pi}} \\, dx \\approx 0.39\\)\n\nThe square root of this value is how much the tanh squeezes the variance of the incoming variable: 0.39 ** .5 ~= 0.63 ~= 3/5 (hence 5/3 is just an approximation of the exact gain)."
  },
  {
    "objectID": "blog/2024-11-26-nn-z2h-p4/index.html#viz-2-backward-pass-gradient-statistics",
    "href": "blog/2024-11-26-nn-z2h-p4/index.html#viz-2-backward-pass-gradient-statistics",
    "title": "NN-Z2H Lesson 4: Building makemore part 3 - Activations & Gradients, BatchNorm",
    "section": "viz #2: backward pass gradient statistics",
    "text": "viz #2: backward pass gradient statistics\nSimilarly, we can do the same thing with gradients. With the setting of gain as 5/3, the distribution of gradients through layers quite the same. Layer by layer, the value of gradients will be shrank close to zero, the distributions would be more and more peak, so the gain here will help expanding those distributions.\n\n\nShow the code\n# visualize histograms\nplt.figure(figsize=(11, 3)) # width and height of the plot\nlegends = []\nfor i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n  if isinstance(layer, Tanh):\n    t = layer.out.grad\n    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n    hy, hx = torch.histogram(t, density=True)\n    plt.plot(hx[:-1].detach(), hy.detach())\n    legends.append(f'layer {i} ({layer.__class__.__name__}')\nplt.legend(legends);\nplt.title('gradient distribution')\n\n\nlayer 1 (      Tanh): mean +0.000010, std 4.205588e-04\nlayer 3 (      Tanh): mean -0.000003, std 3.991179e-04\nlayer 5 (      Tanh): mean +0.000003, std 3.743020e-04\nlayer 7 (      Tanh): mean +0.000015, std 3.290473e-04\nlayer 9 (      Tanh): mean -0.000014, std 3.054035e-04\n\n\nText(0.5, 1.0, 'gradient distribution')"
  },
  {
    "objectID": "blog/2024-11-26-nn-z2h-p4/index.html#the-fully-linear-case-of-no-non-linearities",
    "href": "blog/2024-11-26-nn-z2h-p4/index.html#the-fully-linear-case-of-no-non-linearities",
    "title": "NN-Z2H Lesson 4: Building makemore part 4 - Activations & Gradients, BatchNorm",
    "section": "the fully linear case of no non-linearities",
    "text": "the fully linear case of no non-linearities\n\nTraining Neural Network is like balancing a pencil on a finger"
  },
  {
    "objectID": "blog/2024-11-26-nn-z2h-p4/index.html#viz-3-parameter-activation-and-gradient-statistics",
    "href": "blog/2024-11-26-nn-z2h-p4/index.html#viz-3-parameter-activation-and-gradient-statistics",
    "title": "NN-Z2H Lesson 4: Building makemore part 3 - Activations & Gradients, BatchNorm",
    "section": "viz #3: parameter activation and gradient statistics",
    "text": "viz #3: parameter activation and gradient statistics\nWe can also visualize the distribution of paramaters, here below only weight for simplicity (ignoring gamma, beta, etc…). We observed mean, std, and the grad to data ratio (to see how much the data will be updated).\nProblem for the last layer is shown in code output below, the weights on last layer are 10 times bigger than previous ones, and the grad to data ratio is too high.\nWe can try run 1st 1000 training loops and this can be slight reduced, but since we are using a simple optimizer SGD rather than modern one like Adam, it is still problematic.\n\n\nShow the code\n# visualize histograms\nplt.figure(figsize=(11, 3)) # width and height of the plot\nlegends = []\nfor i,p in enumerate(parameters):\n  t = p.grad\n  if p.ndim == 2:\n    print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n    hy, hx = torch.histogram(t, density=True)\n    plt.plot(hx[:-1].detach(), hy.detach())\n    legends.append(f'{i} {tuple(p.shape)}')\nplt.legend(legends)\nplt.title('weights gradient distribution');\n\n\nweight   (27, 10) | mean -0.000031 | std 1.365078e-03 | grad:data ratio 1.364090e-03\nweight  (30, 100) | mean -0.000049 | std 1.207430e-03 | grad:data ratio 3.871660e-03\nweight (100, 100) | mean +0.000016 | std 1.096730e-03 | grad:data ratio 6.601988e-03\nweight (100, 100) | mean -0.000010 | std 9.893572e-04 | grad:data ratio 5.893091e-03\nweight (100, 100) | mean -0.000011 | std 8.623432e-04 | grad:data ratio 5.158123e-03\nweight (100, 100) | mean -0.000004 | std 7.388576e-04 | grad:data ratio 4.415211e-03\nweight  (100, 27) | mean -0.000000 | std 2.364824e-02 | grad:data ratio 2.328203e+00"
  },
  {
    "objectID": "blog/2024-11-26-nn-z2h-p4/index.html#viz-4-update-data-ratio-over-time",
    "href": "blog/2024-11-26-nn-z2h-p4/index.html#viz-4-update-data-ratio-over-time",
    "title": "NN-Z2H Lesson 4: Building makemore part 3 - Activations & Gradients, BatchNorm",
    "section": "viz #4: update data ratio over time",
    "text": "viz #4: update data ratio over time\nThe grad to data above ratio is at the end not really informative (only at one point in time), what matter is actual amount which we change the data in these tensors (over time). AK introduce a tracking list ud (update to data). This calculates the ratio between (std) of the grad to the data of parameters (and log10() for a nicer viz) without context of gradient.\n\n\nShow the code\nplt.figure(figsize=(11, 3))\nlegends = []\nfor i,p in enumerate(parameters):\n  if p.ndim == 2:\n    plt.plot([ud[j][i] for j in range(len(ud))])\n    legends.append('param %d' % i)\nplt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\nplt.legend(legends);\n\n\nBelow is the visualization from data collected after 1000 training loops:\n\n\n\nViz 4 1000\n\n\nRecall what we did to the last layer, avoiding over confidence, so the pink line looks different among others. In general, the learning process are good, if we change the learning rate to 0.0001, the chart looks much worse.\nBelow are viz 1 after 1000 training loops:\n\n\n\nViz 1 1000\n\n\nand viz 2:\n\n\n\nViz 2 1000\n\n\nand viz 3:\n\n\n\nViz 3 1000\n\n\nPretty decent till now. Let’s bring the BatchNorm back."
  },
  {
    "objectID": "blog/2024-11-26-nn-z2h-p4/index.html#bringing-back-batchnorm-looking-at-the-visualizations",
    "href": "blog/2024-11-26-nn-z2h-p4/index.html#bringing-back-batchnorm-looking-at-the-visualizations",
    "title": "NN-Z2H Lesson 4: Building makemore part 3 - Activations & Gradients, BatchNorm",
    "section": "bringing back batchnorm, looking at the visualizations",
    "text": "bringing back batchnorm, looking at the visualizations\nWe re-define the layers, and change gamma in last layer under no gradient instead of weight. We also dont want the “manual normalization” fan-in, and the gain 5/3 as well:\n\n\nShow the code\nlayers = [\n  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n]"
  },
  {
    "objectID": "blog/2024-11-26-nn-z2h-p4/index.html#summary-of-the-lecture-for-real-this-time",
    "href": "blog/2024-11-26-nn-z2h-p4/index.html#summary-of-the-lecture-for-real-this-time",
    "title": "NN-Z2H Lesson 4: Building makemore part 3 - Activations & Gradients, BatchNorm",
    "section": "summary of the lecture for real this time",
    "text": "summary of the lecture for real this time\n\nIntruduction of Batch Normalization - the 1st one of modern innovation to stablize Deep NN training;\nPyTorch-ifying code;\nIntroduction to some diagnostic tools that we can use to verify the network is in good state dynamically.\n\nWhat he did not try to improve here is the loss of the network. It’s now somehow bottleneck not by the Optimization, but by the Context Length he suspect.\n\nTraining Neural Network is like balancing a pencil on a finger.\n\nFinal network architecture and training:\n\n\nShow the code\n# BatchNorm1D and Tanh are the same\nclass Linear:\n    \"\"\"\n    Simplifying Pytorch Linear Layer: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n    \"\"\"\n    def __init__(self, fan_in, fan_out, bias=True):\n        self.weight = torch.randn((fan_in, fan_out), generator=g) # / fan_in**0.5\n        self.bias = torch.zeros(fan_out) if bias else None\n\n    def __call__(self, x):\n        self.out = x @ self.weight\n        if self.bias is not None:\n            self.out += self.bias\n        return self.out\n\n    def parameters(self):\n        return [self.weight] + ([] if self.bias is None else [self.bias])\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 100 # the number of neurons in the hidden layer of the MLP\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC = torch.randn((vocab_size, n_embd),            generator=g)\nlayers = [\n  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n]\n\nwith torch.no_grad():\n    # last layer: make less confident\n    layers[-1].gamma *= 0.1\n    # all other layers: apply gain\n    for layer in layers[:-1]:\n        if isinstance(layer, Linear):\n            layer.weight *= 1.0 #5/3\n\nparameters = [C] + [p for layer in layers for p in layer.parameters()]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\nud = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    x = emb.view(emb.shape[0], -1) # concatenate the vectors\n    for layer in layers:\n        x = layer(x)\n    loss = F.cross_entropy(x, Yb) # loss function\n\n    # backward pass\n    for layer in layers:\n        layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n    with torch.no_grad():\n        ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n\n    # break\n    # if i &gt;= 1000:\n    #     break # AFTER_DEBUG: would take out obviously to run full optimization\n\n\n47024\n      0/ 200000: 3.2870\n  10000/ 200000: 2.4521\n  20000/ 200000: 2.0847\n  30000/ 200000: 2.1838\n  40000/ 200000: 2.1515\n  50000/ 200000: 2.2246\n  60000/ 200000: 1.9450\n  70000/ 200000: 2.2514\n  80000/ 200000: 2.4420\n  90000/ 200000: 2.0624\n 100000/ 200000: 2.5850\n 110000/ 200000: 2.3225\n 120000/ 200000: 2.2004\n 130000/ 200000: 2.0352\n 140000/ 200000: 1.8516\n 150000/ 200000: 2.0424\n 160000/ 200000: 2.2229\n 170000/ 200000: 2.0384\n 180000/ 200000: 2.2274\n 190000/ 200000: 2.0901\n\n\nFinal visualization:\nViz 1:\n\n\n\nViz 1 final\n\n\nViz 2:\n\n\n\nViz 2 final\n\n\nViz 3:\n\n\n\nViz 3 final\n\n\nViz 4:\n\n\n\nViz 4 final\n\n\nThe final loss on train/val:\n\n\nShow the code\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n  x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n  }[split]\n  emb = C[x] # (N, block_size, n_embd)\n  x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n  for layer in layers:\n    x = layer(x)\n  loss = F.cross_entropy(x, y)\n  print(split, loss.item())\n\n# put layers into eval mode\nfor layer in layers:\n  layer.training = False\nsplit_loss('train')\nsplit_loss('val')\n\n\ntrain 2.103635549545288\nval 2.1365904808044434\n\n\nSample from the model:\n\n\nShow the code\n# sample from the model\ng = torch.Generator().manual_seed(2147483647 + 10)\n\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n      # forward pass the neural net\n      emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n      x = emb.view(emb.shape[0], -1) # concatenate the vectors\n      for layer in layers:\n        x = layer(x)\n      logits = x\n      probs = F.softmax(logits, dim=1)\n      # sample from the distribution\n      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n      # shift the context window and track the samples\n      context = context[1:] + [ix]\n      out.append(ix)\n      # if we sample the special '.' token, break\n      if ix == 0:\n        break\n    \n    print(''.join(itos[i] for i in out)) # decode and print the generated word\n\n\nmona.\nmayannielle.\ndhryah.\nrethan.\nejdraeg.\nadelynnelin.\nshi.\njen.\nedelisson.\narleigh.\nmalaia.\nnosadbergiaghiel.\nkinde.\njennex.\nterofius.\nkaven.\njamyleyeh.\nyuma.\nmyston.\nazhil.\n\n\nHappy learning!"
  },
  {
    "objectID": "blog/2024-11-26-nn-z2h-p4/index.html#exercises",
    "href": "blog/2024-11-26-nn-z2h-p4/index.html#exercises",
    "title": "NN-Z2H Lesson 4: Building makemore part 3 - Activations & Gradients, BatchNorm",
    "section": "Exercises:",
    "text": "Exercises:\n\nE01: I did not get around to seeing what happens when you initialize all weights and biases to zero. Try this and train the neural net. You might think either that 1) the network trains just fine or 2) the network doesn’t train at all, but actually it is 3) the network trains but only partially, and achieves a pretty bad final performance. Inspect the gradients and activations to figure out what is happening and why the network is only partially training, and what part is being trained exactly.\nE02: BatchNorm, unlike other normalization layers like LayerNorm/GroupNorm etc. has the big advantage that after training, the batchnorm gamma/beta can be “folded into” the weights of the preceeding Linear layers, effectively erasing the need to forward it at test time. Set up a small 3-layer MLP with batchnorms, train the network, then “fold” the batchnorm gamma/beta into the preceeding Linear layer’s W,b by creating a new W2, b2 and erasing the batch norm. Verify that this gives the same forward pass during inference. i.e. we see that the batchnorm is there just for stabilizing the training, and can be thrown out after training is done! pretty cool."
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "",
    "text": "This is not orginal content!\n\n\n\nThis is my study notes / codes along with Andrej Karpathy’s “Neural Networks: Zero to Hero” series."
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#reading-and-exploring-the-dataset",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#reading-and-exploring-the-dataset",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "reading and exploring the dataset",
    "text": "reading and exploring the dataset\n\n\nShow the code\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt\"\nwords = pd.read_csv(url, header=None).iloc[:, 0].tolist()\n\nprint(words[:10])\nprint(len(words))\n\n\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n32033\n\n\n\n\nShow the code\nprint(\"No of chars for the shortest word: \", min(len(w) for w in words))\nprint(\"No of chars for the longest word: \", max(len(w) for w in words))\n\n\nNo of chars for the shortest word:  2\nNo of chars for the longest word:  15\n\n\nBy looking into (1) the order of characters in individual word, and (2) that pattern for the whole dataset of 32k words, we will try to infer which character is likely to follow a character or chain of characters.\nWe will first building a bigrams languague model - which only works will 2 characters at a time - look at the current character and try to predict the next one. We are just following this local structure!\nIt’s just a simple (and weak) model but a good way to start."
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#exploring-the-bigrams-in-the-dataset",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#exploring-the-bigrams-in-the-dataset",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "exploring the bigrams in the dataset",
    "text": "exploring the bigrams in the dataset\n\n\nShow the code\nfor w in words[:3]:\n  chs = ['&lt;S&gt;'] + list(w) + ['&lt;E&gt;'] # special start and ending token, `list()` will turn all character in word to list\n  for ch1, ch2 in zip(chs, chs[1:]):\n    print(ch1, ch2)\n\n\n&lt;S&gt; e\ne m\nm m\nm a\na &lt;E&gt;\n&lt;S&gt; o\no l\nl i\ni v\nv i\ni a\na &lt;E&gt;\n&lt;S&gt; a\na v\nv a\na &lt;E&gt;"
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#counting-bigrams-in-a-python-dictionary",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#counting-bigrams-in-a-python-dictionary",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "counting bigrams in a python dictionary",
    "text": "counting bigrams in a python dictionary\nIn order to learn statistics about what character is more likely to follow another character, the simplest way is counting.\n\n\nShow the code\nb = {} # dict to store all pair of character\nfor w in words[:5]: # do it for first five words\n  chs = ['&lt;S&gt;'] + list(w) + ['&lt;E&gt;']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    bigram = (ch1, ch2)\n    b[bigram] = b.get(bigram, 0) + 1\n    # print(ch1, ch2)\n\n\n\n\nShow the code\nsorted(b.items(), key = lambda kv: -kv[1])\n\n\n[(('a', '&lt;E&gt;'), 5),\n (('i', 'a'), 2),\n (('&lt;S&gt;', 'e'), 1),\n (('e', 'm'), 1),\n (('m', 'm'), 1),\n (('m', 'a'), 1),\n (('&lt;S&gt;', 'o'), 1),\n (('o', 'l'), 1),\n (('l', 'i'), 1),\n (('i', 'v'), 1),\n (('v', 'i'), 1),\n (('&lt;S&gt;', 'a'), 1),\n (('a', 'v'), 1),\n (('v', 'a'), 1),\n (('&lt;S&gt;', 'i'), 1),\n (('i', 's'), 1),\n (('s', 'a'), 1),\n (('a', 'b'), 1),\n (('b', 'e'), 1),\n (('e', 'l'), 1),\n (('l', 'l'), 1),\n (('l', 'a'), 1),\n (('&lt;S&gt;', 's'), 1),\n (('s', 'o'), 1),\n (('o', 'p'), 1),\n (('p', 'h'), 1),\n (('h', 'i'), 1)]"
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#counting-bigrams-in-a-2d-torch-tensor-training-the-model",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#counting-bigrams-in-a-2d-torch-tensor-training-the-model",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "counting bigrams in a 2D torch tensor (“training the model”)",
    "text": "counting bigrams in a 2D torch tensor (“training the model”)\nInstead of using Python dictionary, we will use torch 2D array to store this information.\n\n\nShow the code\nimport torch\n\n\n\n\nShow the code\na = torch.zeros((3,5), dtype=torch.int32)\na\n\n\ntensor([[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]], dtype=torch.int32)\n\n\nHow can we access/assign a value in torch array:\n\n\nShow the code\na[1:3] = 10\na\n\n\ntensor([[ 0,  0,  0,  0,  0],\n        [10, 10, 10, 10, 10],\n        [10, 10, 10, 10, 10]], dtype=torch.int32)\n\n\nNow the english alphabet contain 26 characters, we will need to capture the &lt;S&gt; and &lt;E&gt; also. So it would be 28 x 28 array.\n\n\nShow the code\nN = torch.zeros((28,28), dtype=torch.int32)\n\n\nThis will collect all the characters used in our dataset (join all words to a massive string and pass it to a set(), which will remove duplicate). With such a large dataset, all the english characters were used.\n\n\nShow the code\nchars = sorted(list(set(''.join(words))))\nlen(chars) # 26 \n\n# with index\nstoi = {s:i for i,s in enumerate(chars)}\nstoi['&lt;S&gt;'] = 26\nstoi['&lt;E&gt;'] = 27\nstoi\n\n\n{'a': 0,\n 'b': 1,\n 'c': 2,\n 'd': 3,\n 'e': 4,\n 'f': 5,\n 'g': 6,\n 'h': 7,\n 'i': 8,\n 'j': 9,\n 'k': 10,\n 'l': 11,\n 'm': 12,\n 'n': 13,\n 'o': 14,\n 'p': 15,\n 'q': 16,\n 'r': 17,\n 's': 18,\n 't': 19,\n 'u': 20,\n 'v': 21,\n 'w': 22,\n 'x': 23,\n 'y': 24,\n 'z': 25,\n '&lt;S&gt;': 26,\n '&lt;E&gt;': 27}\n\n\n\n\nShow the code\nfor w in words:\n  chs = ['&lt;S&gt;'] + list(w) + ['&lt;E&gt;']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    N[ix1, ix2] += 1"
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#visualizing-the-bigram-tensor",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#visualizing-the-bigram-tensor",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "visualizing the bigram tensor",
    "text": "visualizing the bigram tensor\n\n\nShow the code\nitos = {i:s for s, i in stoi.items()}\n\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize=(16,16))\nplt.imshow(N, cmap='Blues')\nfor i in range(28):\n  for j in range(28):\n    # plot character strings with number of time\n    chstr = itos[i] + itos[j]\n    plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color=\"gray\")\n    plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color=\"gray\")\nplt.axis('off')"
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#deleting-spurious-s-and-e-tokens-in-favor-of-a-single-.-token",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#deleting-spurious-s-and-e-tokens-in-favor-of-a-single-.-token",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "deleting spurious (S) and (E) tokens in favor of a single . token",
    "text": "deleting spurious (S) and (E) tokens in favor of a single . token\n&lt;S&gt;, and &lt;E&gt; look a bit annoying. let’s replace them by simple ..\n\n\nShow the code\nN = torch.zeros((27,27), dtype=torch.int32)\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s, i in stoi.items()}\n\nfor w in words:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    N[ix1, ix2] += 1\n\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize=(16,16))\nplt.imshow(N, cmap='Blues')\nfor i in range(27):\n    for j in range(27):\n        chstr = itos[i] + itos[j]\n        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\nplt.axis('off')"
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#sampling-from-the-model",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#sampling-from-the-model",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "sampling from the model",
    "text": "sampling from the model\nTaking the first column of the array.\n\n\nShow the code\nN[0]\n\n\ntensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n         134,  535,  929], dtype=torch.int32)\n\n\nColumn-wise probability.\n\n\nShow the code\np = N[0].float()\np = p / p.sum()\np\n\n\ntensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,\n        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,\n        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])\n\n\nCreating random number with Pytorch generator at a state.\n\n\nShow the code\ng = torch.Generator().manual_seed(2147483647)\np_test = torch.rand(3, generator=g)\np_test = p_test / p_test.sum()\n\n\n\n\nShow the code\ntorch.multinomial(p_test, num_samples=100, replacement=True, generator=g)\n\n\ntensor([1, 1, 2, 0, 0, 2, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 2, 0, 0,\n        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n        0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0,\n        0, 1, 1, 1])\n\n\nNow back to our data, generate a tensor with 1 value from the p vector.\n\n\nShow the code\ng = torch.Generator().manual_seed(2147483647)\nix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\nitos[ix]\n\n\n'j'\n\n\nLet’s automate it:\n\n\nShow the code\ng = torch.Generator().manual_seed(2147483647)\n\nix = 0\nwhile True:\n  p = N[ix].float()\n  p = p / p.sum()\n  ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n  print(itos[ix])\n  if ix == 0:\n    break\n\n\nj\nu\nn\ni\nd\ne\n.\n\n\nAnd more, joining the last result to single word, and make new 10 names:\n\n\nShow the code\ng = torch.Generator().manual_seed(2147483647)\n\nfor i in range(10):\n  \n  out = []\n  ix = 0\n  while True:\n    p = N[ix].float()\n    p = p / p.sum()\n\n    # p = torch.ones(27) / 27.0\n    # the result look terrible, but compare to an un-trained model for eg p - uncomment to code above, they are still like names.\n    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n    out.append(itos[ix])\n    if ix == 0:\n      break\n  print(''.join(out))\n\n\njunide.\njanasah.\np.\ncony.\na.\nnn.\nkohin.\ntolian.\njuee.\nksahnaauranilevias."
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#efficiency-vectorized-normalization-of-the-rows-tensor-broadcasting",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#efficiency-vectorized-normalization-of-the-rows-tensor-broadcasting",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "efficiency! vectorized normalization of the rows, tensor broadcasting",
    "text": "efficiency! vectorized normalization of the rows, tensor broadcasting\nWe just fetching a row of N from the counts matrix, and then always do the same things: converting to float, dividing. That’s not efficient! We now will optimize this:\n\n\nShow the code\nP = N.float()\n# param 1 helps summing horizontally, by rows\n# keepdim keeps the dimension the output is still 2D array with 1 column for each row, not a vertical vector entirely\n# tensor already support to broadcast the row sum allowing this dividing (keepdim helped not to mess the broadcast)\nP /= P.sum(1, keepdim=True)\n# inplace operator instead of P = P / P.sum(1, keepdim=True), take care of memory!\n\n\n\n\nShow the code\ng = torch.Generator().manual_seed(2147483647)\n\nfor i in range(10):\n  \n  out = []\n  ix = 0\n  while True:\n    p = P[ix]\n    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n    out.append(itos[ix])\n    if ix == 0:\n      break\n  print(''.join(out))\n\n\njunide.\njanasah.\np.\ncony.\na.\nnn.\nkohin.\ntolian.\njuee.\nksahnaauranilevias."
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#loss-function-the-negative-log-likelihood-of-the-data-under-our-model",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#loss-function-the-negative-log-likelihood-of-the-data-under-our-model",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "loss function (the negative log likelihood of the data under our model)",
    "text": "loss function (the negative log likelihood of the data under our model)\nWe’ve just trained and sampled from the model, iteratively sampled the next character and fed it in each time and got the next one. Now we need to somehow measure the quality of the model.\nHow good is it in predicting? Gimme a number!\n\n\nShow the code\n# showing bigram for the first 3 words, along with the probability inferred by our model (`P`)\n# the higher the prob, the better of prediction\n# since a fair (under no data) probability of occuring a character is roughly 1/27 ~ 4%, any prob higher than 4% should be good\n# we need to combine all the prob to a single 1 number, measuring how good is our model?\n# since multiplying all the prob resulting a very very small number, we will approach by the log likelihood function\n# the log likelihood is just the sum of log of individual multiplier\n\nlog_likelihood = 0.0\nfor w in words[:3]:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    prob = P[ix1, ix2]\n    log_prob = torch.log(prob)\n    log_likelihood += log_prob\n    print(f'{ch1}{ch2}: {prob:.4f} {log_prob:.4f}')\n\nprint(f'{log_likelihood=}') # print both the variable name and its value, for the first 3 words\n\n\n.e: 0.0478 -3.0408\nem: 0.0377 -3.2793\nmm: 0.0253 -3.6772\nma: 0.3899 -0.9418\na.: 0.1960 -1.6299\n.o: 0.0123 -4.3982\nol: 0.0780 -2.5508\nli: 0.1777 -1.7278\niv: 0.0152 -4.1867\nvi: 0.3541 -1.0383\nia: 0.1381 -1.9796\na.: 0.1960 -1.6299\n.a: 0.1377 -1.9829\nav: 0.0246 -3.7045\nva: 0.2495 -1.3882\na.: 0.1960 -1.6299\nlog_likelihood=tensor(-38.7856)\n\n\nIf all the probs equal to 1, the logs will be 0. If they close to 0, the logs will be more negative. We want to use this as a loss function, meaning lower the better, so we will invert it:\n\n\nShow the code\nneg_log_likelihood = 0.0\nn = 0\nfor w in words:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    prob = P[ix1, ix2]\n    log_prob = torch.log(prob)\n    neg_log_likelihood += -log_prob\n    n += 1\n\nprint(f'{neg_log_likelihood=}')\nprint(f'{neg_log_likelihood/n}')\n\n\nneg_log_likelihood=tensor(559891.7500)\n2.454094171524048\n\n\nFinally we insert a count and calculate the “normalized” (or average) negative log likelihood. The lower of this number, the better model we have.\nYou can test with your name:\n\n\nShow the code\nneg_log_likelihood = 0.0\nn = 0\nfor w in ['tuan']:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    prob = P[ix1, ix2]\n    log_prob = torch.log(prob)\n    neg_log_likelihood += -log_prob\n    n += 1\n    print(f'{ch1}{ch2}: {prob:.4f} {log_prob:.4f}')\nprint(f'{neg_log_likelihood=}')\nprint(f'{neg_log_likelihood/n}')\n\n\n.t: 0.0408 -3.1983\ntu: 0.0140 -4.2684\nua: 0.0520 -2.9566\nan: 0.1605 -1.8296\nn.: 0.3690 -0.9969\nneg_log_likelihood=tensor(13.2498)\n2.649962902069092\n\n\ntu is not common in our dataset."
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#model-smoothing-with-fake-counts",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#model-smoothing-with-fake-counts",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "model smoothing with fake counts",
    "text": "model smoothing with fake counts\nFor a pair of bigram that does not exist in the dataset, for eg jq, the prob will be zero and log likelihood will be infinity. We can kind of smooth our model by adding constant “fake counts” to the model:\n\n\nShow the code\nP = (N+1).float()\n\n\n1 is decent number, the more you added, you’ll have a more uniformed distribution, the less, the more peaked distribution you have."
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#creating-the-bigram-dataset-for-the-neural-net",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#creating-the-bigram-dataset-for-the-neural-net",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "creating the bigram dataset for the neural net",
    "text": "creating the bigram dataset for the neural net\n\n\nShow the code\n# creating training set of bigram(x, y)\nxs, ys = [], []\n\nfor w in words[:1]:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    xs.append(ix1)\n    ys.append(ix2)\n\nxs = torch.tensor(xs) # both .tensor() and .Tensor() work!\nys = torch.tensor(ys)\n# https://stackoverflow.com/questions/51911749/what-is-the-difference-between-torch-tensor-and-torch-tensor\n# .tensor() infers dtype as int64 while .Tensor() infers dtype as float32, in this case\n\n\nThe input and output tensor for the first word will be look like this:\n&gt; print(ch1,ch2)\n. e\ne m\nm m\nm a\na .\n&gt; xs\ntensor([ 0,  5, 13, 13,  1])\n&gt; ys\ntensor([ 5, 13, 13,  1,  0])"
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#feeding-integers-into-neural-nets-one-hot-encodings",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#feeding-integers-into-neural-nets-one-hot-encodings",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "feeding integers into neural nets? one-hot encodings",
    "text": "feeding integers into neural nets? one-hot encodings\n\n\nShow the code\nimport torch.nn.functional as F\n\nxenc = F.one_hot(xs, num_classes=27).float() # remember to cast integer to float, which can be fed to neural nets\nxenc\n\n\ntensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n\n\n\n\nShow the code\nxenc.shape\n\n\ntorch.Size([5, 27])\n\n\n\n\nShow the code\nplt.imshow(xenc, cmap=\"Blues\")"
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#the-neural-net-one-linear-layer-of-neurons-implemented-with-matrix-multiplication",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#the-neural-net-one-linear-layer-of-neurons-implemented-with-matrix-multiplication",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "the “neural net”: one linear layer of neurons implemented with matrix multiplication",
    "text": "the “neural net”: one linear layer of neurons implemented with matrix multiplication\n\n\nShow the code\nW = torch.randn((27, 1)) # fulfill a tensor with random number followed normal distribution, 1 is indicating 1 single neuron\nxenc @ W # @ is matrix mult operator in PyTorch\n# (5, 27) @ (27, 1) will result (5, 1) matrix\n# no bias is added for now\n\n\ntensor([[ 0.2084],\n        [-0.1912],\n        [-0.7102],\n        [-0.7102],\n        [ 0.5453]])\n\n\nThis is 1 neuron only, now we want to evaluate all 27 characters using only 5 inputs from the first word, we’ll make 27 neurons:\n\n\nShow the code\nW = torch.randn((27, 27))\nxenc @ W \n# (5, 27) @ (27, 27) will result (5, 27) matrix\n# no bias is added for now\n\n\ntensor([[ 1.1742, -0.1228,  0.3312, -1.7074, -0.5036,  0.5717,  0.5462,  1.2425,\n          2.2197,  0.9381, -0.4647, -1.1784, -0.0046,  1.5661,  0.1240, -0.1818,\n         -0.1129,  1.1559, -0.2303, -0.2885, -0.8589, -0.6829,  0.6188,  0.6673,\n         -0.6109,  1.9768,  0.2832],\n        [-1.3979, -1.4019, -3.2251, -0.6767,  0.3323,  0.2407,  0.5753, -0.2401,\n          1.0848,  0.1452,  0.1295,  0.4613,  2.3659,  1.4620,  0.3717, -1.1485,\n         -0.5063, -0.4455, -0.2564, -0.6005,  1.9493, -0.3160,  0.7859, -1.0101,\n          0.3456,  0.9161,  0.5722],\n        [-1.2995, -0.2011,  0.0972,  0.1959,  0.5090,  0.0080, -0.8293, -0.9525,\n         -1.0965,  0.6003,  0.7801, -1.1612, -0.4070,  2.1902, -0.0264,  0.9155,\n          1.4465, -0.3992,  0.0044,  0.0918,  2.1084,  0.7271, -0.5992, -0.1443,\n          0.1657, -0.0258, -0.4744],\n        [-1.2995, -0.2011,  0.0972,  0.1959,  0.5090,  0.0080, -0.8293, -0.9525,\n         -1.0965,  0.6003,  0.7801, -1.1612, -0.4070,  2.1902, -0.0264,  0.9155,\n          1.4465, -0.3992,  0.0044,  0.0918,  2.1084,  0.7271, -0.5992, -0.1443,\n          0.1657, -0.0258, -0.4744],\n        [ 0.4732,  0.0148, -0.4303, -0.3870, -2.1335, -1.9046, -0.8460,  1.0632,\n         -0.7406,  1.9063, -0.7375,  0.8191, -1.1415,  0.1678, -0.1831,  0.5425,\n         -1.5502, -0.3083,  0.4649, -0.8271,  0.8959,  0.6717,  1.1916,  2.1147,\n          0.3208, -1.3000, -1.0078]])"
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#transforming-neural-net-outputs-into-probabilities-the-softmax",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#transforming-neural-net-outputs-into-probabilities-the-softmax",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "transforming neural net outputs into probabilities: the softmax",
    "text": "transforming neural net outputs into probabilities: the softmax\nSo far we have fed 5 inputs to 27 neurons for 27 characters in the first layer of the neural net. We notice that the output number ranges from negative to positive while we want “how likely of the next characters”. It would be counts, or probs, hence we exponentiate the logits, then dividing row-wise total to get the prob of each character.\nThis is call the softmax!\n\n\nShow the code\nlogits = xenc @ W # log-counts\ncounts = logits.exp() # equivalent N\nprobs = counts / counts.sum(1, keepdims=True)\nprobs\n\n\ntensor([[0.0609, 0.0166, 0.0262, 0.0034, 0.0114, 0.0333, 0.0325, 0.0652, 0.1732,\n         0.0481, 0.0118, 0.0058, 0.0187, 0.0901, 0.0213, 0.0157, 0.0168, 0.0598,\n         0.0149, 0.0141, 0.0080, 0.0095, 0.0349, 0.0367, 0.0102, 0.1359, 0.0250],\n        [0.0051, 0.0051, 0.0008, 0.0105, 0.0288, 0.0263, 0.0367, 0.0162, 0.0611,\n         0.0239, 0.0235, 0.0328, 0.2201, 0.0891, 0.0300, 0.0066, 0.0125, 0.0132,\n         0.0160, 0.0113, 0.1451, 0.0151, 0.0453, 0.0075, 0.0292, 0.0516, 0.0366],\n        [0.0059, 0.0177, 0.0239, 0.0264, 0.0361, 0.0218, 0.0095, 0.0084, 0.0072,\n         0.0395, 0.0473, 0.0068, 0.0144, 0.1937, 0.0211, 0.0541, 0.0921, 0.0145,\n         0.0218, 0.0238, 0.1785, 0.0448, 0.0119, 0.0188, 0.0256, 0.0211, 0.0135],\n        [0.0059, 0.0177, 0.0239, 0.0264, 0.0361, 0.0218, 0.0095, 0.0084, 0.0072,\n         0.0395, 0.0473, 0.0068, 0.0144, 0.1937, 0.0211, 0.0541, 0.0921, 0.0145,\n         0.0218, 0.0238, 0.1785, 0.0448, 0.0119, 0.0188, 0.0256, 0.0211, 0.0135],\n        [0.0377, 0.0239, 0.0153, 0.0160, 0.0028, 0.0035, 0.0101, 0.0681, 0.0112,\n         0.1582, 0.0112, 0.0533, 0.0075, 0.0278, 0.0196, 0.0405, 0.0050, 0.0173,\n         0.0374, 0.0103, 0.0576, 0.0460, 0.0774, 0.1949, 0.0324, 0.0064, 0.0086]])"
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#summary-preview-to-next-steps-reference-to-micrograd",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#summary-preview-to-next-steps-reference-to-micrograd",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "summary, preview to next steps, reference to micrograd",
    "text": "summary, preview to next steps, reference to micrograd\n\n\nShow the code\n# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\ng = torch.Generator().manual_seed(2147483647) # to make sure we all have same random\nW = torch.randn((27, 27), generator=g)\nxenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\nlogits = xenc @ W # predict log-counts\ncounts = logits.exp() # counts, equivalent to N\nprobs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n# btw: the last 2 lines here are together called a 'softmax'\n\n\n\n\nShow the code\nprobs.shape\n\n\ntorch.Size([5, 27])\n\n\nBelow is detail explaination for each example from our 5-datapoint dataset.\n\n\nShow the code\nnlls = torch.zeros(5)\nfor i in range(5):\n  # i-th bigram:\n  x = xs[i].item() # input character index\n  y = ys[i].item() # label character index\n  print('--------')\n  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n  print('input to the neural net:', x)\n  print('output probabilities from the neural net:', probs[i])\n  print('label (actual next character):', y)\n  p = probs[i, y]\n  print('probability assigned by the net to the the correct character:', p.item())\n  logp = torch.log(p)\n  print('log likelihood:', logp.item())\n  nll = -logp\n  print('negative log likelihood:', nll.item())\n  nlls[i] = nll\n\nprint('=========')\nprint('average negative log likelihood, i.e. loss =', nlls.mean().item())\n\n\n--------\nbigram example 1: .e (indexes 0,5)\ninput to the neural net: 0\noutput probabilities from the neural net: tensor([0.0609, 0.0166, 0.0262, 0.0034, 0.0114, 0.0333, 0.0325, 0.0652, 0.1732,\n        0.0481, 0.0118, 0.0058, 0.0187, 0.0901, 0.0213, 0.0157, 0.0168, 0.0598,\n        0.0149, 0.0141, 0.0080, 0.0095, 0.0349, 0.0367, 0.0102, 0.1359, 0.0250])\nlabel (actual next character): 5\nprobability assigned by the net to the the correct character: 0.03332865983247757\nlog likelihood: -3.4013376235961914\nnegative log likelihood: 3.4013376235961914\n--------\nbigram example 2: em (indexes 5,13)\ninput to the neural net: 5\noutput probabilities from the neural net: tensor([0.0051, 0.0051, 0.0008, 0.0105, 0.0288, 0.0263, 0.0367, 0.0162, 0.0611,\n        0.0239, 0.0235, 0.0328, 0.2201, 0.0891, 0.0300, 0.0066, 0.0125, 0.0132,\n        0.0160, 0.0113, 0.1451, 0.0151, 0.0453, 0.0075, 0.0292, 0.0516, 0.0366])\nlabel (actual next character): 13\nprobability assigned by the net to the the correct character: 0.08912799507379532\nlog likelihood: -2.4176816940307617\nnegative log likelihood: 2.4176816940307617\n--------\nbigram example 3: mm (indexes 13,13)\ninput to the neural net: 13\noutput probabilities from the neural net: tensor([0.0059, 0.0177, 0.0239, 0.0264, 0.0361, 0.0218, 0.0095, 0.0084, 0.0072,\n        0.0395, 0.0473, 0.0068, 0.0144, 0.1937, 0.0211, 0.0541, 0.0921, 0.0145,\n        0.0218, 0.0238, 0.1785, 0.0448, 0.0119, 0.0188, 0.0256, 0.0211, 0.0135])\nlabel (actual next character): 13\nprobability assigned by the net to the the correct character: 0.19367089867591858\nlog likelihood: -1.6415950059890747\nnegative log likelihood: 1.6415950059890747\n--------\nbigram example 4: ma (indexes 13,1)\ninput to the neural net: 13\noutput probabilities from the neural net: tensor([0.0059, 0.0177, 0.0239, 0.0264, 0.0361, 0.0218, 0.0095, 0.0084, 0.0072,\n        0.0395, 0.0473, 0.0068, 0.0144, 0.1937, 0.0211, 0.0541, 0.0921, 0.0145,\n        0.0218, 0.0238, 0.1785, 0.0448, 0.0119, 0.0188, 0.0256, 0.0211, 0.0135])\nlabel (actual next character): 1\nprobability assigned by the net to the the correct character: 0.01772291027009487\nlog likelihood: -4.032896995544434\nnegative log likelihood: 4.032896995544434\n--------\nbigram example 5: a. (indexes 1,0)\ninput to the neural net: 1\noutput probabilities from the neural net: tensor([0.0377, 0.0239, 0.0153, 0.0160, 0.0028, 0.0035, 0.0101, 0.0681, 0.0112,\n        0.1582, 0.0112, 0.0533, 0.0075, 0.0278, 0.0196, 0.0405, 0.0050, 0.0173,\n        0.0374, 0.0103, 0.0576, 0.0460, 0.0774, 0.1949, 0.0324, 0.0064, 0.0086])\nlabel (actual next character): 0\nprobability assigned by the net to the the correct character: 0.0377422496676445\nlog likelihood: -3.276975154876709\nnegative log likelihood: 3.276975154876709\n=========\naverage negative log likelihood, i.e. loss = 2.954097270965576"
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#vectorized-loss",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#vectorized-loss",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "vectorized loss",
    "text": "vectorized loss\n\n\nShow the code\nloss = - probs[torch.arange(5), ys].log().mean()\nloss\n\n\ntensor(2.9541)"
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#backward-and-update-in-pytorch",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#backward-and-update-in-pytorch",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "backward and update, in PyTorch",
    "text": "backward and update, in PyTorch\n\n\nShow the code\n# backward pass\nW.grad = None # set to zero the gradient\nloss.backward()\n# this can not yet run for now, PyTorch require the specification of require_grad\n\n\n\n\nShow the code\nW.data += -0.1 * W.grad"
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#putting-everything-together",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#putting-everything-together",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "putting everything together",
    "text": "putting everything together\n\n\nShow the code\n# create the dataset\nxs, ys = [], []\nfor w in words:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    xs.append(ix1)\n    ys.append(ix2)\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\nnum = xs.nelement()\nprint('number of examples: ', num)\n\n# initialize the 'network'\ng = torch.Generator().manual_seed(2147483647)\nW = torch.randn((27, 27), generator=g, requires_grad=True)\n\n\nnumber of examples:  228146\n\n\n\n\nShow the code\n# gradient descent\nfor k in range(1): # after run 100 times, shorten the notebook\n  \n  # forward pass\n  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n  logits = xenc @ W # predict log-counts\n  counts = logits.exp() # counts, equivalent to N\n  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean() # regularization loss\n  print(loss.item())\n  \n  # backward pass\n  W.grad = None # set to zero the gradient\n  loss.backward()\n  \n  # update\n  W.data += -50 * W.grad\n\n\n3.768618583679199\n\n\nLooking back to the backprogation in the lesson 1, everything look similar here:\n\n\n\n\n\n\n\n\nPart\nThis neural nets for bigram language modeling\nNeural nets introduced in the lesson 1\n\n\n\n\nForward pass\nprobs , use negative log likelihood as loss function, doing classification\nypred , use MSE for loss function, doing regression\n\n\nBackward pass\nSame, offered by Torch.\nSet grad of params to be zeros and do backpropagation.\n\n\nUpdate loss\nSame\nUpdate the parameters, change the parameters following opposite direction to reduce the loss."
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#note-1-one-hot-encoding-really-just-selects-a-row-of-the-next-linear-layers-weight-matrix",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#note-1-one-hot-encoding-really-just-selects-a-row-of-the-next-linear-layers-weight-matrix",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "note 1: one-hot encoding really just selects a row of the next Linear layer’s weight matrix",
    "text": "note 1: one-hot encoding really just selects a row of the next Linear layer’s weight matrix\nLook at the below code, xenc @ W is (5, 27) @ (27, 27) that will result (5, 27) matrix. Each ix row of that 5-rows result matrix should be the selection of corresponding character rows in the W.\n\n\nShow the code\nlogits = xenc @ W # predict log-counts\ncounts = logits.exp() # counts, equivalent to N\nprobs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n\n\nSo in this gradient-based framework, we start with a random array of parameters. By optimizing the loss function we will get the same result with the bigram approach (W and N are almost the same, it’s log count here why is count in bigram). That’s why we obtained the same loss!\nThe neural networks offer more flexibility!"
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#note-2-model-smoothing-as-regularization-loss",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#note-2-model-smoothing-as-regularization-loss",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "note 2: model smoothing as regularization loss",
    "text": "note 2: model smoothing as regularization loss\nSame with smoothing technique when we’ve doing the bigram model, gradient-based framework have an equivalent way for smoothing. We will try to incentivize W to be near zero. We augment to loss function by adding this: 0.01*(W**2).mean().\n\n\nShow the code\nloss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()"
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#sampling-from-the-neural-net",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#sampling-from-the-neural-net",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "sampling from the neural net",
    "text": "sampling from the neural net\n\n\nShow the code\n# finally, sample from the 'neural net' model\ng = torch.Generator().manual_seed(2147483647)\n\nfor i in range(5):\n  \n  out = []\n  ix = 0\n  while True:\n    \n    # ----------\n    # BEFORE:\n    # p = P[ix]\n    # ----------\n    # NOW:\n    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n    logits = xenc @ W # predict log-counts\n    counts = logits.exp() # counts, equivalent to N\n    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n    # ----------\n    \n    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n    out.append(itos[ix])\n    if ix == 0:\n      break\n  print(''.join(out))\n\n\njuwjdjdjancqydjufhqyywecnw.\n.\noiin.\ntoziasz.\ntwt."
  },
  {
    "objectID": "blog/2024-11-15-nn-z2h-p2/index.html#conclusion",
    "href": "blog/2024-11-15-nn-z2h-p2/index.html#conclusion",
    "title": "NN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore",
    "section": "conclusion",
    "text": "conclusion\nWhat we’ve gone through:\n\nintroduced bigrams language model? how we can train, sample, and evaluate the model;\nwe modeled by 2 different ways:\n\n1st: counted out the freq of bigram and normalized it;\n2nd: used a negative log likelihood loss as a guide to optimizing the counts matrix/array in a gradient-based framework;\nwe obtained the same result!\n\ngradient-based framework is more flexible. We’ve just modeled the simplest/dumpiest language model. In next lessons, we will complexify it.\n\nWe are on the way out to transformer!\nThank you, Andrej!"
  },
  {
    "objectID": "blog/2024-10-25-CEE-in-practice/index.html",
    "href": "blog/2024-10-25-CEE-in-practice/index.html",
    "title": "Keynote on: Causal Effect Estimation in Practice - lessons learned from E-commerce & Banking",
    "section": "",
    "text": "Link: https://www.youtube.com/watch?v=pz7QD2GPBlE\nDanial focuses on the practical challenges and solutions in implementing causal effect estimation in business settings, particularly in finance and banking. The speaker emphasizes the gap between theoretical frameworks and real-world applications, sharing lessons learned from actual implementations."
  },
  {
    "objectID": "blog/2024-10-25-CEE-in-practice/index.html#handling-non-randomization",
    "href": "blog/2024-10-25-CEE-in-practice/index.html#handling-non-randomization",
    "title": "Keynote on: Causal Effect Estimation in Practice - lessons learned from E-commerce & Banking",
    "section": "4.1. Handling Non-randomization",
    "text": "4.1. Handling Non-randomization\n\nUse propensity score modeling to mimic randomization;\nAccount for confounding variables (like age, sex) that affect both treatment assignment and outcomes;\nApply inverse probability weighting to correct for selection bias.\n\n\n\n\n\n\n\n\n\n\nminic randomization"
  },
  {
    "objectID": "blog/2024-10-25-CEE-in-practice/index.html#control-group-selection",
    "href": "blog/2024-10-25-CEE-in-practice/index.html#control-group-selection",
    "title": "Keynote on: Causal Effect Estimation in Practice - lessons learned from E-commerce & Banking",
    "section": "4.2. Control Group Selection",
    "text": "4.2. Control Group Selection\n\nUse rule-based methods to create comparable control groups;\nApply the same filters used in campaign targeting;\nConsider similar products within the same category;\nImplement exclusion criteria to maintain non-interference and consistency;\nEnsure basic criteria matching (e.g., valid email addresses, opt-in status).\n\n\n\n\n\n\n\n\n\n\ngood control"
  },
  {
    "objectID": "blog/2024-10-25-CEE-in-practice/index.html#kpi-consideration",
    "href": "blog/2024-10-25-CEE-in-practice/index.html#kpi-consideration",
    "title": "Keynote on: Causal Effect Estimation in Practice - lessons learned from E-commerce & Banking",
    "section": "4.3. KPI Consideration",
    "text": "4.3. KPI Consideration\n\nUnderstand the propagation of effects through time:\n\nShort-term metrics (email opens, clicks);\nMid-term metrics (product adoption);\nLong-term metrics (revenue generation);\n\nConsider data availability and measurement timing;\nAccount for different variables affecting each level of metrics.\n\n\n\n\n\n\n\n\n\n\nKPIs consideration"
  },
  {
    "objectID": "blog/2024-10-25-CEE-in-practice/index.html#potential-outcomes-framework",
    "href": "blog/2024-10-25-CEE-in-practice/index.html#potential-outcomes-framework",
    "title": "Keynote on: Causal Effect Estimation in Practice - lessons learned from E-commerce & Banking",
    "section": "5.1. Potential Outcomes Framework",
    "text": "5.1. Potential Outcomes Framework\n\nFocuses on treatment assignment mechanisms;\nUses propensity scoring and inverse probability weighting;\nSimpler to implement in practice.\n\n\n\n\n\n\n\n\n\n\npotential outcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npotential outcome assumptions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwhen association is causation?"
  },
  {
    "objectID": "blog/2024-10-25-CEE-in-practice/index.html#structural-causal-models",
    "href": "blog/2024-10-25-CEE-in-practice/index.html#structural-causal-models",
    "title": "Keynote on: Causal Effect Estimation in Practice - lessons learned from E-commerce & Banking",
    "section": "5.2. Structural Causal Models",
    "text": "5.2. Structural Causal Models\n\nBased on causal graphs;\nProvides a more complete picture of relationships;\nRequires careful consideration of mediators, confounders, and colliders.\n\n\n\n\n\n\n\n\n\n\nstructural causal models"
  },
  {
    "objectID": "blog/2024-08-06-bjj-mental-models/index.html",
    "href": "blog/2024-08-06-bjj-mental-models/index.html",
    "title": "Một số khuôn mẫu tư duy trong Brazilian Jiu-jitsu",
    "section": "",
    "text": "lược dịch và/hoặc tóm tắt cho khóa học cũng “Mechanic Models of BJJ: A Crash Course” của Steve Kwan.\n\n1. theory of alignment\nlà lý thuyết tối cao của Jiu-jitsu, bao gồm ba yếu tố:\n\nposture - tư thế: position một cách hiệu quả cổ (neck) và cột sống (spine);\nstructure - cấu trúc: position một cách hiệu quả tay (arms) và chân (legs);\nbase - cở sở: khả năng hấp thụ (absorb) hoặc tạo ra lực liên quan để đạt được mục tiêu.\n\nđể đạt được alignment hiệu quả, luôn ghi nhớ hai mục tiêu sau:\n\nmaintain your alignment;\nbreak your opponent’s alignment;\ndo not proceed 2. if you are failing at 1.\n\ntrong đào tạo, với trọng số của ba yếu tố như nhau, có thể “chấm điểm” (alignment scorecard) với thang từ 0-3 cho mỗi người chơi. điểm càng cao hơn sẽ là dấu hiệu cho việc bạn có một cơ hội tốt để phá vỡ alignment của đối thủ. ví dụ, trong closed guard, bạn ở dưới, và không ai đang khống chế đối phương bằng một hand grip:\nbạn sẽ có 3 điểm nếu:\n\nposture: cổ (neck) và cột sống (spine) ở vị trí tự nhiên, thẳng, không bị kiểm soát. hông (hip) được tự do di chuyển;\nstructure: tay (hands) và chân (legs) tự do tạo ra các grips và khung - frames;\nbase: bạn có thể tạo (gen) hoặc hấp thụ lực (absorb) nhờ vai và vị trí đặt chân.\n\nđối thủ sẽ có 3 điểm nếu, tương ứng:\n\ncổ và cột sống thẳng, nằm chính diện (vertical, centre) giữa cơ thể bạn;\ntay của họ không bị kiểm soát một cách chủ động;\nhọ có thể chống lại lực từ bạn với cột sống thẳng, vai mở. họ có khả năng break guard khi ngồi - dùng tay, hoặc đứng lên.\n\ncả hai đều không có lợi thế đáng kể và nỗ lực submission sẽ dẫn đến escape hoặc reversal.\n\n\n2. core mechanics\nnếu tiếp cận BJJ theo technique approach, sẽ có rất nhiều kỹ thuật cũng các biến thể (variations) mà bạn không thể khi nhớ hết. may mắn là, các kỹ thuật của BJJ đều là sự kết hợp của 6 cơ chế cốt lõi (core mechanics) sau đây:\n\nframe (khung):\n\n\nlà concept phòng thủ quan trọng nhất trong BJJ, nó tạo ra một khung bảo vệ (shield) sử dụng những phần cơ thể cứng (hard) hoặc nhiều xương (bony), thường được sử dụng cho người ở vị trí dưới (bottom);\nframe không phụ thuộc vào cơ bắp, mà là sự căn chỉnh hệ xương. bạn không muốn dùng áp lực đẩy đối thủ ra xa, mà là dùng cấu trúc xương để giữ đối thủ tại một khoảng cách nhất định, tạo không gian để bản thân di chuyển. frame tốt thì phải chắc chắn và không sử dụng quá nhiều sức lực ;\nkhông nên frame với các khớp (joints) dễ bị đối phương khai thác: ví dụ dùng hai tay để đẩy đối thủ, phần cổ (wrist) và khuỷu tay yếu và rất dễ bị counter. tương tự, khi frame bằng chân, bạn nên sử dụng gối (knees) và ống chân (shins) nhờ diện tích lớn, dùng bàn chân (feet) có lợi thế về khoảng cách tạo được tuy nhiên rất dễ bị khai thác.\n\nđọc thêm về solid frame\nlevers (đòn bẩy):\n\n\nđòn bẩy là phép bội lực rất hiệu quả để tạo sơ hở (openings) và tấn công và kiểm soát đối thủ;\ntheo thứ tự từ khỏe đến yếu nhất, 3 công cụ đòn bẩy trên cơ thể bạn chính là chân, tay, và đầu; \nđầu (head) thường được sử dụng để break tư thế (posture) của đối thủ, trong khi tay (arms) và chân (legs) thường được sử dụng để tấn công cấu trúc (structure);\nmỗi chi bạn đều có 3 khớp (joints) chính: với tay là vai (shoulders), khuỷu tay (elbows), cổ tay (wrists), với chân là hông (hips), đầu gối (knees), và cổ chân (ankles);\nđể kiểm soát tốt tay hoặc chân đối phương bạn cần kiểm soát được ít nhất 2/3 khớp; trên trục tay, chân, khớp tạo ra hiệu ứng đòn bẩy tốt nhất là khớp ngoài cùng; \nlúc bạn tấn công, đòn bẩy cho phép bạn tiến tới các vị trí kiểm soát và submission, suy cho cùng thì trong mọi submission hợp lệ, bạn luôn phải tận dụng, kiểm soát được ít nhất một đòn bẩy của đối phương; \nkhi không ai có ưu thế, người đầu tiên kiểm soát được lever của đối phương sẽ kiểm soát trận đấu; \n\nđây chính là anatomic hierarchyđọc thêm về 3-joint ruleđọc thêm về isolate a single targetđọc thêm về grips dictate position\nwedges (nêm/chêm):\n\ncó hai kiểu nêm:\n\nnêm chặn (blocking wedges): cố định một phần cơ thể đối thủ;\nnêm cạy (prying wedge): giúp mở khung (frame) của đối thủ.\n\nmột số ví dụ:\n\ncho tay vào hông (hip) của đối phương để pass guard, không cho đối thủ re-guard chính là ví dụ của nêm chặn;\nđòn cắt gối để pass guard (knee cut pass), khi bạn cố đưa gối vào mở hai chân đối thủ, chính là ví dụ của nêm cạy.\n\nnêm và đòn bẩy thường đi với nhau, bạn cần dùng nêm để sử dụng đòn bẩy hiệu quả hơn: nêm một đầu, đòn bẩy một đầu giúp cố định tốt hơn. gồm hai schemes:\n\nhai đánh một: two-on-one (two limbs controlling one lever);\nbốn đánh một: four-on-one (four limbs controlling one lever).\n\n\nclamps (kẹp):\n\nkẹp thường được sử dụng khi bạn đã khóa (lock) được một phần cơ thể của đối thủ trong một vòng khép kín (closed circuit), siết chặt hai tay và hai chân để tạo ra một chuỗi động lực học. xem thêm kinetic chains\ncơ chế kiểm soát này như một chiếc neo làm chậm đối thủ, kết dính cơ thể bạn với họ.\n\nhooks (móc):\n\ncác móc được tạo ra từ phần cuối mỗi chi (ND: bàn chân, bàn tay). tuy nhiên người ta thường chỉ hook với phần mu bàn chân (instep ~ shoelace area), nó được gọi là butterfly hook. móc tạo ra các chuyển động, lực kéo từ vị trí dưới, yêu cầu bạn phải kiểm soát được không gian phía trong giữa bạn và đối thủ. xem thêm inside channel\nhook nhìn chung cũng yếu, bởi vì bạn kiểm soát từ bên trong (không phải bên ngoài và tạo một vòng khép kín). nó sẽ bị vô hiệu hóa nếu đối phương luồn tay thoát vị trí (pummels) hoặc thoát ly, gỡ khóa (disengages) ~ thường thì họ cũng không mất nhiều sức và thời gian cho chuyện này.\n\nposts (trụ):\n\npost (trụ) được dùng để duy trì base (cơ sở). mindset về giữ vững trụ cho phép sự linh hoạt và chuyển động hông (hip) khi ở vị trí dưới, hay chống bị quét, lật (sweep) khi ở vị trí trên/đứng.\nkhí sử dụng post ở vị trí dưới, điều quan trọng là trụ phải cùng phương với vector lực tới, nếu không post của bạn sẽ support cho trọng lượng của đối thủ. xem thên force vectors\n\n\n3. anatomic hierarchy\ncơ thể con người có 6 cơ quan vũ khí, trong đó core (gòm ngực - chest, bụng - midsection, hông - hips) là phần mạnh nhất.\n\ncore;\n2 legs;\n2 arms;\nhead.\n\nvới bạn, 6 bộ phận này là vũ khí chống lại đối thủ. ở chiều ngược lại - với họ, 6 bộ phận này là mục tiêu để họ tấn công, khống chế, và submits.\nhãy luôn nhớ rằng, bạn phải tận dụng tối đa phần cơ thể khỏe nhất của bạn. vì core là phần mạnh nhất, nếu khai thác được đó, xác xuất là rất cao bạn sử dụng được các kỹ thuật một cách hiệu quả. một sai lầm phổ biến của người mới là sử dụng quá mức năng lượng vào tay và chân. ngoài việc ưu tiên core, bạn cũng cần có tư duy sử dụng mọi bộ phận cơ thể nhiều nhất có thể. xem thêm overwhelming force\nngược lại, nên hạn chế tấn công vào core của đối thủ. sẽ dễ dàng hơn khi tấn công vào 4 góc phần tư của cơ thể (torso) họ - là 4 chi (limbs) và tất nhiên bao gồm hai bên hông và hai vai. kiểm soát levers tạo ra leverages, sẽ dễ phá vỡ được alignment của đối thủ hơn.\n\n\n4. type of guards\ncó hàng chục, thậm chí là hàng trăm các loại guards, chúng cũng đang được thêm mới mỗi ngày. do đó rất cần thiết để hiểu, phân loại các guards dựa trên đặc điểm cơ chế (mechanical characteristics) của chúng. chúng ta sẽ hiểu sâu hơn và dễ dàng áp dụng được hơn.\n\nhook-based;\n\nbạn theo và bám (track and check) chuyển động của đối thủ bằng cách “móc” (hooking) sử dụng tay (bàn tay) và chân (mu bàn chân). tuy chúng lỏng lẻo (shallow), và thường không có tác dụng cố định đối thủ (immobilizing), nhưng là cách rất tốt để cảm nhận vị trí cơ thể và phản ứng lại. hooks thường được dùng để nâng đối thủ lên một cách linh hoạt.\nví dụ:\n\nbutterfly guard;\ninstep/shin-to-shin guard.\n\n\nclamp-based;\n\nbạn sẽ trói (tethering) cơ thể của mình vào cơ thể đối thủ, ngăn cản (hindering) họ chuyển động. nhiều traditional guard là dựa vào việc kẹp, chúng bẫy, làm chậm đối thủ và đưa họ vào tình thế không thi đấu, triển khai được phương án. đối với người thực hiện, chúng cũng nguy hiểm khi cho phép đối thủ nâng bạn lên hoặc tạo áp lực bằng trọng lượng.\nví dụ:\n\nclosed guard;\nhalf guard.\n\n\nframe-based;\n\nsử dụng frame tạo thành từ các chi (limbs) để giữ cho đối thủ ở một khoảng cách an toàn. rất hiệu quả đối với các đối thủ aggressive hoặc khỏe hơn, cũng rất hiệu quả trong MMA và chiến đấu thực tế khi giảm hiểu quả các đòn đánh/đấm của đối phương.\nví dụ:\n\nspider guard;\nknee shield guard.\n\n\nhybrid.\n\ncác guard hiện đại thường là sự kết hợp giữa các thể loại đề cập ở trên.\nví dụ:\n\nde la Riva guard và các biến thể;\nX guard và các biến thể.\n\n\n\n5. breaking mechanics\ncó 4 kiểu khóa gãy (breaks):\n\nlinear locks: như kneebars hoặc armbars;\nrotational locks: như heel hooks hoặc Kimuras;\ncompression locks: như calf slicer;\nhybrid locks: kết hợp các kiểu hình trên.\n\ntương tự như choke, nên kết hợp các cơ chế khóa gãy bằng cách sử dụng hybrid locks. ví dụ: khi thực hiện kneebar ~ linear, nên sử dụng thêm rotation để nó thêm hiệu quả. xem thêm chokes\ncác bước để khóa gãy đối phương:\n\nisolate a lever: cô lập một chi, như chân hoặc tay;\nprevent predictable defenses: dự đoán các cách phòng thủ của đối thủ và đưa ra biện pháp phòng ngừa, cũng nên tập cải thiện alignment của bản thân trước khi phá vỡ alignment của họ;\nmaximize leverage:\n\n\ntriệt để cố định khớp mà bạn có ý định tấn công, sử dụng 1 cái nêm cho khớp đó và khống chế các khớp lân cận.\ncác khớp lân cận nếu còn có thể di chuyển sẽ giảm bớt áp lực và làm suy yếu đòn tấn công của bạn.\n\nđảm bảo là grip là chắc chắn và kéo dài “cánh tay đòn” nhất có thể (maximizing leverage), bạn phải từ từ đào bới “digging” để ra được vị trí đó.\ntrước khi siết lực, hãy đảm bảo mình đang có vị trí tốt nhất. sau khi siết cần hạn chế chuyển động.\ncác chuyển động của hông nên được sử dụng sau cùng;\n\n\napply overwhelming force.\n\nchỉ khi bạn cô lập được levers, ngăn chặn các rủi ro bị phòng ngự, tối ưu cánh tay đòn bạn mới nên bung sức.\nkhi bung sức, cần sử dụng cả core và sức mạnh từ nhiều chi nhất có thể, hãy nhớ chỉ sau khi tận dụng tối đa các ưu thế kĩ thuật (technical leverages), bạn mới bung sức. xem thêm overwhelming force\n\n\n6. choking mechanics\ncó năm loại siết nghẹt:\n\nair chokes: tạo áp lực lên khí quản (trachea/windpipe) để ngăn cản đối phương hô hấp, thở. nó cũng có thể được hiệu chỉnh để tạo ra phản xạ hầu / phản xạ họng (gax reflex),  tạo ra hiệu ứng như blood choke;\nblood chokes:\n\nxem thêm pharyngeal reflex\ntạo áp lực lên động mạnh cảnh (carotid arteries) ở hai bên cổ, ngặn chặn máu lưu thông lên não và làm mất ý thức đối thủ. vì ảnh hưởng rất nhanh, blood chokes thường được coi là hiệu quả hơn air chokes, đa số các đòn chokes trong Jiu-jitsu thuộc loại này.\nmục tiêu của mọi blood choke đều là tìm cách siết chặn động mạnh cảnh, dùng một cái nêm (wedge) để đẩy từ sau đầu (ND: trong trường hợp của RNC là cẳng tay) vào choke.\nhãy hình dung RNC như một tam giác bao quanh đầu đối thủ (hai khuỷu tay và 1 vai), trong đó một góc tam giác - khuỷu tay ở vị trí dưới cằm (chin). công việc của bạn là tăng áp lực lên mỗi “cạnh” của tam giác đó đồng thời ghìm cái góc tam giác đó xuống;\n\n\ncranks: giữ và tạo một lực xoắn/ ngẫu lực (torque) tới cổ (neck) và cột sống (spine), ví dụ đẩy hàm (jaw) vào phía trong sọ, hoặc vặn cổ bằng cách sử dụng hàm như một điểm móc. bạn không nhất thiết phải luồn được tay vào dưới cằm (chin) để thực hiện neck crank. crank thường tạo ra các chấn thương nghiêm trọng và thường hạn chế trong luyện tập, nó cũng bị cấm trong một số giải đấu;\ncompression chokes: ép phổi làm đối phương khó chịu vì không đưa không khí vào phổi được, ví dụ như body triangle hay Bas Rutten body crush. nó cũng có thể làm gãy xương sườn và có thể không hợp lệ trong một số giải đấu;\nhybrid chokes: nên kết hợp nhiều kĩ thuật choke khác nhau, ví dụ blook choke kết hợp với một yếu tố crank.\n\n\n\n7. ratchet control\nkiểm soát đầu hoặc một chi sẽ đơn giản hơn rất nhiều nếu chúng ta xoay trục (rotation) một chút, và tốt nhất chính là để phương tạo lực của chúng ta vuông góc với phương chịu lực của đối thủ (ND: người dịch confused về bản dịch này). phương pháp này gọi là kiểm soát bánh cóc (ratchet control). có 2 loại:\n\ninternal ratchet control: khi bạn xoay một chi ngược lại với chuyển động tự nhiên của nó. ví dụ Kimuras (ngoặt tay đối thủ ra sau lưng) hay heel hooks (trục chân bị xoay với hướng gót ra ngoài);\nexternal ratchet control: khi bạn xoay một chi theo hướng chuyển động tự nhiên của nó, cho tới khi đối thủ không chịu đựng được. ví dụ Americana hay knee flares.\n\ndù external ratchet control rất hữu ích nhưng nó thường mang lại sự thoải mái cho đối thủ trong lúc bạn tấn công (khi chưa tới ngưỡng chịu đựng). nên internal ratchet control thường được lựa chọn hơn khi bắt đối thủ phải quay đi và để hở lưng, ví dụ Kimuras hay backside 50-50.\nmột số ví dụ:\n\nkhi dùng đòn armbar, hãy xoắn (twist) tay đối phương song song với việc duỗi ra (hyper-extending);\nkhi dùng đòn kneebar, hãy kéo gót chân gần về phía bạn để tăng lực xoay;\nkhi kéo đầu đối phương xuống, hãy kéo và vặn/xoắn cổ.\n\nchúng rất hữu dụng vì phá đi posture và structure của đối thủ. khi hệ xương của đối phương không còn ngay hàng thẳng lối, sức mạnh cơ bắp (musculature) cũng sẽ bị hạn chế đi nhiều.\n\n\n8. controlled breathing\nhãy chú ý đế nhịp thở của bạn, hãy duy trí một nhịp thở thoải mái (relaxed cadence) có chủ ý (deliberate).\nchúng ta thường nghĩ tâm trí (mind) sẽ dẫn dắt cơ thể (body), tuy nhiên trên thực tế chúng có mối quan hệ chặt chẽ và đôi khi cơ chế là ngược lại.  ví dụ khi bạn cươi, tâm lý của bạn sẽ vui vẻ hơn. tương tự, kiểm soát hơi thở sẽ giúp bạn thoải mái và tiết kiệm (conserve) năng lượng.ref Smile! It Could Make You Happier\nkiểm soát hơi thở là rất quan trọng khi bạn đối đầu với những người to hoặc nhiều kinh nghiệm hơn. bên cạnh việc bảo toàn được sức lực, nó còn giúp bạn bình tĩnh lại. sự bình tĩnh là tối quan trọng để ngăn sự hoàng loạn (panicking) khi bạn chịu thử thách thực sự.\nđối thủ có kinh nghiệm cũng sẽ cảm nhận (sensing) được sự hoảng loạn nếu có trong nhịp thở của bạn. thở không kiểm soát có thể báo hiệu sự hoảng loạn đó cho đối thủ, khiến anh ta bắt đầu tấn công chớp nhoáng (blitz attacking) hoặc tăng nhịp độ (tempo).\nnó cũng giúp tim phân phối lượng oxy tới cơ bắp đồng đều hơn, rất quan trọng trong việc giữ nhịp tim và tránh mệt mỏi quá mức.\n\n\n9. staying loose\nhãy luôn chú ý một cách có ý thức (conscious attention) tới việc thả lỏng cơ bắp (relaxed & fluid), chỉ gồng/căng (tense) chúng khi mọi thứ đã sẵn sàng. nhưng khi nào thì được gọi là “sẵn sàng”?:\n\nchiếm được một góc đánh ưu thế; \nphá vỡ, gián đoạn một cách có hiệu quả vị trí căn chính của đối phương; \nhoặc bạn đưa đối thủ vào một tình huống tiến thoái lưỡng nan. \n\nxem thêm dominant anglesxem thêm về alignmentxem thêm về dilemagồng cứng cơ bắp khi không cần thiết sẽ:\n\nlàm chúng mệt mỏi (fatigues);\nlàm lộ ý định của bạn;\ntay hoặc chân gồng quá cứng sẽ dễ dàng cho đối phương kiểm soát và khống chế.\n\nnó cũng là hiện tượng dễ hiểu khi cơ chế phản ứng của con người thường là sẽ gồng lại khi cảm thấy bị đe dọa, đó là lý do mà người mới tập thường gồng cứng khi roll. cần tập luyện và kỷ luật để có thể thả lỏng được trước áp lực - calm under fire!\nnếu đa số các kỹ năng / kỹ thuật mà bạn tập trung vào đều phụ thuộc vào sức mạnh cơ bắp, và bạn thường xuyên cảm thấy đuối, mệt mỏi, đó chính là dấu hiệu của việc bạn không thực hiện đúng kỹ năng / kỹ thật đó. cần đánh giá và luyện tập lại.\nchỉ có một tính huống duy nhất mà bạn nên bung sức mạnh cơ bắp, đó là lúc bạn đảm bảo được việc thực hiện thành công một kĩ thuật, đối phương không thể phòng thủ và bạn đã ở tư thế hiệu quả nhất có thể. lúc đó sức mạnh cơ bắp sẽ khuếch đại cao nhất lợi thế bạn đang có.\ntóm lại: chỉ bung cơ bắp khi đã ở trong tính huống hiệu quả nhất trong một đòn thế, còn lại, đừng.\n\n“Notice that the stiffest tree is most easily cracked, while the bamboo or willow survives by bending with the wind.” —- Bruce Lee\n\n\n\n10. limb coiling\ngiữ tứ chi của bạn cuộn sát vào cơ thể, sẵn sáng tấn công. cụ thể:\n\ngiữ cằm của bạn khom xuống, hai vai cũng gù, khom vào để hạn chế việc đối thủ tấn công vào cổ;\nkhuỷu tay luôn khép chặt vào hông, sườn (core, hips);\ngiữ đầu gối gập lại.\n\n\n\n11. elbow-knee connection\n\n\n12. solid frames\n\n\n13. kinetic chains\n\n\n14. body tethering\n\n\n15. inside channel control\n\n\n16. single vs. double level control\n\n\n17. overwhelming force\n\n\n18. surface area\n\n\n19. 3-joint rule\n\n\n20. center of gravity\n\n\n21. critical control points\n\n\n22. direct vs. proxy control\n\n\n23. force compression\n\n\n24. force vectors\n\n\n25. head position\n\n\n26. inertia\n\n\n27. leading edges\n\n\n28. mirrored stances\n\n\n29. momentum\n\n\n30. priit’s 45° rule\n\n\n31. redundancies\n\n\n32. seated vs. supine guards\n\n\n33. stress and recovery\nhttps://www.youtube.com/watch?v=XGZ-0ZoFqKk"
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html",
    "title": "BJJ Beginners Guide",
    "section": "",
    "text": "Đây là bài (lược) dịch từ beginners guide của Reddit community r/bjj. Link gốc bài viết: https://www.reddit.com/r/bjj/wiki/beginners-guide/. Một số thuật ngữ mình có thể sẽ để nguyên bản Tiếng Anh, hoặc để kèm trong ngoặc tròn (), chú thích (nếu có) sẽ được đặt trong dấu ngoặc tròn sau “ND”.\nMục đích đánh giá lại những gì mình đạt được sau 6 tháng theo đuổi BJJ."
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#tổng-quan",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#tổng-quan",
    "title": "BJJ Beginners Guide",
    "section": "Tổng quan",
    "text": "Tổng quan\nHãy lịch sử và tuân thủ các quy tắc. Bạn đang tham gia một cộng đồng đang cố gắng cải thiện kỹ năng của họ, chứ không phải chiến đấu để đứng đầu theo kiểu của bạn."
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#vệ-sinh-và-sức-khỏe",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#vệ-sinh-và-sức-khỏe",
    "title": "BJJ Beginners Guide",
    "section": "Vệ sinh và sức khỏe",
    "text": "Vệ sinh và sức khỏe\n\nĐừng đến phòng tập nếu bạn ốm hoặc có virus hoặc bệnh truyền nhiễm khác;\nMang dép hoặc giày ngoài thảm, đi chân đất trên thảm;\nGiặt, vệ sinh đồ tập sau mỗi buổi học;\nCắt gọn ngón chân / tay để tránh làm bị thương bạn tập;\nSử dụng xịt khử mùi, nước súc miệng nếu cần;\nTrước buổi tập nếu có mồ hôi hoặc mùi, hãy tắm trước;\nTắm càng sớm càng tốt sau buổi tập để tránh nhiễm trùng da;\nChú ý đến da, nếu có vấn đề hãy ngừng tập và gặp bác sĩ."
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#tập-luyện-kĩ-thuật-drilling",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#tập-luyện-kĩ-thuật-drilling",
    "title": "BJJ Beginners Guide",
    "section": "Tập luyện kĩ thuật (Drilling)",
    "text": "Tập luyện kĩ thuật (Drilling)\n\nChia đều thời gian của bạn và bạn tập;\nChỉ nên đưa ra lời khuyên khi bạn thực sự tự tin, nếu không thì nghe coach (ND: Biết thì thưa thốt, không biết dựa cột mà nghe);\nMục tiêu là tập luyện các động tác trong điều kiện không có áp lực từ đối thủ. Nên đừng tạo áp lực quá nhiều cho bạn tập, cũng không nên quá thoải mái như một người nộm;\nTập trung vào các chuyện động (movement) hơn là đạt được kết quả của động tác."
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#tập-luyện-đối-kháng-rolling",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#tập-luyện-đối-kháng-rolling",
    "title": "BJJ Beginners Guide",
    "section": "Tập luyện đối kháng (Rolling)",
    "text": "Tập luyện đối kháng (Rolling)\n\nBạn có quyền từ chối việc roll với bất cứ ai, trong bất cứ thời điểm nào, hoặc bất kì lí do nào. Họ có thể không thích hoặc cho rằng nó thiếu lịch sự, nhưng, bạn có quyền;\nNgười ta thường đập và cụng ta (hand slap and dump) để đánh dấu cho một ca roll bắt đầu;\nCảm ơn bạn tập sau mỗi lần roll, bất kể nó kết thúc như thế nào;\nMục tiêu của roll là học hỏi chứ không phải chiến thắng, để dành cái ham muốn đó cho các trận đánh thực sự;\nĐiều tệ nhất bạn có thể làm là gây ra chấn thương cho bạn tập;\nMột số thế submission có thể chuyển từ không đau cho tới tạo ra chấn thương nghiêm trọng nhất nhanh. Trong các trường hợp đó, hãy siết lực chậm lại để tạo thời gian cho bạn tập tap-out;\nKhi roll với một bạn tập nhỏ hơn mình nhiều, đừng cố gắng dùng trọng lượng để đè họ xuống. Mục tiêu là học hỏi, và 5 phút đè như vậy và chả ai học được gì cả. Hãy cố linh hoạt nhiều tư thế;\nCác va chạm tình cờ vào các vùng nhạy cảm là điều bình thường trong các môn vật. Cứ tiếp tục roll hoặc nói “Sorry” nếu bạn muốn, không nên giải thích dài dòng và nên tập trung vào tập luyện."
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#closed-guard",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#closed-guard",
    "title": "BJJ Beginners Guide",
    "section": "Closed Guard",
    "text": "Closed Guard\nClosed Guard là tư thế phố biến nhất cho người mới bắt đầu, hai chân bạn sẽ vòng qua hông (waist) đối thủ và dùng mắt cá chân khóa (ankles) lại. Lợi thế thuộc về người ở trên (bạn tập của bạn).\nTrong Jiu-jitsu, “guard” mang nghĩa tương tự như “chân”, tư thế này được gọi như vậy vì bạn dùng chân khóa kín đối thủ.\n\n\n\n\n\n\n\n\n\nClosed Guard"
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#open-guard",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#open-guard",
    "title": "BJJ Beginners Guide",
    "section": "Open Guard",
    "text": "Open Guard\nKhi người ở trên có thể thoát, gỡ được móc nối giữa hai mắt cá chân, sẽ hình thành nên tư thế Open Guard. Lúc này “guard” đã được “open”, tư thế này rất linh động và có nhiều biến thể / tên gọi cho từng vị trí cụ thể, như Spider Guard hay Lasso Guard. Nhìn chung tư thế này không có lợi cho cả hai người.\n\n\n\n\n\n\n\n\n\nOpen Guard"
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#side-control",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#side-control",
    "title": "BJJ Beginners Guide",
    "section": "Side Control",
    "text": "Side Control\nKhi một người có thể vòng qua chân của người phía dưới và kiểm soát thân mình (torso) thì người đó được xem là đã “passed the guard”, động tác này kết thúc bằng từ thế Side Control. Lợi thế thuộc về người đó - người ở trên.\n\n\n\n\n\n\n\n\n\nSide Control"
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#mount",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#mount",
    "title": "BJJ Beginners Guide",
    "section": "Mount",
    "text": "Mount\nNếu người ở trên có thể giành được nhiều quyền kiểm soát hơn, họ có thể ngồi lên phần bụng (hips) hoặc ngực (chest), tư thế này được gọi là Mount và nó cực kì có lợi cho người ở trên.\n\n\n\n\n\n\n\n\n\nMount"
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#back",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#back",
    "title": "BJJ Beginners Guide",
    "section": "Back",
    "text": "Back\nCon người giỏi chiến đấu với các mối đe dọa đến trước mặt mình hơn là từ đằng sau. Vì thế lấy lưng là một tư thế tốt cho việc tấn công, và bị lấy lưng sẽ tạo ra sự thất thế cực lớn cho việc phòng ngự.\n\n\n\n\n\n\n\n\n\nBack"
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#turtle",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#turtle",
    "title": "BJJ Beginners Guide",
    "section": "Turtle",
    "text": "Turtle\nTư thế con rùa mang lại một chút lợi thế cho người ở trên. Gọi là con rùa bởi vì tư thế của người ở dưới.\n\n\n\n\n\n\n\n\n\nTurtle"
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#half-guard",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#half-guard",
    "title": "BJJ Beginners Guide",
    "section": "Half Guard",
    "text": "Half Guard\nKhi một người có thể dùng chân khống chế được một chân (thay vì từ phần hông và hai chân) của đối thủ, từ phía dưới, ta gọi đó là Half Guard. Ai có lợi thế còn tùy thuộc vào từng trường hợp cụ thể\n\n\n\n\n\n\n\n\n\nHalf Guard"
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#chấp-nhận-thất-bại-nhất-là-khi-chỉ-mới-bắt-đầu",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#chấp-nhận-thất-bại-nhất-là-khi-chỉ-mới-bắt-đầu",
    "title": "BJJ Beginners Guide",
    "section": "Chấp nhận thất bại, nhất là khi chỉ mới bắt đầu",
    "text": "Chấp nhận thất bại, nhất là khi chỉ mới bắt đầu\n\nRất là bình thường nếu bạn cảm thấy mình tệ. Bạn đang đào sâu vào một lĩnh vực sâu và phức tạp, ngay cả đai đen cũng có lúc nhận thấy nhiều thứ họ chưa thể hiểu được;\nNếu bạn không có kinh nghiệp vật (grappling) trước đó, 12 tháng đầu tiên bạn sẽ học 108 các phong cách tap-out khác nhau. Vậy nên đừng đo lường mức độ tiến bộ của bạn bằng các chiến thắng, mà nên là cách bạn cải thiện việc phòng thủ;\nHãy khiêm tốn và học cách trân trọng từng sự tiến bộ, tập trung vào học hỏi thay vì kết quả cuối cùng. Tất cả những gì bạn có thể đạt được là một chút tiến bộ mỗi ngày, ai cũng thế, ngay cả khi bạn không chứng kiến."
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#việc-khai-triển-kĩ-thuật-khó-hơn-rất-nhiều-khi-roll",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#việc-khai-triển-kĩ-thuật-khó-hơn-rất-nhiều-khi-roll",
    "title": "BJJ Beginners Guide",
    "section": "Việc khai triển kĩ thuật khó hơn rất nhiều khi Roll",
    "text": "Việc khai triển kĩ thuật khó hơn rất nhiều khi Roll\n\nBạn có thể học được một kĩ thuật và thực hành nó một cách mượt mà khi đối thủ gần như không kháng cự khi tập luyện;\nNhưng khi vào live roll thì khác, khi đối thủ biết bạn muốn làm gì, bạn sẽ thậm chí không thế thực hiện được bước đầu tiên và mọi thứ đều đi chệch khỏi hướng. Hoặc, bạn cũng cảm thấy đầu óc bạn hoàn toàn trống rỗng (ND: hành động theo bản năng);\nĐiều này là hoàn toàn bình thường, học các bước cơ bản là khởi đầu cho hành trình dài để trở nên thành thạo. Hãy kiên nhẫn nghiên cứu xem mình đã làm chưa đúng ở bước nào, và tiếp tục thực hành."
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#làm-sao-tiến-bộ-một-cách-có-hiệu-quả",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#làm-sao-tiến-bộ-một-cách-có-hiệu-quả",
    "title": "BJJ Beginners Guide",
    "section": "Làm sao tiến bộ một cách có hiệu quả",
    "text": "Làm sao tiến bộ một cách có hiệu quả\n\nHãy đặt câu hỏi cho coach;\nHãy hỏi các tiền bối đi trước, nhớ rằng cần phải tôn trọng thời gian của họ;\nHãy xem các video hướng dẫn, tài nguyên rất phong phú trên Youtube. Cẩn thận với click-bait và video-brain - hãy xem có chọn lọc:\nClick-bait: nhờ thuật toán của Youtube mà các video với tiêu đề ‘learn this unstoppable sweep’ hoặc ‘never get submitted again ever’ sẽ xuất hiện rất nhiều. Những thứ này không chính xác, hãy học những thứ basic. Những thứ trông fancy sẽ không phù hợp với mức độ kĩ năng của bạn đâu;\nVideo-brain: rất dễ để xem hết hai ba chục video youtube về các kỹ thuật submissions, escapes. Nhưng não bộ bạn sẽ không thể hấp thu hết được chừng đấy các kỹ thuật một cách chi tiết. Nếu bạn muốn áp dụng trong rolling, bạn sẽ thấy đầu ong ong và nhức nhức khi cố nhớ về kỹ thuật nào đó đã xem trong video nào đó trong hàng chục cái video khác. Hãy đi sâu thay vì đi rộng (recommended to go deep rather than broad). Xem nhiều video cho cùng một kĩ thuật, hoặc xem một video lặp lại nhiều lần và cố gắng tiếp thu trước khi move on.\nYêu cầu các bạn tập open mat học một kĩ thuật cụ thể hơn là rolling. Rolling mang lại cho bạn kinh nghiệm, nhưng nó sẽ rất loãng. Nếu bạn tập một tư thế guard hoặc một kỹ thuật với nhiều bạn tập khác nhau trong một thời gian ngắn, bạn sẽ thấy sự cải thiện rõ rệt. Hãy bắt đầu với tư thế mà bạn chưa thành thạo và reset nếu một trong hai thoát được khỏi đó. Tất nhiên cũng nên hỏi bạn tập xem họ muốn tập kĩ thuật nào;\nSau mỗi buổi tập, hãy nghĩ về những thứ mình chưa làm được khi roll. Hãy chọn 1, 2 chủ đề trong đó để tự học qua video, buổi sau bạn có thể thực hành cho tới khi thấy có cải thiện. Cứ thế tiếp tục một chu trình như vậy."
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#gi",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#gi",
    "title": "BJJ Beginners Guide",
    "section": "Gi",
    "text": "Gi\nBạn có thể mua Gi online hoặc ở ngay phòng tập. Tuy nhiên mua online sẽ rẻ hơn. Có một số phòng tập sẽ yêu cầu mua Gi hoặc patch.\nHãy kiểm tra kĩ size chart, nhất là nên hỏi những bạn tập có cùng kích thước với mình. Cũng nên nhớ rằng size chart có thể khác nhau cho các brand khác nhau."
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#rashguard-for-no-gi",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#rashguard-for-no-gi",
    "title": "BJJ Beginners Guide",
    "section": "Rashguard (for No Gi)",
    "text": "Rashguard (for No Gi)\nTODO"
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#quần-áo-lót-underclothes",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#quần-áo-lót-underclothes",
    "title": "BJJ Beginners Guide",
    "section": "Quần áo lót (Underclothes)",
    "text": "Quần áo lót (Underclothes)\nĐối với Gi, bạn có thể mặc đồ lót dưới Gi (ND: vì áo Gi có thể bị kéo bung trong lúc tập luyện và bạn không muốn để ngực trần). Bạn có thể dễ dàng tìm các elastic/compression shirts và pants (spat) online. Hãy nhớ rằng chúng có thể bị nắm/kéo (ND: như là một phần của các kĩ thuật luyện tập với Gi), và có thể làm bạn nóng nực.\nTrong thi đấu, nam không được mặc gì dưới áo Gi (Gi top). Còn nữ thì phải mặc compression shirt, gymnast top, hoặc one-piece swimsuit."
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#bảo-vệ-hàm-mounthguard",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#bảo-vệ-hàm-mounthguard",
    "title": "BJJ Beginners Guide",
    "section": "Bảo vệ hàm (Mounthguard)",
    "text": "Bảo vệ hàm (Mounthguard)\nMặc dù không phải là môn về đấm / đá (striking), sẽ vẫn có khả năng đối thủ, bạn tập của bạn đánh trúng mặt hoặc siết vào hàm của bạn. Do đó, việc đeo bảo vệ hàm sẽ giúp hạn chế các tổn thương tới hàm, răng, môi, lưỡi - những tổn thương rất đau và khó hồi phục."
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#cup",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#cup",
    "title": "BJJ Beginners Guide",
    "section": "… Cup",
    "text": "… Cup\nMột số người dùng cup để vào vệ háng / bẹn (crotch), nhưng nó thực sự không cần thiết. Cup có thể làm tổn thương bạn tập, và bị cấm trong hầu hết các tour đấu."
  },
  {
    "objectID": "blog/2024-07-29-bjj-beginners-guide/index.html#bảo-vệ-đầu-headgear",
    "href": "blog/2024-07-29-bjj-beginners-guide/index.html#bảo-vệ-đầu-headgear",
    "title": "BJJ Beginners Guide",
    "section": "Bảo vệ đầu (Headgear)",
    "text": "Bảo vệ đầu (Headgear)\nBảo về đầu sẽ giúp bạn hạn chế các tổn thương tai - thường dẫn đến “tai súp lơ” (cauliflower ear). Tuy nhiên nó không phổ biến và việc dùng headgear bị cấm trong các tour đấu. Bạn chỉ nên dùng khi đã có các chấn thương và muốn chúng không nặng thêm cho tới khi lành lại."
  },
  {
    "objectID": "blog/2024-07-17-prefect-workshop/index.html",
    "href": "blog/2024-07-17-prefect-workshop/index.html",
    "title": "A Prefect Workshop",
    "section": "",
    "text": "🎞 Video source: Dr. Adam Hill - Empower Your Projects with Prefect’s Pipeline Magic | PyData London 2024\n⛏ Github repo: https://github.com/Cadarn/PyData-Prefect-Workshop"
  },
  {
    "objectID": "blog/2024-07-17-prefect-workshop/index.html#e01-my-first-flow",
    "href": "blog/2024-07-17-prefect-workshop/index.html#e01-my-first-flow",
    "title": "A Prefect Workshop",
    "section": "e01 my first flow",
    "text": "e01 my first flow\nthe basic component of prefect is task and flow. these are decorators to funtions we want to run. we can name and log easily:\n\n\ne01.py\n\n# Example of processing some data\nfrom prefect import task, flow\n\n\n@task(name=\"Addition operator\")\ndef add(a, b):\n    return a + b\n\n\n@task(name=\"Squaring operator\")\ndef square_num(num):\n    if True:\n        raise ValueError\n    return num ** 2\n\n\n@flow(log_prints=True, name=\"My first simple flow\")\ndef add_and_square(a: int = 2, b: int = 3):\n    add_result = add(a, b)\n    square_result = square_num(add_result)\n    print(f\"({a} + {b}) squared = {square_result}\")\n\n\nif __name__ == \"__main__\":\n    add_and_square(4, 8)"
  },
  {
    "objectID": "blog/2024-07-17-prefect-workshop/index.html#e02a-sentiment-pipeline-v1",
    "href": "blog/2024-07-17-prefect-workshop/index.html#e02a-sentiment-pipeline-v1",
    "title": "A Prefect Workshop",
    "section": "e02a sentiment pipeline v1",
    "text": "e02a sentiment pipeline v1"
  },
  {
    "objectID": "blog/2024-07-17-prefect-workshop/index.html#e02b-sentiment-pipeline-v2",
    "href": "blog/2024-07-17-prefect-workshop/index.html#e02b-sentiment-pipeline-v2",
    "title": "A Prefect Workshop",
    "section": "e02b sentiment pipeline v2",
    "text": "e02b sentiment pipeline v2"
  },
  {
    "objectID": "blog/2024-07-17-prefect-workshop/index.html#e03a-kafka-tweet-publisher",
    "href": "blog/2024-07-17-prefect-workshop/index.html#e03a-kafka-tweet-publisher",
    "title": "A Prefect Workshop",
    "section": "e03a kafka tweet publisher",
    "text": "e03a kafka tweet publisher"
  },
  {
    "objectID": "blog/2024-07-17-prefect-workshop/index.html#e03b-kafka-tweet-deployment",
    "href": "blog/2024-07-17-prefect-workshop/index.html#e03b-kafka-tweet-deployment",
    "title": "A Prefect Workshop",
    "section": "e03b kafka tweet deployment",
    "text": "e03b kafka tweet deployment"
  },
  {
    "objectID": "blog/2024-07-17-prefect-workshop/index.html#e04-sentiment-pipeline-v3",
    "href": "blog/2024-07-17-prefect-workshop/index.html#e04-sentiment-pipeline-v3",
    "title": "A Prefect Workshop",
    "section": "e04 sentiment pipeline v3",
    "text": "e04 sentiment pipeline v3"
  },
  {
    "objectID": "blog/2024-07-10-expert-in-python/index.html",
    "href": "blog/2024-07-10-expert-in-python/index.html",
    "title": "Recap: So you want to be a Python expert?",
    "section": "",
    "text": "🔥 source: James Powell: So you want to be a Python expert? | PyData Seattle 2017"
  },
  {
    "objectID": "blog/2024-07-10-expert-in-python/index.html#there-are-2-fundamental-ways-that-people-ussually-use-to-solve-this",
    "href": "blog/2024-07-10-expert-in-python/index.html#there-are-2-fundamental-ways-that-people-ussually-use-to-solve-this",
    "title": "Recap: So you want to be a Python expert?",
    "section": "There are 2 fundamental ways that people ussually use to solve this",
    "text": "There are 2 fundamental ways that people ussually use to solve this\n\n1. meta class\nfuther discussion\n\n\nShow the code\n# meta classes are merely derived from `type`\nclass BaseMeta(type):\n    def __new__(cls, name, bases, namespace):\n        print('BaseMeta.__new__', cls, name, bases, namespace)\n        if name != 'Base' and 'bar' not in namespace:\n            raise TypeError(\"Bad User class, 'bar' is not defined\")\n        return super().__new__(cls, name, bases, namespace)\n\n\n# then our Base should be derived from BaseMeta\nclass Base(metaclass=BaseMeta):\n    def foo(self):\n        return self.bar()\n\n\n\n\n\n\n\n\n\n\n\nmetaclass\n\n\n\n\n\nThat’s it! you can control, constraint the derived class from the base class in class hierachy.\n\n\n2. __init_subclass__\nWe can implement Base like this:\n\n\nShow the code\nclass Base():\n    def foo(self):\n        return self.bar()\n\n    def __init_subclass__(cls, **kwargs):\n        print(\"init subclass\", cls.__dict__, kwargs)\n        if 'bar' not in cls.__dict__:\n            raise TypeError(\"Bad sub class\")\n        super().__init_subclass__(**kwargs)"
  },
  {
    "objectID": "blog/2024-06-26-hopfield-network/index.html",
    "href": "blog/2024-06-26-hopfield-network/index.html",
    "title": "Random youtube video: How are memories stored in neural networks?",
    "section": "",
    "text": "This video came across in my youtube recommendation, the title is “How are memories stored in neural networks? | The Hopfield Network #SoME2” by Layerwise Lectures.\nThis channel is interestingly has only 1 video, bio:\n\nReality comes in layers - layers of abstraction. This channels tries to uncover them to gain insights on neuroscience and machine learning.\n\nYou’ll find how it interesting, below is my notes.\n\nWhere is your memory?\n\nThe Random Access Memory (RAM) of a nowaday typical computer is 8-32GB. That’s the part directly interact with CPU.\nAside from that, you may have hard disk with terabyte or so memory.\nHow about you ~ or your brain? Can we mearsure in bytes?\nBut the question should be asked in the first place is: Where is it?\nBecause memory in computer have physical location, to access a piece of data in RAM you have to know the the binary address associated with that location.\nFor the CPU, the matter comes down to just turning on the right wires to retreive the bits in desired location.\n\n\n\n\n\n\n\n\n\n\nWhere is our memory?\n\n\n\n\n\n\nImagine another kind of memory. Instead of specifying where of a memory, it’s a binary address, how about we could specify what, it’s content.\nOur memory, if we provide a incomplete version of memory, it’s just sort of autocompletes. With the right software, computer can also do this, but it’s not how computer memory work on the basic level.\nThe point of this video is to convince you that autocompleting memories, also know as associative memory, is kind of natural behaviour of networks of neurons.\nIt’ll become clear that it doesn’t really make sense to measure memory capacity in networks of neurons in the same way we measure computer memory.\n\n\n\n\n\n\n\n\n\n\nAssociative Networks\n\n\n\n\n\nThe biggest difference might be: computer memory have fixed location, but as we’ll see, the memories in an associative networks rather have - a time.\n\n\nComputer memory in a nutshell\n\nComputer memory is measured in bits, binary switches of ones and zeros. A string of eight such bits can represent anything from letters to integers.\nHow do I get to a memory once it’s saved, say in RAM? Because on its own it doesn’t do much.\nBroadly speaking, and glossing over tons of technical detail here, every piece of data in RAM is matched to a binary address.\n\n\n\n\n\n\n\n\n\n\nMemories are matched to addresses and that’s ultimately the only way to retrieve them\n\n\n\n\n\n\nEach piece of data is in a different physical location and can only be retrieved by knowing its address. How the reading and writing of memories is accomplished, is really the meat of programming and is another story.\n\n\n\nModeling neural networks\n\nThe aim of the video is to introduce how humain brain memory work, by introducing Hopefield Network.\nMore generally this lecture aim to be a modelling itself, a kind of the art of essential.\n\n\n\n\n\n\n\n\n\n\nHopfield Network\n\n\n\n\n\n\nThis is a picture depicted neuron,\nit integrates electrical signals from other neurons to determine its own activity and then,\nit broadcasts that activity back to the network.\n\n\n\n\n\n\n\n\n\n\nA Neuron\n\n\n\n\n\n\nMathematically the story goes something like this:\nthere’s electrical signals coming in from other neurons, which we will say are just some numbers.\nthen the synapses act as multipliers on these signals - another set of numbers,\nand then the activity of the neuron is based on the sum of the weighted inputs, and by “based on” I mean that it’s fine to apply some function after computing the sum.\n\n\n\n\n\n\n\n\n\n\nMathematical expression of Neuron\n\n\n\n\n\n\nIt gets interesting once we turn this into a network, connecting the outputs of neurons to the inputs of other neurons.\nThis is a special type of neural network. It’s a recurrent network, meaning that there are back and forth connections between the neurons.\n\n\n\n\n\n\n\n\n\n\nRecurrent Network\n\n\n\n\n\n\n\nMemories in dynamical systems\n\nWhat does this have to do with memory? Well it needs to be somewhere in here doesn’t it? Where?\nRemember the idea of an associative memory, which is the ability of a system to sort of “pattern-autocomplete”.\nLet’s try a definition of memory that’s slightly wider than maybe what we’re used to.\n\n\n\n\n\n\n\n\n\n\nA State in memory\n\n\n\n\n\n\nLet a memory system be a system that, after having been in a certain state, a configuration, it has the ability to return to that state later on.\nThe responsibility for this return in our computer is CPU.\nOur network seems different though. So let’s get creative. There’s other things in our everyday lives that fall under our definition of memory.\nFor example, if below bottle’s crushed, in other words its configuration changed, it can sometimes return to its earlier state, which in that sense could be said to have been memorized.\n\n\n\n\n\n\n\n\n\n\nA plastic bottle in our memory\n\n\n\n\n\n\nA neural network is a system with a pattern of activity that dynamically evolves. If, somehow, we could construct our network such that it would have some preferred state and would return to that state over time if it was perturbed, then that could reasonably be qualified as a memory.\n\n\n\n\n\n\n\n\n\n\nPerturbed states\n\n\n\n\n\n\nLet’s take a simple example of 8x8 neurons network. The way our system work is to decribe how the network change over time # Learning\n\n\n\nMemory capacity and conclusion\n\n\nResources\nThis was the submission to the Summer of Math Exposition 2022 (#SoME2). All credit to Layerwise Lectures.\n\nOrginal paper: Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8), 2554-2558.\nImage: Neuron image by Santiago Ramón y Cajal, The pyramidal neuron of the cerebral cortex, 1904 Ink and pencil on paper, 8 5/8 x 6 7/8 in."
  },
  {
    "objectID": "blog/2024-06-19-observable/index.html",
    "href": "blog/2024-06-19-observable/index.html",
    "title": "Observable Framework try-out",
    "section": "",
    "text": "I’ve heard that Observable released their Framework as open source in April 2024, and Quarto also supported Observable natively since version 1.4 (?), thus decided to give it a try. Could not agree more with their slogan:\nThis is my repo for the practice.\n🚀 Observable Framwork documentation.\nIt can be a set of coordinated interactive visualizations for “self-service” analysis, a live dashboard, or a point-in-time report that combines graphics and prose to present in-depth analysis.\nWhy use Framework for your data app?\nLet’s start!"
  },
  {
    "objectID": "blog/2024-06-19-observable/index.html#self-hosting",
    "href": "blog/2024-06-19-observable/index.html#self-hosting",
    "title": "Observable Framework try-out",
    "section": "Self hosting",
    "text": "Self hosting\nWe don’t have to deploy to Observable — Framework projects are simply static sites, so we can host them anywhere. With npm, we can run npm run build, which will generates the dist directory. We can then copy this directory to your static site server or preferred hosting service, like http-server, with command npx http-server dist.\nFor deployment, see the final section at the bottom of this article."
  },
  {
    "objectID": "blog/2024-06-10-python-bitcoin/index.html",
    "href": "blog/2024-06-10-python-bitcoin/index.html",
    "title": "Bitcoin tour in Python",
    "section": "",
    "text": "A from-scratch implementation of Bitcoin in Python under tuturial from Andrej Karpathy."
  },
  {
    "objectID": "blog/2024-06-10-python-bitcoin/index.html#part-1-summary-so-far",
    "href": "blog/2024-06-10-python-bitcoin/index.html#part-1-summary-so-far",
    "title": "Bitcoin tour in Python",
    "section": "Part 1: summary so far",
    "text": "Part 1: summary so far"
  },
  {
    "objectID": "blog/2024-06-05-cnac/index.html",
    "href": "blog/2024-06-05-cnac/index.html",
    "title": "Về quy ước đặt tên dữ liệu",
    "section": "",
    "text": "Note\n\n\n\nCác bạn cũng có thể xem thêm slide về dbtplyr của Emily trong posit::conf ở đây.\nĐây là keynote từ article Column Names as Contracts của Emily Riederer.\nCác sản phẩm phần mềm luôn tuân theo một quy ước nhất định (về hình ảnh, ký tự) nhằm để người dùng có thể ngay lập tức nắm bắt được cách thức sử dụng, giao tiếp với nó. Ví dụ, nút Home thường sẽ đưa người dùng về trang chủ của một mobile app, nút “blog” trên blog của tôi sẽ đưa các bạn tiếp cận danh mục bài viết, etc. Ở layer phía sau, các API được BE viết cần tuân thủ documented inputs & outputs, để các BE khác hay FE có thể sử dụng.\nTuy nhiên các bảng dữ liệu lại nằm ở một vùng màu xám không rõ ràng. Data service (từ producer i.e DE -&gt; user i.e DA, DS) đủ tĩnh để không được xem là normal service, nhưng cũng đủ thô để người ta ít chú ý đến nó về mặt giao diện. DE cứ thể lấy hết mọi dữ liệu trong hệ thống, DA nhìn dữ liệu và ngay lập tức giả định được nội dung của (cột) dữ liệu đó.\nCũng có một cách được nhiều tổ chức sử dụng như là Metadata managament, như:\nTuy nhiên giải pháp này là documentation oriented, producer và user đều phải tuân thủ nghiêm ngặt. Nó không trực diện khi DE đặt tên cột, hay DA đọc dữ liệu.\nÝ tưởng của Emily là chuẩn hóa các quy ước đặt tên dữ liệu bằng Controlled vocabulary. Minh họa bằng pointblank, collapsibleTree, và dplyr."
  },
  {
    "objectID": "blog/2024-06-05-cnac/index.html#level-1-kiểu-dữ-liệu-kiểu-đo-lường",
    "href": "blog/2024-06-05-cnac/index.html#level-1-kiểu-dữ-liệu-kiểu-đo-lường",
    "title": "Về quy ước đặt tên dữ liệu",
    "section": "Level 1: Kiểu dữ liệu, kiểu đo lường",
    "text": "Level 1: Kiểu dữ liệu, kiểu đo lường\n\nID: chỉ đối tượng, dạng numberic, tối ưu cho việc lưu trữ và thường làm khóa chính các bảng’\nIND: giá trị boolean 0/1, tôi thì thường sử dụng các từ như IS, FLAG;\nN: chỉ các dạng thông tin có thể đếm (count) được;\nAMT: chỉ các dạng thông tin có thể tổng, thường đề cập đến currency;\nVAL: cũng là số nhưng không thể tổng được (không có ý nghĩa), ví dụ như kinh độ, vĩ độ;\nDT: ngày;\nTM: giờ hoặc ngày giờ;\nCAT: category\n\nCũng tùy vào dữ liệu, có thể thêm các cụm từ nhất định đề đề cập đến thông tin dữ liệu đó mang lại, ví dụ ADD cho address."
  },
  {
    "objectID": "blog/2024-06-05-cnac/index.html#level-2-đối-tượng-chủ-đề",
    "href": "blog/2024-06-05-cnac/index.html#level-2-đối-tượng-chủ-đề",
    "title": "Về quy ước đặt tên dữ liệu",
    "section": "Level 2: Đối tượng, chủ đề",
    "text": "Level 2: Đối tượng, chủ đề\nMối quan tâm của người dùng dữ liệu cũng giống như nhà quản trị, xung quanh các đối tượng của business: USER, TRANSACTION, etc.\nĐối với ứng dụng đặt xe, các chủ thể có thể cần quan tâm là TRIP, TRAVELER, ACCOMODATION, TRANSPORTATION, etc."
  },
  {
    "objectID": "blog/2024-06-05-cnac/index.html#level-3-n-chi-tiết",
    "href": "blog/2024-06-05-cnac/index.html#level-3-n-chi-tiết",
    "title": "Về quy ước đặt tên dữ liệu",
    "section": "Level 3-n: Chi tiết",
    "text": "Level 3-n: Chi tiết\nHai tầng trên về cơ bản đã cho phép chúng ta mô tả được cột thông tin, chúng đều thể hiện được thuộc tính của chủ thể. Ví dụ:\n\nAMT_TRIP: giá trị deal của Trip;\nADD_DESTINATION: Điểm đến của trip.\n\nTuy nhiên tùy vào từng use case, chúng ta hoàn toàn có thể mở rộng được về cả hai phía. Ví dụ:\n\nVề phía trước: khi làm việc với nhiều layer dữ liệu, ta có thể dùng RAW, STAGING, DW, DL, DM, etc\n\nVề phía sau: DAY hoặc MONTH cho N_DURATION, VND hoặc USD cho currency AMT.\n\nMiễn là có một quy chuẩn, và nó đủ đơn giản để sử dụng."
  },
  {
    "objectID": "blog/2024-06-05-cnac/index.html#kết-hợp-chúng-lại",
    "href": "blog/2024-06-05-cnac/index.html#kết-hợp-chúng-lại",
    "title": "Về quy ước đặt tên dữ liệu",
    "section": "Kết hợp chúng lại",
    "text": "Kết hợp chúng lại\nTất nhiên ràng buộc việc đặt tên như vậy không chỉ để trang trí, nó sẽ giúp chúng ta tự động hóa quy trình kiểm soát chất lượng dữ liệu, .\nTôi sử dụng bộ dữ liệu này để thực hành (cột dữ liệu đã được đổi tên).\n\nhead(trip_data)\n\n# A tibble: 6 × 13\n  ID_TRIP CAT_DESTINATION_CITY DT_START  DT_END N_DURATION_DAY CAT_TRAVELER_NAME\n    &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;            \n1       1 London, UK           5/1/2023  5/8/2…              7 John Smith       \n2       2 Phuket, Thailand     6/15/2023 6/20/…              5 Jane Doe         \n3       3 Bali, Indonesia      7/1/2023  7/8/2…              7 David Lee        \n4       4 New York, USA        8/15/2023 8/29/…             14 Sarah Johnson    \n5       5 Tokyo, Japan         9/10/2023 9/17/…              7 Kim Nguyen       \n6       6 Paris, France        10/5/2023 10/10…              5 Michael Brown    \n# ℹ 7 more variables: VAL_AGE_TRAVELER &lt;dbl&gt;, IND_GENDER_TRAVELER &lt;chr&gt;,\n#   CAT_TRAVELER_NATIONALITY &lt;chr&gt;, CAT_ACCOMMODATION_TYPE &lt;chr&gt;,\n#   AMT_ACCOMMODATION_COST &lt;chr&gt;, CAT_TRANSPORTATION_TYPE &lt;chr&gt;,\n#   AMT_TRANSPORTATION_COST &lt;chr&gt;"
  },
  {
    "objectID": "blog/2024-06-05-cnac/index.html#generating-sql",
    "href": "blog/2024-06-05-cnac/index.html#generating-sql",
    "title": "Về quy ước đặt tên dữ liệu",
    "section": "Generating SQL",
    "text": "Generating SQL\nSử dụng R để generate SQL query\n\nlibrary(dbplyr)\n\n\nAttaching package: 'dbplyr'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    ident, sql\n\nlibrary(RSQLite)\ndf_mem &lt;- memdb_frame(trip_data, .name = \"example_table\")\n\ndf_mem %&gt;%\n  group_by(CAT_TRAVELER_NATIONALITY) %&gt;%\n  summarize_at(vars(starts_with(\"N_\")), mean, na.rm = TRUE) %&gt;%\n  show_query()\n\n&lt;SQL&gt;\nSELECT `CAT_TRAVELER_NATIONALITY`, AVG(`N_DURATION_DAY`) AS `N_DURATION_DAY`\nFROM `example_table`\nGROUP BY `CAT_TRAVELER_NATIONALITY`"
  },
  {
    "objectID": "blog/2024-06-05-cnac/index.html#r---base-data.table",
    "href": "blog/2024-06-05-cnac/index.html#r---base-data.table",
    "title": "Về quy ước đặt tên dữ liệu",
    "section": "R - base & data.table",
    "text": "R - base & data.table\nsử dụng base::grep để tìm tất cả các columns bắt đầu bằng AMT_\n\ncols_n &lt;- grep(\"^AMT_\", names(trip_data), value = TRUE)\nprint(cols_n)\n\n[1] \"AMT_ACCOMMODATION_COST\"  \"AMT_TRANSPORTATION_COST\"\n\n\nsử dụng vector để lưu các cột chúng ta muốn group by:\n\ncols_grp &lt;- c(\"CAT_TRAVELER_NATIONALITY\")\n\nchúng ta có thể dùng vector này trong stats::aggregate\nhoặc trong data.table"
  },
  {
    "objectID": "blog/2024-06-05-cnac/index.html#python-pandas",
    "href": "blog/2024-06-05-cnac/index.html#python-pandas",
    "title": "Về quy ước đặt tên dữ liệu",
    "section": "python pandas",
    "text": "python pandas\ntương tự trong python, sử dụng list comprehension:\n```{python}\n\nimport pandas as pd\ncols_n   = [vbl for vbl in trip_data.columns if vbl[0:2] == 'AMT_']\ncols_grp = [\"CAT_TRAVELER_NATIONALITY\"]\ntrip_data.groupby(cols_grp)[cols_n].sum()\n```"
  },
  {
    "objectID": "blog/2024-06-05-cnac/index.html#concept-map",
    "href": "blog/2024-06-05-cnac/index.html#concept-map",
    "title": "Về quy ước đặt tên dữ liệu",
    "section": "Concept Map",
    "text": "Concept Map\n\n\n\n\n\n%%{init: {'theme':'dark'}}%%\nflowchart LR\n  A(dataframe)-- has --&gt; B(columns)\n  A(dataframe)-- has --&gt; C(names)\n  B -- have --&gt; D(data types)\n  B -- have --&gt; E(units)\n  B -- have --&gt; F(meaning)\n  C -- should encode --&gt; D\n  C -- should encode --&gt; E\n  C -- should encode --&gt; F\n  C -- can support --&gt; G(validation)\n  C -- explained in --&gt; H(documentation)\n  G -- should check consistency of --&gt; C\n  G -- should check consistency of --&gt; H\n  H --&gt; I(data dictionary)\n  H --&gt; K(ERD)"
  },
  {
    "objectID": "blog/2024-06-05-cnac/index.html#new-package-dec-2020",
    "href": "blog/2024-06-05-cnac/index.html#new-package-dec-2020",
    "title": "Về quy ước đặt tên dữ liệu",
    "section": "New Package (Dec 2020)",
    "text": "New Package (Dec 2020)\nconvo là một thư viện released bởi Emily phục vụ cho việc quản lý controlled vocabulary:\n\nlibrary(convo)\n\n\n\n\nvocab.yaml\n\nlevel1:\n  ID:\n    desc: Unique identifier\n    valid:\n      - col_vals_not_null()\n      - col_is_numeric()\n      - col_vals_between(1000, 99999)\n  IND:\n    desc: Binary indicator\n    valid:\n      - col_is_numeric()\n      - col_vals_in_set(c(0,1))\n    rename:\n      - when: SUM\n        then: 'N'\n      - when: AVG\n        then: P\n  AMT:\n    desc: Non-negative, summable quantity\n    valid:\n      - col_is_numeric()\n      - col_vals_gte(0)\n  VAL:\n    desc: Value\n    valid:\n      - col_is_numeric()\n    rename:\n      - when: AVG\n        then: VALAV"
  },
  {
    "objectID": "blog/2024-06-05-cnac/index.html#new-package-april-2021",
    "href": "blog/2024-06-05-cnac/index.html#new-package-april-2021",
    "title": "Về quy ước đặt tên dữ liệu",
    "section": "New Package (April 2021)",
    "text": "New Package (April 2021)\ndbtplyr là một package port tính năng “select helpers” của dplyr sang dbt."
  },
  {
    "objectID": "blog/2024-05-22-swd-p2-improve-table/index.html",
    "href": "blog/2024-05-22-swd-p2-improve-table/index.html",
    "title": "Storytelling with Data: Let’s Practice, Improve this table!",
    "section": "",
    "text": "Lấy cảm hứng từ bài post của thầy Nguyễn Chí Dũng về việc giải các exercise thuộc Chương 2 cuốn sách Storytelling with Data: Let’s Practice của tác giả Cole Nussbaumer Knaflic sử dụng R, mình muốn cải thiện 1 bảng biểu nhưng sử dụng Python (trong cuốn sách gốc, tác giả sử dụng Excel để makeover các graph).\nĐây là một cuốn sách hay về chủ để visualization, dành cho những người sử dụng biểu đồ nhiều để communicate trong công việc. Cụm từ “storytelling” đang được sử dụng rất nhiều, thậm chí như là một buzzword, một keyword cần có trong CV của mỗi người làm data. Đối với mình, “storytelling” chỉ nên được sử dụng khi bạn đã có sẵn các kết quả phân tích, được thực hiện một cách khoa học, và bạn muốn truyền tải nó tới business unit/management một cách hiệu quả. “Storytelling” nên không bao giờ là động cơ để bạn thực hiện một phân tích (thứ thay vào đó nên là một câu hỏi), thiên kiến sẽ ảnh hưởng tới phương pháp thực hiện, và rất dễ nảy sinh ngụy biện khi kết luận. Hãy xem bài viết này của Keith McNulty và discussion trên Linked In.\n\n\n\n\n\n\n\n\n\nPhoto credit to swd"
  },
  {
    "objectID": "blog/2024-05-22-swd-p2-improve-table/index.html#import-the-data",
    "href": "blog/2024-05-22-swd-p2-improve-table/index.html#import-the-data",
    "title": "Storytelling with Data: Let’s Practice, Improve this table!",
    "section": "2.1. Import the data",
    "text": "2.1. Import the data\nOkay đầu tiên cần import và làm sạch dữ liệu. Bảng có sẵn trong file Exercise có vẻ dùng để báo cáo hơn là lưu trữ dữ liệu.\n\nimport pandas as pd\n\ndata = pd.read_excel(\n  \"2.1 EXERCISE.xlsx\",\n  sheet_name = \"EXERCISE 2.1\",\n  skiprows=6\n)\n\ndata = data.drop('Unnamed: 0', axis=1)\ndata = data.rename(columns={ \"Tier\": \"tier\"\n                    ,\"# of Accounts\": \"no_of_account\"\n                    ,\"% Accounts\": \"pct_of_account\"\n                    ,\"Revenue ($M)\": \"revenue\"\n                    ,\"% Revenue\": \"pct_of_revenue\"})"
  },
  {
    "objectID": "blog/2024-05-22-swd-p2-improve-table/index.html#data-manipulation",
    "href": "blog/2024-05-22-swd-p2-improve-table/index.html#data-manipulation",
    "title": "Storytelling with Data: Let’s Practice, Improve this table!",
    "section": "2.2. Data manipulation",
    "text": "2.2. Data manipulation\nYep giờ ta đã có 1 bảng data khá sạch, tuy nhiên lưu ý là bảng đang không thể hiện toàn bộ dữ liệu (total của pct_of_account chỉ là 81.86%). Ta cần tính toán thêm All other và Grand Total (lý do là mình chưa tìm thấy function hiển thị Grand Total với package đang sử dụng, ngay cả trong ví dụ này, GT cũng sử dụng data đã có sẵn Grand Total).\n\nall_other_pct_of_account = 1 - data[\"pct_of_account\"].sum()\nall_other_no_of_account = round(all_other_pct_of_account / data[\"pct_of_account\"][0] * data[\"no_of_account\"][0],0)\n\nall_other_pct_of_revenue = 1 - data[\"pct_of_revenue\"].sum()\nall_other_revenue = round(all_other_pct_of_revenue / data[\"pct_of_revenue\"][0] * data[\"revenue\"][0],3)\n\nall_other = pd.Series({ \"tier\": \"All other\",\n                        \"no_of_account\": all_other_no_of_account,\n                        \"pct_of_account\": all_other_pct_of_account,\n                        \"revenue\":all_other_revenue,\n                        \"pct_of_revenue\": all_other_pct_of_revenue})\n\ndata = pd.concat([data, all_other.to_frame().T], ignore_index=True)\n\ndata.loc[len(data.index)] = data.sum()\ndata.loc[data.index[-1], 'tier'] = 'Grand Total'\n\nShow the data:\n\nfrom great_tables import GT\n\n( \n  GT(data)\n  .fmt_number(columns= [\"no_of_account\",\"pct_of_account\",\"revenue\",\"pct_of_revenue\"],decimals=2))\n\n\n\n\n\n\n\n\ntier\nno_of_account\npct_of_account\nrevenue\npct_of_revenue\n\n\n\n\nA\n77.00\n0.07\n4.67\n0.25\n\n\nA+\n19.00\n0.02\n3.93\n0.21\n\n\nB\n338.00\n0.31\n5.98\n0.32\n\n\nC\n425.00\n0.39\n2.80\n0.15\n\n\nD\n24.00\n0.02\n0.37\n0.02\n\n\nAll other\n205.00\n0.19\n0.94\n0.05\n\n\nGrand Total\n1,088.00\n1.00\n18.70\n1.00"
  },
  {
    "objectID": "blog/2024-05-22-swd-p2-improve-table/index.html#visualization",
    "href": "blog/2024-05-22-swd-p2-improve-table/index.html#visualization",
    "title": "Storytelling with Data: Let’s Practice, Improve this table!",
    "section": "2.3. Visualization",
    "text": "2.3. Visualization\nĐược rồi, giờ ta có thể dùng great_tables để tiến hành visualize. Về cơ bản, chỉ cần khởi tạo một object GT() có thể mở rộng được, các element/hoặc format được định nghĩa về sau.\nDưới đây chúng ta trình bày một bảng đơn giản với tiêu đề, tên cột, cũng như định dạng lại dữ liệu.\n\n(\n  GT(data)\n  .tab_header(title=\"New client tier share\")\n  .cols_label(\n    tier = \"Tier\",\n    no_of_account = \"# of Accounts\",\n    pct_of_account = \"% Accounts\",\n    revenue = \"Revenue ($M)\",\n    pct_of_revenue = \"% Revenue\"\n  )\n  .fmt_number(columns=\"no_of_account\",decimals=0)\n  .fmt_percent(columns=[\"pct_of_account\", \"pct_of_revenue\"], decimals=2)\n  .fmt_currency(columns = \"revenue\", currency=\"USD\", decimals=2)\n  .tab_source_note(source_note=\"Figure 2.1b: Slightly improved table\")\n)\n\n\n\n\n\n\n\n\nNew client tier share\n\n\n\n\nTier\n# of Accounts\n% Accounts\nRevenue ($M)\n% Revenue\n\n\n\n\nA\n77\n7.08%\n$4.67\n25.00%\n\n\nA+\n19\n1.75%\n$3.93\n21.00%\n\n\nB\n338\n31.07%\n$5.98\n32.00%\n\n\nC\n425\n39.06%\n$2.80\n15.00%\n\n\nD\n24\n2.21%\n$0.37\n2.00%\n\n\nAll other\n205\n18.84%\n$0.94\n5.00%\n\n\nGrand Total\n1,088\n100.00%\n$18.70\n100.00%\n\n\n\nFigure 2.1b: Slightly improved table\n\n\n\n\n\n\n\n        \n\n\n\nTrông ổn (ít nhất là trình bày số liệu một cách toàn vẹn) so với bản gốc rồi!\nTiếp tục, ta dùng:\n\nhelper md() để định dạng lại tiêu đề;\nsử dụng .data_color() để tạo heatmapping;\nsử dụng tab_style() để format dòng Grand Total;\nsử dụng .opt_stylize() để theming.\n\n\nfrom great_tables import md, style, loc\n\n(\n  GT(data)\n  .tab_header(title=md(\"&lt;strong&gt;New client tier share&lt;/strong&gt;\"))\n  .opt_align_table_header(align=\"left\")\n  .cols_label(\n    tier = \"Tier\",\n    no_of_account = \"# of Accounts\",\n    pct_of_account = \"% Accounts\",\n    revenue = \"($M) Revenue\", # change the label\n    pct_of_revenue = \"% Revenue\"\n  )\n  .fmt_number(columns=\"no_of_account\",decimals=0)\n  .fmt_percent(columns=[\"pct_of_account\", \"pct_of_revenue\"], decimals=0) # changed decimals\n  .fmt_currency(columns = \"revenue\", currency=\"USD\", decimals=1)# changed decimals\n  .tab_source_note(source_note=\"Figure 2.1c: Table with heatmapping\")\n  .data_color(\n    columns=[\"pct_of_account\", \"pct_of_revenue\"],\n    palette=[\"#57A6A1\", \"#577B8D\", \"#344C64\", \"#240750\"],\n    domain=[0, 1],\n    na_color=\"lightgray\",\n    autocolor_text = False\n  )\n  .tab_style(\n        style=[\n          style.text(style=\"bolder\"),\n          style.fill(\"#EEEEEE\"),\n          style.borders(sides=[\"top\", \"bottom\"], weight='2px', color=\"grey\")\n          ],\n        locations=loc.body(rows=[6])\n  )\n  .opt_stylize(style=5 ,color='blue')\n)\n\n\n\n\n\n\n\n\nNew client tier share\n\n\n\n\nTier\n# of Accounts\n% Accounts\n($M) Revenue\n% Revenue\n\n\n\n\nA\n77\n7%\n$4.7\n25%\n\n\nA+\n19\n2%\n$3.9\n21%\n\n\nB\n338\n31%\n$6.0\n32%\n\n\nC\n425\n39%\n$2.8\n15%\n\n\nD\n24\n2%\n$0.4\n2%\n\n\nAll other\n205\n19%\n$0.9\n5%\n\n\nGrand Total\n1,088\n100%\n$18.7\n100%\n\n\n\nFigure 2.1c: Table with heatmapping\n\n\n\n\n\n\n\n        \n\n\n\nTiếp theo, chúng ta sẽ thử dùng fmt_nanoplot() để tạo một bar plots thay cho hai cột percentage.\nĐây là một tính năng trong giai đoạn thử nghiệm nên mình thấy chưa thực sử ổn định và tuân theo document mà nhà phát triển đề cập. Trong ví dụ dưới đây mình sử dụng data bar, loại bỏ format do hàng cuối cùng. Tuy nhiên, đồ thị vẫn include giá trị 100% ở hàng Grand Total để điều chỉnh bar scale.\nThêm nữa mình cần chuyển dataframe về polar dataframe, khi đó một column của dl.df là iterable.\n\nfrom great_tables import md, style, loc, nanoplot_options\nimport numpy as np\nimport polars as pl\n\ndata.at[6, \"pct_of_account\"] = np.nan\ndata.at[6, \"pct_of_revenue\"] = np.nan\n# so that the data scale looks better\n\npl_data = pl.from_pandas(data)\n\n(\n  GT(pl_data, rowname_col=\"tier\")\n  .tab_header(title=md(\"&lt;strong&gt;New client tier share&lt;/strong&gt;\"))\n  .opt_align_table_header(align=\"left\")\n  .tab_source_note(source_note=\"Figure 2.1d: Table with data bar\")\n  .cols_label(\n    tier = \"Tier\",\n    no_of_account = \"# of Accounts\",\n    pct_of_account = \"% Accounts\",\n    revenue = \"($M) Revenue\", # change the label\n    pct_of_revenue = \"% Revenue\"\n  )\n  .fmt_number(columns=\"no_of_account\",decimals=0)\n  .fmt_currency(columns = \"revenue\", currency=\"USD\", decimals=1)\n  .fmt_percent(columns=[\"pct_of_account\", \"pct_of_revenue\"], decimals=0) # changed decimals\n  .cols_width(\n    cases={\n        \"tier\": \"20%\",\n        \"no_of_account\": \"20%\",\n        \"pct_of_account\": \"20%\",\n        \"revenue\": \"20%\",\n        \"pct_of_revenue\": \"20%\"      \n    }\n    )\n  .fmt_nanoplot(\n    columns=\"pct_of_account\",\n    rows = [0,1,2,3,4,5],\n    plot_type=\"bar\",\n    plot_height = \"3em\",\n    autoscale=True,\n    options=nanoplot_options(\n      data_bar_stroke_color=\"#577B8D\",\n      data_bar_stroke_width=80,\n      data_bar_fill_color=\"#577B8D\",)\n    )\n  .fmt_nanoplot(\n    columns=\"pct_of_revenue\",\n    rows = [0,1,2,3,4,5],\n    plot_type=\"bar\",\n    plot_height = \"3em\",\n    autoscale=True,\n    options=nanoplot_options(\n      data_bar_stroke_color=\"#577B8D\",\n      data_bar_stroke_width=80,\n      data_bar_fill_color=\"#577B8D\",)\n  )\n  .tab_style(\n        style=[\n          style.text(style=\"bolder\"),\n          style.fill(\"#EEEEEE\"),\n          style.borders(sides=[\"top\", \"bottom\"], weight='2px', color=\"grey\")\n          ],\n        locations=loc.body(rows=[6])\n  )\n  .tab_style(\n        style=[\n          style.text(color=\"#EEEEEE\", size=0.5),\n          ],\n        locations=loc.body(rows=[6], columns=[\"pct_of_account\", \"pct_of_revenue\"])\n  )\n  .opt_stylize(style=3 ,color='blue')\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew client tier share\n\n\n\n\n\n# of Accounts\n% Accounts\n($M) Revenue\n% Revenue\n\n\n\n\nA\n77\n\n\n\n\n0.071\n\n\n$4.7\n\n\n\n\n0.25\n\n\n\n\nA+\n19\n\n\n\n\n0.017\n\n\n$3.9\n\n\n\n\n0.21\n\n\n\n\nB\n338\n\n\n\n\n0.31\n\n\n$6.0\n\n\n\n\n0.32\n\n\n\n\nC\n425\n\n\n\n\n0.39\n\n\n$2.8\n\n\n\n\n0.15\n\n\n\n\nD\n24\n\n\n\n\n0.022\n\n\n$0.4\n\n\n\n\n0.020\n\n\n\n\nAll other\n205\n\n\n\n\n0.19\n\n\n$0.9\n\n\n\n\n0.050\n\n\n\n\nGrand Total\n1,088\nNone\n$18.7\nNone\n\n\n\nFigure 2.1d: Table with data bar\n\n\n\n\n\n\n\n        \n\n\n\ngreat_tables vẫn chưa thực sự render ra được bảng biểu đúng như setting.\nBài thực hành đến đây là kết thúc! 🚀"
  },
  {
    "objectID": "blog/2024-04-19-qej-1/index.html",
    "href": "blog/2024-04-19-qej-1/index.html",
    "title": "Kinh tế lượng với Julia 1: Làm quen với Julia",
    "section": "",
    "text": "Với giả định mình đã quen thuộc với các concepts của một ngôn ngữ lập trình như: Biến (Variables), Các kiểu dữ liệu (như Arrays và Vectors), Vòng lặp (Loops), Điều hướng (Conditionals - if/else), mình sẽ thử thực hành một vài ví dụ nhỏ với Julia với mục đích tìm hiểu các cú pháp và cấu trúc dữ liệu cơ bản.\nCode không hẳn đã được tối ưu, nhưng sẽ dần được tối ưu theo các ví dụ về sau!\n\n\n\n\nQuản lý Packages: Có hai cách để quản lý Packages và Versions (Phương pháp thứ 2 được recommend khi Julia cung cấp sẵn một phương pháp SOTA cho quản lý môi trường có khả năng tái tạo lại cao):\n\nadd pakages một cách trực tiếp (ví dụ Pkg.add(\"MyPackage\") hoặc ] add MyPackage):\n\n\nPhải cài đặt IJulia trên toàn cục để sử dụng trong mọi dự án;\n] là phím tắt để truy cập vào quản lý pakages.\n\n\nDùng các file Project.toml và Manifest.toml:\n\n\nNếu dùng notebook, các file này sẽ được tự đông xác định, nhưng cài đặt sẽ không tự động, cần ] instantiate;\nTệp Project có thể nằm ở folder mẹ của Notebook và Sourcecode.\n\n\n\nShow the code\nusing LinearAlgebra, Statistics, Plots, LaTeXStrings\n\n\nSử dụng các hàm: Có những function có sẵn trong Julia base, như randn()\n\n\nShow the code\nrandn()\n\n\n0.09985220377206018\n\n\nMột số hàm cần gọi các thư viện ngoài Base, ví dụ plot() của thư viện Plots:\n\n\nShow the code\nn = 100\nep = randn(n)\nplot(1:n,ep)\n\n    # \\ep\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: thay vì ep chúng ta có thể dùng các ký tự khoa học như epsilon bằng cách \\epsilon&lt;TAB&gt; .\nMảng: ep trong ví dụ trên chính là một mảng (array).\n\n\nShow the code\ntypeof(ep)\n\n\n\nVector{Float64} (alias for Array{Float64, 1})\n\n\n\nTrích xuất 5 thành tố đầu tiên của mảng\n\n\nShow the code\nep[1:5]\n# Julia là một ngôn ngữ 1-index (như MATLAB hay Fortran, chứ không phải 0 như Python hay C)\n\n\n5-element Vector{Float64}:\n 0.8575040389658757\n 0.005335428164054084\n 0.8330974941796573\n 0.673270529185323\n 0.7352249380457324\n\n\nĐể tìm kiếm trợ giúp và ví dụ cho một hàm, dùng ?:\n\n\nShow the code\n# help?&gt; typeof\n# search: typeof typejoin TypeError\n\n#   typeof(x)\n\n#   Get the concrete type of x.\n\n#   See also eltype.\n\n#   Examples\n#   ≡≡≡≡≡≡≡≡≡≡\n\n#   julia&gt; a = 1//2;\n  \n#   julia&gt; typeof(a)\n#   Rational{Int64}\n  \n#   julia&gt; M = [1 2; 3.5 4];\n  \n#   julia&gt; typeof(M)\n#   Matrix{Float64} (alias for Array{Float64, 2})\n\n\nVòng lặp: hãy thử viết lại quy trình tạo random trên bằng for loops.\n\n\nShow the code\n# poor style\nn = 100 \nep = zeros(n) # create an array with 0.0 as initial value\nfor i in 1:n # the index is looped for all 1:n, but none vector of those indices is created.\n    ep[i] = randn()\nend # indicates the end of a loop\n\n\nThe word in from the for loop can be replaced by either ∈ or =.\nCode trên thành công tạo ra các giá trị cho mảng ep, tuy nhiên mối quan hệ giữa i và ep được thể hiện không tường minh. Để cải thiện, hãy dùng eachindex:\n\n\nShow the code\n# better style\nn = 100\nep = zeros(n)\nfor i in eachindex(ep)\n    ep[i] = randn()\nend\n\n\nVòng lặp rất có hiệu quả về mặt bộ nhớ tuy nhiên lợi ích chính của nó là (1) thể hiện mã một cách rõ ràng hơn, ít mắc lỗi chính tả hơn, và (2) cho phép trình biên dịch linh hoạt tạo mã nhanh một cách sáng tạo.\nTrong Julia, chúng ta cũng có thể loop một array trực tiệp, như ví dụ dưới đây:\n\n\nShow the code\nep_sum = 0.0 # need to use 0.0 rather than 0\nm = 5\nfor ep_eval in ep[1:m]\n    ep_sum = ep_sum + ep_eval\nend\nep_mean = ep_sum / m\n\n\n-0.06355262620335231\n\n\nvới ep[1:m] trả về các giá trị của vector 1 đến m.\nJulia cũng có các operator/function có sẵn để kiểm tra các kết quả này.\n\n\nShow the code\nep_mean ≈ mean(ep[1:m])\nisapprox(ep_mean, mean(ep[1:m])) # equivalent\nep_mean ≈ sum(ep[1:m]) / m\n\n\ntrue\n\n\nGõ \\approx&lt;TAB&gt; .\n≈ và isapprox dùng để kiểm tra đẳng thức (equality), khác với giá trị (==), sử dụng cho số và các loại khác.\nĐịnh nghĩa hàm: để thực hành, tiếp tục sử dụng for để viết một hàm biểu diễn lại quá trình trên, nhưng thú vị hơn thì thử bình phương thay vì các trị số random đơn thuần xem sao.\n\n\nShow the code\n# poor style\nfunction generatedata(n)\n    ep = zeros(n)\n    for i in eachindex(ep)\n        ep[i] = (randn())^2 # squaring the result\n    end\n    return ep\nend\n\ndata = generatedata(10)\nplot(data)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHãy nhớ rằng randn() có thể trả về một vector, thử cải thiện xem.\n\n\nShow the code\n# still poor style\nfunction generatedata(n)\n    ep = randn(n) # use built in function\n\n    for i in eachindex(ep)\n        ep[i] = ep[i]^2 # squaring the result\n    end\n\n    return ep\nend\ndata = generatedata(5)\n\n\n5-element Vector{Float64}:\n 0.4020013293531669\n 3.2976715282134874\n 0.2429343751757354\n 0.5387018498871103\n 1.2211267571308764"
  },
  {
    "objectID": "blog/2024-04-19-qej-1/index.html#các-ví-dụ-mở-đầu",
    "href": "blog/2024-04-19-qej-1/index.html#các-ví-dụ-mở-đầu",
    "title": "Kinh tế lượng với Julia 1: Làm quen với Julia",
    "section": "",
    "text": "Quản lý Packages: Có hai cách để quản lý Packages và Versions (Phương pháp thứ 2 được recommend khi Julia cung cấp sẵn một phương pháp SOTA cho quản lý môi trường có khả năng tái tạo lại cao):\n\nadd pakages một cách trực tiếp (ví dụ Pkg.add(\"MyPackage\") hoặc ] add MyPackage):\n\n\nPhải cài đặt IJulia trên toàn cục để sử dụng trong mọi dự án;\n] là phím tắt để truy cập vào quản lý pakages.\n\n\nDùng các file Project.toml và Manifest.toml:\n\n\nNếu dùng notebook, các file này sẽ được tự đông xác định, nhưng cài đặt sẽ không tự động, cần ] instantiate;\nTệp Project có thể nằm ở folder mẹ của Notebook và Sourcecode.\n\n\n\nShow the code\nusing LinearAlgebra, Statistics, Plots, LaTeXStrings\n\n\nSử dụng các hàm: Có những function có sẵn trong Julia base, như randn()\n\n\nShow the code\nrandn()\n\n\n0.09985220377206018\n\n\nMột số hàm cần gọi các thư viện ngoài Base, ví dụ plot() của thư viện Plots:\n\n\nShow the code\nn = 100\nep = randn(n)\nplot(1:n,ep)\n\n    # \\ep\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: thay vì ep chúng ta có thể dùng các ký tự khoa học như epsilon bằng cách \\epsilon&lt;TAB&gt; .\nMảng: ep trong ví dụ trên chính là một mảng (array).\n\n\nShow the code\ntypeof(ep)\n\n\n\nVector{Float64} (alias for Array{Float64, 1})\n\n\n\nTrích xuất 5 thành tố đầu tiên của mảng\n\n\nShow the code\nep[1:5]\n# Julia là một ngôn ngữ 1-index (như MATLAB hay Fortran, chứ không phải 0 như Python hay C)\n\n\n5-element Vector{Float64}:\n 0.8575040389658757\n 0.005335428164054084\n 0.8330974941796573\n 0.673270529185323\n 0.7352249380457324\n\n\nĐể tìm kiếm trợ giúp và ví dụ cho một hàm, dùng ?:\n\n\nShow the code\n# help?&gt; typeof\n# search: typeof typejoin TypeError\n\n#   typeof(x)\n\n#   Get the concrete type of x.\n\n#   See also eltype.\n\n#   Examples\n#   ≡≡≡≡≡≡≡≡≡≡\n\n#   julia&gt; a = 1//2;\n  \n#   julia&gt; typeof(a)\n#   Rational{Int64}\n  \n#   julia&gt; M = [1 2; 3.5 4];\n  \n#   julia&gt; typeof(M)\n#   Matrix{Float64} (alias for Array{Float64, 2})\n\n\nVòng lặp: hãy thử viết lại quy trình tạo random trên bằng for loops.\n\n\nShow the code\n# poor style\nn = 100 \nep = zeros(n) # create an array with 0.0 as initial value\nfor i in 1:n # the index is looped for all 1:n, but none vector of those indices is created.\n    ep[i] = randn()\nend # indicates the end of a loop\n\n\nThe word in from the for loop can be replaced by either ∈ or =.\nCode trên thành công tạo ra các giá trị cho mảng ep, tuy nhiên mối quan hệ giữa i và ep được thể hiện không tường minh. Để cải thiện, hãy dùng eachindex:\n\n\nShow the code\n# better style\nn = 100\nep = zeros(n)\nfor i in eachindex(ep)\n    ep[i] = randn()\nend\n\n\nVòng lặp rất có hiệu quả về mặt bộ nhớ tuy nhiên lợi ích chính của nó là (1) thể hiện mã một cách rõ ràng hơn, ít mắc lỗi chính tả hơn, và (2) cho phép trình biên dịch linh hoạt tạo mã nhanh một cách sáng tạo.\nTrong Julia, chúng ta cũng có thể loop một array trực tiệp, như ví dụ dưới đây:\n\n\nShow the code\nep_sum = 0.0 # need to use 0.0 rather than 0\nm = 5\nfor ep_eval in ep[1:m]\n    ep_sum = ep_sum + ep_eval\nend\nep_mean = ep_sum / m\n\n\n-0.06355262620335231\n\n\nvới ep[1:m] trả về các giá trị của vector 1 đến m.\nJulia cũng có các operator/function có sẵn để kiểm tra các kết quả này.\n\n\nShow the code\nep_mean ≈ mean(ep[1:m])\nisapprox(ep_mean, mean(ep[1:m])) # equivalent\nep_mean ≈ sum(ep[1:m]) / m\n\n\ntrue\n\n\nGõ \\approx&lt;TAB&gt; .\n≈ và isapprox dùng để kiểm tra đẳng thức (equality), khác với giá trị (==), sử dụng cho số và các loại khác.\nĐịnh nghĩa hàm: để thực hành, tiếp tục sử dụng for để viết một hàm biểu diễn lại quá trình trên, nhưng thú vị hơn thì thử bình phương thay vì các trị số random đơn thuần xem sao.\n\n\nShow the code\n# poor style\nfunction generatedata(n)\n    ep = zeros(n)\n    for i in eachindex(ep)\n        ep[i] = (randn())^2 # squaring the result\n    end\n    return ep\nend\n\ndata = generatedata(10)\nplot(data)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHãy nhớ rằng randn() có thể trả về một vector, thử cải thiện xem.\n\n\nShow the code\n# still poor style\nfunction generatedata(n)\n    ep = randn(n) # use built in function\n\n    for i in eachindex(ep)\n        ep[i] = ep[i]^2 # squaring the result\n    end\n\n    return ep\nend\ndata = generatedata(5)\n\n\n5-element Vector{Float64}:\n 0.4020013293531669\n 3.2976715282134874\n 0.2429343751757354\n 0.5387018498871103\n 1.2211267571308764"
  },
  {
    "objectID": "blog/2024-04-19-qej-1/index.html#generic-programming",
    "href": "blog/2024-04-19-qej-1/index.html#generic-programming",
    "title": "Kinh tế lượng với Julia 1: Làm quen với Julia",
    "section": "2.1. Generic Programming",
    "text": "2.1. Generic Programming"
  },
  {
    "objectID": "blog/2024-04-19-qej-1/index.html#general-purpose-packages",
    "href": "blog/2024-04-19-qej-1/index.html#general-purpose-packages",
    "title": "Kinh tế lượng với Julia 1: Làm quen với Julia",
    "section": "2.2. General Purpose Packages",
    "text": "2.2. General Purpose Packages"
  },
  {
    "objectID": "blog/2024-04-19-qej-1/index.html#data-and-statistics-packages",
    "href": "blog/2024-04-19-qej-1/index.html#data-and-statistics-packages",
    "title": "Kinh tế lượng với Julia 1: Làm quen với Julia",
    "section": "2.3. Data and Statistics Packages",
    "text": "2.3. Data and Statistics Packages"
  },
  {
    "objectID": "blog/2024-04-19-qej-1/index.html#solvers-optimizers-and-automatic-differentiation",
    "href": "blog/2024-04-19-qej-1/index.html#solvers-optimizers-and-automatic-differentiation",
    "title": "Kinh tế lượng với Julia 1: Làm quen với Julia",
    "section": "2.4. Solvers, Optimizers, and Automatic Differentiation",
    "text": "2.4. Solvers, Optimizers, and Automatic Differentiation"
  },
  {
    "objectID": "blog/2024-01-19-guitar-tablature/index.html",
    "href": "blog/2024-01-19-guitar-tablature/index.html",
    "title": "Cách đọc Guitar Tab đơn giản",
    "section": "",
    "text": "TIL: tab là tableture"
  },
  {
    "objectID": "blog/2024-01-19-guitar-tablature/index.html#hammer-on---h",
    "href": "blog/2024-01-19-guitar-tablature/index.html#hammer-on---h",
    "title": "Cách đọc Guitar Tab đơn giản",
    "section": "2.1. Hammer On - h",
    "text": "2.1. Hammer On - h\nHướng dẫn: https://www.youtube.com/watch?v=OGgp0uUNG1I\nVí dụ:\ne--------0------------------------------------------------\nB--------1--1h3-------------------------------------------\nG---------------------------------------------------------\nD---------------------------------------------------------\nA---------------------------------------------------------\nE---------------------------------------------------------\nĐánh dây B phím 1 xong dùng ngón tay nhấn đè lên ngăn 3 để tạo hiệu ứng Hammer On"
  },
  {
    "objectID": "blog/2024-01-19-guitar-tablature/index.html#pull-offs---p",
    "href": "blog/2024-01-19-guitar-tablature/index.html#pull-offs---p",
    "title": "Cách đọc Guitar Tab đơn giản",
    "section": "2.2. Pull Offs - p",
    "text": "2.2. Pull Offs - p\nHưỡng dẫn: https://www.youtube.com/watch?v=cdo1H7FVI84\nNote: hammer on và pull offs ngược nhau nên sẽ có tab sử dụng ký hiệu chung là ^ (cười bằng 1 mắt 😜), ta phân biệt hai technique kia dựa trên số trước và sau dấu ^.\nVí dụ:\ne--------0-------5^3--------------------------------------\nB--------1--3p1-------------------------------------------\nG---------------------------------------------------------\nD---------------------------------------------------------\nA---------------------------------------------------------\nE---------------------------------------------------------\nSố trước nhỏ hơn số sau: 7^9 = 7h9 do đánh từ phím 7 lên thì phải là Hammer On\nSố trước lớn hơn số sau: 9^7 = 9p7 do đánh từ phím 9 xuống phải là Pull Offs\nTrong trường hợp này chúng ta hiểu 3p1 và 5^3 đều là Pull Offs."
  },
  {
    "objectID": "blog/2024-01-19-guitar-tablature/index.html#trills---tr",
    "href": "blog/2024-01-19-guitar-tablature/index.html#trills---tr",
    "title": "Cách đọc Guitar Tab đơn giản",
    "section": "2.3. Trills - tr",
    "text": "2.3. Trills - tr\nKý hiệu tr phía trên hai con số, biểu thị Trills\nVí dụ:\n             tr~~~~    \ne------------7--(9)------------------------------------------\nB------7---------------------------------------------\nG---7-----7-------------------------------------------------\nD---------------------------------------------------------\nA---------------------------------------------------------\nE---------------------------------------------------------\nĐánh ngăn 7 xong sau đó Hammer On nhanh lên ngăn 9 rồi Pull Offs ngay ra. \nNói chung Hammer On sau đó Pull Offs thì gọi là Trills!"
  },
  {
    "objectID": "blog/2024-01-19-guitar-tablature/index.html#bend---b",
    "href": "blog/2024-01-19-guitar-tablature/index.html#bend---b",
    "title": "Cách đọc Guitar Tab đơn giản",
    "section": "2.4. Bend - b",
    "text": "2.4. Bend - b\n(hay còn gọi là nhéo dây)\nVí dụ:\ne-------------------5-----7b8------------------------------\nB---------7-----------------------------------------------\nG-----7-------7---------------------------------------------\nD---------------------------------------------------------\nA---------------------------------------------------------\nE---------------------------------------------------------\n7b8 nghĩa là bend ở ngăn 7 lên cao độ ngăn 8 trên dây E (Dây 1)."
  },
  {
    "objectID": "blog/2024-01-19-guitar-tablature/index.html#release---r",
    "href": "blog/2024-01-19-guitar-tablature/index.html#release---r",
    "title": "Cách đọc Guitar Tab đơn giản",
    "section": "2.5. Release - r",
    "text": "2.5. Release - r\n(nhả dây sau khi Bend)\ne------------------------7b8r7----------------------------\nB---------------------------------------------------------\nG---------------------------------------------------------\nD---------------------------------------------------------\nA---------------------------------------------------------\nE---------------------------------------------------------\n7b8r7 nghĩa là cũng bend lên cao độ ngăn 8 xong nhả về 7 bình thường trên dây E (Dây 1)."
  },
  {
    "objectID": "blog/2024-01-19-guitar-tablature/index.html#legato-slide---s-hoặc",
    "href": "blog/2024-01-19-guitar-tablature/index.html#legato-slide---s-hoặc",
    "title": "Cách đọc Guitar Tab đơn giản",
    "section": "2.6. Legato Slide - “s” hoặc “/”",
    "text": "2.6. Legato Slide - “s” hoặc “/”\n(vuốt dây Legato)\nHướng dẫn: https://www.youtube.com/watch?v=QgKk0eV-E8c\ne------------------------7s9------------------------------\nB--------------------------------------------9\\7----------\nG---------------------------------------------------------\nD---------------------------------------------------------\nA---------------------------------------------------------\nE---------------------------------------------------------\nTrong một số tab sẽ ký hiệu là \"/\" với trường hơp vuốt lên như 7/9 hoặc\"\\\" là vuốt về ví dụ như 9\\7\nLegato là đánh xong rồi vuốt"
  },
  {
    "objectID": "blog/2024-01-19-guitar-tablature/index.html#shift-slide---s",
    "href": "blog/2024-01-19-guitar-tablature/index.html#shift-slide---s",
    "title": "Cách đọc Guitar Tab đơn giản",
    "section": "2.7. Shift slide - S",
    "text": "2.7. Shift slide - S\nHướng dẫn (cũng như phân biệt s và S): https://www.youtube.com/watch?v=OWhziAenUeU\ne------------------------7S9------------------------------\nB--------------7-------------------------------------------\nG-----6----------------------------------------------------\nD-----6----------------------------------------------------\nA---------------------------------------------------------\nE---------------------------------------------------------\nVừa đánh vừa vuốt từ ngăn 7 lên ngăn 9"
  },
  {
    "objectID": "blog/2024-01-19-guitar-tablature/index.html#vibrato---v-hoặc",
    "href": "blog/2024-01-19-guitar-tablature/index.html#vibrato---v-hoặc",
    "title": "Cách đọc Guitar Tab đơn giản",
    "section": "2.8. Vibrato - “v” hoặc “~”",
    "text": "2.8. Vibrato - “v” hoặc “~”\nHưỡng dẫn: https://www.youtube.com/watch?v=MY9aTs1YpT4\ne------------------------5v-------------5~----------------\nB-------7--------------------------------------------------\nG--------------5-------------------------------------------\nD---------------------------------------------------------\nA---0------------------------------------------------------\nE---------------------------------------------------------\nKhi thấy ký hiệu \"v\" hoặc \"~\" thì bạn bend nốt đó lên xuống liên tục là vibrato, xem đường link phia trên để hiểu rõ hơn"
  },
  {
    "objectID": "blog/2024-01-19-guitar-tablature/index.html#mute---x",
    "href": "blog/2024-01-19-guitar-tablature/index.html#mute---x",
    "title": "Cách đọc Guitar Tab đơn giản",
    "section": "2.9. Mute - x",
    "text": "2.9. Mute - x\n(Bịt dây đánh câm tiếng)\nBịt dây lại đánh hoàn toàn không ra tiếng\ne----5---x--------------------------------------------------\nB----5---x--------------------------------------------------\nG----6---x--------------------------------------------------\nD----8---x--------------------------------------------------\nA----7---x--------------------------------------------------\nE----5---x--------------------------------------------------"
  },
  {
    "objectID": "blog/2024-01-19-guitar-tablature/index.html#palm-mute---pm",
    "href": "blog/2024-01-19-guitar-tablature/index.html#palm-mute---pm",
    "title": "Cách đọc Guitar Tab đơn giản",
    "section": "2.10. Palm Mute - PM",
    "text": "2.10. Palm Mute - PM\nVẫn đánh ra tiếng nhưng dùng tay phải đè lên phần ngựa để hạn chế độ rung của dây. Tiếng gọn hơn chứ không vang đanh như bình thường.\n                    PM---------- |\nE---------5---------3-----------------------------------\nB------------7------------------------------------------\nG-------------------------------------------------------\nD-------------------------------------------------------\nA------0--------0---------------------------------------\nE---0---------------------------------------------------"
  },
  {
    "objectID": "blog/2024-01-19-guitar-tablature/index.html#harmonic--",
    "href": "blog/2024-01-19-guitar-tablature/index.html#harmonic--",
    "title": "Cách đọc Guitar Tab đơn giản",
    "section": "2.11. Harmonic - <>",
    "text": "2.11. Harmonic - &lt;&gt;\nĐặt tay lên phím (không bấm, chỉ đặt hờ) gảy ở phía cuối dây đàn (nơi bình thường nếu gảy sẽ tạo ra tiếng đanh và sắc nhất), sẽ tạo ra tiếng harmonic.\nE------------------------&lt;7&gt;-------------&lt;12&gt;-------------\nB----------------------------------------&lt;12&gt;-------------\nG----------------------------------------&lt;12&gt;-------------\nD---------------------------------------------------------\nA---------------------------------------------------------\nE---------------------------------------------------------"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Hi, I’m Tuan 🕵️‍♂️",
    "section": "",
    "text": "😎 I am a Data Science Enthusiast and currently working as a Senior Data Analyst at Validus Vietnam.\n🎓 Growing up in a small village next to a rice field, I opted to chase equations instead of grasshoppers by diving into math at the age of 10 (though my current math skills might say otherwise 👀). I later graduated with a degree in Accounting & Auditing from the Foreign Trade University, HCMC Campus.\n📚 When I’m not doing data, I might be:\n\nlearning math/statistics/data stuff;\ndrinking coffee;\nlistening to Vietnamese rap/pop rock;\nreading sci-fi novels, history books;\nplaying pool (mostly ten-ball);\ntraining Brazilian jiu-jitsu;\nand occasionally traveling or hiking.\n\n\n\n\n“It has often been said that a person doesn’t really understand something until he teaches it to someone else. Actually a person doesn’t really understand something until he can teach it to a computer, i.e. express it as an algorithm.” 1\n\nYes, until I can explain the accrual accounting concept to my non-accountant friend or build a pure Python code without dependencies to run a logistic regression, I cannot feel confident that I understand these things.\nAlso inspired by Andrej Karpathy - a great teacher to me, who always repeats the famous quote “What I cannot create, I do not understand” by Richard Feynman, I create this website to capture my learning journey to understand how things work. Most of content is currently things I am learning, but yeah I’ll also write to share and teach things I know and to get feedback, docendo discimus!\nThis is some kind of Sandbox learning method and it’s depicted in the picture below:\n\n\n\nThe Sandbox Method for Self-Education, by Nat Eliason\n\n\nIf you share the same interest, do not hesitate to contact me. I am best reached via Facebook.\nThis blog will be written in Vietnamese/Tiếng Việt (80%), English (10%), and Mandarin Chinese/汉语 (10%). Attaching a comment tool for my website is a TODO item, I am considering utteranc, and BlueSky.\n\n \n  \n   \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Facebook\n  \n  \n    \n     X\n  \n  \n    \n     Email\n  \n  \n    \n     Résumé"
  },
  {
    "objectID": "about/index.html#footnotes",
    "href": "about/index.html#footnotes",
    "title": "Hi, I’m Tuan 🕵️‍♂️",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKnuth, Donald E. “Computer Science and Mathematics: How a New Discipline Presently Interacts with an Old One, and What We May Expect in the Future.” American Scientist, vol. 61, no. 6, 1973, pp. 707–13. JSTOR, http://www.jstor.org/stable/27844074. Accessed 7 June 2024.↩︎"
  },
  {
    "objectID": "blog/2023-12-24-a-first-lession-on-ggplot2/index.html",
    "href": "blog/2023-12-24-a-first-lession-on-ggplot2/index.html",
    "title": "A tutorial of ggplot2\n",
    "section": "",
    "text": "0. Preparation\n\nI need to install the following packages:\n\n# install CRAN packages\npkg_install =   c(\"ggplot2\", \"tibble\", \"tidyr\", \"forcats\", \"purrr\", \"prismatic\", \"corrr\", \n    \"cowplot\", \"ggforce\", \"ggrepel\", \"ggridges\", \"ggsci\", \"ggtext\", \"ggthemes\", \n    \"grid\", \"gridExtra\", \"patchwork\", \"rcartocolor\", \"scico\", \"showtext\", \n    \"shiny\", \"plotly\", \"highcharter\", \"echarts4r\")\ninstall.packages(pkg_install)\n\nIn which:\n\n\nggplot2: part of the tidyverse;\n\ntidyverse:\n\n\ntibble: mordern data frames;\n\ndplyr: data wrangling;\n\ntidyr: data cleaning;\n\nforcats: handling factors;\n\n\n\ncorrr: correlation matrices;\n\ncowplot: composing ggplots;\n\nggforce: sina plots and other cool stuffs;\n\nggrepel: nice text labeling;\n\nggridges: rigde plots;\n\nggsci: nice color palettes;\n\nggtext: advanced text rendering;\n\nggthemes: additional themes;\n\ngrid: creating graphical objects;\n\ngridExtra: additional functions for grid graphics;\n\npatchwork: multiple panel plots;\n\nprismatic: minipulating colors;\n\nrcartocolor: great color palettes;\n\nscico: perceptional uniform palettes;\n\nshowtext: custom fonts;\n\nshiny: interactive applications;\n\ncharter, echarts4r, ggiraph, highcharter, plotly: interactive visualization.\n\nI was facing the error of installing devtools:\n\n# install from GitHub since not on CRAN\ninstall.packages('devtools')\ndevtools::install_github(\"JohnCoene/charter\")\n\nI tried to update R to the latest version (commented the code as it would be run once):\n\nupdate.packages(repos='http://cran.rstudio.com/', ask=FALSE, checkBuilt=TRUE)\n\n–&gt; Not worked.\nOops I was should be using “‘devtools’” instead of “devtools”!!! Problem solved\n1. The Dataset\n\nI was using the dataset: “National Morbidity and Mortality Air Pollution Study (NMMAPS)”\nInstall the readr first:\n\ninstall.packages('readr')\n# install.packages(\"quarto\")\n\nImport data\nThe :: here call the namespace and can be used to access a function without loading the package.\n\nchic &lt;- readr::read_csv(\"https://cedricscherer.com/data/chicago-nmmaps-custom.csv\")\n\nRows: 1461 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): city, season, month\ndbl  (6): temp, o3, dewpoint, pm10, yday, year\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nview some data:\n\ntibble::glimpse(chic)\n\nRows: 1,461\nColumns: 10\n$ city     &lt;chr&gt; \"chic\", \"chic\", \"chic\", \"chic\", \"chic\", \"chic\", \"chic\", \"chic…\n$ date     &lt;date&gt; 1997-01-01, 1997-01-02, 1997-01-03, 1997-01-04, 1997-01-05, …\n$ temp     &lt;dbl&gt; 36.0, 45.0, 40.0, 51.5, 27.0, 17.0, 16.0, 19.0, 26.0, 16.0, 1…\n$ o3       &lt;dbl&gt; 5.659256, 5.525417, 6.288548, 7.537758, 20.760798, 14.940874,…\n$ dewpoint &lt;dbl&gt; 37.500, 47.250, 38.000, 45.500, 11.250, 5.750, 7.000, 17.750,…\n$ pm10     &lt;dbl&gt; 13.052268, 41.948600, 27.041751, 25.072573, 15.343121, 9.3646…\n$ season   &lt;chr&gt; \"Winter\", \"Winter\", \"Winter\", \"Winter\", \"Winter\", \"Winter\", \"…\n$ yday     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ month    &lt;chr&gt; \"Jan\", \"Jan\", \"Jan\", \"Jan\", \"Jan\", \"Jan\", \"Jan\", \"Jan\", \"Jan\"…\n$ year     &lt;dbl&gt; 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1…\n\n\n\nhead(chic,10)\n\n# A tibble: 10 × 10\n   city  date        temp    o3 dewpoint  pm10 season  yday month  year\n   &lt;chr&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n 1 chic  1997-01-01  36    5.66    37.5  13.1  Winter     1 Jan    1997\n 2 chic  1997-01-02  45    5.53    47.2  41.9  Winter     2 Jan    1997\n 3 chic  1997-01-03  40    6.29    38    27.0  Winter     3 Jan    1997\n 4 chic  1997-01-04  51.5  7.54    45.5  25.1  Winter     4 Jan    1997\n 5 chic  1997-01-05  27   20.8     11.2  15.3  Winter     5 Jan    1997\n 6 chic  1997-01-06  17   14.9      5.75  9.36 Winter     6 Jan    1997\n 7 chic  1997-01-07  16   11.9      7    20.2  Winter     7 Jan    1997\n 8 chic  1997-01-08  19    8.68    17.8  33.1  Winter     8 Jan    1997\n 9 chic  1997-01-09  26   13.4     24    12.1  Winter     9 Jan    1997\n10 chic  1997-01-10  16   10.4      5.38 24.8  Winter    10 Jan    1997\n\n\n3. The {ggplot2} Package\n\nA ggplot is built up from a few basic elements:\n\n\nData;\n\nGeometries geom_: the geometric shape (hình học) that will represent the data;\n\nAesthetics aes_: aesthetics (tính thẩm mỹ) of the geometric or statistical objects, such as postition, color, size, shape, and transparency;\n\nScales scale_: map between the data and the aesthetics dimensions (ánh xạ từ dữ liệu đến đồ thị), such as data range to plot width or factor values to colors;\n\nStatistical transformations stat_: statistical summaries (thống kê) of data, such as quantitles, fitted curves, and sums;\n\nCoordinate system coord_: the transformation used for mapping data coordinates into the plane of the data rectangles (hệ tọa độ);\n\nFacets facet_: the arrangement of the data into a grid of plots;\n\nVisual themes theme(): the overall visual defaults of a plot, such as background, grids, axes, default typeface, sizes and colors (tông).\n\n🚀 Không nhất thiết một phần tử được gọi, và chúng cũng có thể được gọi nhiều lần.\n4. A default ggplot\n\n\nLoad the package for ability to use the functionality:\n\nlibrary(ggplot2)\n# library(tidyverse) # can also be imported from the tidy-universe!\n\nA default ggplot needs three things that you have to specify: the data, aesthetics, and a geometry.\n\nstarting define a plot by using ggplot(data = df);\nif we want to plot (in most cases) 2 variables, we must add positional aesthetics aes(x = var1, y = var2);\n\n🚀 Data được đề cập bên ngoài aes(), trong khi đó biến/variables được đề cập bên trong aes().\nVí dụ:\n\n(g &lt;- ggplot(chic, aes(x = date, y = temp)))\n\n\n\n\n\n\n\nJust a blank panel, because ggplot2 does not know how we plot data ~ we still need to provide geometry.\n🚀 ggplot2 cho phép chúng ta lưu ggobject thành một biến, trong trường hợp này là g . Chúng ta có thể mở rộng** g bằng cách thêm cách layers về sau.**\n🚀 Bằng cách dùng dấu (), chúng ta có thể in ngay object được gán ra.\nMany different geometries to use (called geoms because each function usually starts with geom_). For e.g., if we want to plot a scatter plot.\n\ng + geom_point()\n\n\n\n\n\n\n\nalso a lineplot which our managers always like:\n\ng + geom_line()\n\n\n\n\n\n\n\ncool but the plot does not look optimal, we can also using mutiple layers of geometry, where the magic and fun start.\n\n# it's the same if we write g + geom_line() + geom_point() \ng + geom_point() + geom_line()\n\n\n\n\n\n\n\nChange properties of geometries\nTurn all points to large fire-red diamonds:\n\ng + geom_point(color = 'firebrick', shape = 'diamond', size = 2)\n\n\n\n\n\n\n\n🚀 ggplot2 hiểu khi chúng ta dùng color, colour, cũng như col.\n🚀 Có thể dùng màu mặc định hoặc màu hex, hoặc thậm chí là màu RGB/RGBA với hàm rgb(). Ví dụ:\n\ng + geom_point(color = \"#b22222\", shape = \"diamond\", size = 2)\n\n\n\n\n\n\ng + geom_point(color = rgb(178, 34, 34, maxColorValue = 255), shape = \"diamond\", size = 2)\n\n\n\n\n\n\n\nReplacing the default ggplot2 theme\nCalling eg theme_bw() using theme_set(), all following plots will have same blank’n’white theme.\n\ntheme_set(theme_bw())\n\ng + geom_point(color = 'firebrick')\n\n\n\n\n\n\n\n🚀 theme() is also a useful function to modify all kinds of theme elements (texts, rectangles, and lines).\n5. Axes\n\nChange Axis Titles\nUse labs() to assign character string for each lable.\n\nggplot(chic, aes(x = date, y = temp)) +\n    geom_point(color = 'firebrick') +\n    labs(x = 'Year', y = 'Temperature (°F)')\n\n\n\n\n\n\n\nCan also using xlab() and ylab():\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  xlab(\"Year\") +\n  ylab(\"Temperature (°F)\")\n\n\n\n\n\n\n\nNot only the degree symbol before F, but also the supper script:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Year\", y = expression(paste(\"Temperature (\", degree ~ F, \")\"^\"(Hey, why should we use metric units?!)\")))\n\n\n\n\n\n\n\nIncrease space between Axis and Axis Titles.\nOverwrite the default element_text() within the theme() call:\n\nggplot(chic, aes(x = date, y = temp)) +\n    geom_point(color = 'firebrick') +\n    labs(x = 'Year', y = 'Temperature (°F)') +\n    theme(axis.title.x = element_text(vjust = 0, size = 30),\n         axis.title.y = element_text(vjust = 2, size = 30))\n\n\n\n\n\n\n\nvjust refer to vertical alignment. We can also change the distance by specifying the margin of both text elements.\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Year\", y = \"Temperature (°F)\") +\n  theme(axis.title.x = element_text(margin = margin(t = 10), size = 15),\n        axis.title.y = element_text(margin = margin(r = 10), size = 15))\n\n\n\n\n\n\n\nr and t in the margin are top and right. Margin has 4 arguments: margin(t, r, b, l). A good way to remember the order of the margin sides is “t-r-ou-b-l-e”.\nChange Aesthetics of the Axis Titles\nAgain, we use theme() function and modify the axis.tile and/or the subordinated elements axis.tile.x and axis.tile.y . Within element_text() we can modify the default of size, color, and face.\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Year\", y = \"Temperature (°F)\") +\n  theme(axis.title = element_text(size = 15, color = \"firebrick\",\n                                  face = \"italic\"))\n\n\n\n\n\n\n\nthe face argument can be used to make the font bold, italic, or even bold.italic.\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Year\", y = \"Temperature (°F)\") +\n  theme(axis.title.x = element_text(color = \"sienna\", size = 15, face = 'bold'),\n        axis.title.y = element_text(color = \"orangered\", size = 15, face = 'bold.italic'))\n\n\n\n\n\n\n\nYou could also use a combination of axis.title and axis.title.y, since axis.title.x inherits the values from axis.title. Eg:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Year\", y = \"Temperature (°F)\") +\n  theme(axis.title = element_text(color = \"sienna\", size = 15),\n        axis.title.y = element_text(color = \"orangered\", size = 15))\n\n\n\n\n\n\n\nOne can modify some properties for both axis titles and other only for one or properties for each on its own:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Year\", y = \"Temperature (°F)\") +\n  theme(axis.title = element_text(color = \"sienna\", size = 15, face = \"bold\"),\n        axis.title.y = element_text(face = \"bold.italic\"))\n\n\n\n\n\n\n\nChange Aesthetics of Axis Text\nSimilar to the title, we can change the appearance of the axis text (number indeed) by using axis.text and/or the subordinated elements axis.text.x and axis.text.y.\n\nggplot(chic, aes(x = date, y = temp)) +\n    geom_point(color = 'firebrick') +\n    labs(x= \"Year\", y = expression(paste(\"Temperature(\",degree ~ F, \")\"))) +\n    theme(axis.text = element_text(color = \"dodgerblue\", size = 13),\n         axis.text.x = element_text(face = 'italic'))\n\n\n\n\n\n\n\nRotate Axis Text\nSpecifying an angle help us to rotate any text elements. With hjust and vjust we can adjust the position of text afterwards horizontally (0 = left, 1 = right), and vertically (0 = top, 1 = bottom).\n\nggplot(chic, aes(x = date, y = temp)) +\n    geom_point(color = 'firebrick') +\n    labs(x= \"Year\", y = expression(paste(\"Temperature(\",degree ~ F, \")\"))) +\n    theme(axis.text.x = element_text(angle = 50, vjust = 1, hjust = 1, size = 13))\n\n\n\n\n\n\n# 50 means 50 degrees, not % =)))\n\nRemove Axis Text & Ticks\nRarely a reason to do this but this is how it works.\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Year\", y = \"Temperature (°F)\") +\n  theme(axis.ticks.y = element_blank(),\n        axis.text.y = element_blank())\n\n\n\n\n\n\n\n🚀If you want to get rid of a theme element, the element is always element_blank.\nRemove Axis Titles\nWe could again use element_blank() but it is way simpler to just remove the label in the labs() (or xlab()) call:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = NULL, y = \"\")\n\n\n\n\n\n\n\nNote that NULL removes the element (similarly to element_blank()) while empty quotes \"\" will keep the spacing for the axis title and simply print nothing.\nLimit Axis Range\nSome time you want to take a closer look at some range of you data. You can do this without subsetting your data:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Year\", y = \"Temperature (°F)\") +\n  ylim(c(0, 50))\n\nWarning: Removed 777 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nAlternatively you can use scale_y_continuous(limits = c(0, 50)) (subset) or coord_cartesian(ylim = c(0, 50)). The former removes all data points outside the range while the second adjusts the visible area (zooming) and is similar to ylim(c(0, 50)) (subset).\nForce Plot to start at the origin\n\nchic_high &lt;- dplyr::filter(chic, temp &gt; 25, o3 &gt; 20)\n\nggplot(chic_high, aes(x = temp, y = o3)) +\n  geom_point(color = \"darkcyan\") +\n  labs(x = \"Temperature higher than 25°F\",\n       y = \"Ozone higher than 20 ppb\") +\n  expand_limits(x = 0, y = 0)\n\n\n\n\n\n\n\nUsing coord_cartesian(xlim = c(0,NA), ylim = c(0,NA)) will lead to the same result.\n\nchic_high &lt;- dplyr::filter(chic, temp &gt; 25, o3 &gt; 20)\n\nggplot(chic_high, aes(x = temp, y = o3)) +\n  geom_point(color = \"darkcyan\") +\n  labs(x = \"Temperature higher than 25°F\",\n       y = \"Ozone higher than 20 ppb\") +\n  coord_cartesian(xlim = c(0, NA), ylim = c(0, NA))\n\n\n\n\n\n\n\nBut we can also force it to literally start at the origin!\n\nggplot(chic_high, aes(x = temp, y = o3)) +\n  geom_point(color = \"darkcyan\") +\n  labs(x = \"Temperature higher than 25°F\",\n       y = \"Ozone higher than 20 ppb\") +\n  expand_limits(x = 0, y = 0) +\n  coord_cartesian(expand = FALSE, clip = \"off\")\n\n\n\n\n\n\n\n🚀 The argument clip = \"off\" in any coordinate system, always starting with coord_*, allows to draw outside of the panel area. Call it here to make sure that the tick marks at c(0, 0) are not cut.\nAxes with Same Scaling\nUse coord_equal() with default ratio = 1 to ensure the units are equally scaled on the x-axis and on the y-axis. We can set the aspect ratio of a plot with coord_fixed() or coord_equal(). Both use aspect = 1 (1:1) as a default.\n\nggplot(chic, aes(x = temp, y = temp + rnorm(nrow(chic), sd = 20))) +\n  geom_point(color = \"sienna\") +\n  labs(x = \"Temperature (°F)\", y = \"Temperature (°F) + random noise\") +\n  xlim(c(0, 100)) + ylim(c(0, 150)) +\n  coord_fixed()\n\nWarning: Removed 59 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nRatios higher than one make units on the y axis longer than units on the x-axis, and vice versa:\n\nggplot(chic, aes(x = temp, y = temp + rnorm(nrow(chic), sd = 20))) +\n  geom_point(color = \"sienna\") +\n  labs(x = \"Temperature (°F)\", y = \"Temperature (°F) + random noise\") +\n  xlim(c(0, 100)) + ylim(c(0, 150)) +\n  coord_fixed(ratio = 1/5)\n\nWarning: Removed 65 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nUse a Function to Alter Labels\nIn case you want to format (eg adding % sign) without change the data.\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Year\", y = NULL) +\n  scale_y_continuous(label = function(x) {return(paste(x, \"Degrees Fahrenheit\"))})\n\n\n\n\n\n\n\n6. Titles\n\nAdd a Title\nWe can add a title via ggtitle() function.\n\nggplot(chic, aes(x = date, y = temp)) +\n    geom_point(color = \"firebrick\") +\n    labs(x = \"Year\", y = \"Temperature (°F)\") +\n    ggtitle(\"Temperatures in Chicago\")\n\n\n\n\n\n\n\nAlternatively, we can use labs(), where we can add serveral arguments ~ metadata of the plot (a sub-title, a caption, and a tag):\n\nggplot(chic, aes(x = date, y = temp)) +\n    geom_point(color = \"firebrick\") +\n    labs(x = \"Year\", y = \"Temperature (°F)\",\n        title = \"Temperatures in Chicago\",\n        subtitle = \"Seasonal pattern of daily temperatures from 1997 to 2001\",\n        caption = \"Data: NMMAPS\",\n        tag = \"Fig 1\")\n\n\n\n\n\n\n\nMake title bold & add a space at the baseline\n7. Legends\n8. Backgrounds & Grid Lines\n9. Margins\n10. Multi-panel Plots\n11. Colors\n12. Themes\n13. Lines\n14. Text\n15. Coordinates\n16. Chart Types\n17. Ribbons (AUC, CI, etc.)\n18. Smoothings\n19. Interactive Plots\n20. Remarks, Tipps & Resources\nReferences\n\nSource: https://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/"
  },
  {
    "objectID": "blog/2024-02-28-bjj/index.html",
    "href": "blog/2024-02-28-bjj/index.html",
    "title": "I am starting training Brazilian Jiu-jitsu",
    "section": "",
    "text": "It was 2018 when I first stumbled upon the term ‘BJJ’ when I joined a martial discussion group named “DOG Brothers Team Vietnam”. There was an active club named “RONIN BJJ Hanoi”, but I never tried to find out what BJJ was.\nAs an ectomorphic, I’m 1.72 meters tall and weigh only 52kg. My strength lies in endurance, not conditioning1. I can hike or walk for hours, even up to 20 kilometers. However, playing left-back in a 5-a-side football match for 15 minutes straight would be a challenge.\n1 Endurance is the ability to maintain a certain effort with minimal fatigue, while conditioning is the ability to repeat a certain effort with minimal fatigue. See this article.So upgrade my body composition and physical strength is one task in my this year to-do-list. I contacted the Dog Brother club and inquired about Kickboxing class, as I’m not a fan of gyms and don’t want to bulk up. Fortunately, I spoke with Head Coach Vu Dinh Tien, a BJJ expert and one of the pioneers of BJJ in Vietnam. After learning about my goals, Coach Tien recommended BJJ. He explained that BJJ focuses on isometric strength (holding positions) and grappling-specific strength (leverage and technique), which builds lean muscle and core strength, making it a better fit for my desire to achieve a toned physique.\nThis is also the short comparision between Brazillian Jiu-jitsu & Kickboxing that Gemini gave me:\n\n\n\n\n\n\n\n\nFeature\nBJJ\nKickboxing\n\n\n\n\nBody Form Focus\nLean muscle, core strength\nMuscle mass, sculpted physique\n\n\nWeight Impact\nCan aid weight loss\nExcellent for burning calories\n\n\nStrength Focus\nIsometric, grappling-specific\nExplosive power, speed\n\n\n\nHe convinced me so that’s how I started training BJJ! 🔥🔥🔥\n\n\n\n\n\n\n\n\n\nMy very first day training BJJ, me on the top-right corner ↗, photo credit to Sói Jiu-jitsu\n\n\n\n\n\nThe club boasts a friendly atmosphere, with many seasoned athletes who’ve been training for years. They often take the time to help newbies like me during training sessions. Sometimes we also have head coach Tien in the class, his insights on how we and our opponents react in different situations are fascinating ~ we follow principles, not movements. We practice to develop muscle memory for various moves, but more importantly, we learn the principles that guide us in developing strategies for specific games or situations. Just like football, BJJ emphasizes positional control, but the key lies in understanding the underlying principles to adapt strategies in any situation.\nAs of June, I’ve been training in BJJ for almost 4 months. Although I did not gain significant weight because my nutrition did not meet the needs of exercise, my physical strength has improved significantly. In rolling, I was able to do basic control and escape control movements, but still didn’t know how to control my explosive energy during a match. I often use my muscles too much instead of leverage my body weight at the beginning of the match, leading to a rapid decline in fitness and losing position at the end. This definitely takes more time to improve.\nThis article is just my experience so far, so please don’t take the headline too seriously. Let’s just say, I’m not quite at the point where I size up a bigger guy and say (even think): how interesting! lol. (see this video for how interesting a BJJ expert handles a bigger guy with nearly 30kg gap weight)\nHappy training!\nTuan"
  },
  {
    "objectID": "blog/2024-05-21-python-is-cool/index.html",
    "href": "blog/2024-05-21-python-is-cool/index.html",
    "title": "Python is cool ❄",
    "section": "",
    "text": "Đây là một bài thực hành theo một post bởi chị Chip Huyen về một số features đặc biệt của Python. Là một DA không sử dụng Python quá nhiều, chỉ một số feature dưới đây là mình đã từng nghe qua. Hi vọng bài thực hành sẽ giúp mình hứng thú với Python hơn!\n\n\n\n\n\n\n\n\n\nDong Nai Cultural Nature Reserve, a python lying along the stream waiting for the prey. Photo credit to PhucNguyenPhotos"
  },
  {
    "objectID": "blog/2024-05-21-python-is-cool/index.html#unpacking",
    "href": "blog/2024-05-21-python-is-cool/index.html#unpacking",
    "title": "Python is cool ❄",
    "section": "3.1. Unpacking",
    "text": "3.1. Unpacking\nChúng ta có thể “giải nén” một list như thế này:\n\nelems = [1,2,3,4]\na,b,c,d = elems\n\nprint(a,b,c,d)\n\n1 2 3 4\n\n\nCũng có thể làm như thế này:\n\na, *new_elems, d = elems # remember the char * for extended unpacking\n\nprint(a)\nprint(new_elems)\nprint(b)\n\n1\n[2, 3]\n2"
  },
  {
    "objectID": "blog/2024-05-21-python-is-cool/index.html#slicing",
    "href": "blog/2024-05-21-python-is-cool/index.html#slicing",
    "title": "Python is cool ❄",
    "section": "3.2. Slicing",
    "text": "3.2. Slicing\nChúng ta có thể reverse/đảo ngược một list với [::-1]\n\nelem = list(range(10))\nprint(elem)\n\n\nprint(elem[::-1])\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n\n\nCú pháp [x:y:z] có nghĩa là lấy mỗi phần tử thứ z từ index x tới index y. Khi z âm, tương đương với việc lấy theo thứ tự ngược lại. x để trống chỉ việc lấy từ phần tử đầu tiên, y để rỗng chỉ việc lấy tới phần tử cuối cùng.\n\nevens = elem[::2]\nprint(evens)\n\nreversed_evens = elem[2::-2]\nprint(reversed_evens)\n\n[0, 2, 4, 6, 8]\n[2, 0]\n\n\nCũng có thể dùng slicing để xóa các phần tử như thế này:\n\ndel elems[::2]\nprint(elems)\n\n[2, 4]"
  },
  {
    "objectID": "blog/2024-05-21-python-is-cool/index.html#insertion",
    "href": "blog/2024-05-21-python-is-cool/index.html#insertion",
    "title": "Python is cool ❄",
    "section": "3.3. Insertion",
    "text": "3.3. Insertion\nChúng ta có thể thay đổi giá trị một phần tử trong một list như sau:\n\nelems = list(range(10))\n\nelems[1] = 100\nprint(elems)\n\n[0, 100, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nCũng có thể thay thế một giá trị bằng nhiều giá trị:\n\nelems = list(range(10))\nelems[1:2] = [20, 30, 40]\nprint(elems)\n\n[0, 20, 30, 40, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nNếu chúng ta muốn thêm 3 giá trị 0.3, 0.4, 0.5 vào giữa phần tử thứ 0 và 1 của list này, thì:\n\nelems = list(range(10))\nelems[1:1] = [.3, .4, .5]\nprint(elems)\n\n[0, 0.3, 0.4, 0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
  },
  {
    "objectID": "blog/2024-05-21-python-is-cool/index.html#flattening",
    "href": "blog/2024-05-21-python-is-cool/index.html#flattening",
    "title": "Python is cool ❄",
    "section": "3.4. Flattening",
    "text": "3.4. Flattening\nChúng ta có thể flatten một list sử dung sum(0):\n\nlist_of_lists = [[1], [2, 3], [4, 5, 6]]\nsum(list_of_lists, [])\n\n[1, 2, 3, 4, 5, 6]\n\n\nCũng có thể sử dụng recursive lambda (another beauty of lambda)\n\nnested_lists = [[1, 2], [[3, 4], [5, 6], [[7, 8], [9, 10], [[11, [12, 13]]]]]]\nflatten = lambda x: [y for i in x for y in flatten(i)] if type(x) is list else [x]\n\nprint(flatten(nested_lists))\n\n# This line of code is from\n# https://github.com/sahands/python-by-example/blob/master/python-by-example.rst#flattening-lists\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]"
  },
  {
    "objectID": "blog/2024-05-21-python-is-cool/index.html#list-vs-generator",
    "href": "blog/2024-05-21-python-is-cool/index.html#list-vs-generator",
    "title": "Python is cool ❄",
    "section": "3.5. List vs Generator",
    "text": "3.5. List vs Generator\n🚀Generator là cái gì vậy? Trích bài viết:\n\nTừ generator được sử dụng cho cả hàm (hàm generator là hàm đã nói ở trên) và kết quả mà hàm đó sinh ra (đối tượng được hàm generator sinh ra cũng được gọi là generator). Vì vậy đôi khi việc này gây khó hiểu một chút. Hãy xem ví dụ về việc tạo n-grams từ một danh sách tokens dưới đây để hiểu sự khác biệt giữa list và generator:\n\n\ntokens = ['i', 'want', 'to', 'go', 'to', 'school']\n\ndef ngrams(tokens, n):\n    length = len(tokens)\n    grams = []\n    for i in range(length - n + 1):\n        grams.append(tokens[i:i+n])\n    return grams\n\nprint(ngrams(tokens, 3))\n\n[['i', 'want', 'to'], ['want', 'to', 'go'], ['to', 'go', 'to'], ['go', 'to', 'school']]\n\n\nTrong ví dụ này, chúng ta phải lưu toàn bộ n-grams một lúc. Nếu có m tokens, memory requirement là O(nm) - sẽ là vấn đề nếu m lớn. Thay vào đó, chúng ta có thể sử dụng generator để tạo n-grams tiếp theo khi được yêu cầu. Đây gọi là lazy evaluation. Chúng ta có thể tạo một hàm ngrams trả về một generator sử dụng keyword yield, lúc này memory requirement là O(n+m).\n\ndef ngrams(tokens, n):\n    length = len(tokens)\n    for i in range(length - n + 1):\n        yield tokens[i:i+n]\n\nngrams_generator = ngrams(tokens, 3)\nprint(ngrams_generator)\n\nfor ngram in ngrams_generator:\n    print(ngram)\n\n&lt;generator object ngrams at 0x00000239753BDB70&gt;\n['i', 'want', 'to']\n['want', 'to', 'go']\n['to', 'go', 'to']\n['go', 'to', 'school']\n\n\nMột cách khác để tạo n-grams là slice để lấy các sub-list [0, 1, 2, ...,-n], [1, 2, 3, ...,-n+1], [2, 3, 4, ...,-n+2],… [n-1, n, ...,-1], sau đó zip chúng lại:\n\ndef ngrams(tokens, n):\n    length = len(tokens)\n    slices = (tokens[i:length-n+i+1] for i in range(n))\n    return zip(*slices)\n\nngrams_generator = ngrams(tokens, 3)\nprint(ngrams_generator)\n\n\nfor ngram in ngrams_generator:\n    print(ngram)\n\n&lt;zip object at 0x00000239753EE840&gt;\n('i', 'want', 'to')\n('want', 'to', 'go')\n('to', 'go', 'to')\n('go', 'to', 'school')\n\n\nLưu ý chúng ta sử dụng (tokens[...] for i in range(n)), chứ không phải [tokens[...] for i in range(n)]. [] trả về một list, () trả về generator. # 4. Classes & magic methods\n\nTrong Python, magic methods được prefixed và suffixed bởi double underscore __ (aka dunder). Magic method được biết đến rộng rãi nhất là __init__.\n\nclass Node:\n    \"\"\" A struct to denote the node of a binary tree.\n    It contains a value and pointers to left and right children.\n    \"\"\"\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n\nIn ra object, tuy nhiên nhìn không tường minh lắm!\n\nroot = Node(5)\nprint(root) # &lt;__main__.Node object at 0x1069c4518&gt;\n\n&lt;__main__.Node object at 0x00000239753ED910&gt;\n\n\nChúng ta mong muốn khi in ra một Node, giá trị của nó cũng như giá trị của các Node con (nếu có) cũng sẽ được in ra. Chúng ta dùng __repr__:\n\nclass Node:\n    \"\"\" A struct to denote the node of a binary tree.\n    It contains a value and pointers to left and right children.\n    \"\"\"\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n\n    def __repr__(self):\n        strings = [f'value: {self.value}']\n        strings.append(f'left: {self.left.value}' if self.left else 'left: None')\n        strings.append(f'right: {self.right.value}' if self.right else 'right: None')\n        return ', '.join(strings)\n\nleft = Node(4)\nroot = Node(5, left)\nprint(root) # value: 5, left: 4, right: None\n\nvalue: 5, left: 4, right: None\n\n\nChúng ta cũng muốn hai Node có thể được so sánh được với nhau, vì thế tạo ra các magic method để implement các operator: == với __eq__, &gt; với __lt__, ‘&gt;=’ với __ge__:\n\nclass Node:\n    \"\"\" A struct to denote the node of a binary tree.\n    It contains a value and pointers to left and right children.\n    \"\"\"\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n\n    def __eq__(self, other):\n        return self.value == other.value\n\n    def __lt__(self, other):\n        return self.value &lt; other.value\n\n    def __ge__(self, other):\n        return self.value &gt;= other.value\n\n\nleft = Node(4)\nroot = Node(5, left)\nprint(left == root) # False\nprint(left &lt; root) # True\nprint(left &gt;= root) # False\n\nFalse\nTrue\nFalse\n\n\nXem ở đây, hoặc ở đây danh sách đầy đủ các magic method mà Python hỗ trợ.\nMột số magic method khác cần chú ý __len__, __str__, __iter__, and __slots__ (tham khảo đây)"
  },
  {
    "objectID": "blog/2024-06-06-excel-blambda/index.html",
    "href": "blog/2024-06-06-excel-blambda/index.html",
    "title": "TIL: Xử lý dynamic-ragged array trong Excel",
    "section": "",
    "text": "TIL: Hôm nay mình thử giải một challenge của bác Owen Price. Bác là Microsoft MVP, master ở khía cạnh xử lí, làm sạch, thậm chí là tối ưu việc xử lí đó trong Excel, trang blog của bác là FLEX YOUR DATA, mình học được rất nhiều cách sử dụng formulas, đặc biệt là các hàm mới từ LAMBDA(), MAP(), REDUCE() cho đến gần đây là Python in Excel, GROUPBY(), PIVOTBY, và các hàm REGEX_().\nNội dung challenge là, với một phần (1000 records) của bộ dữ liệu Citi Bike, trong đó chứa hai cột start_station_name, end_station_name, mỗi cột lại chứa thông tin các “station” ~ trạm, liệt kê 5 địa chỉ cùng số lần xuất hiện với số lượng lớn nhất!. Hình dưới là 10 dòng đầu tiên của dữ liệu.\n\n\n\n\n\n\n\n\n\n10 dòng đầu tiên của dữ liệu\n\n\n\n\n\nWell lúc đầu mình nghĩ nó cũng đơn giản 😂, thậm chí cũng không hiểu tại sao mỗi start hoặc end station lại có hai station, cách nhau bởi ký tự & (space before & after). Mình lập tức đưa ra lời giải như sau:\n=LET(\n    _d, \" & \",\n    _all_records, TOCOL(\n        BYROW(\n            BYROW(\n                _tbl_CityBike[[start_station_name]:[end_station_name]],\n                LAMBDA(x, TEXTJOIN(_d, TRUE, x))\n            ),\n        LAMBDA(x, TEXTSPLIT(x, _d))\n        )\n    ),\n    _stations, UNIQUE(_all_records),\n    _cnt, BYROW(\n        --(_stations = TRANSPOSE(_all_records)), \n        // \"--\" equals to \"+\" will convert the boolean TRUE/FALSE to value 1/0\n        LAMBDA(x, SUM(x))\n    ),\n    TAKE(SORTBY(HSTACK(_stations, _cnt), _cnt, -1), 5, )\n)\nKết quả:\n\n\n\n\n\n\n\n\n\nKết quả đầu tiên của mình\n\n\n\n\n\nĐối chiếu với các kết quả của những người tham gia khách thì nó sai! Ý tưởng của mình là đầu tiên với mỗi hàng, join chúng lại với & (_d), sau đó lại split chúng ra với cùng _d đó, từ đó có được danh sách occurence của tất cả các địa chỉ. Tuy nhiên mình nhận ra dữ liệu trả về cho _all_records bị thiếu sót. Lý do là các trạm (station) thường là các giao lộ, kết hợp từ hai địa chỉ với dấu &, tuy nhiên có một số trạm lại nằm trên một con đường. Và vì chất lượng dữ liệu, cũng có một số station là null. Hàm TOCOL() là hàm mảng xử lí các mảng con nhận được từ BYROW(...,LAMBDA(...,TEXTSPLIT())), các mảng con này có độ dài không đều, dẫn đến TOCOL() chỉ take record đầu tiên của mỗi mảng con, trả về _all_records chỉ gồm 1000 dòng dữ liệu.\n\n\n\n\n\n\n\n\n\nCác điểm dữ liệu đặc biệt"
  },
  {
    "objectID": "blog/2024-06-06-excel-blambda/index.html#edit-sau-khi-mày-mò-thêm-sửa-lỗi-cho-giải-pháp-trên-mình-back-to-basic-với-lời-giải-sau",
    "href": "blog/2024-06-06-excel-blambda/index.html#edit-sau-khi-mày-mò-thêm-sửa-lỗi-cho-giải-pháp-trên-mình-back-to-basic-với-lời-giải-sau",
    "title": "TIL: Xử lý dynamic-ragged array trong Excel",
    "section": "🚀 Edit: sau khi mày mò thêm sửa lỗi cho giải pháp trên mình back to basic với lời giải sau:",
    "text": "🚀 Edit: sau khi mày mò thêm sửa lỗi cho giải pháp trên mình back to basic với lời giải sau:\n=LET(\n    _data, TOCOL(_tbl_CityBike[[start_station_name]:[end_station_name]], 3),\n    _str, DROP(REDUCE(\"\", _data, LAMBDA(a, x, VSTACK(a, TEXTSPLIT(x, , \" & \")))), 1),\n    _u_str, UNIQUE(_str),\n    _cnt, BYROW(--(_u_str = TRANSPOSE(_str)), LAMBDA(x, SUM(x))),\n    TAKE(SORTBY(HSTACK(_u_str, _cnt), _cnt, -1), 5)\n)\nTất nhiên tốc độ xử lí sẽ chậm hơn BLAMBDAλ() - O_log(n), giải pháp này duyệt qua từng phần tử, nên BigO là O(n)\nThanks Peter & Owen for this great use of recursion presented in such a concise manner."
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "",
    "text": "This is not orginal content!\n\n\n\nThis is my study notes / codes along with Andrej Karpathy’s “Neural Networks: Zero to Hero” series.\nUpfront-note: There are also greate resources in Vietnamese for learning Backpropagation, for e.g.:"
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#intro-micrograd-overview---what-does-your-neural-network-training-look-like-under-the-hood",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#intro-micrograd-overview---what-does-your-neural-network-training-look-like-under-the-hood",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "intro & micrograd overview - what does your neural network training look like under the hood?",
    "text": "intro & micrograd overview - what does your neural network training look like under the hood?\nWhat is MicroGrad ❓: a tiny auto-grad (automatic gradient) engine, implement of back propagation ~ iteratively tune the weight of that neural net to minimize the loss function -&gt; improve the accuracy of the neural network. Backpropagation will be the mathematical core of any modern deep neutral network like, say pytorch, or jaxx.\nInstallation: pip install micrograd\nExample:\n\n\nShow the code\nfrom micrograd.engine import Value\n\n1a = Value(-4.0)\nb = Value(2.0)\nc = a + b\nd = a * b + b**3\nc += c + 1\nc += 1 + c + (-a)\nd += d * 2 + (b + a).relu()\nd += 3 * d + (b - a).relu()\ne = c - d\nf = e**2\ng = f / 2.0\n2g += 10.0 / f\n3print(f'{g.data:.4f}') # prints 24.7041, the outcome of this forward pass\ng.backward()\n4print(f'{a.grad:.4f}') # prints 138.8338, i.e. the numerical value of dg/da\nprint(f'{b.grad:.4f}') # prints 645.5773, i.e. the numerical value of dg/db\n\n\n\n1\n\nMicrograd allows you to build mathematical expressions, in this case a and b are inputs, wrapped in Value object with value equal to -4.0 and 2.0, respectively.\n\n2\n\na and b are transformed to c, d, e and eventually f, g. Mathematical operators are implemented, like +, *, **, even relu().\n\n3\n\nValue object contains data, and grad.\n\n4\n\nCall backpropagation() process.\n\n\n\n\n24.7041\n138.8338\n645.5773"
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#derivative-of-a-simple-function-with-one-input",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#derivative-of-a-simple-function-with-one-input",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "derivative of a simple function with one input",
    "text": "derivative of a simple function with one input\n❓What exactly is derivative❓\n\n\nShow the code\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nA simple quadratic function:\n\n\nShow the code\ndef f(x):\n    return 3*x**2 - 4*x + 5\n\n\n\n\nShow the code\nf(3.0)\n\n\n20.0\n\n\nInput also can be an array, we can plot it for visibility.\n\n\nShow the code\nxs = np.arange(-5, 5, 0.25)\nys = f(xs)\nplt.plot(xs, ys)\n\n\n\n\n\n\n\n\n\nIf we bump up a little value h of x, how f(x) will response?\n\n\nShow the code\n1h = 0.000000000001\nx = 3.0\n( f(x+h) - f(x) ) / h\n\n\n\n1\n\nChange the value of h from 0.0001 to be 0.00000...0001 -&gt; the slope value comes to 14 (at the value of 3.0 of x).\n\n\n\n\n14.001244608152774\n\n\nTry for x = -3.0, x = 5.0, we get different values of the slope, for x = 2/3, the slope is zero. Let’s get more complex."
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#derivative-of-a-function-with-multiple-inputs",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#derivative-of-a-function-with-multiple-inputs",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "derivative of a function with multiple inputs",
    "text": "derivative of a function with multiple inputs\n\n\nShow the code\na = 2.0\nb = -3.0\nc = 10.0\nd = a*b + c\nprint(d)\n\n\n4.0\n\n\nPut our bump-up element to this multi-variables function:\n\n\nShow the code\nh = 0.001\n\n# input\na = 2.0\nb = -3.0\nc = 10.0\n\nd1 = a*b + c\n1a += h\nd2 = a*b + c\n\nprint('d1: ', d1)\nprint('d2: ', d2)\n2print('slope: ', (d2 - d1)/h)\n\n\n\n1\n\nDo the same for b, c, we’ll get different slopes.\n\n2\n\nWe say given b = -3.0 and c = 10.0 are constants, the derivative of d at a = 2.0 is -3.0. The rate of which d will increase if we scale a!\n\n\n\n\nd1:  4.0\nd2:  3.997\nslope:  -3.0000000000001137"
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#starting-the-core-value-object-of-micrograd-and-its-visualization",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#starting-the-core-value-object-of-micrograd-and-its-visualization",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "starting the core Value object of micrograd and its visualization",
    "text": "starting the core Value object of micrograd and its visualization\nSo we now have some intuitive sense of what is derivative is telling you about the function. We now move to the Neural Networks, which would be massive mathematical expressions. We need some data structures that maintain these expressions, we first declare an object Value that holds data.\n\n\nShow the code\nclass Value:\n    def __init__(self, data, \n3                        _children=(),\n5                        _op = '',\n                        label = ''\n                        ): \n        self.data = data\n6        self.grad = 0.0\n7        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n        self.label =  label\n\n    def __repr__(self) -&gt; str: # a nicer looking for class attributes\n        return f\"Value(data={self.data})\"\n    \n4    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other) # turn other to Value object before calculation\n        out = Value(self.data + other.data, (self, other), '+')\n\n8        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other) # turn other to Value object before calculation\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n\n        def _backward():\n            self.grad += (1 - t**2) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        \n        # topo order for all children in the graph\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child) \n                topo.append(v)\n        build_topo(self)\n\n        # sequentially apply the chain rules\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\n\n\n3\n\nthe connective tissue of this expression. We want to keep these expression graphs, so we need to know and keep pointers about what values produce what other values. _children is by default a empty tuple.\n\n4\n\nas we added _children, we also need to point out the father - children relationship in method __add__ and __mul__ as well.\n\n5\n\nwe want to know the operation between father and child, _op is empty string by default, the value + and - will be added to the operator method respectively.\n\n6\n\ninitially assume that node has no impact to the output.\n\n7\n\nthis backward function basically do nothing at the initial.\n\n8\n\nimplement of backward pass for plus node, += represent the accumulate action (rather than overwrite it), assign the gradient behavior for each type of operation, call the _backward concurrently with function.\n\n\n\n\nSetting input and expression:\n\n\nShow the code\na = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(10.0, label='c')\n\n1a + b\n\n2a*b + c\n\n# d = a*b + c rewrite the expression\ne = a*b; e.label = 'e'\nd = e + c; d.label = 'd'\n# d\nf = Value(-2.0, label='f')\nL = d * f; L.label = 'L'\nL\n\n\n\n1\n\nwhich will internally call a.__add__(b)\n\n2\n\nwhich will internally call (a.__mul__(b)).__add__(c)\n\n\n\n\nValue(data=-8.0)\n\n\nSo that we can know the children:\n\n\nShow the code\nd._prev\n\n\n{Value(data=-6.0), Value(data=10.0)}\n\n\nWe can know the operations:\n\n\nShow the code\nd._op\n\n\n'+'\n\n\nNow we know exactly how each value came to be by word expression and from what other values. These will be quite abit larger, so we need a way to nicely visualize these expressions that we’re building out. Below are a-little-scary codes.\n\n\nShow the code\nimport os\n\n# Assuming the Graphviz bin directory path is 'C:/Program Files (x86)/Graphviz2.xx/bin'\nos.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz/bin' # add with the code, Gemini instructed me this 😪\n\nfrom graphviz import Digraph\n\ndef trace(root):\n    # build a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n1            nodes.add(v)\n            for child in v._prev:\n2                edges.add((child, v))\n                build(child)\n    build(root)\n\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = from left to right\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        # for any value in the graph, create a rectangular ('record') node for it\n        dot.node(name=uid, label=\"{ %s | data %.4f | grad %.4f}\" % (n.label, n.data, n.grad), shape='record') # why is (n.data, ), but not (n.data) ???\n        if n._op:\n            # if this value is a result of some operations, create an op node for it\n            dot.node(name = uid + n._op, label = n._op)\n            # and connect the node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        # connect n1 to the op node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n\n\n\n1\n\nThis will collect all nodes to the nodes.\n\n2\n\nThis will iteratively recursively collect all nodes to the nodes, add child and node ralationship information to edges.\n\n\n\n\n\n\nRemember to let graphviz installed on your machine, not only Python package, I also run this:\n\n\nShow the code\nimport os\nos.environ[\"PATH\"] += os.pathsep + 'C:\\Program Files (x86)\\Graphviz\\bin\\dot.exe'\n\n\nNow we can draw 🚀.\n\n\nShow the code\ndraw_dot(d)\n\n\n\n\n\n\n\n\n\nSo far we’ve build out mathematical expressions using only plus + and times *, all Values are only scalar.\nBack to the Value object, we will create 1 more attribute call label, make the expression more complicated by adding intermediate value f, d, out final node will be capital L."
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#manual-backpropagation-example-1-simple-expression",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#manual-backpropagation-example-1-simple-expression",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "manual backpropagation example #1: simple expression",
    "text": "manual backpropagation example #1: simple expression\n\n\n\nShow the code\ndraw_dot(L)\n\n\n\n\n\n\n\n\n\n\nLet’s do the back propagation manually:\n\nFirst we need to calculate the dL/dL, how L will response if we change L a tiny value h. The response simply is 1 so L.grad = 1.0.\nL = d * f, so dL/dd -&gt; (f((x+h)) - f(x))/h = ((d+h)*f - d*f)/h = h*f/h = f = -2.0. Quite straighforward, so d.grad = -2.0.\nSimilarly, f.grad = d = 4.\n\nNext, for dL/dc. We first concern dd/dc, we know d = c + e. Same with (2) we will soon know dd/dc = 1.0, by symmetry dd/de = 1.0. Following the Chain Rules \\(h'(x) = f'(g(x))g'(x)\\), we have: dL/dc = dL/dd * dd/dc = -2.0 * 1 = -2.0. \nBy symmetry, dL/de = -2.0.\ndL/da = dL/de * de/da = -2.0 * b = -2.0 * -3.0 = 6.0.\ndl/db = dL/de * de/db = -2.0 * a = -2.0 * 2.0 = -4.0.\n\nChain Rules WikiWe can also create a function for playing around / gradient check, and not messing up the global scope.\n\n\nShow the code\ndef lol():\n\n    h = 0.0001\n\n    a = Value(2.0, label='a')\n    b = Value(-3.0, label='b')\n    c = Value(10.0, label='c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L1 = L.data\n\n    a = Value(2.0, label='a')\n    b = Value(-3.0, label='b')\n    c = Value(10.0, label='c')\n    c.data += h # dL/dc = -2.0\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    # d.data += h # dL/dd = -2.0\n    f = Value(-2.0 # + h # dL/df = 4.0\n                , label='f') \n    L = d * f; L.label = 'L'\n    L2 = L.data # + h # dL/dL = 1.0\n\n    print((L2 - L1) / h)\n\nlol()\n\n\n-1.9999999999953388\n\n\nSo that is back propagation ~ just recursively applying the Chain Rules, multiplying local derivatives."
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#preview-of-a-single-optimization-step",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#preview-of-a-single-optimization-step",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "preview of a single optimization step",
    "text": "preview of a single optimization step\nWe can change the input that we can control a, b, c, f to see 1 step of the optimization of process.\n\n\nShow the code\na.grad = 6.0\nb.grad = -4.0\nc.grad = -2.0\nf.grad = 4.0\n\na.data += 0.01 * a.grad\nb.data += 0.01 * b.grad\nc.data += 0.01 * c.grad\nf.data += 0.01 * f.grad\n\ne = a * b; e.grad = -2.0; e.label = 'e'\nd = e + c; d.grad = -2.0; d.label = 'd'\nL = d * f\n\nprint(L.data)\n\n\n-7.286496\n\n\nWe can see the changes, L increased a little bit as expected.\n\n\n\nShow the code\ndraw_dot(L)"
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#manual-backpropagation-example-2-a-neuron",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#manual-backpropagation-example-2-a-neuron",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "manual backpropagation example #2: a neuron",
    "text": "manual backpropagation example #2: a neuron\nAnatomy of neurons, we have:\n\naxon as input \\(x_0\\);\nsynapse string as weight \\(w_0\\);\ninformation flows into the cell body will be \\(x_0w_0\\);\nthere are multiple inputs \\(x_iw_i\\) flow into the cell body;\nthe cell body has some bias itself \\(b\\);\nthe cell body processes all information, the output will flow through an activation function ~ which is some kind of a squashing function, like sigmoid, tanh or something like that;\n\n\n\n\n\n\n\n\n\n\nNeural net Structure with an Activation Function, CS231n Stanford 2017\n\n\n\n\n\n🚀How does the tanh look like? this hyperbolic function will squash the output to the edge values: -1.0 or 1.0.\n\n\nShow the code\nplt.plot(np.arange(-5, 5, 0.2), np.tanh(np.arange(-5, 5, 0.2))); plt.grid()\n\n\n\n\n\n\n\n\n\n\n\nWe first implement tanh function to our class Value.\n\n\nShow the code\ndef tanh(self):\n    x = self.data\n    t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n    out = Value(t, (self, ), 'tanh')\n    return out\n\nValue.tanh = tanh\n\n\nLet’s take a simple example of 2-dimensional neuron with 2 inputs x1 and x2:\n\n\nShow the code\n# input x1, x2\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n# weights w1,w2\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n# bias of neuron b\nb = Value(6.88137358, label='b')\n# x1*w1 + x2*w2 + b\nx1w1 = x1*w1; x1w1.label = 'x1w1'\nx2w2 = x2*w2; x2w2.label = 'x2w2'\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\nn = x1w1x2w2 + b; n.label = 'n'\n\no = n.tanh(); o.label = 'o' # not define yet\n\n\n\n\n\nShow the code\ndraw_dot(o)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\no.grad = 1.0\nn.grad = 1.0 - o.data ** 2\nb.grad = n.grad\nx1w1x2w2.grad = n.grad\nx1w1.grad = x1w1x2w2.grad\nx2w2.grad = x1w1x2w2.grad\nx1.grad = w1.data * x1w1.grad\nw1.grad = x1.data * x1w1.grad\nx2.grad = w2.data * x2w2.grad\nw2.grad = x2.data * x2w2.grad\n\n\nFrom here we will manually calculate the gradient again:\n\ndo/do = 1, that’s the base case, so o.grad = 1.0.\no = tanh(n), follow that Wiki link (and of course can be easily proof) we have do/dn = 1 - tanh(x)^2 = 1 - o^2.\nn = x1w1x2w2 + b, this is plus node, which gradient will flow to children equally, do/db = do/dn * dn/db = do/dn * 1.\nBy symmetry, do/dx1w1x2w2 = do/db.\ndo/dx1w1 = do/dx1w1x2w2.\ndo/dx2w2 = do/dx1w1x2w2.\ndo/dx1 = w1 * do/dx1w1.\ndo/dw1 = x1 * do/dx1w1.\ndo/dx2 = w2 * do/dx2w2.\ndo/dw2 = x2 * do/dx2w2.\n\n\n\n\nShow the code\ndraw_dot(o)"
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#implementing-the-backward-function-for-each-operation",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#implementing-the-backward-function-for-each-operation",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "implementing the backward function for each operation",
    "text": "implementing the backward function for each operation\nDoing the back propagation manually is obviously ridiculous and we are now to put an end to this suffering. We will see how we can implement backward pass a bit more automatically.\nWe create _backward operation for each operator, implement the Chain Rules. Activate the _backward call along with function execution.\n\n\nShow the code\no.grad = 1.0\n\no._backward()\nn._backward()\nb._backward()\nx1w1x2w2._backward()\nx1w1._backward()\nx2w2._backward()\n\n\n\n\n\nShow the code\ndraw_dot(o)\n\n\n\n\n\n\n\n\n\n\nWe still need to call the _backward node by node. Now we move to the next step, to implement backward function to whole expression graph."
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#implementing-the-backward-function-for-a-whole-expression-graph",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#implementing-the-backward-function-for-a-whole-expression-graph",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "implementing the backward function for a whole expression graph",
    "text": "implementing the backward function for a whole expression graph\nIn short, we need to do everything after each node before we call the backward function itself. For every node, all dependencies, everything that it depends on has to propagate to it before we can continue backpropagation.\nThis ordering of graph can be archived using something like topological sort.\n\n\n\n\n\n\n\n\n\nTopological Sort, photo credit to Claire Lee\n\n\n\n\n\n\n\n\n\nShow the code\n# we first reset the Values\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\nb = Value(6.88137358, label='b')\nx1w1 = x1*w1; x1w1.label = 'x1w1'\nx2w2 = x2*w2; x2w2.label = 'x2w2'\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\nn = x1w1x2w2 + b; n.label = 'n'\no = n.tanh(); o.label = 'o'\n\n\nBelow is the code:\n\n\nShow the code\ntopo = []\nvisited = set()\n\ndef build_topo(v):\n    if v not in visited:\n        visited.add(v)\n        for child in v._prev:\n            build_topo(child) # recursively look up all children for v\n        topo.append(v)\n\nbuild_topo(o)\ntopo\n\n\n[Value(data=6.88137358),\n Value(data=-3.0),\n Value(data=2.0),\n Value(data=-6.0),\n Value(data=0.0),\n Value(data=1.0),\n Value(data=0.0),\n Value(data=-6.0),\n Value(data=0.88137358),\n Value(data=0.707106777676776)]\n\n\nWe implement the topological sort to backward() (without underscore) function. Now we can trigger the whole process:\n\n\nShow the code\no.backward()\n\n\n\n\n\nShow the code\ndraw_dot(o)"
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#fixing-a-backprop-bug-when-one-node-is-used-multiple-times",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#fixing-a-backprop-bug-when-one-node-is-used-multiple-times",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "fixing a backprop bug when one node is used multiple times ⛔",
    "text": "fixing a backprop bug when one node is used multiple times ⛔\nThis a.grad should be 2.0.\n\n\nShow the code\na = Value(3.0, label='a')\nb = a + a; b.label = 'b' # this case self and other are both a, we should not overwrite the gradient, we should accumulate it.\nb.backward()\ndraw_dot(b)"
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#breaking-up-a-tanh-exercising-with-more-operations",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#breaking-up-a-tanh-exercising-with-more-operations",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "breaking up a tanh, exercising with more operations",
    "text": "breaking up a tanh, exercising with more operations\nSometime we do operations between Value and other, like int. We can not do this unless we add below code to __add__ and __mul__ operations. Now we can Value(1.0) + 1.0, or Value(2.0) * 2.\n\n\nShow the code\nother = other if isinstance(other, Value) else Value(other)\n\n\nBut for 2 * Value(2.0), which will internally call 2.__mul__(Value(2.0)), will not work. We add __rmul__:\n\n\nShow the code\ndef __rmul__(self, other): # other * self\n    return self * other\n\nValue.__rmul__ = __rmul__\n\n\nFor exponential, we add epx:\n\n\nShow the code\ndef exp(self):\n    x = self.data\n    out = Value(math.exp(x), (self, ), 'exp')\n\n    def _backward():\n        self.grad += out.data * out.grad\n    out._backward = _backward\n\n    return out\n\nValue.exp = exp\n\n\nFor division, we add __truediv__:\n\n\nShow the code\ndef __truediv__(self, other): # self / other\n    return self * other**(-1)\n\nValue.__truediv__ = __truediv__\n\n\nFor power, we add __pow__:\n\n\nShow the code\ndef __pow__(self, other): # self ** other\n    assert isinstance(other, (int, float)), \"TypeError: only supporting int/float power for now\"\n    out = Value(self.data**other, (self, ), f'**{other}')\n\n    def _backward():\n        self.grad += other * ( self.data ** (other - 1)) * out.grad\n    out._backward = _backward\n\n    return out\n\nValue.__pow__ = __pow__ \n\n\nFor subtract, we add __neg__ and __sub__:\n\n\nShow the code\ndef __neg__(self): # - self\n    return - self\n\nValue.__neg__ = __neg__ # self - other\n\ndef __sub__(self, other):\n    return self + (-other)\n\nValue.__sub__ = __sub__\n\n\nNow we are ready to try tanh in a different way:\n\n\nShow the code\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\nb = Value(6.88137358, label='b')\nx1w1 = x1*w1; x1w1.label = 'x1w1'\nx2w2 = x2*w2; x2w2.label = 'x2w2'\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\nn = x1w1x2w2 + b; n.label = 'n'\n\ne = (2*n).exp(); e.label = 'e'\no = (e - 1)/(e + 1)\no.label = 'o'\no.backward()\n\n\n\n\n\nShow the code\ndraw_dot(o)"
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#doing-the-same-thing-but-in-pytorch-comparison",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#doing-the-same-thing-but-in-pytorch-comparison",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "doing the same thing but in PyTorch: comparison",
    "text": "doing the same thing but in PyTorch: comparison\n\n\nShow the code\nimport torch\n\nx1 = torch.tensor([2.0]).double(); x1.requires_grad = True\nx2 = torch.tensor([0.0]).double(); x2.requires_grad = True\nw1 = torch.tensor([-3.0]).double(); w1.requires_grad = True\nw2 = torch.tensor([1.0]).double(); w2.requires_grad = True\nb = torch.tensor([6.8813735870195432]).double(); b.requires_grad = True\n\nn = x1*w1 + x2*w2 + b\no = torch.tanh(n)\n\nprint(o.data.item())\no.backward()\n\nprint('------------------')\nprint('x1', x1.grad.item())\nprint('w1', w1.grad.item())\nprint('x2', x2.grad.item())\nprint('w2', w2.grad.item())\nprint('------------------')\n\n\n0.7071066904050358\n------------------\nx1 -1.5000003851533106\nw1 1.0000002567688737\nx2 0.5000001283844369\nw2 0.0\n------------------"
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#building-out-a-neural-net-library-multi-layer-perceptron-in-micrograd",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#building-out-a-neural-net-library-multi-layer-perceptron-in-micrograd",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "building out a neural net library (multi-layer perceptron) in micrograd",
    "text": "building out a neural net library (multi-layer perceptron) in micrograd\nWe are going to build out a two-layer perceptron.\n\n\n\n\n\n\n\n\n\nA 3-layer neural network with three inputs, two hidden layers of 4 neurons each and one output layer. Notice that in both cases there are connections (synapses) between neurons across layers, but not within a layer, photo credit to cs231n\n\n\n\n\n\n\n\nShow the code\nclass Neuron:\n\n1    def __init__(self, nin):\n        self.w = [Value(np.random.uniform(-1,1)) for _ in range(nin)]\n        self.b = Value(np.random.uniform(-1,1))\n    \n2    def __call__(self, x):\n        activation = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n        out = activation.tanh()\n        return out\n\n    def parameters(self):\n        return self.w + [self.b] # list plus list gives you a list\n\nclass Layer:\n\n    def __init__(self, nin, nout):\n3        self.neurons = [Neuron(nin) for _ in range(nout)]\n\n    def __call__(self, x):\n        outs = [n(x) for n in self.neurons]\n        return outs[0] if len(outs) == 1 else outs\n\n    def parameters(self):\n        return [p for neuron in self.neurons for p in neuron.parameters()] # list comprehension\n        # params = []\n        # for neuron in self.neurons:\n        #     ps = neuron.parameters()\n        #     params.extend(ps)\n        # return params\n\nclass MLP:\n\n4    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def parameters(self):\n        return [p for layer in self.layers for p in layer.parameters()] # for neuron in layer.neurons for neuron.parameters()]\n\n\n\n1\n\nNumber of input for the Neuron. w is randomly generated for each input, same for b which is the bias that control “the happiness”.\n\n2\n\nObject as a function: define the forward pass of the Neuron \\(\\sum\\limits_{i=1}^{nin} w_ix_i+b\\), then squash the output using tanh.\n\n3\n\nA Layer is a list of Neurons, nout specifies how many Neurons in the Layer. Each neuron has nin inputs ~ nin-D. We just initialize completely independent neurons with this given dimensionality.\n\n4\n\nA MLP is a sequence of Layers, picture above depicts a 3-layers MLP containing 1 input layer and 3 output layers, we say the size is 4. We sequentially create connection from the input layer to the 1st output layer, 1st output layer to 2nd output layer,…\n\n\n\n\n\n\nShow the code\nnin = 3\nnouts = [2.0, 3.0, -1.0]\n[nin] + nouts\n\n\n[3, 2.0, 3.0, -1.0]\n\n\n\n\nShow the code\nx = [2.0, 3.0]\nn = Neuron(2)\nl = Layer(2, 3)\nn(x)\nl(x)\n\n\n[Value(data=-0.5867207664175761),\n Value(data=0.9886062778428307),\n Value(data=0.9955002730440313)]\n\n\n\n\nShow the code\nx = [2.0, 3.0, -1.0]\nm = MLP(3, [4, 4, 1]) # a MLP with 3-D input, 3 output layers contains 4, 4, 1 neurons in each layer respectively\nm(x)\n\n\nValue(data=0.7110631094754943)\n\n\n\n\n\nShow the code\ndraw_dot(m(x))"
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#creating-a-tiny-dataset-writing-the-loss-function",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#creating-a-tiny-dataset-writing-the-loss-function",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "creating a tiny dataset, writing the loss function",
    "text": "creating a tiny dataset, writing the loss function\nA simple data set, m() is the MLP we defined above.\n\n\nShow the code\nxs = [\n    [2.0, 3.0, -1.0],\n    [3.0, -1.0, 0.5],\n    [0.5, 1.0, 1.0],\n    [1.0, 1.0, -1.0]\n]\nys = [1.0, -1.0, -1.0, 1.0] # designed targets\n\n\nWriting the loss function. I was unable to sum a list of Value, found the solution here; Edit: I used Numpy random instead of random\n\n\nShow the code\nypred = [m(x) for x in xs]\nloss = np.array([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]).sum()\n\nloss\n\n\nValue(data=4.273206861488075)\n\n\nBackpropagation the loss, some magical here:\n\n\nShow the code\nloss.backward()\n\n\nWe can look into the gradient of weight of the first neuron of the first layer (input layer)\n\n\nShow the code\nprint('value of 1st neuron in 1st layer: ',m.layers[0].neurons[0].w[0].data)\nprint('grad of 1st neuron in 1st layer: ',m.layers[0].neurons[0].w[0].grad)\n\n\nvalue of 1st neuron in 1st layer:  -0.1315259199945451\ngrad of 1st neuron in 1st layer:  0.0890351163825127\n\n\n\n\n\nShow the code\ndraw_dot(loss)"
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#collecting-all-of-the-parameters-of-the-neural-net",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#collecting-all-of-the-parameters-of-the-neural-net",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "collecting all of the parameters of the neural net",
    "text": "collecting all of the parameters of the neural net\nWe aim to produce the fitness ypred. xs is the data, the input of problem, we can not change it. ys is the ground true, can not changes as well. What we can change is the “parameters” of each neuron, which is weight w and bias b.\nWe add in to each class a parameters() function to collect those. Finally we can get all the parameters of the MLP:\n\n\nShow the code\nlen(m.parameters())\n\n\n41"
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#doing-gradient-descent-optimization-manually-training-the-network",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#doing-gradient-descent-optimization-manually-training-the-network",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "doing gradient descent optimization manually, training the network",
    "text": "doing gradient descent optimization manually, training the network\nNow we will try to change the parameters to minimize the loss, which means our prediction will be more close to the ground true.\nForward pass, calculate the loss:\n\n\nShow the code\nypred = [m(x) for x in xs]\nloss = np.array([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]).sum()\nloss\n\n\nValue(data=4.273206861488075)\n\n\nBackward pass, calculate the parameters:\n\n\nShow the code\nloss.backward()\n\n\nUpdate the parameters, change the parameters following opposite direction to reduce the loss:\n\n\nShow the code\nfor p in m.parameters():\n    p.data += -0.01 * p.grad # we want the p.data go on opposite direction of the loss\n\n\n0.01 is the learning rate!\nNew loss\n\n\nShow the code\nypred = [m(x) for x in xs]\nloss = np.array([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]).sum()\nloss\n\n\nValue(data=2.6769450595254147)\n\n\nYeah the loss decreased. In short, the process is:\n\n\n\n\n\n%%{init: {'theme':'dark'}}%%\nflowchart LR\n\nP1(Updated parameters) -- Forward Pass --&gt; L(Loss)\nL(Loss) -- Backward Pass --&gt; P2(Parameters to update) \nP2(Parameters to update)  -- Update Pamameters --&gt; P1(Updated parameters)\n\n\n\n\n\n\nAutomate the training loop:\n\n\n\n\n\n\nWarning\n\n\n\nFor each process, there remained a subtle bug above that we didn’t flush the grads before back propagation. Because we did not overwrite the gradients (remember the +=), they kept accumulated. The next action of backward and changing parameters using learning rate and grad (which produce a massive step size) become wrong! We must set the grad to zero before backward pass.\n\n\n\nCommon guilties when training the NN\n\n\n\n\n\n\nShow the code\nfor k in range(20):\n    # forward pass:\n    ypred = [m(x) for x in xs]\n    loss = np.array([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]).sum()\n\n    # backward pass:\n    for p in m.parameters():\n        p.grad = 0.0\n    loss.backward()\n\n    # update params:\n    for p in m.parameters():\n        p.data += -0.01 * p.grad\n\n    print(k, loss.data)\n\n\n0 2.6769450595254147\n1 2.071186773259925\n2 1.6553095000151627\n3 1.3747465447804739\n4 1.175614462288813\n5 1.0253739615104944\n6 0.9066645062590994\n7 0.8099556765272622\n8 0.7295348947415733\n9 0.6616589001674095\n10 0.6037038116501692\n11 0.5537439350176029\n12 0.5103211378534444\n13 0.4723064068959759\n14 0.4388105836138193\n15 0.4091237571814982\n16 0.38267256401718325\n17 0.3589892725779239\n18 0.3376889268409917\n19 0.3184521607517571\n\n\n\n\nShow the code\nypred\n\n\n[Value(data=0.7454183490141355),\n Value(data=-0.742127817744707),\n Value(data=-0.6669516627171552),\n Value(data=0.7239183338483085)]"
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#summary-of-what-we-learned-how-to-go-towards-modern-neural-nets",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#summary-of-what-we-learned-how-to-go-towards-modern-neural-nets",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "summary of what we learned, how to go towards modern neural nets",
    "text": "summary of what we learned, how to go towards modern neural nets\n\nWhat are Neural Nets: they are mathematical expressions, in case of MLP it takes: (1) data as the input, and (2) weights and biases as parameters to build out expression for the forward pass followed by the loss function.\nThe loss function is kind of measure for the accuracy of predictions. The low loss implies that predicted values are matching our targets and the networks are behaving well.\nThe process of Gradient Descent is for each step, we calculate the loss (output of the nets), backwarding it to get parameters, then updating data (which we can change - weights and biases) follow the opposite side of the loss (negative grad * learning rate). We’ll get a lower loss, and backwarding again and again. This process will find the local minimum of the loss."
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#walkthrough-of-the-full-code-of-micrograd-on-github",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#walkthrough-of-the-full-code-of-micrograd-on-github",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "walkthrough of the full code of micrograd on github",
    "text": "walkthrough of the full code of micrograd on github\nSame with which we built today:\n\nengine: Value\nnn: Neuron, Layer, MLP, and modulize the zero grad process to class Module\ntest: sanity check - compare the backward with torch, also for the forward pass\ndemo: a bit complicated example with sklearn dataset, using batch processing when the dataset come large, the loss is slightly different - SVM max-margin loss and using of auto L2 regularization\nlearning rate decay: is a scaled as a function of number of iterations, high at begin and low at the end"
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#real-stuff-diving-into-pytorch-finding-their-backward-pass-for-tanh",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#real-stuff-diving-into-pytorch-finding-their-backward-pass-for-tanh",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "real stuff: diving into PyTorch, finding their backward pass for tanh",
    "text": "real stuff: diving into PyTorch, finding their backward pass for tanh\nThese libraries unfortunately grow in size and entropy, if you just search for tanh it’ll give you thousands of results."
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#conclusion",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#conclusion",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "conclusion",
    "text": "conclusion\nThere will be follow up session yeah haha."
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#outtakes",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#outtakes",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "outtakes :)",
    "text": "outtakes :)\nPytorch self-defined autograd.\nHapply learning! 🤙🚀🔥"
  },
  {
    "objectID": "blog/2024-06-24-docker/index.html",
    "href": "blog/2024-06-24-docker/index.html",
    "title": "Hi Docker 🐳",
    "section": "",
    "text": "Edit: an other great source for Docker - https://levelup.gitconnected.com/working-with-docker-and-docker-compose-9773295b4d51"
  },
  {
    "objectID": "blog/2024-06-24-docker/index.html#giới-thiệu-về-script-python",
    "href": "blog/2024-06-24-docker/index.html#giới-thiệu-về-script-python",
    "title": "Hi Docker 🐳",
    "section": "Giới thiệu về script Python",
    "text": "Giới thiệu về script Python\nMình có một file python đơn giản, huấn luyện một mô hình ml_project Random forest để nhận diện hoa diễn vĩ từ bộ iris dataset với thư viện sklearn như sau:\n\n\niris_classification.py\n\n#| eval: false\n# importing required libraries\n# importing Scikit-learn library and datasets package\nfrom sklearn import datasets\n# Splitting arrays or matrices into random train and test subsets\nfrom sklearn.model_selection import train_test_split\n# importing random forest classifier from assemble module\nfrom sklearn.ensemble import RandomForestClassifier\n# importing scaler\nfrom sklearn.preprocessing import StandardScaler\n# metrics are used to find accuracy or error\nfrom sklearn.metrics import accuracy_score \n\n# Loading the iris plants dataset (classification)\niris = datasets.load_iris()\n# dividing the datasets into two parts i.e. training datasets and test datasets\nX = iris.data[:, [2, 3]]\ny = iris.target\n# i.e. 70 % training dataset and 30 % test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=0)\n\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(X_train)\nx_test = scaler.transform(X_test)\n\nmodel = RandomForestClassifier()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\n\nfor pred, label in zip(y_pred, y_test):\n    print(\"Prediction: {}. Label: {}\".format(pred, label))\n\nprint(\"Accuracy: %.2f\" % accuracy_score(y_test, y_pred))"
  },
  {
    "objectID": "blog/2024-06-24-docker/index.html#tìm-base-image-trên-docker-hub",
    "href": "blog/2024-06-24-docker/index.html#tìm-base-image-trên-docker-hub",
    "title": "Hi Docker 🐳",
    "section": "Tìm Base Image trên Docker Hub",
    "text": "Tìm Base Image trên Docker Hub\nChúng ta có thể tìm các class cha ~ base image trên hub.docker.com, chúng ta muốn app ml này chạy trên ubuntu, do đó có thể start từ image ubuntu.\nMình xây dựng Dockerfile một cách đơn giản như sau:\n\n\nDockerfile\n\nFROM ubuntu\n\nTừ Dockerfile này mình có thể build image thông qua command sau:\ndocker -t ml_project .\nRun một docker container (container vừa run sẽ lập tức exit):\ndocker run image_name container_name\nNếu muốn “chui” vào bên trong container:\ndocker run -it image_name container_name bash\n-it nghĩa là chúng ta chạy container dưới interactive mode, bash nghĩa là chạy trong bash mode, chúng ta có thể thực hiện các câu lệnh bash từ đây."
  },
  {
    "objectID": "blog/2024-06-24-docker/index.html#cài-đặt-các-thư-viện-trong-docker-image",
    "href": "blog/2024-06-24-docker/index.html#cài-đặt-các-thư-viện-trong-docker-image",
    "title": "Hi Docker 🐳",
    "section": "Cài đặt các thư viện trong Docker Image",
    "text": "Cài đặt các thư viện trong Docker Image\nBây giờ ta mới chỉ có duy nhất hệ điều hành ubuntu trong container, chưa có python để chạy ứng dụng. Trong ubuntu, ta dùng apt-get để cài đặt python. Chúng ta sẽ thực hiện các lệnh trên bash ở container hiện tại trước, sau đó mới đưa vào Dockerfile với lệnh RUN. Dưới đây là Dockerfile cập nhật:\n\n\nDockerfile\n\nFROM ubuntu\n\nRUN apt-get update\nRUN apt-get -y install python3\n# -y tự động điền yes khi có các câu hỏi Y/N\n\n\n\n\n\n\n\nTip\n\n\n\nSử dụng exit() hoặc Ctrl-D để thoát Python mode hoặc Container trong Powershell.\n\n\nChúng lại truy cập bash của container đang chạy từ Docker desktop hoặc Powershell:\n\n\n\n\n\n\n\n\n\nBash inside container"
  },
  {
    "objectID": "blog/2024-06-24-docker/index.html#copy-dữ-liệu-từ-host-vào-docker-image",
    "href": "blog/2024-06-24-docker/index.html#copy-dữ-liệu-từ-host-vào-docker-image",
    "title": "Hi Docker 🐳",
    "section": "Copy dữ liệu từ host vào Docker Image",
    "text": "Copy dữ liệu từ host vào Docker Image\nBây giờ chúng ta muốn rằng sau khi vào container, chúng ta không ở thư mục root nữa mà ở src ~ ứng chúng của chúng ta, với mục đích dễ làm việc hơn. Chúng ta thêm cú pháp WORKDIR /src. Đồng thời cũng cần cài đặt sklearn để ứng dụng có thể chạy. Đồng thời, sau khi tất cả được cài đặt, mình cũng muốn chạy luôn ứng dụng, sử dụng CMD.\nCập nhật Dockerfile như sau:\n\n\nDockerfile\n\nFROM ubuntu\n\nWORKDIR /src\n# Khi ở trong docker container, mặc định chúng ta sẽ ở root, giờ ta muốn khi vào container, chúng ta sẽ vào /src\n\nRUN apt-get update\nRUN apt-get -y install python3\nRUN apt-get -y install python3-sklearn\n# -y tự động điền yes khi có các câu hỏi Y/N\n\nCOPY iris_classification.py ./iris_classification.py \n\nCMD [ \"python3\", \"iris_classification.py\" ]"
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html",
    "href": "blog/2024-07-08-advanced-python/index.html",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "",
    "text": "This lesson was found on Juan Luis Cano Rodríguez’s github profile when I came across his talk “Building the composable Python data stack with Kedro & Ibis” (bookmarked for learning later) in Pydata London 2024.\nThis is my notes:"
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#learning-objectives",
    "href": "blog/2024-07-08-advanced-python/index.html#learning-objectives",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "learning objectives",
    "text": "learning objectives\n\nLearn modern software engineering practices using Python\nUnderstand the value of automation in the software engineering process\nGain insight into how Data Science projects are put in production\nLearn better techniques to collaborate in software projects\n\nThe instructor used Linux / Conda, err I just have Window here so there is no other way but right I will try to make stuffs work in Window. Let’s get started!\n\n\n\n\n\n\n\n\n\nstolen from the lecture"
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#glossary",
    "href": "blog/2024-07-08-advanced-python/index.html#glossary",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "glossary",
    "text": "glossary\n\nRepository: Directory tracked by git, contains a .git folder and it’s created by $ git init;\nCommit: State or snapshot of the repository, they are created by $ git commit;\nBranch: A parallel or separate line of development, the default one is master and they are created by $ git branch or $ git checkout -b."
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#git-uses-a-linux-like-cli-so-now-linux-cli-101-and-how-to-do-the-same-on-window-pwsh",
    "href": "blog/2024-07-08-advanced-python/index.html#git-uses-a-linux-like-cli-so-now-linux-cli-101-and-how-to-do-the-same-on-window-pwsh",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "git uses a Linux-like cli so now Linux cli 101 (and how to do the same on Window pwsh)",
    "text": "git uses a Linux-like cli so now Linux cli 101 (and how to do the same on Window pwsh)\n\nwhoami: who am i\npwd: print working directory\nls: list all file in the dir, . for current folder, .. for parent one (seems there is no --color or -a in pwsh)\ncd: change dir\ntouch: create empty file (in pwsh we use echo \"\" &gt;&gt; file_name)\ncat: concatenate, print file contents\nnano: edit a file from the command line (there is a way that allows us to do the same in win10, but ya, nope)"
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#workflow",
    "href": "blog/2024-07-08-advanced-python/index.html#workflow",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "workflow",
    "text": "workflow\nI did these in pwsh:\n\nCreate a directory mkdir test_project and navigate there cd test_project;\nInit a git repository git init;\nCheck status git status (“on branch master, no commits yet, nothing to commit”);\nCreate some files echo \"#Hello, world!\" &gt;&gt; readme.md;\nStage the files git add readme.md;\nCommit the changes git commit -m \"initial commit\";\n\n\n\n\n\n\n\nWarning\n\n\n\nDo not run git init on your home directory, as it can lead to confusion and potential data loss. If git status gives a lot of untracked files unrelated to your project, you might want to rm -rf .git and start in another directory. Notice that this command removes all git history."
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#branching",
    "href": "blog/2024-07-08-advanced-python/index.html#branching",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "branching",
    "text": "branching\n\nCreate and checkout to new branch git switch -c branch1 (-c stands for create);\nCommit there (see above);\nGo back to main branch git switch master;\nMerge changes git merge branch1;\nDelete branch git branch -d branch1 (-d stands for delete, don’t forget this step!).\n\n\n\n\n\n\n\nTip\n\n\n\n\nNormally, the git merge step happens online using pull requests or merge requests, which are not git concepts, but GitHub/GitLab concepts.\nIf git switch does not work for you, you might have an older version of Git. Consider upgrading, or alternatively replace all git switch -c with git checkout -b.\n\n\n\n\nPull requests (PR) let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch. – GitHub\n\n\nA merge request (MR) is a proposal to incorporate changes from a source branch to a target branch. – GitLab\n\n\nRemember to never commit to master. – Git Workflow"
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#merging",
    "href": "blog/2024-07-08-advanced-python/index.html#merging",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "merging",
    "text": "merging\n2 types of merging:\n\nFast-forward merge: There is no diverging history, and git just “advances the pointer” of the current branch. git merge new-branch --ff-only will fail if a fast-forward merge is not possible;\nNon fast-forward merge: The history diverged, and git will create a merge commit (hence ask for a commit message) with two parents that combines the two branches. git merge new-branch --no-ff always creates a merge commit even if a fast-forward merge is possible.\n\nGitHub use --no--ff option for pull requests, see here, and this old-but-gold discussion.\n\nNon fast-forward merges can end up in conflicts. In that case, git will halt the merge operation and leave traces in the affected files;\nTo abort a merge git merge --abort (useful if we are scared and don’t know what to do);\nTo merge overriding everything with the upcoming branch git merge new-branch --strategy-option theirs;\nTo merge overriding everything with the current branch git merge new-branch --strategy-option ours.\n\nBe careful while editing files that are in conflict."
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#other",
    "href": "blog/2024-07-08-advanced-python/index.html#other",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "other",
    "text": "other\n\nIgnoring files .gitignore;\nAmend the last commit: git commit --amend;\nShow pretty history: git log --graph --oneline --decorate --all;\nConfiguring git aliases: git config --global alias.lg \"log --graph --oneline --decorate\" (and now you have git lg!).\n\nThis excellent chart will help you in git workflow decision making.\n\n\n\n\n\n\n\n\n\ngit flowchart, photo credit to this SO thread"
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#triangular-workflows-in-git",
    "href": "blog/2024-07-08-advanced-python/index.html#triangular-workflows-in-git",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "triangular workflows in git",
    "text": "triangular workflows in git\nWhen collaborating with a project hosted online on GitHub or GitLab, the most common setup is having a central repository, one remote fork per user, and local clones/checkouts:\n\n\n\n\n\n\n\n\n\ntriangular workflows in git, source\n\n\n\n\n\nNotice the different naming conventions between this website and the first image:\n\nConvention 1: upstream/origin/local\nConvention 2: origin//local\n\nWe will be consistent with the Aaron Meurer guide and therefore use Convention 2 all the time."
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#after-creating-a-pr",
    "href": "blog/2024-07-08-advanced-python/index.html#after-creating-a-pr",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "after creating a PR",
    "text": "after creating a PR\nAfter your pull request has been merged to master, your local master and &lt;username&gt;/master will be outdated with respect to origin/master. On the other hand, you should avoid working on this branch anymore in the future: remember branches should be ephemeral and short-lived.\nTo put yourself in a clean state again, you have to:\n\nClick “remove branch” in the pull request (don’t click “remove fork”!);\ngit checkout master (go back to master);\ngit fetch origin (never, ever use git pull unless you know exactly what you’re doing)\ngit merge --ff-only origin master (update your local master with origin/master, and fail if you accidentally made any commit in master)\ngit fetch -p &lt;username&gt; (✨ acknowledge the removal of the remote branch ✨)\ngit branch -d old-branch (remove the old branch)\ngit push &lt;username&gt; master (update your fork with respect to origin)\ngit checkout -b new-branch (start working in the new feature!)\n\nThis process has to be repeated after every pull request.\nSome organizations where all the members are trusted do not use forks, and everybody pushes their branches to the same repository instead. While this simplifies some parts of the workflow, it also requires proper checks in place to prevent bad code to be merged - for example, by requiring a minimum number of reviews or some automated status checks."
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#how-does-import-work",
    "href": "blog/2024-07-08-advanced-python/index.html#how-does-import-work",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "how does import work?",
    "text": "how does import work?\nHow do import os, pandas work? If pandas was not installed, what happen?"
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#how-can-i-import-my-code",
    "href": "blog/2024-07-08-advanced-python/index.html#how-can-i-import-my-code",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "how can I import my code?",
    "text": "how can I import my code?\nThere are three ways to import our own code:\n\nBeing on the same directory: This is the quickest, however it scales quite poorly (imagine having all of pandas and scikit-learn in a single directory to do any data analysis project!)\nAppending our code location to PYTHONPATH: This is effective, but we will try to avoid it because it can bring problems in the future.\nMaking our code installable: Since any code that’s installed can be imported, this shifts the question to “how to make our code installable”."
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#my-first-python-lib",
    "href": "blog/2024-07-08-advanced-python/index.html#my-first-python-lib",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "my first python lib",
    "text": "my first python lib\nWe will create a new Python library, “IE Titanic utils”, to analyze the Titanic dataset. I will create a project ie-titanic-utils, add readme.md and .gitignore files.\nmkdir it-titanic-utils\ngit init\n\necho \"# This is utility to help analyze Titanic dataset\" &gt;&gt; readme.md\nInvoke-WebRequest -Uri \"https://www.toptal.com/developers/gitignore/api/python,jupyternotebooks\" | Select-Object -ExpandProperty Content | Out-File -FilePath .gitignore -Encoding utf8\n\ngit add readme.md .gitignore\ngit commit -m \"initial commit\"\nNow I will create str_utils.py file (using VS code for convenience), with a function called tokenize that takes a str sentence and splits it into a list of words.\n\n\nstr_utils.py\n\ndef tokenize(sentence: str) -&gt; list[str]:\n    return sentence.split()\n\nIn pwsh, I can import and use this function\npython\nPython 3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; from str_utils import tokenize\n&gt;&gt;&gt; tokenize(\"Toi la Le Khac Tuan\")\n['Toi', 'la', 'Le', 'Khac', 'Tuan']\n&gt;&gt;&gt;"
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#the-pythonpath",
    "href": "blog/2024-07-08-advanced-python/index.html#the-pythonpath",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "the PYTHONPATH",
    "text": "the PYTHONPATH\nWe saw above that we could easily import our tokenize function. However, this only works if we are in the same directory. Why? Python looks in some predefined locations to know where to find what we want to import, called the “PATH”.\n\n\nShow the code\nimport sys\nsys.path # i will not execute this code\n\n\nTherefore, there are two ways of making our code globally importable:\n\nModify the “PATH”\nPut our code inside a location predefined in the “PATH”\n\nThe first option can be achieved like this:\n\n\nShow the code\n&gt;&gt;&gt; sys.path.insert(0, \"/home/username/ie-titanic-utils\")\n&gt;&gt;&gt; import str_utils  # Works!\n\n\nOr, alternatively, from outside of the interpreter: export PYTHONPATH=/home/username/ie-titanic-utils.\nHowever, both are bad practices and should be avoided."
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#what-does-import-do",
    "href": "blog/2024-07-08-advanced-python/index.html#what-does-import-do",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "what does import do?",
    "text": "what does import do?\nPython code is normally written in .py scripts. These scripts can be imported in the same way that any model or package from the standard library can:\n$ python3\n&gt;&gt;&gt; import math  # Works, because it's in stdlib\n&gt;&gt;&gt; import numpy as np  # Works if you ran `pip install numpy` in advance\n&gt;&gt;&gt; import str_utils  # Works if you are in the same directory\n['Hello,', 'world!']\n&gt;&gt;&gt; \nWhen the user imports a script, Python runs the script. That’s the way all the possible functions and classes inside it are available."
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#how-to-separate-running-code-from-reusable-pieces",
    "href": "blog/2024-07-08-advanced-python/index.html#how-to-separate-running-code-from-reusable-pieces",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "how to separate “running code” from reusable pieces",
    "text": "how to separate “running code” from reusable pieces\nA Python module (any .py script) might contain code that we want to run, as well as code that we only want to import. To separate these, we use this trick:\n ie-titanic-utils  get-content -Tail 2 .str_utils.py\nif __name__ == \"__main__\":\n    print(tokenize(\"hello, world!\"))\n\n ie-titanic-utils  python .str_utils.py # The `print` runs\n['hello,', 'world!']\n\n ie-titanic-utils  python\n&gt;&gt;&gt; from str_utils import tokenize\n&gt;&gt;&gt; tokenize(\"Hi, world!\")\n['Hi,', 'world!'] # The `print` doesn't run!\n&gt;&gt;&gt;\n\n\n\n\n\n\nTip\n\n\n\nHere is what Claude sonnet 3.5 gave me:\nTo separate code that we want to run directly from code that we only want to import, we should use the if __name__ == \"__main__\": idiom in Python. This is a common pattern that allows a Python script to be both importable and executable. Here’s an explanation:\n\nWhen a Python file is run directly, Python sets the special __name__ variable to “__main__”.\nWhen a Python file is imported as a module,__name__is set to the name of the module.\n\nBy using this idiom, we can control which code runs when the script is executed directly versus when it’s imported as a module."
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#managing-python-environments",
    "href": "blog/2024-07-08-advanced-python/index.html#managing-python-environments",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "managing python environments",
    "text": "managing python environments\n\n\n\n\n\n\n\n\n\nstolen from the lecture\n\n\n\n\n\n\nSimple is better than complex.\nComplex is better than complicated.\n\nHow do people install and upgrade Python? -&gt; Most of people choose Python.org, the same for me!\n\n\n\n\n\n\nCaution\n\n\n\nThis way ships a tool to create development environments (venv). However, venv cannot create environments with different Python versions (you’re tied to the one you downloaded) and certain packages will require extra steps to be installed. Therefore, it is not for everyone.\n\n\nJuan chose to use conda. As I am learning Docker, I choose Docker for this tutorial.\nHow do people create isolated development environments? -&gt; The most popular is Virtualenv. But normally when doing an analysis task, I use venv which is built-in python lib. Recently I followed my dev team to use pipenv, I do also see poetry is worth-learning approach.\n\n“More than a half of the users of Jupyter Notebook and JupyterLab choose Conda”\n\nAs I think a model which is not deployed yet is useless model, I choose VS Code and Docker - more deployment-oriented."
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#summary",
    "href": "blog/2024-07-08-advanced-python/index.html#summary",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "summary",
    "text": "summary\n\nFor the user, the most salient distinction is probably this: pip installs python packages within any environment; conda installs any package within conda environments.\n—Jake Vanderplas\n\nHowever, I will be using pipenv to achieve what Juan done in upcoming sections. I will specify version of each package I used."
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#pip-and-pypi",
    "href": "blog/2024-07-08-advanced-python/index.html#pip-and-pypi",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "pip and PyPI",
    "text": "pip and PyPI\npip is the default Python installer. By default, it fetches packages from https://pypi.org/, which is the community repository for Python packages."
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#creating-a-package",
    "href": "blog/2024-07-08-advanced-python/index.html#creating-a-package",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "creating a package",
    "text": "creating a package\n\nrun flit init to create the metadata\n\n ie-titanic-utils  flit init\npyproject.toml exists - overwrite it? [y/N]: y\nModule name [ie_titanic_utils]: ie_utils\nAuthor: Tuan Le Khac\nAuthor email: tuan.lekhac0905@gmail.com\nHome page: \nChoose a license (see http://choosealicense.com/ for more info)\n1. MIT - simple and permissive\n2. Apache - explicitly grants patent rights\n3. GPL - ensures that code based on this is shared with the same terms\n4. Skip - choose a license later\nEnter 1-4: 4\n\nWritten pyproject.toml; edit that file to add optional extra info.\n\nplace some code under the source directory. In __init__.py there must be a docstring giving a description of the project and a __version__ variable indicating the version:\n\n\n\n__init__.py\n\n\"\"\"IE utils (test package).\"\"\"\n\n__version__ = \"0.1.0\"\n\n\nInstall the code using pip install! (this did not work for me currently. edit: the project name should match the src/package_name omg)\n\n ie-titanic-utils  pip install .\nProcessing .ie-titanic-utils\n...\nSuccessfully installed ie_titanic_utils-0.1.0\n ie-titanic-utils  python\nPython 3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import ie_titanic_utils\n&gt;&gt;&gt; ie_titanic_utils.__version__\n'0.1.0'\n&gt;&gt;&gt;    \n\nreadme.md and a .gitignore files were created.\ncommit the change\n\ngit checkout -b first_module # i should be using switch (-c) haha, checkout is old syntax\ngit status\ngit add .\ngit commit -m \"initial very first module\"\ngit checkout master\n\n ie-titanic-utils  git merge --no-ff first_module\nMerge made by the 'ort' strategy.\n pyproject.toml                    | 30 ++++++++++++++++++++++++++++++\n src/ie_titanic_utils/__init__.py  |  6 ++++++\n src/ie_titanic_utils/str_utils.py |  6 ++++++\n 3 files changed, 42 insertions(+)\n create mode 100644 pyproject.toml\n create mode 100644 src/ie_titanic_utils/__init__.py\n create mode 100644 src/ie_titanic_utils/str_utils.py\n\ngit branch -d first_module\n# git push origin master\n# as I will not upload this code to GitHub\n# https://stackoverflow.com/questions/32238616/git-push-fatal-origin-does-not-appear-to-be-a-git-repository-fatal-could-n\nNow if I log the git, I will see this:\n ie-titanic-utils  git log --graph --oneline --decorate --all\n*   39105c7 (HEAD -&gt; master) Merge branch 'first_module'\n|\n| * 7a33844 initial very first module\n|/\n* 793bd67 initial commit\nYeah til now I can create a “package” in my computer and install so I can use it globally. But I have not use any env management yet."
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#intermezzo-version-numbers",
    "href": "blog/2024-07-08-advanced-python/index.html#intermezzo-version-numbers",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "intermezzo: version numbers",
    "text": "intermezzo: version numbers\n\nVersion numbers for Python packages are explained in PEP 440\nFor libraries, the most widely used convention is semantic versioning: X.Y.Z\n\nZ must be incremented if only backwards compatible bug fixes are introduced (a bug fix is defined as an internal change that fixes incorrect behavior)\nY must be incremented every time there is new, backwards-compatible functionality\nX must be incremented every time there are backwards-incompatible changes\n\nBetween releases, the version should have the .dev0 suffix\nRecommendation: start with 0.1.dev0 (development version), then make a 0.1.0 release, then progress to 0.1.1 for quick fixes and 0.2.0 for new functionality, and when you want to make a promise of relative stability jump to 1.0.0.\nFor applications, other conventions are more appropriate, like calendar versioning: [YY]YY.MM.??"
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#project-requirements",
    "href": "blog/2024-07-08-advanced-python/index.html#project-requirements",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "project requirements",
    "text": "project requirements\nSometimes our project will depend on third-party libraries (pandas, scikit-learn). To make pip install those dependencies automatically, we can add them to our pyproject.toml under the [tool.flit.metadata] section, using the requires option:\n[build-system]\nrequires = [\"flit_core&gt;=3.4\"]\nbuild-backend = \"flit_core.buildapi\"\n\n[project]\nname = \"ie_titanic_utils\"\nauthors = [{name = \"Tuan Le Khac\", email = \"tuan.lekhac0905@gmail.com\"}]\nreadme = \"readme.md\"\nrequires-python = \"&gt;=3.11\"\ndynamic = [\"version\", \"description\"]\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\",\n]\nrequires = [\n    \"pandas\",\n    \"matplotlib&gt;=2\",\n]\nWe might want to specify optional dependencies that should only be installed upon request, or for some specific purposes. A typical example will be development dependencies: we will need things like pytest and black, but we don’t want the user to install them as part as our library. To do that, we can specify groups of optional dependencies under the tool.flit.metadata.requires-extra section:\nIn my case, I use `project.optional-dependencies`: flit_core.config.ConfigError: Use [project] table for metadata or [tool.flit.metadata], not both.\n[project.optional-dependencies]\ndev = [\n    \"pytest&gt;=6.0\",\n    \"black&gt;=20.8b1\",\n]\nThat way, they will only get installed when [dev] is added after the name of our library:\n ie-titanic-utils  pip install .[dev]\n# Successfully installed black-24.4.2 ie_titanic_utils-0.1.0 iniconfig-2.0.0 pytest-8.2.2"
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#test-driven-development",
    "href": "blog/2024-07-08-advanced-python/index.html#test-driven-development",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "test-driven development",
    "text": "test-driven development\nThe “test-driven development mantra” is Red - Green - Refactor:\n\n\n\n\n\n\n\n\n\nMake it work. Make it right. Make it fast.\n\n\n\n\n\n\nWrite a test. Watch it fail.\nWrite just enough code to pass the test.\nImprove the code without breaking the test.\n\nRepeat."
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#testing-in-python",
    "href": "blog/2024-07-08-advanced-python/index.html#testing-in-python",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "testing in Python",
    "text": "testing in Python\nSummary: use pytest. Everybody does. It rocks.\npytest is a testing framework for Python that makes writing tests extremely easy. It is much more powerful than the standard library equivalent, unittest. We can use by install it first pip install pytest.\nWe can write a function that test the tokenize funtion:\n\n\ntests/test_tokenize.py\n\nfrom ie_titanic_utils import tokenize  # This will fail right away!\n\n\ndef test_tokenize_returns_expected_list():\n    sentence = \"This is a sentence\"\n    expected_tokens = [\"This\", \"is\", \"a\", \"sentence\"]\n\n    tokens = tokenize(sentence)\n\n    assert tokens == expected_tokens\n\nand we run it from the command line:\n ie-titanic-utils  pytest\n============================================================================================ test session starts ============================================================================================\nplatform win32 -- Python 3.11.4, pytest-8.2.2, pluggy-1.5.0\nrootdir: .ie-titanic-utils\nconfigfile: pyproject.toml\nplugins: anyio-3.7.1, time-machine-2.14.0\ncollected 1 item\n\nteststest_tokenize.py .                                                                                                                                                                               [100%] \n\n============================================================================================= 1 passed in 0.02s ============================================================================================= \nThe test successed after I fixed the __init__.py:\n\n\nsrc/./__init__.py\n\nfrom ie_titanic_utils.str_utils import tokenize\n__all__ = [\"tokenize\"]"
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#what-are-objects-anyway",
    "href": "blog/2024-07-08-advanced-python/index.html#what-are-objects-anyway",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "what are “objects” anyway?",
    "text": "what are “objects” anyway?\nlike pandas’s DataFrame or matplotlib’s Figure, an object is sthg that has:\n\nobject-bound variables: call properties;\nobject-bound functions: call methods.\n\nif the object’s properties can change, we say they have states, in that case they are mutable. otherwise, they are stateless and immutable. a typical example is list (mutable) and tuple (immutable).\n\n\nShow the code\nmy_list = [1, 2, 3]\nprint(my_list)\nmy_list.append(4)\nprint(my_list)\n\n\n[1, 2, 3]\n[1, 2, 3, 4]\n\n\n\n\nShow the code\n# The operator that creates tuples is not parentheses:\n# is the comma!\nmy_tuple = 1, 2, 3  # Notice that I don't need parentheses!\nmy_tuple\n\n\n(1, 2, 3)\n\n\n\n\nShow the code\nprint(dir(my_tuple))  # Nothing that allows us to change the state of the tuple\n\n\n['__add__', '__class__', '__class_getitem__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'count', 'index']\n\n\n\n\nShow the code\nmy_tuple[0] = 99 # This wont work, \"TypeError: 'tuple' object does not support item assignment\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nImmutable objects have the advantage that they can be hashed, that is: they can be transformed, using some cryptographical function, into something that uniquely represents that object. Mutable objects can’t, because the hash would have to change every time the state of the object changed. Dictionary keys have to be hashable objects.\n\n\n\n\nShow the code\n{\n    my_tuple: \"my_tuple\"\n}\n\n\n{(1, 2, 3): 'my_tuple'}\n\n\n\n\nShow the code\nhash(my_tuple)\n\n\n529344067295497451\n\n\n\n\nShow the code\n# this wont work: \"TypeError: unhashable type: 'list'\"\n{\n    my_list: \"my_list\"\n}\n\n\n\n\nShow the code\n# this wont work: \"TypeError: unhashable type: 'list'\"\nhash(my_list)"
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#classes-and-instances",
    "href": "blog/2024-07-08-advanced-python/index.html#classes-and-instances",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "classes and instances",
    "text": "classes and instances\nobjects are defined by instantiating a class. a class is a template for objects, where we define it’s behaviours, an instance is a particular realization of that class.\n\nExample\nwe want model the User of our company’s product, to later study their behaviours:\n\n\nShow the code\nclass User:\n    pass\ntype(User)\n\n\ntype\n\n\nUser class is of type type, which means that is can be used to created new objects. Let’s create 2 instances:\n\n\nShow the code\nuser1 = User()\nuser2 = User()\nprint(user1)\nprint(user2)\n\n\n&lt;__main__.User object at 0x000001B1AD8D4F90&gt;\n&lt;__main__.User object at 0x000001B1AD8D4D90&gt;\n\n\nwith a slight abuse of notation, we would say we have 2 User objects, or just 2 Users.\n\n\nUsing the instance: self\nlet’s add a very simple method to demonstrate explicit self, a very important concept. a method is like a function bounded to the object, an can use it’s properties:\n\n\nShow the code\nclass User:\n    def whoami(self):\n        print(f'This is : {self}')\n\n\n\n\nShow the code\nuser1 = User()\nuser1.whoami()\n\n\nThis is : &lt;__main__.User object at 0x000001B1AD8D4590&gt;\n\n\nwhy are methods (instead of plain functions) interesting? Because of duck typing:\n\n“If it walks like a duck and it quacks like a duck, then it must be a duck” – https://en.wikipedia.org/wiki/Duck_typing\n\nif something has a method that I need, I don’t care about its type.\n\n\nShow the code\ndef do_stuff(obj):\n    return obj.mean()\n\n\n\n\nShow the code\nimport numpy as np\nimport pandas as pd\n\nprint(do_stuff(np.arange(5)))\nprint(do_stuff(pd.Series([0, 1, 2, 3, 4])))\n\n\n2.0\n2.0\n\n\nnotice how we called user1.test() without passing an extra argument? This is because Python is automatically passing the instance. It’s the equivalent of doing this (never do this):\n\n\nShow the code\nUser.whoami(user1)\n\n\nThis is : &lt;__main__.User object at 0x000001B1AD8D4590&gt;\n\n\nin fact, if we define a method without a first parameter, it will fail when we call it:\n\n\nShow the code\nclass TestClass:\n    def test():\n        pass  # Don't do anything\n\nt = TestClass()\nt.test()  # Fails: \"TypeError: test() takes 0 positional arguments but 1 was given\"\n\n\nthis first parameter can be called anything, but everybody uses self. Remember, conventions are important to minimize surprise and enhance collaboration!\n\n\nintermezzo: f-strings\n\n\nShow the code\nprint(f\"This is {user1}\")  # Python &gt;= 3.6\nprint(\"This is {}\".format(user1))  # Python &lt; 3.6, equivalent\nprint(User.whoami(user1))  # DON'T use! (Although it's equivalent)   \n# %timeit User.whoami(user1)  # They have about the same performance \n# %timeit user1.whoami()\n\n\nThis is &lt;__main__.User object at 0x000001B1AD8D4590&gt;\nThis is &lt;__main__.User object at 0x000001B1AD8D4590&gt;\nThis is : &lt;__main__.User object at 0x000001B1AD8D4590&gt;\nNone\n\n\n\n\ninitializing our instances\nThe ellipsis (...) is a built-in constant in Python. It’s an instance of the ellipsis (dấu chấm lửng) class.\n\n\nShow the code\n...\n\n\nEllipsis\n\n\ncommon uses of ellipsis:\n\nas a placeholder in function definitions or class bodies\nin type hinting (especially for variable-length tuples)\nin slicing operations (especially for multidimensional arrays in libraries like NumPy)\n\n\n\nShow the code\n# As a placeholder\ndef function_to_be_implemented_later():\n    ...\n\n# In type hinting\nfrom typing import Tuple\n\ndef process_points(*points: Tuple[int, ...]) -&gt; None:\n    for point in points:\n        print(f\"Processing point: {point}\")\n\n# Usage\nprocess_points((1, 2), (3, 4, 5), (6,))\n\n\nProcessing point: (1, 2)\nProcessing point: (3, 4, 5)\nProcessing point: (6,)\n\n\n\n\nShow the code\nuser1.this_property = ...  # \"99\", whatever\nuser1.this_property\n\n\nEllipsis\n\n\nhowever, this is considered a bad practice, and can confuse editors and static analysis tools. These properties should be specified on creation, in a way that I cannot have a user without name and signup_date. Python provides us a special method, __init__ (this should not be confused with file __init__.py we put to project to tell Python our code is a package), that initializes1 the object:\n1 This philosophy used to be summarized by the sentence “we are all consenting adults here”, which is nowadays being less used.\n\nShow the code\nclass User:\n    # \"dunder init\" = double underscore init\n    def __init__(self, name, signup_date):\n        self.name = name\n        self.signup_date = signup_date\n\n\n\n\nShow the code\nimport datetime as dt\nuser1 = User(name=\"John Doe\", signup_date=dt.datetime.now())\nprint(user1.name, user1.signup_date, sep='\\n')\n\n\nJohn Doe\n2024-08-01 23:07:08.587740\n\n\nthat’s something! However, there are several things we can improve:\n\nit can be cumbersome to specify the date every time, and it would be nice to have some default.\nthe default representation of the instances contains some hexadecimal memory address and nothing else. It would be nice to at least see the user name and the signup date\nnothing stops me from changing the name and signup_date of a existing user:\n\n\n\nexercise\n\nmake signup_date optional by providing a default value;\nmake the __repr__ method return a string containing the name and signup_date, which will override the default.\n\n\n\nShow the code\nclass User:\n    def __init__(self, name, signup_date=None):\n        if signup_date is None:\n            signup_date = dt.datetime.now() # Watch out with default parameters! They are created when the function is defined.\n\n        self.name = name\n        self.signup_date = signup_date\n\n    def __repr__(self):\n        return f\"User(name='{self.name}', signup_date={repr(self.signup_date)})\"\n\n\n\n\nShow the code\nuser1 = User(\"John Doe\")\nuser1\n\n\nUser(name='John Doe', signup_date=datetime.datetime(2024, 8, 1, 23, 7, 8, 618751))\n\n\n\n\nextra: date formatting\n\n\nShow the code\ndt.datetime.now().isoformat()  # ISO 8601\n# If you don't like it, there's http://strftime.org/\n\n\n'2024-08-01T23:07:08.625045'\n\n\n\n\nShow the code\nuser1.signup_date.strftime(\"%Y ::: %d\")\n\n\n'2024 ::: 01'\n\n\n\n\nprotecting properties\nin Python, there are no private attributes (neither properties nor methods), and in fact everything can be accessed 2. However, we can “hide” them by default in autocomplete and other environments by using a leading underscore _: this is usually called protected variables.\n2 This philosophy used to be summarized by the sentence “we are all consenting adults here”, which is nowadays being less used.there is a common pattern in which, if I want to make some property read-only, we can:\n\nmake it protected\ncreate a “getter” using the @property decorator, which gets the value of the protected property with a public name\n\n\n\nShow the code\nclass User:\n    def __init__(self, name, signup_date=None):\n        if signup_date is None:\n            signup_date = dt.datetime.now()\n\n        self._name = name\n        self._signup_date = signup_date\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def signup_date(self):\n        return self._signup_date\n\n    def __repr__(self):\n        return f\"User(name='{self.name}', signup_date='{self.signup_date}')\"\n\n\n\n\nShow the code\nuser1 = User(\"Tuan Le\")\nuser1.name\n\n\n'Tuan Le'\n\n\n\n\nShow the code\n# this wont work: \"AttributeError: can't set attribute\"\nuser1.name = \"Tan Le\"\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you see tutorials mentioning “true private variables”, they are wrong!\n\n\n\n\nShow the code\nclass Test:\n    def __init__(self, name):\n        self.__name = name  # Not what you think!\nt1 = Test(\"This name\")\n\n\n\n\nShow the code\n# this wont work\nt1.__name\n\n\n\n\nShow the code\nt1._Test__name  # These are *NOT* \"private\" properties\n\n\n'This name'\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe behavior you’re experiencing is due to a feature in Python called “name mangling” for private attributes. Let’s break down what’s happening:\nDouble underscore prefix:\n\nWhen you define an attribute with a double underscore prefix (__) in a class, Python automatically mangles the name to avoid naming conflicts in inherited classes.\nName mangling process: Python changes the name from __name to _ClassName__name. In your case, it becomes _Test__name.\nAccessing the attribute: Because of this name mangling, you can’t access t1.__name directly. Instead, you would need to use the mangled name.\n\n\n\nShow the code\nclass Test:\n    def __init__(self, name):\n        self.__name = name  # This gets mangled\n\nt1 = Test(\"This name\")\n\n# This will raise an AttributeError\ntry:\n    print(t1.__name)\nexcept AttributeError as e:\n    print(f\"AttributeError: {e}\")\n\n# This will work\nprint(t1._Test__name)\n\n# You can see all attributes, including mangled ones\nprint(dir(t1))\n\n\nAttributeError: 'Test' object has no attribute '__name'\nThis name\n['_Test__name', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__']"
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#inheritance",
    "href": "blog/2024-07-08-advanced-python/index.html#inheritance",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "inheritance",
    "text": "inheritance\n\n\nShow the code\nclass SpecialUser(User):\n    def __init__(self, name, age, signup_date=None):\n        # Initializes self._name and self._signup_date\n        super().__init__(name, signup_date)\n\n        self._age = age\n\n    @property\n    def age(self):\n        return self._age\n\n    def greet(self):\n        print(f\"Hi! I'm {self.name}\")\n\n\n\n\nShow the code\ns_user1 = SpecialUser(\"John Doe\", 27)\n#s_user1\n\n\n\n\nShow the code\ns_user1.name\n\n\n'John Doe'\n\n\n\n\nShow the code\ns_user1.greet()\n\n\nHi! I'm John Doe\n\n\n\nthe diamond problem: https://www.wikiwand.com/en/Multiple_inheritance#/The_diamond_problem\n???: https://softwareengineering.stackexchange.com/questions/238176/why-would-square-inheriting-from-rectangle-be-problematic-if-we-override-the-set/238184#238184\nliskov substitution principle: https://www.wikiwand.com/en/Liskov_substitution_principle\ncomposition and inheritance: https://www.thedigitalcatonline.com/blog/2014/08/20/python-3-oop-part-3-delegation-composition-and-inheritance/\n\n\nmore special methods\ngo search for python data model"
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#where-do-we-go-from-here",
    "href": "blog/2024-07-08-advanced-python/index.html#where-do-we-go-from-here",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "where do we go from here",
    "text": "where do we go from here\n\nKeep improving the art of Python packaging\nExplore other options for high performance Python\nHelp “bridging the gap”\nEngage with the (open source) Python community"
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#keep-improving-the-art-of-python-packaging",
    "href": "blog/2024-07-08-advanced-python/index.html#keep-improving-the-art-of-python-packaging",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "keep improving the art of python packaging",
    "text": "keep improving the art of python packaging\n\nrecommend: pip-tools + requirements.in = requirements.txt\nPoetry, Pipenv… Yes, they work, but they are way more complex and have “lock-in”\nAll companies I worked for struggle sooner or later with their code deployment practices. Now, you know better"
  },
  {
    "objectID": "blog/2024-07-08-advanced-python/index.html#explore-other-options-for-high-performance-python",
    "href": "blog/2024-07-08-advanced-python/index.html#explore-other-options-for-high-performance-python",
    "title": "A lesson of advanced python from Juan Rodríguez",
    "section": "explore other options for high performance python",
    "text": "explore other options for high performance python\n\nJuan mentioned: Numba, Modin, Vaex, Dask, Spark, Coiled, Prefect\nNow we have more: polar, pandas 2.0, aiflow, dagster, dbt, etc\n\n\n\n\n\n\n\n\n\n\nstolen from the lecture"
  },
  {
    "objectID": "blog/2024-07-12-vim/index.html",
    "href": "blog/2024-07-12-vim/index.html",
    "title": "Vim 101",
    "section": "",
    "text": "source: Vim tutorial for Beginners by freeCodeCamp.\nin an effort not to give Vim up from the very first day. inspired by James Powell’s Vim skill in his every Python lecture.\nafter a few days of rolling with neovim i already have a somewhat calls sense of how to install, make the nvim config folder, do some git clone stuffs to download a bunch of i have no idea what they are to use vim in my damn window machine. can do nvim file1, :qw, :below terminal, :vsplit, but in general lots of actions i am not yet familiar with so actually could not stick with this editor.\nnow back to basics, i am spending Friyay night starting from scratch. Florian Dedov uses vim while i use nvim expecting both to be the same.\n\n\n\n\n\n\n\n\n\nsource programmerhumor\n\n\n\n\n\n\nintro & why vim?\n\nless stick with your mouse, speed up your coding speed;\nall settings are customizable;\ndont need to stick with your terminal, you can use Vim keybindings in any IDE or text editor: Jupyter Notebook, VS Code, etc\n\n\n\ninstallation\n\ni use window, i dont want to install more package manager like scoop, i dont like to use WSL, so just simply run winget install Neovim.Neovim; \nthis neovim version will not include Neovim QT, alright, i will use my terminal pwsh;\n\nneovim installation page\n\nbasics\n\nnvim file_name will open a file or create a file if it did not exist (but the file will not be actually created if we dont write :w ~ this is just buffer);\n\n\n\nexit\n\nnow if we create a file (e.g. client.py), when we press :q in will return this message:\n\nE37: No write since last change\nE162: No write since last change for buffer \"client.py\"   \n\nwhat you need to do is :q! (exclamation mark ~ saying you quite and dismiss any change you’ve made);\nnotice that q first and ! after. :! will open the terminal, so :!q will pass the command q to the terminal (window CMD), which returns “‘q’ is not recognized as an internal or external command, operable program or batch file.”\n\n\n\ninsert and normal modes\n\nwhen we enter vim, we are in NORMAL mode. press the i or I will get you into the INSERT mode, press esc to enter the NORMAL mode;\nINSERT mode: any thing we press does not have functionality, just text;\nNORMAL mode: execute a command;\nin normal mode, press :w to save/write file;\n:wq write and quit;\nin INSERT mode, cursor points between 2 characters; in NORMAL mode cursor points a character;\nfrom INSERT mode, press esc and cursor will point to the up-front character, press i in the cursor will point to before that character, press a and for cursor after character;\nI = shift + i will let you get into insert mode with cursor at the begin of the line;\nA = shift + a for the end of the line;\nfrom normal mode, o let you create a new line below and cursor at that line;\nO = shift + o for creating line above.\n\n\n\nline numbers\n\nin normal mode, :set number will activate line number;\n:set nonumber to turn off the line number;\nnow this is important pattern: “an action can be repeated x times” ~ in normal mode we can move cursor up and down using up and down arrow -&gt; if we press 5 + down arrow we will move to the 5th line below;\n10 + right arrow will move the cursor to 10th character to the right, in the same line (?, or in the end of the line);\nyou can also use h, j, k, l instead of the arrow keys, they stand for ⬅, ⬇, ⬆, ➡, respectively (extremely usefull if you can type with 10 fingers);\n\n\n\nrelative line numbers\n\ncertainly you can use number with h, j, k, l. now if you :set relativenumber, you can have your current line indexed as 0 and easily know the number to move up & down-ward;\n\n\n\n\n\n\n\n\n\n\nnow if i type 14 + k i will move to the 3rd line\n\n\n\n\n\n\neasy to know that :set norelativenumber will turn this option off.\n\n\n\nvarious options\n\n:set mouse=a activate the mouse, you can scroll or select text;\n:set mouse-=a to inactivate, the pattern -= is to inactivate any option;\n:set tabstop=4 set tab as 4 spaces\n:set shiftwidth=4 set shift width as 4 spaces \n:colorscheme slate to set the color scheme, before type “state” you can tab to select the scheme too (remember the space);\n\nthis is recommended\n\n\n\n\n\n\n\n\n:colorscheme and tab to select color scheme\n\n\n\n\n\n\nevery single time you close and re-open vim, these settings is by far - all gone.\n\n\n\n.vimrc\n\nso you need a configuration file and every time vim or nvim is opened your config will be loaded;\nneovim will lookfor init.vim or init.lua when start; \nin terminal (pwsn in user folder window), input:\n\nrefer herecd %AppData%\ncd .. # to get out of Roaming folder\ncd .\\Local\\\nmkdir nvim\ncd nvim\nnvim init.vim\n\nthen i set these config in my init.vim file:\n\nset number\nset relativenumber\nset tabstop=4\nset shiftwidth=4\nset autoindent\nset mouse=a\ncolorscheme slate\n\nthen :wq. now we’ve already have some handy settings when go into nvim.\n\n\n\nkeybindings\n\nhelpful keybindings\n\n\n\n\n\n\nKeybindings\nActions\n\n\n\n\nu\nundid the actions, vim show “… change; before #x xx seconds ago”\n\n\nCtrl + R\nredo\n\n\nntimes u\nntimes Ctrl + R\nrepeat to press u for multiple undo, or you can 3 + u for 3 times undo. same for redo\n\n\ni, I, a, A, o, O\nback to insert mode in different ways\n\n\nv\nenter VISUAL mode\n\n\nV\nenter VISUAL model and select the whole line\n\n\nd\ndeleting\n\n\ny\nyanking ~ equivalent to copying\n\n\np\npasting what we yank before (after or below the cursor)\n\n\ndd\ndelete the whole line\n\n\n5dd\ndelete the next whole 5 lines\n\n\nD\ndelete from the cursor to the end of line\n\n\nyy\nyank the full line in NORMAL mode, same with V then y ~ select the whole line and yank\n\n\nY\nsame at yy\n\n\nP\npasting what we yank before (before or above the cursor)\n\n\n\n\n\n\nTip\n\n\n\nagain remember the pattern of lowercase and uppercase\n\n\n\n\nc\ndelete the selection (from VISUAL mode) and enter to INSERT mode\n\n\ncc\ndelete the whole text in the line (the line was kept) enter to INSERT mode\n\n\nC\ndelete from cursor to end of line (what c does is change, what d does is delete)\n\n\nr\nreplacing the current cursor in NORMAL mode with 1 character press after r\nreplacing all selection (include cursor) in VISUAL mode\n\n\nw\nJump to the next word (split by space or “-”)\n\n\nW\nJump to the next word (accept only space at delimiter)\n\n\nb, B\nThe same thing for backward\n\n\ndw\ndelete a word\n\n\n2dw , d2w\ndelete 2 words (forward, current cursor and the next one)\n\n\n2db, d2b\ndelete 2 words (backward, before the cursor)\n\n\ndiw\ndelete in a words\n\n\nciw , cw, cb\nchange in a words, next word, previous word and go into INSERT mode\n\n\ne\nJump to the end of the word (E will be more strictly, only accept space as delimiter)\n\n\n0, $\nJump to the begin / and the end of the line respectively\n\n\nd0, d$\ndelete everything from cursor to the begin / end of the line, respectively\n-&gt; think of c0, and c$ for aha\n\n\n:h ...\nseek help, :q to quite the help window\n\n\n\n\n\nintermediate stuff\ngo and cook your meal:\n\ni - before and I - beginning, a - after and A - ending for navigating to INSERT mode;\no - below, O - above to insert line;\nu - undo, Ctrl + R - redo;\nv - individual visual, V - visual the whole line;\ny - yanking current, Y, yy - yanking whole line;\np - pasting after or below, P - pasting before or above;\nd - deleting current, dd - deleting whole line, D - deleting to the end of line;\nc - same with d but for changing;\nr - replacing current with 1 character, R - enter the REPLACE mode and replace with multiple characters;\nw, W - jump word forward, b, B - jump word backward;\ni - inside the word, e - end of the word;\n0 - begin, $ - end of the line;\nrepeat by press key multiple times or n - number before command;\n% - jump to the closing bracket if you are on the opening one;\nt follow by a character will let you jump cursor to before the next nearest one;\nf same with t, but to the character’s position;\nT and F for the backward;\ngg - go to the begin of the line, G for the end of the file;\n123G or :123 will bring you to the line 123; ….\n\nnow:\n\nciw - changing the whole word; but\ncib , ciB - changing the text inside the current set of parenthesis () or braces {}, respectively, you can also ci( , ci{ , ci&lt; for the specific.\nci\" - changing the text inside double quote;\nif you want delete d, or yank y - replace the c;\n5dw will delete 5 words, 5d5w will delete 5 words 5 times!;\n5yy copying 5 lines;\ndt( deleting everything up till the opening bracket, df( will delete the bracket also; …\n\neven more advanced stuff:\n\nindentation: &gt;&gt; to the right, &lt;&lt; to the left;\nV for Visual Line mode, which will automatically select entire lines;\nCtrl + v for Block Visual mode, which will select rectangular regions of the text;\n= for auto indentation;\ngg=G will start at the begin of the file, auto indentation till the end of file (and end of at the end of file);\n/ and word follow to search the word, then n to jump to the next found, N to jump to the previous one;\n\n\n\n\n\n\n\nNote\n\n\n\nRe-note about the search:\n\n/pattern - search for pattern;\n?pattern - search backward for pattern;\n\\vpattern - ‘very magic’ pattern: non-alphanumeric characters are interpreted as special regex symbols (no escaping needed);\nn - repeat search in same direction;\nN - repeat search in opposite direction;\n# go up, * go down.\n\n\n\n\nma: mark A -&gt; to explore later;\nzz: centre the screen;\n:%s/old/new/g - replace all old with new throughout file;\n:%s/old/new/gc - replace all old with new throughout file with confirmations (without c is without confirmation);\n. repeat the last command;\n\"+ and \"* are the special registers, which you copied in the system\n\n\n\nregisters and macros\n\nd deleting is also copying ~ cutting, so if you y then d you will lost the current clipboard\n:reg to see the history of the clipboard;\n\"7p will paste the 7th register;\n\"7yy yanks the whole line into 7th register;\n\"0p the last thing that i actually yanked, not d;\nqa recording macro @a, q quits recording;\nyou can see the macro in :reg, @a to use the macro; …\n\n\n\nneovim & plugins\n\nbetter, have more pluggins supported;\nconfig in ./nvim using init.vim or init.lua;\n\n\n\nvim support\n\ncan use vim bindings in VS Code, Pycharm\n\n\n\noutro\n\nit is recommended to use vim bindings while coding, even if you dont like the command line.\n\n\n\nother resources\n\nVim cheat sheet\nVim plug"
  },
  {
    "objectID": "blog/2024-07-18-claude-summary/index.html",
    "href": "blog/2024-07-18-claude-summary/index.html",
    "title": "Another James Powell’s impromptu talk, summary by Clauder 3.5 Sonnet",
    "section": "",
    "text": "source: James Powell - Whatever I Can do to Entertain You in 30 Minutes | PyData London 2022\n\n\n\n\n\n\n\n\n\nWhatever I Can do to Entertain You in 30 Minutes\n\n\n\n\n\ni was using this extension Language Reactor to export transcript into Excel and convert it to json. here is data in short:\n{   \n    \"5s\": \"okay let me get started i'll give you the\",\n    \"7s\": \"premise the premise is the issues are not av related the issues are that my\",\n    \"12s\": \"laptop where i was up until 4am last night writing this talk\",\n    \"16s\": \"did not boot at all and as a consequence of that my notes that i spent quite a\",\n    \"21s\": \"bit of time putting together are not available to you but i am still on the\",\n    \"24s\": \"hook to keep you entertained for 30 minutes so we're going to see what i can\",\n    \"...\": \"...\",\n    \"25:55\": \"that makes it possible for us to write code that\",\n    \"25:58\": \"put some structuring around the core computation now with that said i hope i\",\n    \"26:03\": \"was able to entertain you i think we're just at the time that we have\",\n    \"26:07\": \"thank you everybody i'm james powell hopefully i'll give the real talk\",\n}\nand this is summary by Clauder AI:\nJames Powell’s impromptu talk at PyData London 2022 covers several key points about Python programming, particularly aimed at data scientists. Here are the main points he’s trying to convey:\n\nPython is more than just APIs and function calls: Many data scientists focus solely on memorizing APIs (like Pandas or NumPy) without truly understanding the language’s depth and richness.\nGenerators are underutilized: Generators can be powerful tools, especially for simulations or non-closed form operations where each incremental step is computationally expensive.\nLazy vs. Eager computation: Understanding when to use lazy or eager computation can significantly impact performance, especially in data science tasks.\nFunctions should not be overused: There’s a tendency to create functions for any repeated code, but this isn’t always the best approach. Powell distinguishes between intentional and coincidental repetition.\nAvoid premature abstraction: Creating functions deep in libraries, especially for data loading, can lead to issues with testing and maintenance. Sometimes, ad-hoc behavior is preferable.\nIteration helpers and intentionality: Tools like enumerate, zip, and custom iteration helpers can make code more expressive and reveal the programmer’s intentions more clearly.\nUnderstanding Python conventions: Looking beyond the surface-level functionality of Python’s standard library (e.g., itertools) can reveal patterns and conventions that lead to better code.\nCode should express human intention: Loops in data science code often represent human-level processes rather than low-level computations. The code structure should reflect this.\nFlexibility in code structure: Allowing code to diverge when necessary, rather than forcing premature unification, can lead to more maintainable and adaptable codebases.\nCritical thinking about basic tools: Even when using basic Python constructs, data scientists should think deeply about why they’re using them and what alternatives might exist.\n\nOverall, Powell encourages data scientists to look beyond superficial knowledge of Python and its libraries, urging them to understand the language more deeply and use its features more thoughtfully to write more intentional, maintainable, and expressive code."
  },
  {
    "objectID": "blog/2024-07-30-docker-init/index.html",
    "href": "blog/2024-07-30-docker-init/index.html",
    "title": "Now I know docker init is a thing",
    "section": "",
    "text": "TIL that Docker has an example for containerizing Python apps. It uses a simple FastAPI example for demonstration. We can download the project as below:\ngit clone https://github.com/estebanx64/python-docker-example\nAfter navigating to our project, we can easily set up our project for containerization with docker init. This will create:\n\n.dockerignore\nDockerfile\ncompose.yaml\nREADME.Docker.md\n\ndocker init\n\nWelcome to the Docker Init CLI!\n\nThis utility will walk you through creating the following files with sensible defaults for your project:\n  - .dockerignore\n  - Dockerfile\n  - compose.yaml\n  - README.Docker.md\n\nLet's get started!\n\n? What application platform does your project use? Python\n? What version of Python do you want to use? (3.11.4)\n\n? What version of Python do you want to use? 3.11.4\n? What port do you want your app to listen on? (8000)\n\n? What port do you want your app to listen on? 8000\n? What is the command you use to run your app? (uvicorn 'app:app' --host=0.0.0.0 --port=8000)\n\n? What is the command you use to run your app? uvicorn 'app:app' --host=0.0.0.0 --port=8000\n\n✔ Created → .dockerignore\n✔ Created → Dockerfile\n✔ Created → compose.yaml\n✔ Created → README.Docker.md\n\n→ Your Docker files are ready!\n  Review your Docker files and tailor them to your application.\n  Consult README.Docker.md for information about using the generated files.\n\n! Warning → Make sure your requirements.txt contains an entry for the uvicorn package, which is required to run your application.\n\nWhat's next?\n  Start your application by running → docker compose up --build\n  Your application will be available at http://localhost:8000\nI can access the application right after running docker compose up --build. The image buidling process was fast, and image size was only 203.21MB.\nLet’s take a look at the Dockerfile. IMHO, this is not yet a optimal Dockerized Python project:\n\nLack off a dependency management tool (for e.g, pipenv, poetry both are good, production-ready);\nThis is single-step builder. Should we split it to dependencies and runtime steps, which will limit the objects in the runtime image to only those needed to run the application? \n\nfurther reading, “A perfect way to Dockerize your Pipenv Python application”I do see some good practices here that we should run our apps in a non-privileged user rather than root as well as mount cache and bind, using Docker’s BuildKit feature, which allows more advanced mounting capabilities during build time:\n\nfirst --mount=type=cache,target=/root/.cache/pip option:\n\n\nThis creates a cache mount for pip’s cache directory;\nIt speeds up subsequent builds by reusing cached pip packages;\nThe cache persists between builds, saving time and bandwidth.\n\n\nsecond --mount=type=bind,source=requirements.txt,target=requirements.txt option:\n\n\nThis creates a bind mount for the requirements.txt file;\nIt allows access to the requirements.txt file without copying it into the image layer;\nThis is useful for keeping the image size smaller and allowing changes to requirements.txt without rebuilding all layers.\n\nIn conclusion, benefits are:\n\nFaster builds: By using a cache mount for pip, subsequent builds can reuse cached packages, significantly speeding up the process.\nSmaller image size: The bind mount for requirements.txt means the file doesn’t need to be copied into the image, keeping the image size smaller.\nBetter caching: Changes to requirements.txt don’t invalidate the entire layer cache, only the parts that have changed.\nSeparation of concerns: Downloading dependencies is done as a separate step, which can be beneficial for Docker’s layer caching mechanism.\n\n# syntax=docker/dockerfile:1\n\n# Comments are provided throughout this file to help you get started.\n# If you need more help, visit the Dockerfile reference guide at\n# https://docs.docker.com/go/dockerfile-reference/\n\n# Want to help us make this template better? Share your feedback here: https://forms.gle/ybq9Krt8jtBL3iCk7\n\nARG PYTHON_VERSION=3.11.4\nFROM python:${PYTHON_VERSION}-slim as base\n\n# Prevents Python from writing pyc files.\nENV PYTHONDONTWRITEBYTECODE=1\n\n# Keeps Python from buffering stdout and stderr to avoid situations where\n# the application crashes without emitting any logs due to buffering.\nENV PYTHONUNBUFFERED=1\n\nWORKDIR /app\n\n# Create a non-privileged user that the app will run under.\n# See https://docs.docker.com/go/dockerfile-user-best-practices/\nARG UID=10001\nRUN adduser \\\n    --disabled-password \\\n    --gecos \"\" \\\n    --home \"/nonexistent\" \\\n    --shell \"/sbin/nologin\" \\\n    --no-create-home \\\n    --uid \"${UID}\" \\\n    appuser\n\n# Download dependencies as a separate step to take advantage of Docker's caching.\n# Leverage a cache mount to /root/.cache/pip to speed up subsequent builds.\n# Leverage a bind mount to requirements.txt to avoid having to copy them into\n# into this layer.\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=requirements.txt,target=requirements.txt \\\n    python -m pip install -r requirements.txt\n\n# Switch to the non-privileged user to run the application.\nUSER appuser\n\n# Copy the source code into the container.\nCOPY . .\n\n# Expose the port that the application listens on.\nEXPOSE 8000\n\n# Run the application.\nCMD uvicorn 'app:app' --host=0.0.0.0 --port=8000\nHappy coding!"
  },
  {
    "objectID": "blog/2024-08-08-note-muln/index.html",
    "href": "blog/2024-08-08-note-muln/index.html",
    "title": "Note on Mathematics as the Universal Language of Nature",
    "section": "",
    "text": "mình nghe bài nói chuyện “Mathematics as the Universal Language of Nature” của R. Seiringer, tổ chức tại Trường Đại học Sư phạm Huế, ngày 8 tháng 8 năm 2024. Dưới đây là note của mình.\n\n\n\n\n\n\n\n\n\ntalk cover\n\n\n\n\n\nRobert là 1 giáo sư chuyên, đầu ngành về Toán và Vật lý, từng giảng dạy tại Princeton, hiện nay công tác tại Viện Kỹ Thuật và Khoa học của Áo. Bài giảng này của giáo sư nhằm nêu bật vẻ đẹp của Toán và Vật lý, dùng Toán để giải thích tự nhiên.\n\nHistory\n\nNewton: calculus, motion of earth around the sun, theory of gravity;\nFourier: Fourier series to understand hear transfer & conduction (also green house effect!);\nRieman: thuyết tương đối tổng quát, lý thuyết trường lượng tử, lý thuyết dây.\n\n\n\n\n\n\n\ngreen house effect\n\n\n\n\n\nJean-Baptiste Joseph Fourier, nhà toán học và vật lý học người Pháp, là người đầu tiên mô tả hiệu ứng nhà kính (greenhouse effect) vào năm 1824. Hãy giải thích về khái niệm này:\n\nĐịnh nghĩa: Hiệu ứng nhà kính của Fourier đề cập đến quá trình mà khí quyển của Trái Đất giữ nhiệt, làm cho bề mặt hành tinh ấm hơn so với trường hợp không có khí quyển.\nCơ chế cơ bản:\n\n\nBức xạ mặt trời đi qua khí quyển và được hấp thụ bởi bề mặt Trái Đất.\nTrái Đất phát ra bức xạ hồng ngoại (nhiệt).\nMột phần bức xạ này bị các khí nhà kính trong khí quyển hấp thụ và tái phát ra theo mọi hướng.\nQuá trình này giữ nhiệt trong khí quyển, làm tăng nhiệt độ trung bình của Trái Đất.\n\n\nĐóng góp của Fourier:\n\n\nFourier nhận ra rằng khí quyển đóng vai trò như một “tấm chăn”, giữ nhiệt cho Trái Đất.\nÔng so sánh quá trình này với cách hoạt động của nhà kính trồng cây.\n\n\nTầm quan trọng:\n\n\nHiệu ứng nhà kính tự nhiên làm cho Trái Đất có thể ở được, duy trì nhiệt độ trung bình khoảng 15°C thay vì -18°C nếu không có nó.\nHiểu biết này đặt nền móng cho nghiên cứu về biến đổi khí hậu hiện đại.\n\n\nPhát triển sau này:\n\n\nJohn Tyndall (1859) xác định các khí cụ thể gây ra hiệu ứng nhà kính.\nSvante Arrhenius (1896) tính toán định lượng ảnh hưởng của CO2 đối với nhiệt độ Trái Đất.\n\n\nKhác biệt với hiểu biết hiện đại:\n\n\nFourier chưa biết về các khí nhà kính cụ thể hoặc cơ chế phân tử của quá trình này.\nHiểu biết hiện đại về hiệu ứng nhà kính phức tạp hơn nhiều, bao gồm các phản hồi và tương tác khí hậu.\n\n\nÝ nghĩa lịch sử:\n\n\nCông trình của Fourier đặt nền móng cho khoa học khí hậu hiện đại và hiểu biết của chúng ta về cách Trái Đất duy trì nhiệt độ.\n\nHiệu ứng nhà kính của Fourier là một ví dụ tuyệt vời về cách một khái niệm khoa học cơ bản có thể dẫn đến những hiểu biết sâu sắc về hệ thống Trái Đất và cuối cùng ảnh hưởng đến chính sách toàn cầu về biến đổi khí hậu.\n\n\n\n\n\nMath Desc. of Laws of Nature\nthe unreasonable effectiveness of math in the natural sciences (Eugene P. Wigner), formulation of the basic laws of nature in mathematically precise terms.\nví dụ: Phương trình Schrodinger -&gt; áp dụng từ atomic nuclei to neutron stars.\n\n\n\n\n\n\nPhương Trình Schrödinger: Cánh Cửa Vào Thế Giới Lượng Tử\n\n\n\n\n\nPhương trình Schrödinger là một phương trình vi phân từng phần cơ bản trong cơ học lượng tử, được đặt tên theo nhà vật lý Erwin Schrödinger. Nó mô tả sự biến đổi theo thời gian của hàm sóng của một hệ lượng tử. Hàm sóng này chứa đựng tất cả thông tin về trạng thái của hệ thống, bao gồm cả vị trí, động lượng và các tính chất khác.\nÝ nghĩa Vật lý\n\nMô tả sự chuyển động của các hạt ở cấp độ lượng tử: Nếu trong cơ học cổ điển, chúng ta sử dụng các phương trình Newton để mô tả chuyển động của các vật thể, thì trong cơ học lượng tử, phương trình Schrödinger đóng vai trò tương tự. Nó cho phép chúng ta tính toán xác suất tìm thấy một hạt tại một vị trí nào đó tại một thời điểm nhất định.\nHàm sóng: Hàm sóng không phải là một đại lượng vật lý có thể đo trực tiếp, mà là một hàm toán học phức. Bình phương mô đun của hàm sóng tại một điểm trong không gian cho ta xác suất tìm thấy hạt tại điểm đó.\n\nỨng dụng của Phương Trình Schrödinger. Phương trình Schrödinger có ứng dụng rộng rãi trong nhiều lĩnh vực của vật lý, bao gồm:\n\nVật lý nguyên tử: Dùng để mô tả cấu trúc của nguyên tử, phân tử và tính toán các mức năng lượng của chúng.\nVật lý hạt nhân: Áp dụng để nghiên cứu cấu trúc của hạt nhân nguyên tử và các quá trình tương tác hạt nhân.\nVật lý chất rắn: Được sử dụng để mô tả các tính chất điện, từ, và quang học của vật liệu.\nVật lý thiên văn: Áp dụng để nghiên cứu các vật thể thiên văn như sao, tinh vân và lỗ đen.\n\nTại Sao Phương Trình Schrödinger Áp Dụng Rộng Rãi?\n\nTính phổ quát: Phương trình Schrödinger là một phương trình cơ bản, áp dụng cho mọi hệ lượng tử, từ các hạt đơn lẻ đến các hệ phức tạp như nguyên tử, phân tử và vật liệu rắn.\nĐộ chính xác cao: Các kết quả tính toán dựa trên phương trình Schrödinger đã được kiểm nghiệm thực nghiệm một cách chính xác cao, khẳng định tính đúng đắn của lý thuyết lượng tử.\nKhả năng dự đoán: Phương trình Schrödinger cho phép chúng ta dự đoán kết quả của các thí nghiệm lượng tử, từ đó mở ra nhiều ứng dụng trong công nghệ hiện đại.\n\nTừ Nguyên Tử đến Sao Neutron\n\nNguyên tử: Phương trình Schrödinger được sử dụng để mô tả sự chuyển động của electron xung quanh hạt nhân, từ đó giải thích quang phổ nguyên tử và các tính chất hóa học của các nguyên tố.\nHạt nhân: Phương trình Schrödinger (ở dạng tương đối tính) được sử dụng để mô tả cấu trúc của hạt nhân nguyên tử, các tương tác giữa các nucleon (proton và neutron) bên trong hạt nhân.\nSao neutron: Sao neutron là những ngôi sao đã chết, cực kỳ đặc, chủ yếu gồm các neutron. Mặc dù cấu trúc bên trong của sao neutron rất phức tạp, nhưng các nhà vật lý vẫn sử dụng các phiên bản mở rộng của phương trình Schrödinger để mô tả hành vi của vật chất trong điều kiện cực đoan bên trong sao neutron.\n\nTóm lại, phương trình Schrödinger là một công cụ toán học mạnh mẽ, cho phép chúng ta hiểu sâu sắc về thế giới lượng tử. Sự thành công của phương trình này trong việc giải thích các hiện tượng từ cấp độ nguyên tử đến cấp độ vũ trụ đã khẳng định vị trí trung tâm của nó trong nền tảng của vật lý hiện đại.\n\n\n\n\n\nTechnology Development\nHow quantum mechanic works, it’s understanding from human lies at the heart of technological advances in the past century\n\nphone\ncomputer\nsatellite\n…\n\n\n\nComplexity\nthe more basic physical laws are stated, the more complex be allowed for calculating their solution.\neg: description of the air in this lecture room, there will me rought ly 10^30 parameters\neg: phase transitions -&gt; one small change in system’s params lead to a masive change of its behaviour.\nread more: “More is Different - broken symertry and the nature of hierarchical structure of science” – P. W. Anderson – (link).\n\n\nAims of Mathematical Physics\n\ndev new math tools, allowing deduction of …\nimprove understanding of physical systems, dưới nhiều góc nhìn hơn.\n\neg: fruitful interaction, Toán là nền tảng, Lý thể hiện các câu hỏi một cách tự nhiên để mang lại sự phát triển cho sự khám phá Toán học.\n\n\nStability of Atoms\nTại sao electron không spiral vào nguyên tử, giải phóng một năng lượng vô hạn.\n\nHeisenberg uncertainty principle\nIn math, non-commutativity implies…\n\n\n\n\nstability of atoms\n\n\n? electromagtic radiation\n\n\n\n\n\n\nelectromagtic radiation\n\n\n\n\n\nBức xạ điện từ là một dạng năng lượng truyền đi dưới dạng sóng điện từ. Sóng điện từ này bao gồm cả trường điện từ và trường từ trường dao động vuông góc với nhau và vuông góc với phương truyền sóng.\nĐặc điểm chính của bức xạ điện từ:\n\nTốc độ: Tất cả các sóng điện từ đều truyền đi với tốc độ ánh sáng trong chân không (khoảng 3 x 10^8 m/s).\nTần số và bước sóng: Các sóng điện từ khác nhau bởi tần số (số lần dao động trong một giây) và bước sóng (khoảng cách giữa hai đỉnh sóng liên tiếp).\nPhổ điện từ: Phổ điện từ là một dải liên tục các loại bức xạ điện từ, từ sóng vô tuyến có bước sóng dài đến tia gamma có bước sóng ngắn.\nTính chất sóng-hạt lưỡng tính: Bức xạ điện từ thể hiện cả tính chất sóng (giao thoa, nhiễu xạ) và tính chất hạt (hiệu ứng quang điện).\n\nCác loại bức xạ điện từ phổ biến:\n\nSóng vô tuyến: Sử dụng trong truyền thông, radio, truyền hình.\nVi sóng: Sử dụng trong lò vi sóng, radar.\nTia hồng ngoại: Tạo ra nhiệt, được sử dụng trong điều khiển từ xa, thiết bị nhìn đêm.\nÁnh sáng khả kiến: Ánh sáng mà mắt người có thể nhìn thấy.\nTia tử ngoại: Sử dụng trong khử trùng, sản xuất vitamin D.\nTia X: Sử dụng trong y tế để chụp X-quang.\nTia gamma: Có năng lượng rất cao, được sử dụng trong điều trị ung thư.\n\nỨng dụng của bức xạ điện từ: Bức xạ điện từ có rất nhiều ứng dụng trong cuộc sống hàng ngày và trong các lĩnh vực khoa học, công nghệ. Ví dụ:\n\nTruyền thông: Điện thoại, truyền hình, internet.\nY tế: Chụp X-quang, điều trị ung thư.\nCông nghiệp: Hàn, cắt kim loại.\nQuân sự: Radar, sóng âm.\n\n\n\n\n\n\n\n\n\n\nTính Không Giao Hoán (Non-Commutativity) trong Toán học\n\n\n\n\n\nTính không giao hoán là một khái niệm quan trọng trong toán học, đặc biệt trong đại số trừu tượng. Nó mô tả một tính chất của các phép toán, khi mà việc đổi chỗ các đối tượng trong phép toán đó lại cho kết quả khác nhau.\nVí dụ minh họa:\n\nPhép nhân số: Phép nhân số là một phép toán giao hoán. Ví dụ: 2 x 3 = 3 x 2.\nPhép trừ số: Phép trừ số không phải là phép toán giao hoán. Ví dụ: 5 - 3 ≠ 3 - 5.\nPhép nhân ma trận: Phép nhân ma trận nói chung không giao hoán. Tức là, nếu A và B là hai ma trận, thì thường AB ≠ BA.\n\nỨng dụng của tính không giao hoán:\n\nĐại số trừu tượng: Tính không giao hoán xuất hiện trong nhiều cấu trúc đại số như nhóm, vành, trường.\nVật lý lượng tử: Trong cơ học lượng tử, các toán tử đại diện cho các đại lượng vật lý thường không giao hoán. Điều này dẫn đến nguyên lý bất định Heisenberg, một trong những nguyên lý cơ bản của vật lý lượng tử.\nKhoa học máy tính: Tính không giao hoán xuất hiện trong các cấu trúc dữ liệu như hàng đợi, ngăn xếp, và trong các ngôn ngữ lập trình.\n\nÝ nghĩa của tính không giao hoán:\n\nThứ tự quan trọng: Khi thực hiện các phép toán không giao hoán, thứ tự các đối tượng đóng vai trò quyết định trong việc xác định kết quả.\nMô hình hóa các hệ thống phức tạp: Tính không giao hoán cho phép mô hình hóa các hệ thống phức tạp trong đó các yếu tố tương tác với nhau theo một cách không đối xứng.\n\nVí dụ cụ thể trong vật lý lượng tử:\nTrong cơ học lượng tử, vị trí (x) và động lượng (p) của một hạt là hai đại lượng vật lý không giao hoán. Quan hệ không giao hoán này được biểu diễn bởi công thức:\n\\([x, p] = xp - px = iħ\\)\nTrong đó:\n\n\\(x\\) là toán tử vị trí\n\\(p\\) là toán tử động lượng\n\\(ħ\\) là hằng số Planck rút gọn\n\\(i\\) là đơn vị ảo\n\nCông thức trên cho thấy, việc đo vị trí và động lượng của một hạt cùng một lúc sẽ có độ chính xác bị giới hạn. Đây chính là nội dung của nguyên lý bất định Heisenberg.\nTóm lại, tính không giao hoán là một khái niệm quan trọng trong toán học và vật lý, giúp chúng ta hiểu rõ hơn về sự tương tác giữa các đối tượng và các hệ thống phức tạp.\n\n\n\n\n\nStability of Matter\nnếu nguyên tử là ổn định, điều gì xảy ra nếu chúng ta assemble chúng lại với nhau, như 10^30 nguyên tử trong 1 viên gạch. việc 1 viên gạch tại sao lớn như vậy, chứ không thể đặc hơn, khó có thể coi là hiển nhiên, và uncertainty principle ko thể tự mình giải thích.\nextensitivity: electrons là fermions, thỏa Pauli exclusion principle.\nwhy a brick is a brick?\nbook recommendation: “Stability of Matter in Quantum Mechanics”\n\n\n\n\n\n\nFermions and Bosons\n\n\n\n\n\nThese are two fundamental types of particles in physics. They differ in their quantum properties.\n\nFermions: These particles obey the Pauli Exclusion Principle, which means that no two identical fermions can occupy the same quantum state simultaneously. Examples of fermions include electrons, protons, and neutrons.  \nBosons: Unlike fermions, multiple bosons can occupy the same quantum state. This property is essential for phenomena like superconductivity and laser light. Examples of bosons include photons (particles of light) and the Higgs boson.\n\n\n\n\n\n\nGravitational (In) Stability of Stars\nscale up to Stars, where the gravitation becomes a major factor -&gt; can be taken in to account.\nChandrasekhar limit - for the maximun mass of stable white dwarf (amounts roughly 1.4 solar masses)\nSchrodinger equation can be applied to a huge range of scalces, from atoms to stars!\n\n\nPhase Transitions\nphương trình cơ bản có dạng đơn giản, nhưng lời giải có thể ở rất nhiều dạng do có rất nhiều biến và tham số.\nví dụ như nước, có thể khí, lỏng và rắn - phase transitions.\n\n\nMore Phase Transitions\n\nmagnetism;\ntraffic jams: hociminh city, tiny change can make huge effect?;\nsuperfluidity (siêu chảy, một trạng thái “kỳ lạ” - exotic của vật chất);\nsuperconductivity (siêu dẫn).\n\n\n\n\n\n\n\nsuperfluidity\n\n\n\n\n\n\nĐịnh nghĩa siêu chảy: Siêu chảy là trạng thái của vật chất, trong đó chất lỏng có độ nhớt bằng không và chảy mà không có ma sát. Điều này xảy ra ở nhiệt độ cực thấp, gần bằng không tuyệt đối.\nĐặc điểm chính:\n\n\nKhông có ma sát nội: Chất lỏng siêu chảy có thể chảy mà không mất năng lượng.\nHiệu ứng phun tia: Có thể leo lên thành bình chứa.\nDẫn nhiệt vô hạn: Truyền nhiệt cực kỳ hiệu quả.\nXoáy lượng tử: Tạo ra các xoáy với động lượng góc lượng tử hóa.\n\n\nVí dụ về chất siêu chảy:\n\n\nHelium-4 lỏng dưới 2.17 K (điểm lambda).\nHelium-3 lỏng dưới khoảng 0.0025 K.\n\nTại sao gọi là “exotic” (kỳ lạ):\n\nHiếm gặp trong tự nhiên:\n\n\nChỉ xuất hiện ở nhiệt độ cực thấp, hiếm khi gặp trong điều kiện tự nhiên.\n\n\nVi phạm trực giác thông thường:\n\n\nChảy mà không có ma sát, trái ngược với hầu hết các chất lỏng thông thường.\nCó thể “leo” ra khỏi bình chứa, dường như vi phạm trọng lực.\n\n\nHành vi lượng tử vĩ mô:\n\n\nHiển thị các hiệu ứng lượng tử ở quy mô có thể quan sát được.\nThể hiện tính chất sóng-hạt của vật chất ở mức độ lớn.\n\n\nLiên quan đến vật lý cơ bản:\n\n\nNghiên cứu siêu chảy giúp hiểu sâu hơn về cơ học lượng tử và vật lý trạng thái ngưng tụ.\n\n\nỨng dụng tiềm năng độc đáo:\n\n\nCó thể được sử dụng trong các thiết bị làm lạnh siêu dẫn và các ứng dụng công nghệ cao khác.\n\nTóm lại, siêu chảy được coi là “exotic” vì nó thể hiện các tính chất kỳ lạ, hiếm gặp và vi phạm trực giác thông thường về cách chất lỏng hoạt động. Nó là một ví dụ về cách các hiệu ứng lượng tử có thể biểu hiện ở quy mô vĩ mô, làm cho nó trở thành một chủ đề thú vị và quan trọng trong vật lý hiện đại.\n\n\n\n\n\n\n\n\n\nsuperconductivity\n\n\n\n\n\n\nĐịnh nghĩa: Siêu dẫn là trạng thái của vật chất trong đó điện trở suất của vật liệu giảm xuống bằng không và từ trường bị đẩy ra khỏi vật liệu (hiệu ứng Meissner) khi nhiệt độ giảm xuống dưới một nhiệt độ tới hạn nhất định.\nĐặc điểm chính:\n\n\nĐiện trở bằng không: Dòng điện có thể chảy mà không mất năng lượng.\nHiệu ứng Meissner: Từ trường bị đẩy ra khỏi vật liệu siêu dẫn.\nNhiệt độ tới hạn (Tc): Nhiệt độ mà dưới đó vật liệu trở thành siêu dẫn.\n\n\nLoại siêu dẫn:\n\n\nSiêu dẫn loại I: Thường là kim loại nguyên chất, chuyển đột ngột sang trạng thái siêu dẫn.\nSiêu dẫn loại II: Thường là hợp kim hoặc hợp chất phức tạp, có hai nhiệt độ tới hạn và có thể duy trì trạng thái siêu dẫn trong từ trường mạnh hơn.\n\n\nCơ chế:\n\n\nLý thuyết BCS (Bardeen-Cooper-Schrieffer): Giải thích siêu dẫn thông qua sự hình thành các cặp Cooper (electron ghép đôi).\nSiêu dẫn nhiệt độ cao: Cơ chế chưa được hiểu đầy đủ, là một lĩnh vực nghiên cứu đang phát triển.\n\n\nỨng dụng:\n\n\nNam châm siêu dẫn mạnh cho MRI và máy gia tốc hạt.\nMáy dò SQUID (Superconducting Quantum Interference Device) để đo từ trường cực nhỏ.\nTiềm năng trong máy tính lượng tử và truyền tải điện không tổn thất.\n\n\nThách thức:\n\n\nTìm kiếm vật liệu siêu dẫn ở nhiệt độ cao hơn, lý tưởng là ở nhiệt độ phòng.\nHiểu rõ hơn về cơ chế của siêu dẫn nhiệt độ cao.\n\n\nLịch sử và phát triển:\n\n\nPhát hiện năm 1911 bởi Heike Kamerlingh Onnes trong thủy ngân ở 4.2K.\nLý thuyết BCS được đề xuất năm 1957.\nSiêu dẫn nhiệt độ cao được phát hiện năm 1986 bởi Bednorz và Müller.\n\nSiêu dẫn là một lĩnh vực nghiên cứu đang phát triển mạnh mẽ trong vật lý hiện đại, với tiềm năng ứng dụng rộng rãi trong công nghệ và kỹ thuật.\n\n\n\n\n\nSummary\n\ntoán không phải là ngôn ngữ vũ trụ, nhưng rất mạnh mẽ để giải thích tự nhiên;\ncác luật vật lý cơ bản được diễn giải bằng ngôn ngữ toán, rất thú ví là nhiều định luật vật lý cơ bản lại giải thích được rất nhiều hiện tượng khác nhau, như chuyển thể;\nviệc hiểu các định luật vật lý tạo ra các tiến bộ công nghệ;\nmục tiêu của toán vật lý là phát triển các công cụ toán để hiểu các hệ quả của các định luật vật lý cơ bản và khai phá các cơ chế cũng như nguyên lý đằng sau các hiện tượng quan sát được;\nmột trong các thành tựu của toán vật lý chính là nghiên cứu về tính ổn định của vật chất.\n\n\n\n\n\n\n\n\n\n\nsummary\n\n\n\n\n\n\n\nq&a and outro\n\nhamiltonian mechanics;\nnon-commutativity;\nfermions, fundamental particle, bosons, 4-dimensions space;\nschrodinger equation, lagrangian;\nphase transitions, is this similar to the butterfly effect (notion of chaos)? eg double pendulum;\nwhen was quantum mechanics been cared;\nin classical physics, can predict trajectory; but why in quantum mechanics, we can not. (electron does not have definite position, read more on bell inequalities);\ngiven the particles moving very fast, will they exchange the energy (read more on kinetic energy);\nstanding wave.\n\n\n\n\n\n\n\n\n\n\nbye bye"
  },
  {
    "objectID": "blog/2024-11-04-intro-to-mamba/index.html",
    "href": "blog/2024-11-04-intro-to-mamba/index.html",
    "title": "Data Science Project with Mamba, Python, and VS Code on Window",
    "section": "",
    "text": "Là một non-tech data coder, mình vẫn loay hoay việc setup một dự án có thể được tái sử dụng và dễ dàng deploy, đặc biệt là với dự án dạng data - khi mà DS/DA sẽ làm việc nhiều với notebook - rất anti production.\n\n\n\n\n\nArggg! Cơn đau đầu của non-tech làm data, nguồn ảnh Analytics Vidhya\n\n\n\n\n\n\nMình tìm hiểu một số công cụ và dưới đây là so sánh:\n\nMamba ✅\n\n\nƯu điểm:\n\nNhanh hơn Conda nhiều lần\nTương thích hoàn toàn với Conda\nQuản lý được cả Python và non-Python dependencies\nTích hợp tốt với notebooks\n\n\n\nPoetry\n\n\nƯu điểm:\n\nDependency resolution tốt\nLock file chính xác\n\nNhược điểm:\n\nKhó xử lý non-Python dependencies\nCần thêm setup cho notebook kernels\nKhông phù hợp lắm với data science\n\n\n\nvenv\n\n\nQuá đơn giản cho data science\nKhông xử lý được dependencies phức tạp\nCần nhiều cấu hình thủ công\n\n\npipenv\n\n\nTương tự Poetry nhưng ít tính năng hơn\nKhông phù hợp với data science\n\n\nuv\n\n\nMới và nhanh\nChưa đủ chín muồi cho data science\nThiếu nhiều tính năng cần thiết\n\nXem ra mamba có vẻ ổn nhất, hãy đào sâu hơn về ưu điểm của nó:\n\nXử lý dependencies phức tạp\n\n\nData science thường cần nhiều thư viện với dependencies phức tạp (numpy, pandas, scipy, pytorch…)\nMamba giải quyết dependencies nhanh và hiệu quả hơn Conda\nXử lý tốt các thư viện có binary dependencies (như CUDA)\n\n\nQuản lý môi trường kernel cho notebooks\n\n\nTự động tích hợp với Jupyter notebooks\nDễ dàng switch giữa các môi trường trong notebook\nKhông cần cấu hình thêm cho notebook kernels\n\nOke giờ hãy thử xem làm thế nào để tổ chức phát triển một dự án với mamba - VS Code trên Window."
  },
  {
    "objectID": "blog/2024-11-04-intro-to-mamba/index.html#cài-đặt-mamba-trên-windows",
    "href": "blog/2024-11-04-intro-to-mamba/index.html#cài-đặt-mamba-trên-windows",
    "title": "Data Science Project with Mamba, Python, and VS Code on Window",
    "section": "1. Cài đặt Mamba trên Windows",
    "text": "1. Cài đặt Mamba trên Windows\n\nCách 1: Tải trực tiếp\n\nTải Mambaforge cho Windows từ: https://github.com/conda-forge/miniforge#mambaforge\nChạy file installer và làm theo hướng dẫn.\n\n\n\nCách 2: Dùng Windows Terminal/PowerShell\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Windows-x86_64.exe -OutFile mambaforge.exe\nstart /wait \"\" mambaforge.exe /InstallationType=JustMe /RegisterPython=0 /S /D=%UserProfile%\\Mambaforge"
  },
  {
    "objectID": "blog/2024-11-04-intro-to-mamba/index.html#tích-hợp-với-vs-code",
    "href": "blog/2024-11-04-intro-to-mamba/index.html#tích-hợp-với-vs-code",
    "title": "Data Science Project with Mamba, Python, and VS Code on Window",
    "section": "2. Tích hợp với VS Code",
    "text": "2. Tích hợp với VS Code\n\nCài đặt Extensions\n\nPython (Microsoft)\nJupyter (Microsoft)\n\n\n\nTạo và Cấu hình Môi trường\nTạo file environment.yml:\nname: ds-project\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.10\n  - ipykernel\n  - pandas\n  - numpy\n  - matplotlib\n  - scikit-learn\n  - jupyter\nTạo môi trường từ file:\nmamba env create -f environment.yml"
  },
  {
    "objectID": "blog/2024-11-04-intro-to-mamba/index.html#cấu-hình-vs-code",
    "href": "blog/2024-11-04-intro-to-mamba/index.html#cấu-hình-vs-code",
    "title": "Data Science Project with Mamba, Python, and VS Code on Window",
    "section": "3. Cấu hình VS Code",
    "text": "3. Cấu hình VS Code\n\nChọn Python Interpreter\n\nMở Command Palette (Ctrl + Shift + P)\nTìm “Python: Select Interpreter”\nChọn môi trường Mamba vừa tạo\n\n\n\nCấu hình Notebooks\n\nVS Code tự động nhận diện kernel từ môi trường Mamba\nKernel có thể được chọn ở góc phải trên của notebook"
  },
  {
    "objectID": "blog/2024-11-04-intro-to-mamba/index.html#cấu-trúc-project-đề-xuất",
    "href": "blog/2024-11-04-intro-to-mamba/index.html#cấu-trúc-project-đề-xuất",
    "title": "Data Science Project with Mamba, Python, and VS Code on Window",
    "section": "4. Cấu trúc Project Đề Xuất",
    "text": "4. Cấu trúc Project Đề Xuất\nproject/\n│\n├── .vscode/                      # VS Code settings\n│   ├── settings.json\n│   └── launch.json\n│\n├── data/\n│   ├── raw/\n│   └── processed/\n│\n├── notebooks/\n│   ├── 01_exploration.ipynb\n│   └── 02_analysis.ipynb\n│\n├── src/\n│   └── processing/\n│       ├── __init__.py\n│       └── data_processing.py\n│\n├── environment.yml\n├── .gitignore\n└── README.md"
  },
  {
    "objectID": "blog/2024-11-04-intro-to-mamba/index.html#cấu-hình-vs-code-settings",
    "href": "blog/2024-11-04-intro-to-mamba/index.html#cấu-hình-vs-code-settings",
    "title": "Data Science Project with Mamba, Python, and VS Code on Window",
    "section": "5. Cấu hình VS Code Settings",
    "text": "5. Cấu hình VS Code Settings\nTạo file .vscode/settings.json:\n{\n    \"python.defaultInterpreterPath\": \"${env:USERPROFILE}\\\\Mambaforge\\\\envs\\\\ds-project\\\\python.exe\",\n    \"jupyter.notebookFileRoot\": \"${workspaceFolder}\",\n    \"python.analysis.extraPaths\": [\"${workspaceFolder}/src\"],\n    \"python.formatting.provider\": \"black\",\n    \"editor.formatOnSave\": true,\n    \"ruff.enable\": true,\n    \"ruff.format.args\": [\"--config=pyproject.toml\"],\n    \"ruff.lint.args\": [\"--config=pyproject.toml\"],\n    \"mypy.enabled\": true,\n    \"mypy.configFile\": \"pyproject.toml\"\n}"
  },
  {
    "objectID": "blog/2024-11-04-intro-to-mamba/index.html#cấu-hình-development-tools",
    "href": "blog/2024-11-04-intro-to-mamba/index.html#cấu-hình-development-tools",
    "title": "Data Science Project with Mamba, Python, and VS Code on Window",
    "section": "6. Cấu hình Development Tools",
    "text": "6. Cấu hình Development Tools\n\n6.1 Cài đặt Development Dependencies\nCập nhật environment.yml:\nname: ds-project\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.10\n  - ipykernel\n  - pandas\n  - numpy\n  - matplotlib\n  - scikit-learn\n  - jupyter\n  # Dev dependencies\n  - black\n  - ruff\n  - mypy\n  - pre-commit\n  - nbqa\n  - jupytext\n\n\n6.2 Cấu hình pyproject.toml\nTạo file pyproject.toml:\n[tool.black]\nline-length = 88\ntarget-version = [\"py310\"]\ninclude = '\\.pyi?$'\n\n[tool.ruff]\nline-length = 88\ntarget-version = \"py310\"\nselect = [\n    \"E\",  # pycodestyle\n    \"F\",  # pyflakes\n    \"I\",  # isort\n    \"UP\", # pyupgrade\n]\nignore = []\n\n[tool.ruff.isort]\nknown-first-party = [\"src\"]\n\n[tool.mypy]\npython_version = \"3.10\"\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = true\ncheck_untyped_defs = true\n\n[[tool.mypy.overrides]]\nmodule = [\n    \"pandas.*\",\n    \"numpy.*\",\n    \"matplotlib.*\",\n    \"seaborn.*\"\n]\nignore_missing_imports = true\n\n\n6.3 Cấu hình Pre-commit\nTạo file .pre-commit-config.yaml:\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.5.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: end-of-file-fixer\n    -   id: check-yaml\n    -   id: check-json\n    -   id: check-added-large-files\n        args: ['--maxkb=5000']\n\n-   repo: https://github.com/psf/black\n    rev: 24.1.1\n    hooks:\n    -   id: black\n\n-   repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.2.1\n    hooks:\n    -   id: ruff\n        args: [--fix]\n    -   id: ruff-format\n\n-   repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.8.0\n    hooks:\n    -   id: mypy\n        additional_dependencies: [types-all]\n\n-   repo: https://github.com/nbQA-dev/nbQA\n    rev: 1.7.1\n    hooks:\n    -   id: nbqa-black\n        additional_dependencies: [black==24.1.1]\n    -   id: nbqa-ruff\n        additional_dependencies: [ruff==0.2.1]\n\n\n6.4 Cấu hình Git Ignore\nTạo file .gitignore:\n# Python\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# Jupyter Notebook\n.ipynb_checkpoints\n*/.ipynb_checkpoints/*\nprofile_default/\nipython_config.py\n\n# VS Code\n.vscode/*\n!.vscode/settings.json\n!.vscode/tasks.json\n!.vscode/launch.json\n!.vscode/extensions.json\n\n# Environment\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n.conda/\nconda-env/\n\n# Data\ndata/raw/*\n!data/raw/.gitkeep\ndata/processed/*\n!data/processed/.gitkeep\n*.csv\n*.xlsx\n*.xls\n*.db\n*.sqlite\n*.h5\n\n# Logs\nlogs/\n*.log\n.hypothesis/\n.pytest_cache/\n.coverage\nhtmlcov/\n\n# OS specific\n.DS_Store\n.DS_Store?\n._*\n.Spotlight-V100\n.Trashes\nehthumbs.db\nThumbs.db\n\n\n6.5 Khởi tạo Pre-commit\n# Activate môi trường\nmamba activate ds-project\n\n# Cài đặt pre-commit hooks\npre-commit install\n\n# (Tùy chọn) Chạy pre-commit trên tất cả files\npre-commit run --all-files\n\n\n6.6 Extensions VS Code cho Notebooks\nCài thêm các extensions sau:\n\n“Jupyter Notebook Renderers”\n“Jupytext for Notebooks”\n“Black Formatter”\n“Ruff”\n“Mypy Type Checker”\n\n\n\n6.7 Cấu hình Jupytext\nTạo file .jupytext.toml:\nformats = \"ipynb,qmd\"\n\n[formats]\nipynb = {}\nqmd = {}"
  },
  {
    "objectID": "blog/2024-11-04-intro-to-mamba/index.html#pro-tips-cho-windows",
    "href": "blog/2024-11-04-intro-to-mamba/index.html#pro-tips-cho-windows",
    "title": "Data Science Project with Mamba, Python, and VS Code on Window",
    "section": "7. Pro Tips cho Windows",
    "text": "7. Pro Tips cho Windows\n\nPath Management\nSử dụng pathlib để xử lý đường dẫn:\nfrom pathlib import Path\ndata_path = Path(\"data\") / \"raw\" / \"file.csv\"\n\n\nGit Configuration\nTạo file .gitignore:\n# .gitignore\ndata/raw/*\ndata/processed/*\n.ipynb_checkpoints/\n__pycache__/\n*.pyc\n\n\nJupyter trong VS Code\n\nChạy cell: Shift + Enter\nVariable explorer có sẵn\nPlot viewer tích hợp\nIntellisense hoạt động tốt\n\n\n\nDebugging\n\nDebugger tích hợp cho cả .py và .ipynb\nCó thể đặt breakpoints trong notebooks"
  },
  {
    "objectID": "blog/2024-11-04-intro-to-mamba/index.html#xử-lý-lỗi-thường-gặp",
    "href": "blog/2024-11-04-intro-to-mamba/index.html#xử-lý-lỗi-thường-gặp",
    "title": "Data Science Project with Mamba, Python, and VS Code on Window",
    "section": "8. Xử lý Lỗi Thường Gặp",
    "text": "8. Xử lý Lỗi Thường Gặp\n\nPath quá dài trên Windows\n\nSử dụng đường dẫn ngắn hơn\nKích hoạt hỗ trợ đường dẫn dài trong Windows\n\nConflict với môi trường Python khác\n\nĐảm bảo PATH được cập nhật đúng\nKiểm tra where python trong terminal\n\nJupyter Kernel không hiển thị\n\nCài lại ipykernel\nKiểm tra jupyter kernelspec list"
  },
  {
    "objectID": "blog/2024-11-04-intro-to-mamba/index.html#resources-hữu-ích",
    "href": "blog/2024-11-04-intro-to-mamba/index.html#resources-hữu-ích",
    "title": "Data Science Project with Mamba, Python, and VS Code on Window",
    "section": "9. Resources Hữu ích",
    "text": "9. Resources Hữu ích\n\nMamba Documentation\nVS Code Python Documentation\nJupyter in VS Code"
  },
  {
    "objectID": "blog/2024-11-04-intro-to-mamba/index.html#workflow-đề-xuất",
    "href": "blog/2024-11-04-intro-to-mamba/index.html#workflow-đề-xuất",
    "title": "Data Science Project with Mamba, Python, and VS Code on Window",
    "section": "10. Workflow Đề Xuất",
    "text": "10. Workflow Đề Xuất\n\nKhởi tạo project mới:\n\n# Tạo và activate môi trường\nmamba env create -f environment.yml\nmamba activate ds-project\n\n# Cài đặt pre-commit hooks\npre-commit install\n\nPhát triển:\n\n\nCode trong VS Code với các extensions đã cài\nNotebooks sẽ tự động được format bởi nbQA\nPre-commit sẽ kiểm tra code trước mỗi commit\n\n\nCommit Changes:\n\ngit add .\ngit commit -m \"your message\"\n# Pre-commit sẽ tự động chạy các checks\n\nUpdate Dependencies:\n\n# Thêm package mới\nmamba install package-name\n# Export environment\nmamba env export &gt; environment.yml"
  },
  {
    "objectID": "blog/2024-11-04-intro-to-mamba/index.html#bonus-nếu-muốn-import-một-module-từ-src-trong-notebook.ipynb-thì-làm-thế-nào",
    "href": "blog/2024-11-04-intro-to-mamba/index.html#bonus-nếu-muốn-import-một-module-từ-src-trong-notebook.ipynb-thì-làm-thế-nào",
    "title": "Data Science Project with Mamba, Python, and VS Code on Window",
    "section": "Bonus: nếu muốn import một module từ src trong notebook.ipynb thì làm thế nào?",
    "text": "Bonus: nếu muốn import một module từ src trong notebook.ipynb thì làm thế nào?\n\n\nShow the code\n# Trong notebook của bạn (ví dụ notebooks/analysis.ipynb)\n\n# 1. Setup path\nimport sys\nfrom pathlib import Path\nproject_root = str(Path.cwd().parent)\nif project_root not in sys.path:\n    sys.path.append(project_root)\n\n# 2. Import function\nfrom src.processing.data_preprocessing import process_data, clean_data\n\n# 3. Sử dụng function\ndf_processed = process_data(df_raw)\n\n\nVới lưu ý:\n\nĐảm bảo có file __init__.py trong mỗi thư mục Python\nKhông nên dùng relative imports (from ...src.processing) vì dễ gây lỗi\nNên đặt tên module và function theo PEP 8\nTrong data_preprocessing.py nên có docstring mô tả function\n\n\n\nShow the code\n# src/processing/data_preprocessing.py\n\ndef process_data(df):\n    \"\"\"Process the input dataframe.\n    \n    Args:\n        df (pd.DataFrame): Input dataframe\n        \n    Returns:\n        pd.DataFrame: Processed dataframe\n    \"\"\"\n    # your code here\n    return processed_df"
  },
  {
    "objectID": "blog/2024-11-20-nn-z2h-p3/index.html",
    "href": "blog/2024-11-20-nn-z2h-p3/index.html",
    "title": "NN-Z2H Lesson 3: Building makemore part 2 - MLP",
    "section": "",
    "text": "This is not orginal content!\n\n\n\nThis is my study notes / codes along with Andrej Karpathy’s “Neural Networks: Zero to Hero” series.\nIn the previous lecture, we built a simple bigram character-level language model, using 2 different approaches that are (1) count, and (2) 1 layer neural network. They produced the same (and both poor - since the context is 1 character only) result but the neural network option offers more flexibility so that we can complexify our model to get better performance.\nIn this lecture we are going to implement 20-years ago neural probabilistic language model by Bengio et al. (2003)."
  },
  {
    "objectID": "blog/2024-11-20-nn-z2h-p3/index.html#bengio-et-al.-2003-mlp-language-model-paper-walkthrough",
    "href": "blog/2024-11-20-nn-z2h-p3/index.html#bengio-et-al.-2003-mlp-language-model-paper-walkthrough",
    "title": "NN-Z2H Lesson 3: Building makemore part 2 - MLP",
    "section": "Bengio et al. 2003 (MLP language model) paper walkthrough",
    "text": "Bengio et al. 2003 (MLP language model) paper walkthrough\n\nSummary\nProblem Statement:\n\nTraditional n-gram language models suffer from the curse of dimensionality: they can’t effectively generalize to word sequences not seen in training data;\nThe core issue is treating words as atomic units with no inherent similarity to each other;\nFor example, if we’ve seen “dog is eating” in training but never “cat is eating”, n-gram models can’t leverage the similarity between “dog” and “cat”;\nThis leads to poor probability estimates for rare or unseen word sequences.\n\nSolution:\n\nLearn a distributed representation (embedding) for each word in a continuous vector space where similar words are close to each other;\nUse a neural network architecture with:\n\nInput layer: concatenated embeddings of n-1 previous words;\nHidden layer: dense neural network with tanh activation;\nOutput layer: softmax over entire vocabulary to predict next word probability.\n\n\nThe model simultaneously learns:\n\nWord feature vectors (embeddings) that capture semantic/syntactic word similarities;\nNeural network parameters that combine these features to estimate probability distributions.\n\nKey advantages:\n\nWords with similar meanings get similar feature vectors, enabling better generalization;\nThe probability function is smooth with respect to word embeddings, so similar words yield similar predictions;\nCan generalize to unseen sequences by leveraging learned word similarities.\n\n\n\nMethodology:\n\nTraditional Problem:\n\nIn n-gram models, each word sequence of length n is a separate parameter;\nFor vocabulary size \\(|V|\\), need \\(|V|^n\\) parameters;\nMost sequences never appear in training, leading to poor generalization;\n\nSolution via Distributed Representation:\n\nEach word mapped to a dense vector in \\(R^m\\) (typically m=50-100);\nSimilar words get similar vectors through training;\nProbability function is smooth w.r.t these vectors;\nKey benefit: If “dog” and “cat” have similar vectors, model can generalize from “dog is eating” to “cat is eating”;\nNumber of parameters reduces to \\(O(|V|×m + m×h + h×|V|)\\), where \\(h\\) is hidden layer size;\nThis is much smaller than \\(|V|^n\\) and allows better generalization;\n\n\n\n\nNeural architecture:\nInput Layer:\n\nTakes \\(n-1\\) previous words (context window);\nEach word i mapped to vector \\(C(i) ∈ R^m\\) via lookup table;\nConcatenates these vectors: \\(x = [C(wₜ₋ₙ₊₁), ..., C(wₜ₋₁)]\\);\n\\(x\\) dimension is \\((n-1)×m\\);\n\nHidden Layer:\n\nDense layer with tanh activation;\nComputation: \\(h = tanh(d + Hx)\\);\n\\(H\\) is weight matrix, \\(d\\) is bias vector;\nMaps concatenated context to hidden representation;\n\nOutput Layer:\n\nComputes probability distribution over all words;\n\\(y = b + Wx + Uh\\);\nSoftmax activation: \\(P(wₜ|context) = exp(yᵢ)/Σⱼexp(yⱼ)\\);\n\\(W\\) provides “shortcut” connections from input to output;\nDirect connection helps learn simpler patterns;\n\nTraining:\n\nMaximizes log-likelihood of training data;\nUses stochastic gradient descent;\nLearns both word vectors \\(C(i)\\) and neural network parameters \\((H, d, W, U, b)\\);\nWord vectors capture similarities as they help predict similar contexts;\nCan initialize word vectors randomly or with pretrained vectors.\n\n\n\n\nNeural Language Model proposed by (Bengio et al., 2003). C(i) is the i th word embedding."
  },
  {
    "objectID": "blog/2024-11-20-nn-z2h-p3/index.html#re-building-our-training-dataset",
    "href": "blog/2024-11-20-nn-z2h-p3/index.html#re-building-our-training-dataset",
    "title": "NN-Z2H Lesson 3: Building makemore part 2 - MLP",
    "section": "(re-)building our training dataset",
    "text": "(re-)building our training dataset\nLoading library, reading data, building dictionary:\n\n\nShow the code\nimport torch\nimport torch.nn.functional as F\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n\n\nShow the code\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt\"\nwords = pd.read_csv(url, header=None).iloc[:, 0].tolist()\nwords[:8]\n\n\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n\n\n\n\nShow the code\nlen(words)\n\n\n32033\n\n\n\n\nShow the code\n# build the vocabulary of characters and mapping to/from integer\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i, s in enumerate(chars)}\nstoi['.'] = 0\n\nitos = {i: s for s, i in stoi.items()}\nitos\n\n\n{1: 'a',\n 2: 'b',\n 3: 'c',\n 4: 'd',\n 5: 'e',\n 6: 'f',\n 7: 'g',\n 8: 'h',\n 9: 'i',\n 10: 'j',\n 11: 'k',\n 12: 'l',\n 13: 'm',\n 14: 'n',\n 15: 'o',\n 16: 'p',\n 17: 'q',\n 18: 'r',\n 19: 's',\n 20: 't',\n 21: 'u',\n 22: 'v',\n 23: 'w',\n 24: 'x',\n 25: 'y',\n 26: 'z',\n 0: '.'}\n\n\nBuilding the dataset:\n\n\nShow the code\nblock_size = 3 # the context length: how many characters do we take to predict the next one?\nX, Y = [], []\n\nfor w in words[:5]:\n    print(w)\n    context = [0] * block_size # 0 so context will be padded by '.'\n    for ch in w + '.':\n        ix = stoi[ch]\n        X.append(context)\n        Y.append(ix)\n        print(''.join(itos[i] for i in context), '-----&gt;', itos[ix] )\n        context = context[1:] + [ix] # rolling to the next one\n\nX = torch.tensor(X)\nY = torch.tensor(Y)\n\n\nemma\n... -----&gt; e\n..e -----&gt; m\n.em -----&gt; m\nemm -----&gt; a\nmma -----&gt; .\nolivia\n... -----&gt; o\n..o -----&gt; l\n.ol -----&gt; i\noli -----&gt; v\nliv -----&gt; i\nivi -----&gt; a\nvia -----&gt; .\nava\n... -----&gt; a\n..a -----&gt; v\n.av -----&gt; a\nava -----&gt; .\nisabella\n... -----&gt; i\n..i -----&gt; s\n.is -----&gt; a\nisa -----&gt; b\nsab -----&gt; e\nabe -----&gt; l\nbel -----&gt; l\nell -----&gt; a\nlla -----&gt; .\nsophia\n... -----&gt; s\n..s -----&gt; o\n.so -----&gt; p\nsop -----&gt; h\noph -----&gt; i\nphi -----&gt; a\nhia -----&gt; .\n\n\n\n\nShow the code\nX.shape, X.dtype, Y.shape, Y.dtype\n\n\n(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
  },
  {
    "objectID": "blog/2024-11-20-nn-z2h-p3/index.html#implementing-the-embedding-lookup-table",
    "href": "blog/2024-11-20-nn-z2h-p3/index.html#implementing-the-embedding-lookup-table",
    "title": "NN-Z2H Lesson 3: Building makemore part 2 - MLP",
    "section": "implementing the embedding lookup table",
    "text": "implementing the embedding lookup table\nIn the paper they cram 17k word into as-low-as-possible 30 dimensions space, for our data, we just cram words into 2D space.\n\n\nShow the code\nC = torch.randn((27, 2))\n\n\nWe can access the element of torch.tensor by:\n\n\nShow the code\nC[5] # can be integer, list [5, 6, 7], or torch.tensor([5,6,7])\n# &gt; tensor([1.0825, 0.2010])\n\n# or\n\nF.one_hot(torch.tensor(5), num_classes=27).float() @ C\n# produce identical result, remember torch.tensor() infer long dtype int64, so we need to cast to float\n\n\ntensor([ 0.3055, -0.4069])\n\n\n…but in this lecture accessing by C[5] would be sufficient. We can even access using a more than 1 dimension tensor:\n\n\nShow the code\nprint(C[X].shape)\nprint(X[13, 2]) # integer 1 for 13rd index of 2nd dimension\nprint(C[X][13,2]) # will be the embedding of that element\nprint(C[1]) # so C[X][13,2] = C[1]\n\n\ntorch.Size([32, 3, 2])\ntensor(1)\ntensor([0.0748, 0.8711])\ntensor([0.0748, 0.8711])\n\n\nPyTorch is great for embedding words:\n\n\nShow the code\nemb = C[X]\nemb.shape\n\n\ntorch.Size([32, 3, 2])\n\n\nWe’ve compeleted the first layer with context and lookup table!"
  },
  {
    "objectID": "blog/2024-11-20-nn-z2h-p3/index.html#implementing-the-hidden-layer-internals-of-torch.tensor-storage-views",
    "href": "blog/2024-11-20-nn-z2h-p3/index.html#implementing-the-hidden-layer-internals-of-torch.tensor-storage-views",
    "title": "NN-Z2H Lesson 3: Building makemore part 2 - MLP",
    "section": "implementing the hidden layer + internals of torch.Tensor: storage, views",
    "text": "implementing the hidden layer + internals of torch.Tensor: storage, views\n\n\nShow the code\n# input of tanh layer will be 6 (3 words in context x 2 dimensions)\n# and the number or neurons is up to us - let's set it 100\nW1 = torch.randn((6, 100))\nb1 = torch.randn(100)\n\n\nNow we need to do something like emb @ W1 + b1, but emb.shape is [32, 3, 2] and W1.shape is [6, 100]. We need to somehow concatnate/transform:\n\n\nShow the code\n# emb[:, 0, :] is tensor for each input in the 3-words context, shape is [32, 2]\n# cat 3 of them using the 2nd dimension (index 1) -&gt; so we set dim = 1\ntorch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], dim=1).shape\n\n\ntorch.Size([32, 6])\n\n\nHowever this code does not change dynamically when we change the block size. We will be using torch.unbind()\n\n\nShow the code\n# this is good!\ntorch.cat(torch.unbind(emb, 1), 1).shape\n# new memory for storage is created, so it is not efficient\n\n\ntorch.Size([32, 6])\n\n\nThis works, but we have a better and more efficient way to do this. Since:\n\nevery torch.Tensor have .storage() which is one-dimensional vector tensor;\nwhen we call .view(), we instruct how this vector tensor is interpreted;\nno memory is being changed/copied/moved/or created. the storage is identical.\n\nReadmore: http://blog.ezyang.com/2019/05/pytorch-internals/\nSo this hidden layer can be declared:\n\n\nShow the code\n# instead or 32 we can write emb.shape[1], or -1 (whatever fitted)\nh = emb.view(-1, 6) @ W1 + b1\nh.shape\n\n\ntorch.Size([32, 100])\n\n\nNotice that in the final operation, b1 will be broadcasted."
  },
  {
    "objectID": "blog/2024-11-20-nn-z2h-p3/index.html#implementing-the-output-layer",
    "href": "blog/2024-11-20-nn-z2h-p3/index.html#implementing-the-output-layer",
    "title": "NN-Z2H Lesson 3: Building makemore part 2 - MLP",
    "section": "implementing the output layer",
    "text": "implementing the output layer\n\n\nShow the code\nW2 = torch.randn((100, 27))\nb2 = torch.randn(27)\n\n\nIn Deep Learning, people use logits for what raw output that range from negative inf to positive inf.\n\n\nShow the code\nlogits = h @ W2 + b2\n\n\n\n\nShow the code\nlogits.shape\n\n\ntorch.Size([32, 27])\n\n\nNow we need to exponentiate it and get the probability.\n\n\nShow the code\ncounts = logits.exp()\n\n\n\n\nShow the code\nprobs = counts / counts.sum(1, keepdims=True)\n\n\n\n\nShow the code\nprobs.shape\n\n\ntorch.Size([32, 27])\n\n\nEvery row of probs has sum of 1.\n\n\nShow the code\nprobs[0].sum()\n\n\ntensor(1.)\n\n\nAnd this is the probs of each ground true Y in current output of the neural nets:\n\n\nShow the code\nprobs[torch.arange(32), Y]\n\n\ntensor([2.0660e-21, 3.6184e-15, 1.5675e-11, 1.2796e-08, 4.4312e-02, 2.4003e-12,\n        3.5811e-13, 1.3876e-18, 3.3465e-14, 2.5158e-22, 8.3230e-35, 3.4999e-08,\n        7.5305e-10, 6.8868e-24, 1.8081e-28, 8.6262e-08, 4.0514e-18, 4.2847e-19,\n        4.9013e-15, 1.0952e-10, 8.4563e-11, 2.1141e-26, 4.4209e-22, 6.9570e-30,\n        3.9779e-10, 3.2419e-13, 2.2802e-07, 6.5380e-23, 3.0035e-37, 0.0000e+00,\n        0.0000e+00, 3.2723e-26])\n\n\nResult is not good as we’ve not trained the network yet!"
  },
  {
    "objectID": "blog/2024-11-20-nn-z2h-p3/index.html#implementing-the-negative-log-likelihood-loss",
    "href": "blog/2024-11-20-nn-z2h-p3/index.html#implementing-the-negative-log-likelihood-loss",
    "title": "NN-Z2H Lesson 3: Building makemore part 2 - MLP",
    "section": "implementing the negative log likelihood loss",
    "text": "implementing the negative log likelihood loss\nWe define the negative log likelihood as:\n\n\nShow the code\nloss = - probs[torch.arange(32), Y].log().mean()\nloss\n\n\ntensor(inf)"
  },
  {
    "objectID": "blog/2024-11-20-nn-z2h-p3/index.html#summary-of-the-full-network",
    "href": "blog/2024-11-20-nn-z2h-p3/index.html#summary-of-the-full-network",
    "title": "NN-Z2H Lesson 3: Building makemore part 2 - MLP",
    "section": "summary of the full network",
    "text": "summary of the full network\nDataset:\n\n\nShow the code\nX.shape, Y.shape\n\n\n(torch.Size([32, 3]), torch.Size([32]))\n\n\nNeural network layers:\n\n\nShow the code\ng = torch.Generator().manual_seed(2147483647)\nC = torch.randn((27, 2), generator=g)\nW1 = torch.randn((6, 100), generator=g)\nb1 = torch.randn(100, generator=g)\nW2 = torch.randn((100, 27), generator=g)\nb2 = torch.randn(27, generator=g)\n\nparameters = [C, W1, b1, W2, b2]\n\n\nSize of the network:\n\n\nShow the code\nsum(p.nelement() for p in parameters)\n\n\n3481\n\n\nConstructing forward pass:\n\n\nShow the code\nemb = C[X] # (32, 3, 2)\nh = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\nlogits = h @ W2 + b2 # (32, 27)\ncounts = logits.exp()\nprobs = counts / counts.sum(1, keepdims=True)\nloss = - probs[torch.arange(32), Y].log().mean()\nloss\n\n\ntensor(17.7697)"
  },
  {
    "objectID": "blog/2024-11-20-nn-z2h-p3/index.html#introducing-f.cross_entropy-and-why",
    "href": "blog/2024-11-20-nn-z2h-p3/index.html#introducing-f.cross_entropy-and-why",
    "title": "NN-Z2H Lesson 3: Building makemore part 2 - MLP",
    "section": "introducing F.cross_entropy and why",
    "text": "introducing F.cross_entropy and why\nWe re-define loss:\n\n\nShow the code\nloss = F.cross_entropy(logits, Y)\nloss\n\n\ntensor(17.7697)\n\n\nWhy?\n\nPytorch will create more intermediate tensor for every assignment: counts, probs -&gt; more memory;\nBackward pass will be more optimized, because the expressions are much analytically and mathematically interpreted;\nCross entropy can be significantly & numerically well behaved (for eg when we exponentiate a large positive number we got inf, PyTorch cross entropy will calculate the max of set and subtract it - which will not impact the exp result)"
  },
  {
    "objectID": "blog/2024-11-20-nn-z2h-p3/index.html#implementing-the-training-loop-overfitting-one-batch",
    "href": "blog/2024-11-20-nn-z2h-p3/index.html#implementing-the-training-loop-overfitting-one-batch",
    "title": "NN-Z2H Lesson 3: Building makemore part 2 - MLP",
    "section": "implementing the training loop, overfitting one batch",
    "text": "implementing the training loop, overfitting one batch\nSo the forward pass, backward pass, and update loop will be implemented as below:\n\n\nShow the code\nfor p in parameters:\n    p.requires_grad = True\n\n\n\n\nShow the code\nfor _ in range(10):\n    # forward pass:\n    emb = C[X] # (32, 3, 2)\n    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n    logits = h @ W2 + b2 # (32, 27)\n    loss = F.cross_entropy(logits, Y)\n    print(loss.item())\n    # backward pass:\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    # update\n    for p in parameters:\n        p.data += -0.1 * p.grad\n\nprint(loss.item())\n\n\n17.76971435546875\n13.656400680541992\n11.298768997192383\n9.452457427978516\n7.984262466430664\n6.891321182250977\n6.100014686584473\n5.452036380767822\n4.898152828216553\n4.4146647453308105\n4.4146647453308105\n\n\nWe are fitting 32 examples to a neural nets of 3481 params, so it’s super easy to be overfitting. We got a low final loss, but it would never be 0, because the output can varry for the same input, for eg, ....\n\n\nShow the code\nlogits.max(1)\n\n\ntorch.return_types.max(\nvalues=tensor([10.7865, 12.2558, 17.3982, 13.2739, 10.6965, 10.7865,  9.5145,  9.0495,\n        14.0280, 11.8378,  9.9038, 15.4187, 10.7865, 10.1476,  9.8372, 11.7660,\n        10.7865, 10.0029,  9.2940,  9.6824, 11.4241,  9.4885,  8.1164,  9.5176,\n        12.6383, 10.7865, 10.6021, 11.0822,  6.3617, 17.3157, 12.4544,  8.1669],\n       grad_fn=&lt;MaxBackward0&gt;),\nindices=tensor([ 1,  8,  9,  0, 15,  1, 17,  2,  9,  9,  2,  0,  1, 15,  1,  0,  1, 19,\n         1,  1, 16, 10, 26,  9,  0,  1, 15, 16,  3,  9, 19,  1]))"
  },
  {
    "objectID": "blog/2024-11-20-nn-z2h-p3/index.html#training-on-the-full-dataset-minibatches",
    "href": "blog/2024-11-20-nn-z2h-p3/index.html#training-on-the-full-dataset-minibatches",
    "title": "NN-Z2H Lesson 3: Building makemore part 2 - MLP",
    "section": "training on the full dataset, minibatches",
    "text": "training on the full dataset, minibatches\nWe can deploy our code to all the dataset, un-fold the below code block to see full code.\n\n\nShow the code\nblock_size = 3\nX, Y = [], []\n\n# Dataset\nfor w in words:\n    # print(w)\n    context = [0] * block_size\n    for ch in w + '.':\n        ix = stoi[ch]\n        X.append(context)\n        Y.append(ix)\n        # print(''.join(itos[i] for i in context), '-----&gt;', itos[ix] )\n        context = context[1:] + [ix] # rolling to the next one\n\n# Input and ground true\nX = torch.tensor(X)\nY = torch.tensor(Y)\nprint(\"Data size\", X.shape, Y.shape)\n\n# Lookup table\ng = torch.Generator().manual_seed(2147483647)\nC = torch.randn((27, 2), generator=g)\nemb = C[X] # (32, 3, 2)\n\n# Layer 1 - tanh\nW1 = torch.randn((6, 100), generator=g)\nb1 = torch.randn(100, generator=g)\nh = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n\n# Layer 2 - softmax\nW2 = torch.randn((100, 27), generator=g)\nb2 = torch.randn(27, generator=g)\nlogits = h @ W2 + b2 # (32, 27)\nloss = F.cross_entropy(logits, Y)\n\n# All params\nparameters = [C, W1, b1, W2, b2]\nprint(\"No of params: \", sum(p.nelement() for p in parameters))\n\n# Pre-training\nfor p in parameters:\n    p.requires_grad = True\n\n\nData size torch.Size([228146, 3]) torch.Size([228146])\nNo of params:  3481\n\n\nWe notice that it takes a bit long time for each training in the loop. In practice, we will perform the forward/backward passes and update parameters for a small batch of the dataset. The minibatch construction is added/modified for lines of code with #👈.\nRead more: https://nttuan8.com/bai-10-cac-ky-thuat-co-ban-trong-deep-learning/\n\n\nShow the code\n# Training\nfor _ in range(10000):\n    # minibatch construct                                           #👈\n    ix = torch.randint(0, X.shape[0], (32,))                        #👈\n\n    # forward pass:\n    emb = C[X[ix]] # (32, 3, 2)                                     #👈\n    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n    logits = h @ W2 + b2 # (32, 27)\n    loss = F.cross_entropy(logits, Y[ix])                           #👈\n    if _ &gt;= 9990: print(f\"___after running {_} time: \", loss.item())\n    # backward pass:\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    # update\n    for p in parameters:\n        p.data += -0.1 * p.grad\n\nprint(\"final minibatch loss: \", loss.item())\n\n\n___after running 9990 time:  2.4769816398620605\n___after running 9991 time:  2.719482660293579\n___after running 9992 time:  2.5886943340301514\n___after running 9993 time:  2.2563774585723877\n___after running 9994 time:  2.584904432296753\n___after running 9995 time:  2.8459784984588623\n___after running 9996 time:  2.4365286827087402\n___after running 9997 time:  2.2122902870178223\n___after running 9998 time:  2.440680742263794\n___after running 9999 time:  2.183750867843628\nfinal minibatch loss:  2.183750867843628\n\n\nThe loss decrease much much better, although the direction of gradient might be not correct direction. But it is good enough for an approximation. Notice the loss for a minibatch is not the loss of whole dataset.\n\n\nShow the code\nemb = C[X] # (32, 3, 2)                                    \nh = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\nlogits = h @ W2 + b2 # (32, 27)\nloss = F.cross_entropy(logits, Y)   \nloss.item()\n\n\n2.492901086807251\n\n\nWe archived 2.39 loss for final minibatch and 2.5 on overall network."
  },
  {
    "objectID": "blog/2024-11-20-nn-z2h-p3/index.html#finding-a-good-initial-learning-rate",
    "href": "blog/2024-11-20-nn-z2h-p3/index.html#finding-a-good-initial-learning-rate",
    "title": "NN-Z2H Lesson 3: Building makemore part 2 - MLP",
    "section": "finding a good initial learning rate",
    "text": "finding a good initial learning rate\nNow we’re continuing the optimization, let’s focus on how much we update the data from the gradient p.data += -0.1 * p.grad. We do not know if we step too little or too much.\nWe can create 1000 learning rates to use along with the training loop and see which one offers more stable convergence.\n\n\nShow the code\nlre = torch.linspace(-3, 0, 1000)\nlrs = 10**lre\n\n\nReset the code:\n\n\nShow the code\nblock_size = 3\nX, Y = [], []\n\n# Dataset\nfor w in words:\n    # print(w)\n    context = [0] * block_size\n    for ch in w + '.':\n        ix = stoi[ch]\n        X.append(context)\n        Y.append(ix)\n        # print(''.join(itos[i] for i in context), '-----&gt;', itos[ix] )\n        context = context[1:] + [ix] # rolling to the next one\n\n# Input and ground true\nX = torch.tensor(X)\nY = torch.tensor(Y)\nprint(\"Data size\", X.shape, Y.shape)\n\n# Lookup table\ng = torch.Generator().manual_seed(2147483647)\nC = torch.randn((27, 2), generator=g)\nemb = C[X] # (32, 3, 2)\n\n# Layer 1 - tanh\nW1 = torch.randn((6, 100), generator=g)\nb1 = torch.randn(100, generator=g)\nh = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n\n# Layer 2 - softmax\nW2 = torch.randn((100, 27), generator=g)\nb2 = torch.randn(27, generator=g)\nlogits = h @ W2 + b2 # (32, 27)\nloss = F.cross_entropy(logits, Y)\n\n# All params\nparameters = [C, W1, b1, W2, b2]\nprint(\"No of params: \", sum(p.nelement() for p in parameters))\n\n# Pre-training\nfor p in parameters:\n    p.requires_grad = True\n\n\nData size torch.Size([228146, 3]) torch.Size([228146])\nNo of params:  3481\n\n\nTraining and tracking stats:\n\n\nShow the code\nlri = []\nlossi = []\n\nfor i in range(1000):\n    # minibatch construct                                           \n    ix = torch.randint(0, X.shape[0], (32,))                        \n    # forward pass:\n    emb = C[X[ix]] # (32, 3, 2)                                    \n    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n    logits = h @ W2 + b2 # (32, 27)\n    loss = F.cross_entropy(logits, Y[ix])                           \n    # backward pass:\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    # update\n    lr = lrs[i]\n    for p in parameters:\n        p.data += - lr * p.grad\n\n    # track stats\n    lri.append(lre[i])\n    lossi.append(loss.item())\n\nloss.item()\n\n\n8.216608047485352\n\n\nPlotting, we see a good exponential element of learning rate turn out to be around -1.\n\\(10^{-1}\\) is 0.1 so our initial guess seems good.\n\n\nShow the code\nplt.plot(lri, lossi)"
  },
  {
    "objectID": "blog/2024-11-20-nn-z2h-p3/index.html#splitting-up-the-dataset-into-trainvaltest-splits-and-why",
    "href": "blog/2024-11-20-nn-z2h-p3/index.html#splitting-up-the-dataset-into-trainvaltest-splits-and-why",
    "title": "NN-Z2H Lesson 3: Building makemore part 2 - MLP",
    "section": "splitting up the dataset into train/val/test splits and why",
    "text": "splitting up the dataset into train/val/test splits and why\nNow we can keep lengthening the training loop to continue decreasing loss. We can try some techniques like change the learning rate to 0.001 after 20k, 30k loops of training with 0.1.\nBut it will come to be overfitting when we try to keep training or increase the size of network to achieve a lower loss. The model just memorizing our training set verbatim, so if we try to sample from the model it just gives us the same thing in the dataset. Or if we calculate the loss on another dataset, it might be very high.\nSo another industry standard is we will split the data set into 3 pieces: (1) training set; (2) dev/validation set; and (3) test set, they can be 80% - 10% - 10% roughly and respectively.\n\nTraining split: train the parameters;\nDev/validation split: train the hyperparamerters (size of hidden layer, size of embedding, streng of regularization, etc);\nTest split: evaluate the performance of the model at the end, we only work on this a very very few times, otherwise we learn from it and repeat overfitting.\n\nWe are going to implement this train/dev/test splits:\n\n\nShow the code\n# build the dataset\ndef buid_dataset(words):\n    block_size = 3\n    X, Y = [], []\n\n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix]\n\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\nXtr, Ytr = buid_dataset(words[:n1])\nXdev, Ydev = buid_dataset(words[n1:n2])\nXte, Yte = buid_dataset(words[n2:])\n\n\ntorch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n\n\nNow we’re already to train on splits of the dataset, but let’s hold on as we are talking abount overfitting. As discussed, overfitting also come from using a complex (too many parameters) for a small data set.\nOur dataset has roughly 228k records, while the size of network is only 3.4k. So we are still underfitting, let’s continue to complexify our neural networks.\n2 things to consider here:\n\nthe size of tanh - hidden layer; and\ndimensions of embedding space."
  },
  {
    "objectID": "blog/2024-11-20-nn-z2h-p3/index.html#visualizing-the-loss-character-embeddings",
    "href": "blog/2024-11-20-nn-z2h-p3/index.html#visualizing-the-loss-character-embeddings",
    "title": "NN-Z2H Lesson 3: Building makemore part 2 - MLP",
    "section": "visualizing the loss, character embeddings",
    "text": "visualizing the loss, character embeddings\nFirst we want to see: - how the loss decrease with 200k training loop with current network setting, learning rate decay to 0.01 after first 100k; and - how the current character embeddings recognize the similarity between characters in (2D) space.\nTraining on the Xtr, Ytr:\n\n\nShow the code\n# Lookup table\ng = torch.Generator().manual_seed(2147483647)\nC = torch.randn((27, 2), generator=g)\n\n# Layer 1 - tanh\nW1 = torch.randn((6, 100), generator=g)\nb1 = torch.randn(100, generator=g)\n\n# Layer 2 - softmax\nW2 = torch.randn((100, 27), generator=g)\nb2 = torch.randn(27, generator=g)\n\n# All params\nparameters = [C, W1, b1, W2, b2]\nprint(\"No of params: \", sum(p.nelement() for p in parameters))\n\n# Pre-training\nfor p in parameters:\n    p.requires_grad = True\n\n# Stats holders\nlossi = []\nstepi = []\n\n# Training on Xtr, Ytr\nfor i in range(200_000):\n    # minibatch construct                                           \n    ix = torch.randint(0, Xtr.shape[0], (32,))                       #👈\n    # forward pass:\n    emb = C[Xtr[ix]] # (32, 3, 2)                                    #👈\n    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n    logits = h @ W2 + b2 # (32, 27)\n    loss = F.cross_entropy(logits, Ytr[ix])                          #👈\n    # backward pass:\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    # update\n    lr = 0.1 if i &lt;= 100_000 else 0.01                                #👈\n    for p in parameters:\n        p.data += - lr * p.grad\n\n    # track stats\n    lossi.append(loss.item())\n    stepi.append(i)\n\nprint(\"Loss on minibatch: \", loss.item())\n\n\nNo of params:  3481\nLoss on minibatch:  2.1069579124450684\n\n\nLoss on whole training dataset:\n\n\nShow the code\nemb = C[Xtr] # (32, 3, 2)                                    \nh = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\nlogits = h @ W2 + b2 # (32, 27)\nloss = F.cross_entropy(logits, Ytr)   \nloss.item()\n\n\n2.259035110473633\n\n\nLoss on dev/validation dataset, it’s not much different from loss on training as the model is still underfitting, it still generalizes thing:\n\n\nShow the code\nemb = C[Xdev] # (32, 3, 2)                                    \nh = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\nlogits = h @ W2 + b2 # (32, 27)\nloss = F.cross_entropy(logits, Ydev)   \nloss.item()\n\n\n2.257272958755493\n\n\nVisualizing loss, we can see the loss shaking significantly as the batch size still small - 32.\n\n\nShow the code\nplt.plot(stepi, lossi)\n\n\n\n\n\n\n\n\n\nVisualizing the character embeddings, we can see the model can cluster for eg. vowels a, e, i, o, u.\n\n\nShow the code\nplt.figure(figsize=(8,8))\nplt.scatter(C[:,0].data, C[:, 1].data, s=200)\nfor i in range(C.shape[0]):\n    plt.text(C[i,0].item(),C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\")\nplt.grid('minor')"
  },
  {
    "objectID": "blog/2024-11-20-nn-z2h-p3/index.html#experiment-larger-hidden-layer-larger-embedding-size",
    "href": "blog/2024-11-20-nn-z2h-p3/index.html#experiment-larger-hidden-layer-larger-embedding-size",
    "title": "NN-Z2H Lesson 3: Building makemore part 2 - MLP",
    "section": "experiment: larger hidden layer, larger embedding size",
    "text": "experiment: larger hidden layer, larger embedding size\nNow we can experiment a larger hidden layer (300), and larger embedding_size (10). Below is the whole code:\n\n\nShow the code\n# hyper-parameters\nblock_size = 3 # number of chracters / inputs to predict the nextone\nno_chars = 27 # number of possible chracters, include '.'\nemb_size = 10 # no of dimensions of the embedding space.\nhidden_size = 300 # size of the hidden - tanh layer\nbatch_size = 32 # minibatch size for training, 2, 4, 8, 16, 32, 64, etc\n\n# build the dataset\ndef buid_dataset(words):\n\n    X, Y = [], []\n\n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix]\n\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\n# 80 - 10 - 10 splits\nXtr, Ytr = buid_dataset(words[:n1])\nXdev, Ydev = buid_dataset(words[n1:n2])\nXte, Yte = buid_dataset(words[n2:])\n\n# Lookup table - 10 dimensional space\ng = torch.Generator().manual_seed(2147483647) # for reproductivity\nC = torch.randn((no_chars, emb_size), generator=g)\n\n# Layer 1 - tanh - 300 neurons\nW1 = torch.randn((block_size * emb_size, hidden_size), generator=g)\nb1 = torch.randn(hidden_size, generator=g)\n\n# Layer 2 - softmax\nW2 = torch.randn((hidden_size, no_chars), generator=g)\nb2 = torch.randn(no_chars, generator=g)\n\n# All params\nparameters = [C, W1, b1, W2, b2]\nprint(\"No of params: \", sum(p.nelement() for p in parameters))\n\n# Pre-training\nfor p in parameters:\n    p.requires_grad = True\n\n# Stats holders\nlossi = []\nstepi = []\n\n# Training on Xtr, Ytr\nfor i in range(200_000):\n    # minibatch construct                                           \n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))                      \n    # forward pass:\n    emb = C[Xtr[ix]]                                                \n    h = torch.tanh(emb.view(-1, block_size * emb_size) @ W1 + b1)\n    logits = h @ W2 + b2\n    loss = F.cross_entropy(logits, Ytr[ix]) \n    # backward pass:\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    # update\n    lr = 0.1 if i &lt;= 100_000 else 0.01 \n    for p in parameters:\n        p.data += - lr * p.grad\n\n    # track stats\n    lossi.append(loss.item())\n    stepi.append(i)\n\nprint(\"Loss on minibatch: \", loss.item())\n\n\ntorch.Size([182580, 3]) torch.Size([182580])\ntorch.Size([22767, 3]) torch.Size([22767])\ntorch.Size([22799, 3]) torch.Size([22799])\nNo of params:  17697\nLoss on minibatch:  2.0859084129333496\n\n\n\n\nShow the code\nemb = C[Xtr]                                 \nh = torch.tanh(emb.view(-1, block_size * emb_size) @ W1 + b1)\nlogits = h @ W2 + b2\nloss = F.cross_entropy(logits, Ytr)   \nprint(\"Loss on whole training set: \", loss.item())\n\nemb = C[Xdev]                                \nh = torch.tanh(emb.view(-1, block_size * emb_size) @ W1 + b1)\nlogits = h @ W2 + b2\nloss = F.cross_entropy(logits, Ydev)   \nprint(\"Loss on dev/validation set: \", loss.item())\n\nemb = C[Xte]                                \nh = torch.tanh(emb.view(-1, block_size * emb_size) @ W1 + b1)\nlogits = h @ W2 + b2\nloss = F.cross_entropy(logits, Yte)   \nprint(\"Loss on test set: \", loss.item())\n\n\nLoss on whole training set:  2.117095947265625\nLoss on dev/validation set:  2.1767637729644775\nLoss on test set:  2.1748299598693848"
  },
  {
    "objectID": "blog/2024-11-20-nn-z2h-p3/index.html#summary-of-our-final-code-conclusion",
    "href": "blog/2024-11-20-nn-z2h-p3/index.html#summary-of-our-final-code-conclusion",
    "title": "NN-Z2H Lesson 3: Building makemore part 2 - MLP",
    "section": "summary of our final code, conclusion",
    "text": "summary of our final code, conclusion\n\n\nShow the code\nplt.plot(stepi, lossi)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nplt.figure(figsize=(8,8))\nplt.scatter(C[:,0].data, C[:, 1].data, s=200)\nfor i in range(C.shape[0]):\n    plt.text(C[i,0].item(),C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\")\nplt.grid('minor')\n\n\n\n\n\n\n\n\n\nWe can see the loss on validation set and test set are quite similar as we are not try different scenarios to calibrate/tune hyperparamters much. So they both have the same suprise to the model training by Xtr.\nWe still have rooms for improvement!"
  },
  {
    "objectID": "blog/2024-11-20-nn-z2h-p3/index.html#sampling-from-the-model",
    "href": "blog/2024-11-20-nn-z2h-p3/index.html#sampling-from-the-model",
    "title": "NN-Z2H Lesson 3: Building makemore part 2 - MLP",
    "section": "sampling from the model",
    "text": "sampling from the model\nBut our networks now can generate more name-like name!\n\n\nShow the code\ng = torch.Generator().manual_seed(2147483647 + 10)\n\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n      emb = C[torch.tensor([context])] # (1,block_size,d)\n      h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n      logits = h @ W2 + b2\n      probs = F.softmax(logits, dim=1)\n      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n      context = context[1:] + [ix]\n      out.append(ix)\n      if ix == 0:\n        break\n    \n    print(''.join(itos[i] for i in out))\n\n\neria.\nkayanniee.\nmad.\nrylle.\nevers.\nendra.\nkalie.\nkaillie.\nshivonna.\nkeisenna.\naraelyzion.\nkalin.\nshubergenghies.\nkindrendy.\npan.\npuon.\nubertedir.\nyarleyel.\nyule.\nmyshelda."
  },
  {
    "objectID": "blog/2024-11-20-nn-z2h-p3/index.html#google-collab-new-notebook-advertisement",
    "href": "blog/2024-11-20-nn-z2h-p3/index.html#google-collab-new-notebook-advertisement",
    "title": "NN-Z2H Lesson 3: Building makemore part 2 - MLP",
    "section": "google collab (new!!) notebook advertisement",
    "text": "google collab (new!!) notebook advertisement\nColab link: https://colab.research.google.com/drive/1YIfmkftLrz6MPTOO9Vwqrop2Q5llHIGK?usp=sharing\nThanks Andrej!"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Blog",
    "section": "",
    "text": "Let’s build GPT, in code, spelled out!\n\n\n\n\n\n\ntil\n\n\npython\n\n\nandrej karpathy\n\n\nnn-z2h\n\n\nneural networks\n\n\n\nBuild a Generatively Pretrained Transformer (GPT), following the paper ‘Attention is All You Need’ and OpenAI’s GPT-2 / GPT-3\n\n\n\n\n\nApr 27, 2025\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nI completed 25 days of DAX (Friday) challenge, in 2 days!\n\n\n\n\n\n\nDAX\n\n\n\nby Curbal AB, Edition 3, on Northwind dataset\n\n\n\n\n\nFeb 12, 2025\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Large Language Models: A Summary of Andrej Karpathy’s Talk\n\n\n\n\n\n\nandrej karpathy\n\n\nllm\n\n\nneural networks\n\n\n\nSummary of Andrej Karpathy’s “Intro to Large Language Models” talk, deeply diving into the core concepts, current state, future directions, and security challenges surrounding LLMs.\n\n\n\n\n\nDec 12, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nNN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet\n\n\n\n\n\n\ntil\n\n\npython\n\n\nandrej karpathy\n\n\nnn-z2h\n\n\nneural networks\n\n\n\nCNN/WaveNet and a lot of reading of documentation, keeping track of multidimensional tensor shapes, moving between jupyter notebooks and repository code, …\n\n\n\n\n\nDec 9, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced SQL\n\n\n\n\n\n\ntil\n\n\nsql\n\n\n\nSQL things that I wish I could know earlier.\n\n\n\n\n\nDec 6, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nNN-Z2H Lesson 5: Building makemore part 4 - Becoming a Backprop Ninja\n\n\n\n\n\n\ntil\n\n\npython\n\n\nandrej karpathy\n\n\nnn-z2h\n\n\nneural networks\n\n\nbackpropagation\n\n\n\nBecome swole doge: build a 2-layer MLP and deep dive to how gradients flow backwards with cross entropy loss, 2nd linear layer, tanh, batchnorm, 1st linear layer, and the embedding table.\n\n\n\n\n\nDec 2, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nNN-Z2H Lesson 4: Building makemore part 3 - Activations & Gradients, BatchNorm\n\n\n\n\n\n\ntil\n\n\npython\n\n\nandrej karpathy\n\n\nnn-z2h\n\n\nneural networks\n\n\n\ndive into the internals of MLPs, scrutinize the statistics of the forward pass activations, backward pass gradients, understand the health of your deep network, introduce batch normalization\n\n\n\n\n\nNov 26, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nNN-Z2H Lesson 3: Building makemore part 2 - MLP\n\n\n\n\n\n\ntil\n\n\npython\n\n\nandrej karpathy\n\n\nnn-z2h\n\n\nneural networks\n\n\n\nimplement a multilayer perceptron (MLP) character-level language model, introduce model training, learning rate tuning, hyperparameters, evaluation, train/dev/test splits, under/overfitting, etc.\n\n\n\n\n\nNov 20, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nNN-Z2H Lesson 2: The spelled-out intro to language modeling - building makemore\n\n\n\n\n\n\ntil\n\n\npython\n\n\nandrej karpathy\n\n\nnn-z2h\n\n\nbigram\n\n\nneural networks\n\n\n\nimplement a bigram character-level language model, focus on (1) introducing torch, and (2) the overall framework of language modeling that includes model training, sampling, and the evaluation of a loss\n\n\n\n\n\nNov 15, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Project with Mamba, Python, and VS Code on Window\n\n\n\n\n\n\npython\n\n\n\nIn search for the best environment & dependencies management tool for data project\n\n\n\n\n\nNov 4, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nKeynote on: Causal Effect Estimation in Practice - lessons learned from E-commerce & Banking\n\n\n\n\n\n\ncausal inference\n\n\n\nby Danial Senejohnny, PyData Amsterdam 2024. Thumbnail credit to this post\n\n\n\n\n\nOct 25, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nDAX exercises by WiseOwl Training\n\n\n\n\n\n\nDAX\n\n\n\nI am trying to solve all 59 DAX exercises from WiseOwl Training Platform\n\n\n\n\n\nSep 30, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nMột số khuôn mẫu tư duy trong Brazilian Jiu-jitsu\n\n\n\n\n\n\nbjj\n\n\n\na quick and short note on “Mechanic Models of BJJ: A Crash Course” by Steve Kwan\n\n\n\n\n\nAug 6, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nNote on Mathematics as the Universal Language of Nature\n\n\n\n\n\n\nmath\n\n\n\ntalk by Robert Seiringer (IST Austria), hosted by VIASM, Hue University, August 8 2024\n\n\n\n\n\nAug 8, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nA Prefect Workshop\n\n\n\n\n\n\npython\n\n\npydata\n\n\nprefect\n\n\n\nby Dr. Adam Hill in PyData London 2024\n\n\n\n\n\nJul 17, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nA lesson of advanced python from Juan Rodríguez\n\n\n\n\n\n\npython\n\n\npydata\n\n\n\nThe lecture by Juan Luis Cano Rodríguez in Master in Business Analytics and Big Data, 2021-2022, which focused on modern SWE practices using Python and the insight we can gain on how Data Science projects are put into production.\n\n\n\n\n\nJul 8, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nNow I know docker init is a thing\n\n\n\n\n\n\ndocker\n\n\npython\n\n\n\nHow to properly use Docker to containerize your Python project\n\n\n\n\n\nJul 30, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nBJJ Beginners Guide\n\n\n\n\n\n\nbjj\n\n\n\nThumbnail image credit to this post\n\n\n\n\n\nJul 29, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nAnother James Powell’s impromptu talk, summary by Clauder 3.5 Sonnet\n\n\n\n\n\n\npython\n\n\npydata\n\n\n\nLook how he has confidently presenting and live coding with broken laptop, bad keyboard\n\n\n\n\n\nJul 18, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nRecap: So you want to be a Python expert?\n\n\n\n\n\n\npython\n\n\npydata\n\n\n\nby James Powell, PyData Seattle 2017\n\n\n\n\n\nJul 10, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nVim 101\n\n\n\n\n\n\npython\n\n\nvim\n\n\n\nactually NeoVim which is, actually Vim\n\n\n\n\n\nJul 12, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nVề quy ước đặt tên dữ liệu\n\n\n\n\n\n\ndata quality\n\n\nname convention\n\n\ncontrolled vocabulary\n\n\nmetadata\n\n\n\nDanh pháp quy ước để đặt tên cho dữ liệu dạng bảng, tăng tốc độ hiểu biết và sử dụng dữ liệu\n\n\n\n\n\nJun 5, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nRandom youtube video: How are memories stored in neural networks?\n\n\n\n\n\n\nneural networks\n\n\n\nbro made a channel dedicated to neural networks, posted one video, gained 24k subs, decided this was enough and left - @ginqus\n\n\n\n\n\nJun 26, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nHi Docker 🐳\n\n\n\n\n\n\ndocker\n\n\npython\n\n\n\nA single-member full-stack data team me learning Docker 😂\n\n\n\n\n\nJun 24, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nI am starting training Brazilian Jiu-jitsu\n\n\n\n\n\n\nlife\n\n\nbjj\n\n\n\nWhen you do Jiu-jitsu, you won’t look at a bigger guy and say: Oh No!, you’ll look and say: How Interesting! - Rener Gracie.\n\n\n\n\n\nFeb 28, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nBitcoin tour in Python\n\n\n\n\n\n\ntil\n\n\ncrypto\n\n\nbitcoin\n\n\npython\n\n\nandrej karpathy\n\n\n\nNhân việc thánh Andrej Karpathy ra MV tutorial mới về build GPT-2 from scratch, tôi lại ghé thăm trang web cá nhân của anh ấy.\n\n\n\n\n\nJun 10, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nObservable Framework try-out\n\n\n\n\n\n\nDashboard\n\n\nObservable\n\n\ntil\n\n\n\nMy first Observable Framework project\n\n\n\n\n\nJun 19, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nNN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd\n\n\n\n\n\n\ntil\n\n\npython\n\n\nandrej karpathy\n\n\nnn-z2h\n\n\nbackpropagation\n\n\nneural networks\n\n\n\nbackpropagation, from neuron to neural network, from micro grad to pytorch, and more\n\n\n\n\n\nJun 16, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nTIL: Xử lý dynamic-ragged array trong Excel\n\n\n\n\n\n\nexcel\n\n\nlambda\n\n\ntil\n\n\n\nTừ khi các hàm mảng như VTACK() xuất hiện, mình thường xuyên sử dụng các patterns như REDUCE/VSTACK để mở rộng khả năng xử lí các phần tử của mảng: thay vì chỉ tính toán với đầu ra là aggregate value, nay đã có thể tạo ra các mảng con. Tuy nhiên không phải lúc nào cũng có thể làm việc với mảng trong mảng một cách đơn giản\n\n\n\n\n\nJun 6, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nStorytelling with Data: Let’s Practice, Improve this table!\n\n\n\n\n\n\npython\n\n\nvisualization\n\n\n\nPracticing to improve a cross-tab report table using Python\n\n\n\n\n\nMay 22, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nPython is cool ❄\n\n\n\n\n\n\npython\n\n\nlambda\n\n\n\nLess-known Python features\n\n\n\n\n\nMay 21, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nKinh tế lượng với Julia 1: Làm quen với Julia\n\n\n\n\n\n\nQEJ\n\n\nJulia\n\n\nQuant Econ\n\n\n\nĐây là bài khởi đầu, làm quen với ngôn ngữ Julia trong chuỗi series các mô hình Kinh tế lượng - nguồn\n\n\n\n\n\nApr 24, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nCách đọc Guitar Tab đơn giản\n\n\n\n\n\n\nGuitar\n\n\n\nBạn có thể không biết đọc nhạc nhưng ít nhất bạn phải biết cách đọc tab guitar!\n\n\n\n\n\nJan 24, 2024\n\n\nTuan Le Khac\n\n\n\n\n\n\n\n\n\n\n\n\nA tutorial of ggplot2\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\nData Visualization\n\n\n\nThis is a again a lesson on ggplot2\n\n\n\n\n\nDec 24, 2023\n\n\nTuan Le Khac\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "curriculum/resume/index.html",
    "href": "curriculum/resume/index.html",
    "title": "Résumé",
    "section": "",
    "text": "Download my résumé"
  },
  {
    "objectID": "jiu_jitsu_journal/index.html",
    "href": "jiu_jitsu_journal/index.html",
    "title": "My Jiu Jitsu Journal",
    "section": "",
    "text": "There was no note for April haha :“&gt;\n\n\n\n\n\n\n05.05.2025 | Lockdown\n\n\n\n\n\n\nLockdown là một thế khóa chân rất hiệu quả trong việc kiểm soát một cách cứng nhắc một chân của đối thủ. Nó hiệu quả khi ngăn cho chúng ta không bị pass guard, nhưng cũng làm chúng ta bế tắc khi hai chân cũng tự khóa cứng;\nChìa khóa là sự linh hoạt của hông/hip, khi lockdown, chân của chúng ta sẽ dễ dàng đảo theo trục ngang - như con lắc đơn - hơn là co duỗi theo trục dọc. Ta có thể tận dụng nó để làm bánh đà xoay hông;\nVì thế luôn có hai lựa chọn đặt hông: (1) xoay vào trong như half guard, (2) hoặc xoay ra ngoài tiếp cận lưng của đối thủ;\nChúng tôi drill lockdown từ những vị trí bị động như (1) khi cố thoát mount, hoặc chủ động như (2) half guard. Mục tiêu của các bài tập vẫn là “Crafting a door”, xây dựng thêm một điểm bản lề phía upper - ví dụ chính là vai. Khi đó cùng với bản lề thứ 2 là chân lock down, ta có thể sweep được đối thủ;\nHôm nay có fighing pant mới hehe - 悟空-黑神话🙊🙉🙈🐵.\n\n\n\n\n\n\n\n\n\n\n\n\n\n03.22.2025 | Crafting a door\n\n\n\n\n\nMọi kỹ thuật sweep trong Jiu-jitsu đều là hình thái của việc tạo ra một cánh cửa và “mở” nó: tạo và cố định 2 bản lề (hinges), sau đó tìm một điểm làm tay nắm cửa (door handle). Ví dụ như: X-guard sweep - bạn có thể tưởng tượng ra chứ.\nTuy nhiên trong buổi hôm nay, chúng tôi drill lại butterfly sweep. Sifu muốn nhắc nhở lại rằng sweep đối phương sang hai bên khi ở butterfly guard là không hiệu quả - vì hai chân của đối phương đang base theo hướng này! Chúng ta cần sweep được đối thủ qua vai. Để làm được điều này hãy cố gắng đạt được finger-4 grip trên một tay và chôn nó vào hông đối diện của đối thủ. Lúc này với grip tay là 1 hinge, hook chân tương ứng với tay đó là 1 hinge, còn hook chân còn lại chính là door handle -&gt; ta có thể sweep đối thủ chéo qua vai.\n\n\n\nAnatomy of a Door\n\n\n\nQuote 1: Khi không thể chống lại một lực từ đối phương, hãy chuyển hướng lực của bạn theo hướng đó (cộng thêm lực của bạn) để đạt được một vị trí có lợi hơn. – cái này nghe giống Aikido.\n\n\nQuote 2: Trong tập luyện, nên tập theo xu hướng thích nghi và xử lí các tình huống mà đối phương mang đến cho bạn, hơn là áp đặt đối phương vào một trạng thái/thế đánh mà mình mong muốn.\n\n\n\n\n\n\n\n\n\n\n03.08.2025 | Count your opponent’s knee\n\n\n\n\n\nMột buổi chiều thứ 7 tôi chống lại sự lười bằng cách xách ba lô lên và đi tập. Lớp 3h chiều cũng gần như chỉ lác đác vài học viên.\nChúng tôi học cách đẩy đối thủ ngã về phía trước từ đòn single leg - bằng chân (với chân còn lại chống xuống đất). Đồng thời cố gắng spin quanh chân bị lock để finish. Chúng tôi tìm kiếm các phương án xử lí khi base của họ quá vững chắc để sweep như: (a) X-guard để dàn trọng tâm ra, (b) chuyển thành reverse single leg, hay (c) ăn sâu chân đối thủ vào hip hơn nữa để tăng khả năng sweep.\nThứ khiến mình suprised nhiều nhất cho buổi học này chính là cách nhìn nhận của sifu trong bottom game: thay vì ghi nhớ guard nào đánh như thế nào, hãy trigger một thói quen đánh khi quan sát một biểu hiện đơn giản hơn của đối thủ - có mấy đầu gối chạm đất - một yếu tố quan trọng khi correlate tới position/alignment của đối thủ.\nMột model rất dễ vận hành - easy to observe predictors, và hiệu quả rất cao.\n\nCó bao nhiêu loại guards đi chăng nữa, khi các bạn ở bottom, cũng sẽ chỉ có 3 trường hợp cho đối thủ của bạn: (1) hai đầu gối chạm đất, (2) một đầu gối chạm đất, (3) không có đầu gối nào chạm đất. Xây dựng gameplan dựa trên ba hình thái này hoàn toàn đơn giản hơn nhiều so với việc memorizing all guard techniques. Vu Dinh Tien - trích tương đối.\n\n\nĐối thủ có 1 base rất chắc chắn, trọng tâm thấp: đánh tay, cố gắng lấy overhook từ đó đạt được sweep hoặc backtake;\nĐối thủ có sự cân bằng giữa base và flexibility: xu hướng của sifu sẽ là hook chân (như mọi bài đánh chân khác) vào cái chân quỳ, hai tay sẽ đánh chân còn lại và tiến tới sweep;\nSingle leg, SL-X, reverse single leg takedown: hướng tới sweep và leglock cho trường hợp này.\n\n\n\n\n\n\n\n\n\n\n\n\n\n02.21.2025 | Attention is all you need\n\n\n\n\n\nChúng tôi chú ý tới các tiểu tiết (nhưng quan trọng) nhiều hơn khi thực hiện đòn ankle lock:\n\nLuôn giữ áp lực và đe dọa khi slide vào vị trí single leg;\nHai chân kẹp chặt với mục tiêu cố định đầu gối;\nHãy luyện tập phản xạ dùng một tay (không phải tay dùng để leg lock) giữ gót đối phương và đưa nó vào armpit;\nHãy làm cong đầu gối: khi chân đối phương thẳng, ta đang bẻ cả trục chân của đối phương. Khi đầu gối cong, khớp gối tự nhiên sẽ thành một đầu khóa và đòn khóa chân sẽ hiệu quả hơn;\nKhông ankle lock bằng lực siết tay, mà bằng cách đẩy hông ra phía trước.\n\n\n\n\nBuổi dạy của sf và Huân\n\n\n\n\n\n\n\n\n\n\n\n02.19.2025 | Heel hook\n\n\n\n\n\nLiên tiếp là các bài học về đánh chân:\n\nTừ vị trí ankle lock kiểu truyền thống đã học hôm trước, ta có thể chuyển qua kiểu hiện đoạn. Spin quanh trục chân (luôn giữ áp lực) để tìm cơ hội grab luôn chân còn lại của đối thủ. Từ đó:\n\nVẫn có thể tiếp tục ankle lock cái chân đang giữ, tuy nhiên tỷ lệ thoát khá cao;\nTa sẽ không quá commit và ngay khi đối thủ cố thoát một chân, ta có thể heel hook chân còn lại.\n\nViệc thực hiện đúng các setup khi heel hook cũng quan trọng như việc giữ được nó khi đối thủ cố thoát bằng cách roll/spin. Ta cần follow up.\nCó hai lựa chọn đánh heel hook khi đang pass guard đối thủ.\n\n\n\n\n\n\n\n\n\n\n02.17.2025 | The first leg-lock lesson\n\n\n\n\n\nSau Tết quá lười, mãi tới hôm nay mình mới đi tập được buổi thứ 2 (hôm đầu lười viết 😊). Hôm nay lần đầu tiên mình được dạy một đòn leg submission ở Lò - ankle lock - đòn khóa mắt cá chân, thứ được xem là signature của Sói Jiu-jitsu.\nPipeline để đạt được vị trí này vẫn là bài học quen thuộc: single leg -&gt; x-guard -&gt; sweep. Tuy nhiên lần này ta không cố gắng sit up trước đối thủ để giành lấy lợi thế khi pass guard, mà setup lại vị trí single leg - ở vị trí cả 2 cùng nằm. Bây giờ có mấy điểm quan trọng cần ghi nhớ sau:\n\nPhải cố định được đầu gối và làm gập chân đối thủ: Như single leg, ta ở tư thế ôm gấu bông, dùng hai chân kẹp để giữ chặt đầu gối đối thủ. Thêm nữa, nếu chân của họ thẳng, phần cổ chân sẽ rất khỏe, ta cần làm gập nó;\nDùng tay lock cổ chân của đối phương vào armpit: Siết tay vừa phải và ngả tay ra sau cho tới khi cảm nhận được gót + mu bàn chân đối thủ.\n\nĐể finish, nằm nghiêng xuống, giấu vai đi, đẩy bụng vào.\nMình thực hiện không mượt mà lắm. Sẽ cần luyện tập nhiều hơn.\nĐọc thêm về Ankle Lock: https://bjjfanatics.com/blogs/news/ankle-lock-bjj. Happy training!\n\n\n\n\n\n\n\n\n\n\n\n\n01.08.2025 | Stand up and fight, pal!\n\n\n\n\n\nCách đây hàng chục triệu cho tới hàng triệu năm, trong kỷ Neogene (23-2.58 mya), cụ thể là trong thời kỳ Miocene muộn và Pliocene, tổ tiên của loài người đã - lần đầu tiên - phát triển khả năng lưỡng cự - bipedalism, tức đi bằng hai chân.\nHôm nay, sau gần một năm lăn lê bò trườn lộn dưới mặt đất, mình lần đầu tiên thấy sư phụ dạy đánh đứng - what an evolution!\n\n\n\nLịch sử tiến hóa của loài người, nguồn science.org\n\n\nLà một buổi học đơn giản - chúng tôi luyện tập cách xử lí cơ bản khi bị đối thủ collar tie hoặc neck tie - một phần trong hand fighting:\n\nThế tấn: cánh tay đối thủ dùng để neck tie thường ứng với chân đứng sau;\nTa dùng tay tương ứng để grip (nghĩa là tay trái - tay trái - chúng sẽ nằm chéo khi hai người đối diện);\nTuy nhiên không dùng lực tay để phá neck tie, thay vào đó đưa vai lên cao, backstep chữ L - pivot \\(90\\degree\\) về sau lưng, tay grip chỉ cần giữ. Áp lực từ vai chúng ta vào cổ tay đối thủ sẽ làm việc;\nNó tự nhiên sẽ cho ta 1 vị trí tay 2 đánh 1, kiểu figure 4, lúc đó ta chỉ cần đơn giản là sit down - on our knees, là đã có thể kéo đối thủ xuống. Một cách tự nhiên, khi chúng ta đưa tay còn lại móc vào hướng chân xa của đối thủ, ta sẽ luôn có được 1 trong các kiểm soát sau:\n\nNếu chân xa ở phạm vi gần, ta có thể pick vào bắp chân và sweep;\nNếu không móc vào được chân, ta có khả năng sẽ móc được vào armpit. Lúc này với việc circle 2 tay, ta có thể kiểm soát cả hai tay đối phương. Vì chỉ còn trụ bằng hai chân, đối thủ sẽ dễ bị kéo ngã về phía trước. Di chuyển linh hoạt và lấy side control;\nNếu hụt mất armpit, ta có thể móc vào cổ đối phương, kết nối hai tay và circle, ta có được anaconda grip, ngả người về phía cánh tay khi khống chế - mất trụ để finish;\n\nNếu đối thủ đưa 1 chân lại gần, vào giữa central line để rút ngắn khoảng cách, ta có thể linh hoạt sử dụng đòn gạt chân tương tự Osoto Gari (大外刈) trong Judo để takedown đối phương.\n\nĐể kết thúc được đối thủ bằng anaconda hoặc d’arce choke, ta cần đầu của đối thủ nằm gọn trước ngực (đỉnh đầu vào chấn thủy). Việc tập luyện tăng dung tích phổi: đưa hết không khí ra ngoài khi setup đòn, và bơm không khí vào trong khi siết sẽ mang lại hiệu quả lớn.\nTrust me bro, tôi đã thử bị sf siết, hoàn toàn không dùng lực tay. Oss!\n\n\n\n\n\n\n\n\n\n01.04.2025 | Losing position the right way\n\n\n\n\n\n Một buổi tập chiều thứ 7, đầu năm mới 2025, và rất nhiều thứ thú vị từ Head Coach. \n\n\nTrong hai tuần vừa rồi chúng ta đã học và tập luyện một phương thức tấn công rất đặc trưng của lò Sói, từ vị trí half guard - nhện dệt lưới - i.e. cố gắng đan các đường chéo lên các bộ phận cơ thể của đối phương. Nếu thực hiện đúng, ta có thể đạt được một vị trí mà ở đó chân half guard ngoài có thể hook vào chân của đối phương (gần giống lockdown), và control được tay cùng bên với figure-4-shape grip.\nTừ vị trí này - với bàn chân bị kéo lên không, đối thủ đã rất khó có thể thoát ra được, vì bất cứ lúc nào họ muốn generate force để thoát ra, họ sẽ luôn đưa một chi cơ thể vào phạm vi tấn công. Trong hình minh họa với 0 là chân bị kiểm soát, 1,2,3 lần lượt là 2 tay và chân theo thứ tự xa dần trong phạm vi tấn công: khi đối thủ muốn gỡ tay 1 thì bắt buộc phải đưa tay 2 lại gần để tạo lực. And so on, đối thủ sẽ luôn tự đưa mình vào một vị trí bất lợi khác.\n\n\n\nMinh họa half guard từ Lachlan Giles - mình không chụp ảnh và không tìm được hình vẽ nào trên mạng mô tả chính xác đòn đánh của Sói, nguồn flograppling\n\n\nVậy muốn thoát ra, họ sẽ muốn gỡ được chân 0 ra khỏi vị trí bị hook, và đứng dậy - điều này khó nhưng là có thể làm được. Hôm tay ta sẽ học cách ứng xử với trước hợp này!\n\n\n\nĐiều tiên quyết và quan trọng nhất trong trường hợp này, chíng là chuyển chân bên ngoài - sau khi đã mất hook - vào vị trí shin-to-shin, đồng thời ngồi dậy dán body vào chân đối thủ. Chúng ta sẽ muốn giữ wrist grip khi còn có thể và lợi dụng lực kéo của đối phương để ngồi dậy, hoặc pin tay của đối thủ bằng grip đó xuống sàn và ngồi dậy bằng khuỷu tay. Chiếc chân bên trong vẫn nên cố gắng hook vào sau bắp chân - đầu gối, khi ta ngồi dậy có thể chuyển sang hook/wedge chân đối diện.\nBây giờ có hai khả năng xảy ra:\n\nĐối thủ có thể đứng gần như thẳng (chân rộng bằng vai, khuỵu gối) hoặc (đứng) thẳng;\nĐối thủ đứng nhưng base chân tay rộng ra.\n\nCái gì cũng có sự trade off, với trường hợp một, chân của họ sẽ nằm trong phạm vi tấn công của chúng ta và rất dễ để take down, nhưng ta sẽ nói về khả năng đó sau. Với trường hợp 2, khi đối thủ base càng rộng, sẽ tạo ra một tình thế mà đầu của ta nằm cao hơn lưng đối thủ, từ đây:\n\nTa có thể xoay trục cơ thể từ vị trí shin-to-shin với body stick vào chân, chúng ta đeo thêm một cục tạ lên cái chân bị đè. Lưu ý rằng chỉ phân bổ khối lượng cơ thể vào từ dưới vùng xương chậu. Từ đây ta có thể sweep và lấy side control;\nThứ 2, với nhận định rằng cái chân còn lại không có nhiều giá trị trong việc hook/wedge chân còn lại của đối thủ - vốn có không gian hoạt động rất rộng, ta có thể dùng nó như một cánh tay đà để roll xuống giữa hai chân đối thủ. Nhấc bàn chân bị shin-to-shin lên, với việc kiểm soát đầu gối tương ứng, cũng có thể đưa ta về vị trí side control sau khi sweep.\n\n\n\n\nGiờ hãy nói về trường hợp 1 ở trên:\n\nNếu đối thủ đứng bằng hai chân: double legs take down;\nNếu đối thủ cố gắng đưa chân còn lại ra xa: ankle pick -&gt; single legs take down;\nTa có thể kết hợp với việc sự dụng đầu gối ở vị trí shin-to-shin, đẩy vào phía sau đầu gối đối phương, đưa no cong về phía trước. Đồng thời hai ta bốc và xoắn double legs - làm họ ngã về sau. 🚫 Tuy nhiên điều này không được thực hành tại lò, vì có nguy cơ gây chấn thương gối rất cao.\n\n Happy training. Oss!"
  },
  {
    "objectID": "jiu_jitsu_journal/index.html#november",
    "href": "jiu_jitsu_journal/index.html#november",
    "title": "My Jiu Jitsu Journal",
    "section": "November",
    "text": "November\n\n\n\n\n\n\n11.29.2024 | Put constant pressure on your opponent\n\n\n\n\n\n\nTa phải đánh đổi giữa sự linh hoạt (flexibility) của bản thân và áp lực (pressure) lên người đối thủ.\n\nHôm nay tập hand fighting và làm quen kiểm soát tay từ butterfly guard (bottom), hai thứ quan trọng cần nhớ vẫn là:\n\nTác dụng lực vào tay theo trục vuông góc với mắt - cơ thể hai đấu thủ;\nCố gắng đưa khuỷu tay ra xa.\n\nArm lock có thể dùng là outside overhook với 4-shape hand grip, inside overhook (mình hiểu outside nghĩa là body mình nằm ngoài trục cơ thể đối phương và ngược lại cho inside), hoặc underhook với cách-khóa-giống-rnc. Hãy học các linh hoạt switch giữa các kiểu control này.\nCuối cùng thì 1 dạng omoplata với hai-tay-grip-vào-nhau. Việc hai tay luân phiên kiểm soát được phần vai + tay của đối phương (luôn có áp lực) trước khi đạt được grip là thứ thú vị nhất hôm nay. Đòn này cũng có thể kết thúc bằng triangle and/or armbar.\nOss!\n\n\n\n\n\n\n\n\n\n11.27.2024 | Some details to focus\n\n\n\n\n\nHình như từ lúc ghi cái nhật ký này thì tần suất đi tập bị thưa đi thì phải =))))\nHôm này, sau một tuần về Nghệ An, mình tập lại và vẫn là bài thực hành kiểm soát khoảng cách, chuyển đổi phương án tấn công.\n\nCó 1 điểm mới là tại trạng thái sắp lost contact, với hand grip ta có thể áp dụng việc ngồi lên -&gt; shin to shin -&gt; ankle pick -&gt; single leg takedown;\nSf có nói thêm về cách thoát khi bị single leg takedown: cần đưa đầu của đối thủ ra khỏi centre line của mình, dùng tay đưa vào phần cổ của đối thủ để tạo không gian cho hông mình thoát ra, tay đó cũng sẽ là tay lock anaconda -&gt; drag đối thủ xuống và chiếm lấy vị trí có lợi;\nBonus 1 - Thong Le: triangle - armbar ở tư thế mình ở bottom / mount khác với thông thường, mình sẽ không lock 2 chân lại. Hãy dùng gót / heel của chân nằm dười làm blocking wedge / nêm chặn để giữ shin chân trên, rất linh động và dễ thực hiện, dễ siết;\nBonus 2 - Huy + Thong Le: pass guard để ý tay và chân, khép tay lại. Khi pass cái tay nằm dưới của đối thủ là thứ ta cần kiểm soát. Hãy học cách dùng trọng lượng thay vì sức lực. Không move around nhiều mà đưa một chân ngay vào centre line của đối thủ.\n\n\n\n\nẢnh minh họa single leg takedown, bài hướng dẫn rất hay từ Lachlan Giles\n\n\n\n\n\n\n\n\n\n\n\n11.16.2024 | Distance control and matter of inside or outside\n\n\n\n\n\nKhẩu quyết đầu ngày:\n\nMột đối thủ với base vững chắc, co cụm và gồng cứng sẽ ít cho ta cơ hội để thực hiện một đòn thế. Ta cần quấy rối, “làm mềm” để từ từ mở ra cơ hội tấn công / kết thúc. ———- đại ý, không phải nguyên văn từ coach\n\n80% (số-này-được-mình-bịa-ra) thời gian của một game đấu là thời gian hai đấu thủ tìm kiếm một vị trí thuận lợi / setup được position cho các đòn thế được giảng dạy trong hầu hết các giáo trình BJJ. Việc giảng dạy kiểu technique-based rất có lợi trong các khoảnh khắc quyết định, tuy nhiên đường đi đến các khoảnh khắc đó - tức đa số thời gian thi đấu - cũng không kém phần quan trọng. Bản thân mình cũng rất loay hoay, bối rối khi học được một vài kỹ thuật khóa siết, kiểm soát nhưng thực tế không thể tiến tới được. Như mọi bài giảng khác, hôm nay head coach lại nói về cách tiếp cận để kiểm soát body đối phương, limbs motion và positioning mindset.\nTiếp tục về chủ đề control the game ở note trước, từ vị trí bottom, thầy cho ghi nhớ và làm quen các move ở hai yếu tố sau - nhằm học cách kiểm soát khoảng cách với đối thủ và có các hành động tương ứng:\n\nPhần thân dưới:\n\nkhi đầu gối đối phương chạm chất, ta ưu tiên tấn công từ phía ngoài: closed guard -&gt; ankle hook -&gt; leg sprawl. Vì lẽ nó còn cho phép ta giữ cho đối thủ không chạy đi bằng việc móc chân giữ lại;\nkhi đối phương chống chân lên, gối nhấc lên, ta mất cả năng outside hook, ta ưu tiên chuyển qua inside hook hoặc wedge;\n\nPhần thân trên, song song với thân dưới, ta cũng cần kết hợp giữ tay - vai - đầu của đối thủ:\n\nta muốn đưa ít nhất một tay của họ khỏi được central line ra bên ngoài hoặc bên trong;\nta muốn chỏ của họ ra xa khỏi lườn, có thể bắt đầu từ việc 2-đánh-1 vào cổ tay, kéo/đẩy nó theo phương vuông góc với body line của 2 người;\ntừ đó có thể đạt được 1 grip kimura (in/out) với 4-shape, hoặc overhook/cross underhook với tay còn lại ghì đầu/cổ đối thủ xuống.\n\n\nTa cần làm quen với việc kết hợp cả tay và chân từ khoảng cách gần nhất - closed guard, tới xa nhất là hand wrist grip -&gt; transition sang đánh chân như chuỗi tấn công đã nhắc ở note trước. Nếu làm đúng, đối thủ sẽ tiêu hao rất nhiều sức lực để thoát ra, đó là một số nguyên lý cần ghi nhớ khi ở vị trí bottom.\nTất nhiên cũng có ngoại lệ, như:\n\nButterfly guard với double underhook là một inside attack khi đối thủ base hai đầu gối (tuy nhiên mình chỉ có thể dùng lực chân để nâng đối thủ lên chứ ko thể hook -&gt; sprawl đối thủ ra được, trong MMA hoặc street fight, đối thủ có thể chỏ vào đầu nếu mình dùng đòn này);\nDe la Riva cũng có thể coi là một inside (khi hông của mình nằm giữa chân). Tuy nhiên fact là chân thầy bị xoắn khi thực hiện nên lò Sói không dạy đòn này, reversed De la Riva - dễ thực hiện hơn - có thể sẽ được giảng dạy trong tương lai.\n\n\n\n\nẢnh minh họa Âm Dương/Yin Yang/阴阳: Phần Âm càng ít đi, phần Dương càng lớn hơn và ngược lại, ta mất đi thứ gì đó thì song song cũng nhận lại được thứ gì đó, hoặc sẽ có cơ hội khác mở ra. Một tình huống cụ thể luôn gợi ý ta tấn công theo một phương pháp nhất định hơn là một phương pháp khác. Tất nhiên, cũng luôn có ngoại lệ như hai chấm tròn trên hình. Nguồn ảnh: CleanPNG\n\n\nBonus quote 2:\n\nTrong Jiu Jitsu, mình - cũng như đối thủ - sẽ cố gắng “dán” các đường chéo lên/cắt ngang người đối phương. Càng nhiều đường chéo được tạo ta, ta càng kiểm soát đối thủ tốt hơn.\n\nBonus quote 3:\n\nMục tiêu của các bạn khi drill/roll ở lò này không phải là submit bạn tập, mà là kiểm soát họ.\n\nOss!\n\n\n\n\n\n\n\n\n\n11.08.2024 | Control the game from bottom\n\n\n\n\n\n\nNếu bạn có một gameplan đánh chân đủ tốt, bạn sẽ luôn chủ động trong mọi cuộc đấu.\n———- Vũ Đình Tiến (không hẳn là nguyên văn, là đại ý mà mình nhớ được)\n\nMình nhớ trong một rolling session, Lâm Đoàn đã nói chân là thứ rất quan trọng nếu muốn kiểm soát trận đấu. Đối thủ chỉ cần ôm lấy đùi bạn từ vị trí top và giữ được nó, họ gần như sẽ chiến thắng trận đấu, ta rất khó để tiến tới một vị trí có lợi hơn khi mà chân bị khóa cứng.\nLuôn có lựa chọn để đánh chân khi ta ở bottom (ví dụ half/closed guard), hôm nay Thầy cho drill mấy điểm sau:\n\nKhi hai đầu gối của đối phương base dưới đất (trọng tâm dồn vào đấy) -&gt; ta có thể dùng chân hook vào phần mu cổ chân và kéo dãn chân, phá cái base đó đi;\nPhản ứng tự nhiên là họ sẽ cố xây lại base bằng cách chống chân -&gt; ta có thể hook vào phần trong đùi, hoặc wedge (nêm chặn) vào hông của đối thủ;\nTrong các tình huống trên, hãy luôn cố gắng kiểm soát phần trên bằng tay: có thể là over hook + ghì đầu đối thủ, có thể là under hook tay nghịch, over hook với 4-shape, thậm chí tệ nhất là wrist grip. Hãy học cách kết hợp tay và các hook chân để kiểm soát cơ thể đối phương;\nRồi giờ hãy để ý, nếu ta thực hiện được điều 3, vị trí đối phương sẽ luôn trong tầm tấn công. Đặc biệt, khi stand up, chân của đối phương sẽ luôn expose để ta thực hiện 1 đòn grip chân;\nNhư vậy ta có thể tạo ra một chuỗi tấn công: Gối chạm đất -&gt; hook chân sprawl ra -&gt; hook đùi trong hoặc wedge vào hip -&gt; đối phương stand up -&gt; triển khai single leg X, X-guard sweep, K-guard (đánh được triangle, omoplata, hay reversed closed guard) -&gt; nếu đối phương có thể thoát được, khả năng cao gối của đối phương sẽ lại chạm đất, ta lại bắt đầu lại.\n\nOss!\n\n\n\nẢnh không liên quan, congandanhdan by Minh Tân\n\n\n\n\n\n\n\n\n\n\n\n11.02.2024 | Saturday guard passing\n\n\n\n\n\nHôm nay mình học pass guard từ một vị trí open - cụ thể là half guard - hoặc tương tự thế, chúng ta thường tiếp cận bằng cách đưa một chân vào vùng hai chân của đối thủ.\nCần ghi nhớ:\n\nKhi guard mở, hai chân của đối phương nâng lên defense sẽ làm giảm tính linh hoạt, giờ họ chỉ di chuyển bằng hông;\nNgay cả khi set up được guard, việc khống chế sao cho bàn chân của đối phương bị nhấc lên cũng sẽ hạn chế khả năng dùng lực chân của họ;\nChúng ta sẽ cần pin được 1 đầu gối (như hình, của chân c1), và kiểm soát tay nằm chéo chiếc chân này, khống chế cơ thể của đối phương bằng một đường chéo (màu xanh lá như hình).\n\nTất nhiên chân c2 của họ sẽ shield lại ngăn cản. Chúng ta có thể tấn công theo hướng này (đường xanh lá) - đùng đầu và lườn hook/đẩy vào dưới cánh tay - sẽ tiêu hao rất nhiều sức lực, và cũng nên cẩn thận việc bị neck crank. Trong quá trình đó ta vẫn cần kiểm soát chân c1, cố gắng gạt chân c2 ra.\nHoặc chúng ta có thể lựa chọn tấn công theo đường màu tím - ngang thân. Ta cần làm được:\n\nKiểm soát chân c1 bằng hai chân của mình, cố gắng nâng bàn chân đó lên khỏi mặt đất - có thể bằng cách móc hai chân mình lại với nhau, sau đó duỗi ra trong khi đang ngồi lên chiếc gối đó. Việc này cũng sẽ hạn chế việc chân c2 khi bị gỡ shield sẽ quay sang tấn công chân trong của mình;\nCố định hông của đối thủ, do ta đã pin được chân c1, thế nên nếu kiểm soát được chân c2 tự nhiên hông của họ sẽ không di chuyển được. Đầu tiên hãy xoay trục cơ thể về đường tím, sau đó ta có hai hướng:\n\na) Đẩy chân c2 - phần gối về hướng h1, hãy dùng khối lượng cơ thể bằng cách kết hợp việc duỗi chân ra. Đối phương sẽ tiêu hao nhiều sức lực hơn và khi pass được đầu gối c2, ta có thể pivot/xoay trục về đường xanh lá để đánh tay;\nb) Đẩy chân c2 - phần gối về hướng h2, ta có thể hook tay phải vào sau chiếc gối này và đấm tay xuống đất (trước bụng đối phương) - lúc này hông của họ cũng không di chuyển được. Sau khi đè được gối c2 xuống, ta có thể:\n\ni) pass kiểu knee slice về phía trước; hoặc\nii) duỗi người, thẳng chân, di chuyển bằng mũi ngón chân để pass về phía sau - giữ cho trọng lượng cơ thể vẫn áp lực lên người đối thủ.\n\n\n\nBonus: Đòn đánh 2.b.ii có thể kết thúc bằng outside heel hook, khi mà chân c2 đã ở trong hông, tay phải mình cũng sẽ đang - một cách tự nhiên - giữ lấy nó.\nNói tóm lại, các tư duy cần ghi nhớ sau buổi học này:\n\nKhông nên dùng sức mà nên dựa nhiều vào trọng lượng cơ thể;\nKhông tấn công được theo hướng này thì nên pivot ra hướng khác, không ai có thể mạnh trên mọi hướng (– said Tiến Sói 🐺);\nHông / hip là một phần quan trọng trong việc trực tiếp maintain posture, gián tiếp ảnh hưởng tới khả năng trụ chân để maintain base, cũng như sự tự do của tay chân để maintain structure, ba yếu tố trong alignment của bạn 👊.\n\n\n\n\nHình minh họa, hình gốc credit to Bernardo Faria BJJ Fanatics\n\n\nKết thúc, một buổi chiều quá nóng, may có ly nước mía!"
  },
  {
    "objectID": "学汉语的日记/2024-06-03_hskk-langdu-wuxiaxiaoshuo/index.html",
    "href": "学汉语的日记/2024-06-03_hskk-langdu-wuxiaxiaoshuo/index.html",
    "title": "HSKK, 朗读：武侠小说",
    "section": "",
    "text": "金庸，最著名的中国武侠小说作者，神雕侠侣是我最喜欢的 (aka The Return of the Condor Heroes)，photo credit to tv.cctv\n\n\n\n\n\n\n🚢武侠小说\n优秀的武侠小说不仅从另一个角度反映了时代风貌和各色人等的心理历程，也铸造了独特的艺术风格。它们多是线条粗矿，没有雕琢，甚至略显仓促，但让人读后心跳加逮、热血涌动，透出一股逼人的热气。这就是它们中的佼佼者共同具有的豪放美。这种美的形态是从宏伟的力量、崇高的精神呈现出耒的，它往往引发人们十分强烈的感情，或促人奋发昂扬，或迫人扼腕悲愤，或令人仰天长啸，或使人悲歌慷慨。这种气势美，恰恰就在于它表现出我们民族精神面貌里的一种豪放、一种对理想境界的追求、一种价值判断的准则。\n已故著名数学大师华罗庚先生曾根据自己读武侠小说的感受，称其为“成人的童话”。这是指武侠小说都建构了一个想象的世界，它以虚构的梦幻的形式，揭示历史、人生及人性的现实。\n\n\n✍越南语翻译 Dịch nghĩa\nTiểu thuyết võ hiệp xuất sắc không chỉ phản ánh diện mạo thời đại và quá trình tâm lý các màu sắc, mà còn tạo nên phong cách nghệ thuật độc đáo. Chúng nó phần lớn là đường nét thô khoáng, không có tạo hình, thậm chí hơi có vẻ vội vàng, nhưng làm cho người ta đọc xong tim đập thình thịch, nhiệt huyết bắt đầu khởi động, lộ ra một cỗ nhiệt khí bức người. Đây chính là vẻ đẹp hào phóng mà những người nổi bật trong chúng cùng có. Hình thái đẹp này xuất phát từ sức mạnh to lớn, tinh thần cao thượng, nó thường dẫn đến tình cảm vô cùng mãnh liệt của mọi người, hoặc thúc đẩy người ta hăng hái dâng trào, hoặc bức bách người ta bóp cổ tay bi phẫn, hoặc khiến người ta ngửa mặt lên trời thét dài, hoặc khiến người ta bi ca hào phóng. Vẻ đẹp khí thế này, vừa vặn chính là nó thể hiện ra một loại hào phóng, một loại theo đuổi cảnh giới lý tưởng, một loại chuẩn tắc phán đoán giá trị trong diện mạo tinh thần dân tộc chúng ta.\nCố đại sư toán học nổi tiếng Hoa La Canh từng căn cứ vào cảm nhận của mình khi đọc tiểu thuyết võ hiệp, gọi nó là “truyện cổ tích của người lớn”. Đây là chỉ tiểu thuyết võ hiệp đều xây dựng một thế giới tưởng tượng, nó lấy hình thức hư cấu mộng ảo, tiết lộ hiện thực lịch sử, nhân sinh và nhân tính.\n\n\n📚生词\n\n\n参考资料\n新 HSK 速成强化教程 口试 (高级), 作者：金舒年 编著， 第85页"
  },
  {
    "objectID": "学汉语的日记/2024-07-23_hskk-jiangnan-de-chunyu/index.html",
    "href": "学汉语的日记/2024-07-23_hskk-jiangnan-de-chunyu/index.html",
    "title": "HSKK, 朗读：江南的春雨",
    "section": "",
    "text": "几日来，是蒙蒙细雨，密密地织着江南的春了。\n椎开窗儿，大堤对面的湖里，满是淡淡的黑雾，看不见往曰温柔清澈的微波。湖的那岸，是隐隐约约的树和房予，仝都藏在细雨里，看不真切。窗前，檐雨不时滴下来，东一滴西一滴，很轻很轻，仿佛是始娘羞怯时多情的泪珠了!窗台上的那盆水仙，湿湿的，润润的，绿得青翠。我双手撑着书案，漫元目的地望着满夭的细雨，似乎在想，又似乎什么都忘了。\n窗前的大堤湿涯涯的，远处的树、远处的农家湿媲混的。江南呵!在细细的春雨里，轻轻地、轻轻地湿了。甚至江南的人，江南的风俗，甚至窗前的那盆水仙，连我铺在书桌上准备写江南雨的稿予，连我童年的记忆都湿了!\n\n\n\n\n\n\n\n\n\n江南的春雨，credit"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第02课-留串钥匙给父母/index.html",
    "href": "学汉语的日记/HSK5上-第02课-留串钥匙给父母/index.html",
    "title": "HSK5上 | 第02课：留串钥匙给父母",
    "section": "",
    "text": "留串钥匙给父母\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n串\nchuàn - measure word for bunch, string\n\n\n2\n一辈子\nyíbèizi - all one’s life\n\n\n3\n农村\nnóng cūn - countryside\n\n\n4\n屋子\nwūzi - house\n\n\n5\n断\nduàn - to cut off, to stop\n\n\n6\n以来\nyǐlái - since\n\n\n7\n姥姥\nlǎo lao - maternal grandma\n\n\n8\n舅舅\njiùjiu - mother’s brother (uncle)\n\n\n9\n姑姑\ngūgu - father’s sister (aunt)\n\n\n10\n坚决\njiān jué - resolute, determined\n\n\n11\n打工\ndǎgōng - to work for others, to do a temporary job\n\n\n12\n挣\nzhèng - to earn (money)\n\n\n13\n县\nxiàn - county\n\n\n14\n套\ntào - measure word for suite or set\n\n\n15\n装修\nzhuāng xiū - to decorate; to renovate\n\n\n16\n不得了\nbù dé liǎo - extreme, exceeding\n\n\n17\n醉\nzuì - to be drunk\n\n\n18\n强烈\nqiáng liè - strong, vehement\n\n\n19\n锁\nsuǒ - lock; to lock up\n\n\n20\n临\nlín - about to, just before\n\n\n21\n悄悄\nqiāo qiāo - quietly, secretly\n\n\n22\n晒\nshài - to dry in the sun\n\n\n23\n被子\nbèi zi - quilt\n\n\n24\n长途\nchángtú - long distance\n\n\n25\n冻\ndòng - to freeze\n\n\n26\n想象\nxiǎng xiàng - to imagine\n\n\n27\n灰尘\nhuī chén - dust, dirt\n\n\n28\n微笑\nwēi xiào - to smile\n\n\n29\n温暖\nwēn nuǎn - warm\n\n\n30\n立刻\nlì kè - immediately, at once\n\n\n31\n扑\npū - to pounce on, to dash at\n\n\n32\n卧室\nwòshì - bedroom\n\n\n33\n铺\npū - to spread, to unfold\n\n\n34\n飘\npiāo - to float (in the air), to waft\n\n\n35\n阵\nzhèn - measure word for a short period\n\n\n36\n感受\ngǎn shòu - to feel; feeling\n\n\n37\n流泪\nliú lèi - to shed tears"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第02课-留串钥匙给父母/index.html#生词",
    "href": "学汉语的日记/HSK5上-第02课-留串钥匙给父母/index.html#生词",
    "title": "HSK5上 | 第02课：留串钥匙给父母",
    "section": "",
    "text": "留串钥匙给父母\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n串\nchuàn - measure word for bunch, string\n\n\n2\n一辈子\nyíbèizi - all one’s life\n\n\n3\n农村\nnóng cūn - countryside\n\n\n4\n屋子\nwūzi - house\n\n\n5\n断\nduàn - to cut off, to stop\n\n\n6\n以来\nyǐlái - since\n\n\n7\n姥姥\nlǎo lao - maternal grandma\n\n\n8\n舅舅\njiùjiu - mother’s brother (uncle)\n\n\n9\n姑姑\ngūgu - father’s sister (aunt)\n\n\n10\n坚决\njiān jué - resolute, determined\n\n\n11\n打工\ndǎgōng - to work for others, to do a temporary job\n\n\n12\n挣\nzhèng - to earn (money)\n\n\n13\n县\nxiàn - county\n\n\n14\n套\ntào - measure word for suite or set\n\n\n15\n装修\nzhuāng xiū - to decorate; to renovate\n\n\n16\n不得了\nbù dé liǎo - extreme, exceeding\n\n\n17\n醉\nzuì - to be drunk\n\n\n18\n强烈\nqiáng liè - strong, vehement\n\n\n19\n锁\nsuǒ - lock; to lock up\n\n\n20\n临\nlín - about to, just before\n\n\n21\n悄悄\nqiāo qiāo - quietly, secretly\n\n\n22\n晒\nshài - to dry in the sun\n\n\n23\n被子\nbèi zi - quilt\n\n\n24\n长途\nchángtú - long distance\n\n\n25\n冻\ndòng - to freeze\n\n\n26\n想象\nxiǎng xiàng - to imagine\n\n\n27\n灰尘\nhuī chén - dust, dirt\n\n\n28\n微笑\nwēi xiào - to smile\n\n\n29\n温暖\nwēn nuǎn - warm\n\n\n30\n立刻\nlì kè - immediately, at once\n\n\n31\n扑\npū - to pounce on, to dash at\n\n\n32\n卧室\nwòshì - bedroom\n\n\n33\n铺\npū - to spread, to unfold\n\n\n34\n飘\npiāo - to float (in the air), to waft\n\n\n35\n阵\nzhèn - measure word for a short period\n\n\n36\n感受\ngǎn shòu - to feel; feeling\n\n\n37\n流泪\nliú lèi - to shed tears"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第02课-留串钥匙给父母/index.html#听力",
    "href": "学汉语的日记/HSK5上-第02课-留串钥匙给父母/index.html#听力",
    "title": "HSK5上 | 第02课：留串钥匙给父母",
    "section": "2.1. 听力",
    "text": "2.1. 听力"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第02课-留串钥匙给父母/index.html#阅读",
    "href": "学汉语的日记/HSK5上-第02课-留串钥匙给父母/index.html#阅读",
    "title": "HSK5上 | 第02课：留串钥匙给父母",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n19.\n20.\n21.\n22.\n23-25.\n23.\n24.\n25.\n26-28.\n26.\n27.\n28."
  },
  {
    "objectID": "学汉语的日记/HSK5上-第02课-留串钥匙给父母/index.html#书写",
    "href": "学汉语的日记/HSK5上-第02课-留串钥匙给父母/index.html#书写",
    "title": "HSK5上 | 第02课：留串钥匙给父母",
    "section": "2.3. 书写",
    "text": "2.3. 书写"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第04课-子路背米/index.html",
    "href": "学汉语的日记/HSK5上-第04课-子路背米/index.html",
    "title": "HSK5上 | 第04课：子路背米",
    "section": "",
    "text": "子路背米\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n背\nbēi - to carry on the back\n\n\n2\n从前\ncóng qián - before, in the past\n\n\n3\n时期\nshí qī - period, stage\n\n\n4\n流传\nliú chuán - to spread, to hand down\n\n\n5\n至今\nzhì jīn - up to now\n\n\n6\n孝敬\nxiàojìng - to show filial respect for\n\n\n7\n农民\nnóngmín - farmer; peasant\n\n\n8\n战争\nzhànzhēng - war\n\n\n9\n满足\nmǎn zú - to be satisfied\n\n\n10\n惭愧\ncán kuì - ashamed\n\n\n11\n决心\njué xīn - determination; to make up one’s mind\n\n\n12\n委屈\nwěi qu - feel wronged; to do wrong\n\n\n13\n打听\ndǎ ting - to inquire about\n\n\n14\n主人\nzhǔrén - master, owner\n\n\n15\n结实\njiē shi - sturdy, strong\n\n\n16\n勤奋\nqín fèn - diligent\n\n\n17\n银（子）\nyín (zi) - silver\n\n\n18\n老实\nlǎo shi - honest, frank\n\n\n19\n镇\nzhèn - town\n\n\n20\n后背\nhòu bèi - back (of human body)\n\n\n21\n滑\nhuá - slippery, to slip, to slide\n\n\n22\n甩\nshuǎi - to throw off, to swing\n\n\n23\n顶\ndǐng - to go against, to resist, top, m. for hats\n\n\n24\n扶\nfú - support with the hand\n\n\n25\n不行\nbùxíng - not be allowed, terribly, extremely\n\n\n26\n团圆\ntuányuán - to be reunited (as a family)\n\n\n27\n去世\nqù shì - to pass away; to die\n\n\n28\n国君\nguójūn - king\n\n\n29\n本领\nběn lǐng - ability, capability\n\n\n30\n人才\nrén cái - talented person\n\n\n31\n官\nguān - govt. official\n\n\n32\n物质\nwù zhì - material\n\n\n33\n反而\nfǎn ér - on the contrary, instead\n\n\n34\n诚恳\nchéng kěn - sincere, earnest\n\n\n35\n成就\nchéng jiù - achievement, accomplishment\n\n\n36\n古代\ngǔ dài - ancient times\n\n\n37\n孝顺\nxiàoshùn - show filial piety; obedient & respectful to parents\n\n\n38\n美德\nměidé - virtue, goodness\n\n\n39\n占\nzhàn - to occupy, to take up\n\n\n40\n食物\nshí wù - food"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第04课-子路背米/index.html#生词",
    "href": "学汉语的日记/HSK5上-第04课-子路背米/index.html#生词",
    "title": "HSK5上 | 第04课：子路背米",
    "section": "",
    "text": "子路背米\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n背\nbēi - to carry on the back\n\n\n2\n从前\ncóng qián - before, in the past\n\n\n3\n时期\nshí qī - period, stage\n\n\n4\n流传\nliú chuán - to spread, to hand down\n\n\n5\n至今\nzhì jīn - up to now\n\n\n6\n孝敬\nxiàojìng - to show filial respect for\n\n\n7\n农民\nnóngmín - farmer; peasant\n\n\n8\n战争\nzhànzhēng - war\n\n\n9\n满足\nmǎn zú - to be satisfied\n\n\n10\n惭愧\ncán kuì - ashamed\n\n\n11\n决心\njué xīn - determination; to make up one’s mind\n\n\n12\n委屈\nwěi qu - feel wronged; to do wrong\n\n\n13\n打听\ndǎ ting - to inquire about\n\n\n14\n主人\nzhǔrén - master, owner\n\n\n15\n结实\njiē shi - sturdy, strong\n\n\n16\n勤奋\nqín fèn - diligent\n\n\n17\n银（子）\nyín (zi) - silver\n\n\n18\n老实\nlǎo shi - honest, frank\n\n\n19\n镇\nzhèn - town\n\n\n20\n后背\nhòu bèi - back (of human body)\n\n\n21\n滑\nhuá - slippery, to slip, to slide\n\n\n22\n甩\nshuǎi - to throw off, to swing\n\n\n23\n顶\ndǐng - to go against, to resist, top, m. for hats\n\n\n24\n扶\nfú - support with the hand\n\n\n25\n不行\nbùxíng - not be allowed, terribly, extremely\n\n\n26\n团圆\ntuányuán - to be reunited (as a family)\n\n\n27\n去世\nqù shì - to pass away; to die\n\n\n28\n国君\nguójūn - king\n\n\n29\n本领\nběn lǐng - ability, capability\n\n\n30\n人才\nrén cái - talented person\n\n\n31\n官\nguān - govt. official\n\n\n32\n物质\nwù zhì - material\n\n\n33\n反而\nfǎn ér - on the contrary, instead\n\n\n34\n诚恳\nchéng kěn - sincere, earnest\n\n\n35\n成就\nchéng jiù - achievement, accomplishment\n\n\n36\n古代\ngǔ dài - ancient times\n\n\n37\n孝顺\nxiàoshùn - show filial piety; obedient & respectful to parents\n\n\n38\n美德\nměidé - virtue, goodness\n\n\n39\n占\nzhàn - to occupy, to take up\n\n\n40\n食物\nshí wù - food"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第04课-子路背米/index.html#听力",
    "href": "学汉语的日记/HSK5上-第04课-子路背米/index.html#听力",
    "title": "HSK5上 | 第04课：子路背米",
    "section": "2.1. 听力",
    "text": "2.1. 听力"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第04课-子路背米/index.html#阅读",
    "href": "学汉语的日记/HSK5上-第04课-子路背米/index.html#阅读",
    "title": "HSK5上 | 第04课：子路背米",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n19.\n20.\n21.\n22.\n23-25.\n23.\n24.\n25.\n26-28.\n26.\n27.\n28."
  },
  {
    "objectID": "学汉语的日记/HSK5上-第04课-子路背米/index.html#书写",
    "href": "学汉语的日记/HSK5上-第04课-子路背米/index.html#书写",
    "title": "HSK5上 | 第04课：子路背米",
    "section": "2.3. 书写",
    "text": "2.3. 书写"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第06课-除夕的由来/index.html",
    "href": "学汉语的日记/HSK5上-第06课-除夕的由来/index.html",
    "title": "HSK5上 | 第06课：除夕的由来",
    "section": "",
    "text": "除夕的由来\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n除夕\nchú xī - Chinese New Year Eve\n\n\n2\n由来\nyóulái - origin, source\n\n\n3\n农历\nnóng lì - lunar calendar\n\n\n4\n守岁\nshǒu suì - to stay up all night on new years eve\n\n\n5\n怪物\nguài wù - monster\n\n\n6\n伤害\nshāng hài - to hurt, to harm\n\n\n7\n恨\nhèn - to hate\n\n\n8\n骨头\ngǔtou - bone\n\n\n9\n无奈\nwú nài - to have no way out; to be helpless\n\n\n10\n英雄\nyīng xióng - hero\n\n\n11\n英俊\nyīngjùn - handsome\n\n\n12\n咬\nyǎo - to bite\n\n\n13\n外公\nwàigōng - maternal grandfather\n\n\n14\n询问\nxún wèn - to ask, to inquire\n\n\n15\n天真\ntiān zhēn - naive, innocent\n\n\n16\n杀\nshā - to kill\n\n\n17\n代替\ndài tì - for, on behalf of\n\n\n18\n除\nchú - to get rid of, to remove\n\n\n19\n制造\nzhì zào - to make; to product; to manufacture\n\n\n20\n灾害\nzāihài - disaster, calamity\n\n\n21\n逃\ntáo - to escape\n\n\n22\n影子\nyǐng zi - shadow\n\n\n23\n此外\ncǐ wài - in addition, moreover\n\n\n24\n说不定\nshuōbudìng - perhaps, possibly, it’s likely that\n\n\n25\n熬夜\náo yè - to stay up late\n\n\n26\n赶紧\ngǎn jǐn - immediately, at once\n\n\n27\n果然\nguǒ rán - as expected, really\n\n\n28\n姑娘\ngū niang - girl\n\n\n29\n锅\nguō - pot, pan\n\n\n30\n盆子\npén zi - basin, tub\n\n\n31\n整个\nzhěng gè - whole, entire\n\n\n32\n吓\nxià - to frighten, to scare\n\n\n33\n似的\nshì de - like, as, similar\n\n\n34\n追\nzhuī - to chase, to go after\n\n\n35\n箭\njiàn - arrow\n\n\n36\n射（击）\nShè (jī) - to shoot\n\n\n37\n纷纷\nfēn fēn - one after another, all at once\n\n\n38\n表达\nbiǎo dá - to express, to voice\n\n\n39\n意义\nyì yì - meaning / significance\n\n\n40\n鞭炮\nbiānpào - firecracker\n\n\n41\n风俗\nfēng sú - custom, convention"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第06课-除夕的由来/index.html#生词",
    "href": "学汉语的日记/HSK5上-第06课-除夕的由来/index.html#生词",
    "title": "HSK5上 | 第06课：除夕的由来",
    "section": "",
    "text": "除夕的由来\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n除夕\nchú xī - Chinese New Year Eve\n\n\n2\n由来\nyóulái - origin, source\n\n\n3\n农历\nnóng lì - lunar calendar\n\n\n4\n守岁\nshǒu suì - to stay up all night on new years eve\n\n\n5\n怪物\nguài wù - monster\n\n\n6\n伤害\nshāng hài - to hurt, to harm\n\n\n7\n恨\nhèn - to hate\n\n\n8\n骨头\ngǔtou - bone\n\n\n9\n无奈\nwú nài - to have no way out; to be helpless\n\n\n10\n英雄\nyīng xióng - hero\n\n\n11\n英俊\nyīngjùn - handsome\n\n\n12\n咬\nyǎo - to bite\n\n\n13\n外公\nwàigōng - maternal grandfather\n\n\n14\n询问\nxún wèn - to ask, to inquire\n\n\n15\n天真\ntiān zhēn - naive, innocent\n\n\n16\n杀\nshā - to kill\n\n\n17\n代替\ndài tì - for, on behalf of\n\n\n18\n除\nchú - to get rid of, to remove\n\n\n19\n制造\nzhì zào - to make; to product; to manufacture\n\n\n20\n灾害\nzāihài - disaster, calamity\n\n\n21\n逃\ntáo - to escape\n\n\n22\n影子\nyǐng zi - shadow\n\n\n23\n此外\ncǐ wài - in addition, moreover\n\n\n24\n说不定\nshuōbudìng - perhaps, possibly, it’s likely that\n\n\n25\n熬夜\náo yè - to stay up late\n\n\n26\n赶紧\ngǎn jǐn - immediately, at once\n\n\n27\n果然\nguǒ rán - as expected, really\n\n\n28\n姑娘\ngū niang - girl\n\n\n29\n锅\nguō - pot, pan\n\n\n30\n盆子\npén zi - basin, tub\n\n\n31\n整个\nzhěng gè - whole, entire\n\n\n32\n吓\nxià - to frighten, to scare\n\n\n33\n似的\nshì de - like, as, similar\n\n\n34\n追\nzhuī - to chase, to go after\n\n\n35\n箭\njiàn - arrow\n\n\n36\n射（击）\nShè (jī) - to shoot\n\n\n37\n纷纷\nfēn fēn - one after another, all at once\n\n\n38\n表达\nbiǎo dá - to express, to voice\n\n\n39\n意义\nyì yì - meaning / significance\n\n\n40\n鞭炮\nbiānpào - firecracker\n\n\n41\n风俗\nfēng sú - custom, convention"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第06课-除夕的由来/index.html#听力",
    "href": "学汉语的日记/HSK5上-第06课-除夕的由来/index.html#听力",
    "title": "HSK5上 | 第06课：除夕的由来",
    "section": "2.1. 听力",
    "text": "2.1. 听力"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第06课-除夕的由来/index.html#阅读",
    "href": "学汉语的日记/HSK5上-第06课-除夕的由来/index.html#阅读",
    "title": "HSK5上 | 第06课：除夕的由来",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n19.\n20.\n21.\n22.\n23-25.\n23.\n24.\n25.\n26-28.\n26.\n27.\n28."
  },
  {
    "objectID": "学汉语的日记/HSK5上-第06课-除夕的由来/index.html#书写",
    "href": "学汉语的日记/HSK5上-第06课-除夕的由来/index.html#书写",
    "title": "HSK5上 | 第06课：除夕的由来",
    "section": "2.3. 书写",
    "text": "2.3. 书写"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第08课-朝三暮四/index.html",
    "href": "学汉语的日记/HSK5上-第08课-朝三暮四/index.html",
    "title": "HSK5上 | 第08课：朝三暮四",
    "section": "",
    "text": "朝三暮四\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n朝三暮四\nzhāosān-mùsì - to give 3 in the morning & 4 at night\n\n\n2\n词汇\ncí huì - vocabulary\n\n\n3\n固定\ngù dìng - fixed, settled, to fix\n\n\n4\n结构\njié gòu - structure\n\n\n5\n整体\nzhěng tǐ - whole, entirety\n\n\n6\n综合\nzōng hé - to synthesize, to summarize, comprehensive, integrated\n\n\n7\n完整\nwán zhěng - complete\n\n\n8\n哲学家\nzhéxuéjiā - philosopher\n\n\n9\n寓言\nyùyán - fable\n\n\n10\n喂养\nwèiyǎng - feed, to raise\n\n\n11\n群\nqún - measure word for group, herd, flock\n\n\n12\n猴子\nhóu zi - monkey\n\n\n13\n宠物\nchǒngwù - pet\n\n\n14\n相处\nxiāngchǔ - to get along (with one another)\n\n\n15\n彼此\nbǐ cǐ - each other\n\n\n16\n表情\nbiǎoqíng - facial expression\n\n\n17\n行为\nxíng wéi - behavior\n\n\n18\n对方\nduì fāng - to other side/party\n\n\n19\n蔬菜\nshū cài - vegetable\n\n\n20\n粮食\nliáng shi - grain, cereal, food\n\n\n21\n家庭\njiātíng - family\n\n\n22\n财产\ncái chǎn - property, fortune\n\n\n23\n消费\nxiāofèi - to consume\n\n\n24\n节省\njié shěng - to save, to economize\n\n\n25\n限制\nxiàn zhì - to limit, to restrict\n\n\n26\n猪\nzhū - pig\n\n\n27\n调皮\ntiáo pí - naughty, mischievous\n\n\n28\n淘气\ntáo qì - naughty, mischievous\n\n\n29\n橡子\nxiàng zi - acorn\n\n\n30\n果实\nguǒ shí - fruit\n\n\n31\n不足\nbùzú - insufficient, to be less than\n\n\n32\n馒头\nmán tou - steamed bun\n\n\n33\n颗\nkē - measure word for small and roundish things\n\n\n34\n似乎\nsì hū - it seems that…, seemingly\n\n\n35\n吃亏\nchī kuī - to suffer losses\n\n\n36\n安慰\nān wèi - to comfort / console\n\n\n37\n要不\nyào bù - otherwise, or else, how about\n\n\n38\n显得\nxiǎndé - to seem, to appear\n\n\n39\n格外\ngé wài - especially, extraordinarily\n\n\n40\n情景\nqíng jǐng - scene, sight\n\n\n41\n哈\nhā - sound of laughter"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第08课-朝三暮四/index.html#生词",
    "href": "学汉语的日记/HSK5上-第08课-朝三暮四/index.html#生词",
    "title": "HSK5上 | 第08课：朝三暮四",
    "section": "",
    "text": "朝三暮四\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n朝三暮四\nzhāosān-mùsì - to give 3 in the morning & 4 at night\n\n\n2\n词汇\ncí huì - vocabulary\n\n\n3\n固定\ngù dìng - fixed, settled, to fix\n\n\n4\n结构\njié gòu - structure\n\n\n5\n整体\nzhěng tǐ - whole, entirety\n\n\n6\n综合\nzōng hé - to synthesize, to summarize, comprehensive, integrated\n\n\n7\n完整\nwán zhěng - complete\n\n\n8\n哲学家\nzhéxuéjiā - philosopher\n\n\n9\n寓言\nyùyán - fable\n\n\n10\n喂养\nwèiyǎng - feed, to raise\n\n\n11\n群\nqún - measure word for group, herd, flock\n\n\n12\n猴子\nhóu zi - monkey\n\n\n13\n宠物\nchǒngwù - pet\n\n\n14\n相处\nxiāngchǔ - to get along (with one another)\n\n\n15\n彼此\nbǐ cǐ - each other\n\n\n16\n表情\nbiǎoqíng - facial expression\n\n\n17\n行为\nxíng wéi - behavior\n\n\n18\n对方\nduì fāng - to other side/party\n\n\n19\n蔬菜\nshū cài - vegetable\n\n\n20\n粮食\nliáng shi - grain, cereal, food\n\n\n21\n家庭\njiātíng - family\n\n\n22\n财产\ncái chǎn - property, fortune\n\n\n23\n消费\nxiāofèi - to consume\n\n\n24\n节省\njié shěng - to save, to economize\n\n\n25\n限制\nxiàn zhì - to limit, to restrict\n\n\n26\n猪\nzhū - pig\n\n\n27\n调皮\ntiáo pí - naughty, mischievous\n\n\n28\n淘气\ntáo qì - naughty, mischievous\n\n\n29\n橡子\nxiàng zi - acorn\n\n\n30\n果实\nguǒ shí - fruit\n\n\n31\n不足\nbùzú - insufficient, to be less than\n\n\n32\n馒头\nmán tou - steamed bun\n\n\n33\n颗\nkē - measure word for small and roundish things\n\n\n34\n似乎\nsì hū - it seems that…, seemingly\n\n\n35\n吃亏\nchī kuī - to suffer losses\n\n\n36\n安慰\nān wèi - to comfort / console\n\n\n37\n要不\nyào bù - otherwise, or else, how about\n\n\n38\n显得\nxiǎndé - to seem, to appear\n\n\n39\n格外\ngé wài - especially, extraordinarily\n\n\n40\n情景\nqíng jǐng - scene, sight\n\n\n41\n哈\nhā - sound of laughter"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第08课-朝三暮四/index.html#听力",
    "href": "学汉语的日记/HSK5上-第08课-朝三暮四/index.html#听力",
    "title": "HSK5上 | 第08课：朝三暮四",
    "section": "2.1. 听力",
    "text": "2.1. 听力"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第08课-朝三暮四/index.html#阅读",
    "href": "学汉语的日记/HSK5上-第08课-朝三暮四/index.html#阅读",
    "title": "HSK5上 | 第08课：朝三暮四",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n19.\n20.\n21.\n22.\n23-25.\n23.\n24.\n25.\n26-28.\n26.\n27.\n28."
  },
  {
    "objectID": "学汉语的日记/HSK5上-第08课-朝三暮四/index.html#书写",
    "href": "学汉语的日记/HSK5上-第08课-朝三暮四/index.html#书写",
    "title": "HSK5上 | 第08课：朝三暮四",
    "section": "2.3. 书写",
    "text": "2.3. 书写"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第10课-争论的奇迹/index.html",
    "href": "学汉语的日记/HSK5上-第10课-争论的奇迹/index.html",
    "title": "HSK5上 | 第10课：争论的奇迹",
    "section": "",
    "text": "争论的奇迹\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n争论\nzhēnglùn - to argue, to dispute, to debate\n\n\n2\n奇迹\nqí jì - miracle; wonder\n\n\n3\n围绕\nwéi rào - to center around\n\n\n4\n奔跑\nbēn pǎo - to gallop, to run\n\n\n5\n蹄子\ntízi - hoof\n\n\n6\n辩论\nbiànlùn - to debate\n\n\n7\n青蛙\nqīng wā － frog\n\n\n8\n啦\nla - particle express exclamation, interrogation, etc.\n\n\n9\n始终\nshǐ zhōng － from beginning to end; all along\n\n\n10\n脖子\nbó zi - neck\n\n\n11\n说服\nshuō fú - to persuade, to convince\n\n\n12\n摄影师\nshè yǐng shī - photographer\n\n\n13\n毕竟\nbì jìng - after all, in the final analysis\n\n\n14\n操场\ncāo chǎng - playground, sports ground\n\n\n15\n洞\ndòng - hole\n\n\n16\n插\nchā - to insert, to stick in\n\n\n17\n棍\ngùn - stick, club\n\n\n18\n系\njì - to tie; to fasten\n\n\n19\n匹\npǐ - measure word for horses\n\n\n20\n拦\nlán - to block, to hold back\n\n\n21\n拍\npāi - to take a photo, to shoot a video, to clap\n\n\n22\n差距\nchājù - gap; difference; disparity\n\n\n23\n显示\nxiǎn shì - to show; to display\n\n\n24\n意识\nyìshí - consciousness, to be aware of\n\n\n25\n艰苦\njiān kǔ - arduous, difficult, tough\n\n\n26\n试验\nshìyàn - trial, test, to make a test\n\n\n27\n逐渐\nzhú jiàn - gradually\n\n\n28\n改进\ngǎi jìn - to improve, to make better\n\n\n29\n成熟\nchéng shú - to ripen, mature\n\n\n30\n兄弟\nxiōngdì - brother\n\n\n31\n播放\nbō fàng - to broadcast; to play (music or video)\n\n\n32\n纪念\njì niàn - to commemorate / to remember\n\n\n33\n导演\ndǎo yǎn - director of show or movie; to direct\n\n\n34\n瞬间\nshùn​jiān​ - a short period of time\n\n\n35\n请求\nqǐng qiú - to request, to ask\n\n\n36\n或许\nhuò xǔ - maybe, perhaps\n\n\n37\n重大\nzhòngdà - great, significant"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第10课-争论的奇迹/index.html#生词",
    "href": "学汉语的日记/HSK5上-第10课-争论的奇迹/index.html#生词",
    "title": "HSK5上 | 第10课：争论的奇迹",
    "section": "",
    "text": "争论的奇迹\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n争论\nzhēnglùn - to argue, to dispute, to debate\n\n\n2\n奇迹\nqí jì - miracle; wonder\n\n\n3\n围绕\nwéi rào - to center around\n\n\n4\n奔跑\nbēn pǎo - to gallop, to run\n\n\n5\n蹄子\ntízi - hoof\n\n\n6\n辩论\nbiànlùn - to debate\n\n\n7\n青蛙\nqīng wā － frog\n\n\n8\n啦\nla - particle express exclamation, interrogation, etc.\n\n\n9\n始终\nshǐ zhōng － from beginning to end; all along\n\n\n10\n脖子\nbó zi - neck\n\n\n11\n说服\nshuō fú - to persuade, to convince\n\n\n12\n摄影师\nshè yǐng shī - photographer\n\n\n13\n毕竟\nbì jìng - after all, in the final analysis\n\n\n14\n操场\ncāo chǎng - playground, sports ground\n\n\n15\n洞\ndòng - hole\n\n\n16\n插\nchā - to insert, to stick in\n\n\n17\n棍\ngùn - stick, club\n\n\n18\n系\njì - to tie; to fasten\n\n\n19\n匹\npǐ - measure word for horses\n\n\n20\n拦\nlán - to block, to hold back\n\n\n21\n拍\npāi - to take a photo, to shoot a video, to clap\n\n\n22\n差距\nchājù - gap; difference; disparity\n\n\n23\n显示\nxiǎn shì - to show; to display\n\n\n24\n意识\nyìshí - consciousness, to be aware of\n\n\n25\n艰苦\njiān kǔ - arduous, difficult, tough\n\n\n26\n试验\nshìyàn - trial, test, to make a test\n\n\n27\n逐渐\nzhú jiàn - gradually\n\n\n28\n改进\ngǎi jìn - to improve, to make better\n\n\n29\n成熟\nchéng shú - to ripen, mature\n\n\n30\n兄弟\nxiōngdì - brother\n\n\n31\n播放\nbō fàng - to broadcast; to play (music or video)\n\n\n32\n纪念\njì niàn - to commemorate / to remember\n\n\n33\n导演\ndǎo yǎn - director of show or movie; to direct\n\n\n34\n瞬间\nshùn​jiān​ - a short period of time\n\n\n35\n请求\nqǐng qiú - to request, to ask\n\n\n36\n或许\nhuò xǔ - maybe, perhaps\n\n\n37\n重大\nzhòngdà - great, significant"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第10课-争论的奇迹/index.html#听力",
    "href": "学汉语的日记/HSK5上-第10课-争论的奇迹/index.html#听力",
    "title": "HSK5上 | 第10课：争论的奇迹",
    "section": "2.1. 听力",
    "text": "2.1. 听力"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第10课-争论的奇迹/index.html#阅读",
    "href": "学汉语的日记/HSK5上-第10课-争论的奇迹/index.html#阅读",
    "title": "HSK5上 | 第10课：争论的奇迹",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n19.\n20.\n21.\n22.\n23-25.\n23.\n24.\n25.\n26-28.\n26.\n27.\n28."
  },
  {
    "objectID": "学汉语的日记/HSK5上-第10课-争论的奇迹/index.html#书写",
    "href": "学汉语的日记/HSK5上-第10课-争论的奇迹/index.html#书写",
    "title": "HSK5上 | 第10课：争论的奇迹",
    "section": "2.3. 书写",
    "text": "2.3. 书写"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第12课-外国用户玩儿微信/index.html",
    "href": "学汉语的日记/HSK5上-第12课-外国用户玩儿微信/index.html",
    "title": "HSK5上 | 第12课：外国用户玩儿微信",
    "section": "",
    "text": "外国用户玩儿微信\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n用户\nyònghù - n. user\n\n\n2\n颠球\ndiān qiú - to juggle a soccer ball\n\n\n3\n明星\nmíng xīng - n. star, celebrity\n\n\n4\n直播\nzhíbō - v. to make a live broadcast\n\n\n5\n宝贝\nbǎo bèi - n. baby\n\n\n6\n逗\ndòu - v. to amuse, to tease\n\n\n7\n宣传\nxuān chuán - v. to publicize, to promote\n\n\n8\n手笔\nshǒubǐ - n. style (of handling affairs)\n\n\n9\n推广\ntuī guǎng - v. to popularize, to promote\n\n\n10\n注册\nzhù cè - v. to register\n\n\n11\n召开\nzhào kāi - v. to hold (a meeting), to convene\n\n\n12\n合作\nhé zuò - v. to cooperate, to work together\n\n\n13\n伙伴\nhuǒ bàn - n. partner\n\n\n14\n总裁\nzǒng cái - n. president (of a company)\n\n\n15\n实现\nshí xiàn - v. to realize, to achieve\n\n\n16\n覆盖\nfùgài - v. to cover\n\n\n17\n移动\nyí dòng - v. to move\n\n\n18\n通信\ntōngxìn - v. to communicate\n\n\n19\n应用\nyìng yòng - v. to apply\n\n\n20\n企业\nqǐ yè - n. enterprise, company\n\n\n21\n称霸\nchēng bà - v. to dominate, to maintain hegemony\n\n\n22\n背景\nbèijǐng - n. background\n\n\n23\n高级\ngāo jí - adj. senior, high-ranking\n\n\n24\n副\nfù - adj. deputy, vice\n\n\n25\n开发\nkāi fā - v. to develop, to exploit\n\n\n26\n中心\nzhōngxīn - n. center\n\n\n27\n相关\nxiāng guān - v. to correlate, to be relevant\n\n\n28\n业务\nyè wù - n. professional work, business\n\n\n29\n现实\nxiàn shí - n. reality\n\n\n30\n个人\ngè rén - n. individual\n\n\n31\n以及\nyǐjí - conj. and also, as well\n\n\n32\n程度\nchéngdù - n. degree, level\n\n\n33\n发达\nfā dá - adj. developed, advanced\n\n\n34\n创新\nchuàngxīn - v. to create something new, to innovate\n\n\n35\n领导\nlǐng dǎo - n. leader\n\n\n36\n地位\ndì wèi - n. position, status\n\n\n37\n经营\njīng yíng - v. to manage, to run\n\n\n38\n销售\nxiāo shòu - v. to sell, to market\n\n\n39\n针对\nzhēn duì - v. to be targeted at\n\n\n40\n当地\ndāng dì - n. local, in the locality\n\n\n41\n代言\ndàiyán - v. to speak on behalf of, to star in a commercial\n\n\n42\n华裔\nhuá yì - n. foreign citizen of Chinese origin\n\n\n43\n移民\nyímín - n. immigrant"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第12课-外国用户玩儿微信/index.html#生词",
    "href": "学汉语的日记/HSK5上-第12课-外国用户玩儿微信/index.html#生词",
    "title": "HSK5上 | 第12课：外国用户玩儿微信",
    "section": "",
    "text": "外国用户玩儿微信\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n用户\nyònghù - n. user\n\n\n2\n颠球\ndiān qiú - to juggle a soccer ball\n\n\n3\n明星\nmíng xīng - n. star, celebrity\n\n\n4\n直播\nzhíbō - v. to make a live broadcast\n\n\n5\n宝贝\nbǎo bèi - n. baby\n\n\n6\n逗\ndòu - v. to amuse, to tease\n\n\n7\n宣传\nxuān chuán - v. to publicize, to promote\n\n\n8\n手笔\nshǒubǐ - n. style (of handling affairs)\n\n\n9\n推广\ntuī guǎng - v. to popularize, to promote\n\n\n10\n注册\nzhù cè - v. to register\n\n\n11\n召开\nzhào kāi - v. to hold (a meeting), to convene\n\n\n12\n合作\nhé zuò - v. to cooperate, to work together\n\n\n13\n伙伴\nhuǒ bàn - n. partner\n\n\n14\n总裁\nzǒng cái - n. president (of a company)\n\n\n15\n实现\nshí xiàn - v. to realize, to achieve\n\n\n16\n覆盖\nfùgài - v. to cover\n\n\n17\n移动\nyí dòng - v. to move\n\n\n18\n通信\ntōngxìn - v. to communicate\n\n\n19\n应用\nyìng yòng - v. to apply\n\n\n20\n企业\nqǐ yè - n. enterprise, company\n\n\n21\n称霸\nchēng bà - v. to dominate, to maintain hegemony\n\n\n22\n背景\nbèijǐng - n. background\n\n\n23\n高级\ngāo jí - adj. senior, high-ranking\n\n\n24\n副\nfù - adj. deputy, vice\n\n\n25\n开发\nkāi fā - v. to develop, to exploit\n\n\n26\n中心\nzhōngxīn - n. center\n\n\n27\n相关\nxiāng guān - v. to correlate, to be relevant\n\n\n28\n业务\nyè wù - n. professional work, business\n\n\n29\n现实\nxiàn shí - n. reality\n\n\n30\n个人\ngè rén - n. individual\n\n\n31\n以及\nyǐjí - conj. and also, as well\n\n\n32\n程度\nchéngdù - n. degree, level\n\n\n33\n发达\nfā dá - adj. developed, advanced\n\n\n34\n创新\nchuàngxīn - v. to create something new, to innovate\n\n\n35\n领导\nlǐng dǎo - n. leader\n\n\n36\n地位\ndì wèi - n. position, status\n\n\n37\n经营\njīng yíng - v. to manage, to run\n\n\n38\n销售\nxiāo shòu - v. to sell, to market\n\n\n39\n针对\nzhēn duì - v. to be targeted at\n\n\n40\n当地\ndāng dì - n. local, in the locality\n\n\n41\n代言\ndàiyán - v. to speak on behalf of, to star in a commercial\n\n\n42\n华裔\nhuá yì - n. foreign citizen of Chinese origin\n\n\n43\n移民\nyímín - n. immigrant"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第12课-外国用户玩儿微信/index.html#听力",
    "href": "学汉语的日记/HSK5上-第12课-外国用户玩儿微信/index.html#听力",
    "title": "HSK5上 | 第12课：外国用户玩儿微信",
    "section": "2.1. 听力",
    "text": "2.1. 听力"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第12课-外国用户玩儿微信/index.html#阅读",
    "href": "学汉语的日记/HSK5上-第12课-外国用户玩儿微信/index.html#阅读",
    "title": "HSK5上 | 第12课：外国用户玩儿微信",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n19.\n20.\n21.\n22.\n23-25.\n23.\n24.\n25.\n26-28.\n26.\n27.\n28."
  },
  {
    "objectID": "学汉语的日记/HSK5上-第12课-外国用户玩儿微信/index.html#书写",
    "href": "学汉语的日记/HSK5上-第12课-外国用户玩儿微信/index.html#书写",
    "title": "HSK5上 | 第12课：外国用户玩儿微信",
    "section": "2.3. 书写",
    "text": "2.3. 书写"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第14课-北京的四合院/index.html",
    "href": "学汉语的日记/HSK5上-第14课-北京的四合院/index.html",
    "title": "HSK5上 | 第14课：北京的四合院",
    "section": "",
    "text": "北京的四合院\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n四合院\nsìhéyuàn - n. quadrangle courtyard\n\n\n2\n民居\nmínjū - n. civilian dwelling (place of residence)\n\n\n3\n组合\nzǔ hé - v./n. to combine; combination\n\n\n4\n建筑\njiànzhú - n. building, architecture\n\n\n5\n形式\nxíngshì - n. form, mode\n\n\n6\n所谓\nsuǒ wèi - adj. so-called\n\n\n7\n方\nfāng - adj. square\n\n\n8\n广泛\nguǎng fàn - adj. wide, extensive\n\n\n9\n样式\nyàng shì - n. style, pattern\n\n\n10\n通常\ntōng cháng - adv. usually\n\n\n11\n并列\nbìng liè - v. to stand side by side\n\n\n12\n组成\nzǔ chéng - v. to form, to constitute\n\n\n13\n长辈\nzhǎng bèi - n. senior member of a family\n\n\n14\n具备\njù bèi - v. to have, to possess\n\n\n15\n日常\nrìcháng - adj. day-to-day, daily\n\n\n16\n起居\nqǐ jū - n. daily life\n\n\n17\n接待\njiē dài - v. to receive, to entertain\n\n\n18\n功能\ngōng néng - n. function\n\n\n19\n厢房\nxiāngfáng - b. wing, wing room\n\n\n20\n走廊\nzǒuláng - n. corridor, passageway\n\n\n21\n空间\nkōngjiān - n. space\n\n\n22\n竹子\nzhúzi - n. bamboo\n\n\n23\n金鱼\njīn yú - n. goldfish\n\n\n24\n创造\nchuàng zào - v. to create, to produce\n\n\n25\n情趣\nqíngqù - n. emotional appeal, interest\n\n\n26\n因而\nyīn ér - conj. therefore, thus\n\n\n27\n关闭\nguān bì - v. to close, to shut\n\n\n28\n封闭\nfēng bì - v. to close, to seal\n\n\n29\n打交道\ndǎ jiāo dào - to make contact with\n\n\n30\n日子\nrìzi - n. life, livelihood\n\n\n31\n充分\nchōng fèn - adj. ample, sufficient\n\n\n32\n令\nlìng - v. to make, to cause\n\n\n33\n亲切\nqīn qiè - adj. warm, cordial, affectionate\n\n\n34\n劳动\nláo dòng - n./v. work, labor; to work, to do physical labor\n\n\n35\n人民\nrénmín - n. people\n\n\n36\n矛盾\nmáodùn - n./adj. conflict; conflicting\n\n\n37\n浓\nnóng - adj. strong, deep"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第14课-北京的四合院/index.html#生词",
    "href": "学汉语的日记/HSK5上-第14课-北京的四合院/index.html#生词",
    "title": "HSK5上 | 第14课：北京的四合院",
    "section": "",
    "text": "北京的四合院\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n四合院\nsìhéyuàn - n. quadrangle courtyard\n\n\n2\n民居\nmínjū - n. civilian dwelling (place of residence)\n\n\n3\n组合\nzǔ hé - v./n. to combine; combination\n\n\n4\n建筑\njiànzhú - n. building, architecture\n\n\n5\n形式\nxíngshì - n. form, mode\n\n\n6\n所谓\nsuǒ wèi - adj. so-called\n\n\n7\n方\nfāng - adj. square\n\n\n8\n广泛\nguǎng fàn - adj. wide, extensive\n\n\n9\n样式\nyàng shì - n. style, pattern\n\n\n10\n通常\ntōng cháng - adv. usually\n\n\n11\n并列\nbìng liè - v. to stand side by side\n\n\n12\n组成\nzǔ chéng - v. to form, to constitute\n\n\n13\n长辈\nzhǎng bèi - n. senior member of a family\n\n\n14\n具备\njù bèi - v. to have, to possess\n\n\n15\n日常\nrìcháng - adj. day-to-day, daily\n\n\n16\n起居\nqǐ jū - n. daily life\n\n\n17\n接待\njiē dài - v. to receive, to entertain\n\n\n18\n功能\ngōng néng - n. function\n\n\n19\n厢房\nxiāngfáng - b. wing, wing room\n\n\n20\n走廊\nzǒuláng - n. corridor, passageway\n\n\n21\n空间\nkōngjiān - n. space\n\n\n22\n竹子\nzhúzi - n. bamboo\n\n\n23\n金鱼\njīn yú - n. goldfish\n\n\n24\n创造\nchuàng zào - v. to create, to produce\n\n\n25\n情趣\nqíngqù - n. emotional appeal, interest\n\n\n26\n因而\nyīn ér - conj. therefore, thus\n\n\n27\n关闭\nguān bì - v. to close, to shut\n\n\n28\n封闭\nfēng bì - v. to close, to seal\n\n\n29\n打交道\ndǎ jiāo dào - to make contact with\n\n\n30\n日子\nrìzi - n. life, livelihood\n\n\n31\n充分\nchōng fèn - adj. ample, sufficient\n\n\n32\n令\nlìng - v. to make, to cause\n\n\n33\n亲切\nqīn qiè - adj. warm, cordial, affectionate\n\n\n34\n劳动\nláo dòng - n./v. work, labor; to work, to do physical labor\n\n\n35\n人民\nrénmín - n. people\n\n\n36\n矛盾\nmáodùn - n./adj. conflict; conflicting\n\n\n37\n浓\nnóng - adj. strong, deep"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第14课-北京的四合院/index.html#听力",
    "href": "学汉语的日记/HSK5上-第14课-北京的四合院/index.html#听力",
    "title": "HSK5上 | 第14课：北京的四合院",
    "section": "2.1. 听力",
    "text": "2.1. 听力"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第14课-北京的四合院/index.html#阅读",
    "href": "学汉语的日记/HSK5上-第14课-北京的四合院/index.html#阅读",
    "title": "HSK5上 | 第14课：北京的四合院",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n19.\n20.\n21.\n22.\n23-25.\n23.\n24.\n25.\n26-28.\n26.\n27.\n28."
  },
  {
    "objectID": "学汉语的日记/HSK5上-第14课-北京的四合院/index.html#书写",
    "href": "学汉语的日记/HSK5上-第14课-北京的四合院/index.html#书写",
    "title": "HSK5上 | 第14课：北京的四合院",
    "section": "2.3. 书写",
    "text": "2.3. 书写"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第16课-体重与节食/index.html",
    "href": "学汉语的日记/HSK5上-第16课-体重与节食/index.html",
    "title": "HSK5上 | 第16课：体重与节食",
    "section": "",
    "text": "体重与节食\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n节食\njiéshí - v. to go on a diet\n\n\n2\n报道\nbào dào - n./v. to report, to cover; report\n\n\n3\n营养\nyíng yǎng - n. nutrition\n\n\n4\n摄入\nshè rù - v. to take in, to ingest\n\n\n5\n模式\nmóshì - n. model, pattern\n\n\n6\n波动\nbō dòng - v. to undulate, to rise and fall\n\n\n7\n总共\nzǒng gòng - adv. altogether, in total\n\n\n8\n参与\ncān yù - v. to take part in\n\n\n9\n人员\nrényuán - n. personnel, staff\n\n\n10\n相对\nxiāng duì - adj. relative, comparative\n\n\n11\n类型\nlèi xíng - n. type, category\n\n\n12\n称\nchēng - v. to weigh\n\n\n13\n可靠\nkě kào - adj. reliable, dependable\n\n\n14\n纳入\nnàrù - v. to include, to incorporate into\n\n\n15\n分析\nfēn xī - v. to analyze\n\n\n16\n志愿者\nzhì yuàn zhě - n. volunteer\n\n\n17\n跟踪\ngēn zōng - v. to follow, to track\n\n\n18\n成果\nchéngguǒ - n. result, achievement\n\n\n19\n清晰\nqīngxī - adj. clear, distinct\n\n\n20\n即\njí - v./adv. to be; namely\n\n\n21\n升\nshēng - v. to rise, to go up\n\n\n22\n意外\nyìwài - adj./n. unexpected; accident\n\n\n23\n存在\ncúnzài - v. to exist\n\n\n24\n明显\nmíng xiǎn - adj. obvious, apparent\n\n\n25\n补偿\nbǔ cháng - v. to compensate\n\n\n26\n立即\nlì jí - adv. immediately, at once\n\n\n27\n趋势\nqū shì - n. trend, tendency\n\n\n28\n差异\nchā yì - n. difference\n\n\n29\n联合\nlián hé - v./adj. to unite, to ally; joint, combined\n\n\n30\n个别\ngè bié - adv./adj. one or two; exceptional\n\n\n31\n表明\nbiǎo míng - v. to indicate, to manifest\n\n\n32\n临时\nlín shí - adv./adj. for a short time; temporary\n\n\n33\n现象\nxiàn xiàng - n. phenomenon\n\n\n34\n非\nfēi - v. to be not\n\n\n35\n就餐\njiù cān - v. to have one’s meal\n\n\n36\n放纵\nfàngzòng - v. to indulge, to be unrestrained\n\n\n37\n苗条\nmiáotiáo - adj. slim, slender\n\n\n38\n借口\njiè kǒu - n./v. excuse, pretext; to use as an excuse\n\n\n39\n采取\ncǎi qǔ - v. to take, to adopt\n\n\n40\n措施\ncuò shī - n. measure, step"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第16课-体重与节食/index.html#生词",
    "href": "学汉语的日记/HSK5上-第16课-体重与节食/index.html#生词",
    "title": "HSK5上 | 第16课：体重与节食",
    "section": "",
    "text": "体重与节食\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n节食\njiéshí - v. to go on a diet\n\n\n2\n报道\nbào dào - n./v. to report, to cover; report\n\n\n3\n营养\nyíng yǎng - n. nutrition\n\n\n4\n摄入\nshè rù - v. to take in, to ingest\n\n\n5\n模式\nmóshì - n. model, pattern\n\n\n6\n波动\nbō dòng - v. to undulate, to rise and fall\n\n\n7\n总共\nzǒng gòng - adv. altogether, in total\n\n\n8\n参与\ncān yù - v. to take part in\n\n\n9\n人员\nrényuán - n. personnel, staff\n\n\n10\n相对\nxiāng duì - adj. relative, comparative\n\n\n11\n类型\nlèi xíng - n. type, category\n\n\n12\n称\nchēng - v. to weigh\n\n\n13\n可靠\nkě kào - adj. reliable, dependable\n\n\n14\n纳入\nnàrù - v. to include, to incorporate into\n\n\n15\n分析\nfēn xī - v. to analyze\n\n\n16\n志愿者\nzhì yuàn zhě - n. volunteer\n\n\n17\n跟踪\ngēn zōng - v. to follow, to track\n\n\n18\n成果\nchéngguǒ - n. result, achievement\n\n\n19\n清晰\nqīngxī - adj. clear, distinct\n\n\n20\n即\njí - v./adv. to be; namely\n\n\n21\n升\nshēng - v. to rise, to go up\n\n\n22\n意外\nyìwài - adj./n. unexpected; accident\n\n\n23\n存在\ncúnzài - v. to exist\n\n\n24\n明显\nmíng xiǎn - adj. obvious, apparent\n\n\n25\n补偿\nbǔ cháng - v. to compensate\n\n\n26\n立即\nlì jí - adv. immediately, at once\n\n\n27\n趋势\nqū shì - n. trend, tendency\n\n\n28\n差异\nchā yì - n. difference\n\n\n29\n联合\nlián hé - v./adj. to unite, to ally; joint, combined\n\n\n30\n个别\ngè bié - adv./adj. one or two; exceptional\n\n\n31\n表明\nbiǎo míng - v. to indicate, to manifest\n\n\n32\n临时\nlín shí - adv./adj. for a short time; temporary\n\n\n33\n现象\nxiàn xiàng - n. phenomenon\n\n\n34\n非\nfēi - v. to be not\n\n\n35\n就餐\njiù cān - v. to have one’s meal\n\n\n36\n放纵\nfàngzòng - v. to indulge, to be unrestrained\n\n\n37\n苗条\nmiáotiáo - adj. slim, slender\n\n\n38\n借口\njiè kǒu - n./v. excuse, pretext; to use as an excuse\n\n\n39\n采取\ncǎi qǔ - v. to take, to adopt\n\n\n40\n措施\ncuò shī - n. measure, step"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第16课-体重与节食/index.html#听力",
    "href": "学汉语的日记/HSK5上-第16课-体重与节食/index.html#听力",
    "title": "HSK5上 | 第16课：体重与节食",
    "section": "2.1. 听力",
    "text": "2.1. 听力"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第16课-体重与节食/index.html#阅读",
    "href": "学汉语的日记/HSK5上-第16课-体重与节食/index.html#阅读",
    "title": "HSK5上 | 第16课：体重与节食",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n19.\n20.\n21.\n22.\n23-25.\n23.\n24.\n25.\n26-28.\n26.\n27.\n28."
  },
  {
    "objectID": "学汉语的日记/HSK5上-第16课-体重与节食/index.html#书写",
    "href": "学汉语的日记/HSK5上-第16课-体重与节食/index.html#书写",
    "title": "HSK5上 | 第16课：体重与节食",
    "section": "2.3. 书写",
    "text": "2.3. 书写"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第18课-抽象艺术美不美/index.html",
    "href": "学汉语的日记/HSK5上-第18课-抽象艺术美不美/index.html",
    "title": "HSK5上 | 第18课：抽象艺术美不美",
    "section": "",
    "text": "抽象艺术美不美\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n抽象\nchōu xiàng - adj./v. abstract; to draw a correct conclusion from objective facts\n\n\n2\n古典\ngǔ diǎn - adj. classical\n\n\n3\n欣赏\nxīn shǎng - v. to enjoy, to admire\n\n\n4\n布\nbù - n. cloth\n\n\n5\n规则\nguī zé -adj./n. regular; rule\n\n\n6\n派\npài - n. faction, school, group of people sharing identical ideas, style or tastes\n\n\n7\n作品\nzuòpǐn - n. work (of art or literature)\n\n\n8\n洒\nsǎ - v. to sprinkle, to spray\n\n\n9\n极其\njí qí - adv. extremely\n\n\n10\n神秘\nshén mì - adj. mysterious, mystical\n\n\n11\n丑（陋）\nchǒu (lòu) - adj. ugly\n\n\n12\n自由\nzìyóu - n./adj. freedom; free\n\n\n13\n设计\nshè jì - v./n. to design; design\n\n\n14\n组\nzǔ - n./m. group, team, set\n\n\n15\n幅\nfú - m. used for paintings and scrolls etc.\n\n\n16\n出自\nchūzì - v. to come from, to stem from\n\n\n17\n业余\nyèyú - adj. amateur\n\n\n18\n婴儿\nyīng’ér - n. infant, baby\n\n\n19\n黑猩猩\nhēixīngxīng - n. chimpanzee\n\n\n20\n涂鸦\ntúyā - v. to scrawl, to scribble, graffiti\n\n\n21\n签\nqiān - v. to sign; to autograph\n\n\n22\n其余\nqí yú - pron. the rest, the others\n\n\n23\n身份\nshēn fèn - n. identity\n\n\n24\n确认\nquè rèn - v. to confirm, to affirm\n\n\n25\n随手\nsuíshǒu - adv. randomly, without extra trouble\n\n\n26\n分辨\nfēnbiàn - v. to distinguish, to differentiate\n\n\n27\n挥\nhuī - v. to wave, to wield\n\n\n28\n可见\nkějiàn - conj. it can be seen that..\n\n\n29\n哪怕\nnǎ pà - conj. even if\n\n\n30\n元素\nyuánsù - n. element\n\n\n31\n调整\ntiáo zhěng - v./n. to adjust, to revise; adjustment\n\n\n32\n位置\nwèizhì - n. position, location\n\n\n33\n含意\nhányì - n. implied meaning, implication\n\n\n34\n区域\nqūyù - n. region, district\n\n\n35\n活跃\nhuó yuè - adj./v. active, dynamic; to activate\n\n\n36\n布局\nbù jú - n. composition (of a piece of art or writing)\n\n\n37\n事实\nshì shí - n. fact\n\n\n38\n目前\nmù qián - n. now, present\n\n\n39\n证据\nzhèng jù - n. proof, evidence\n\n\n40\n话题\nhuà tí - n. topic"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第18课-抽象艺术美不美/index.html#生词",
    "href": "学汉语的日记/HSK5上-第18课-抽象艺术美不美/index.html#生词",
    "title": "HSK5上 | 第18课：抽象艺术美不美",
    "section": "",
    "text": "抽象艺术美不美\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n抽象\nchōu xiàng - adj./v. abstract; to draw a correct conclusion from objective facts\n\n\n2\n古典\ngǔ diǎn - adj. classical\n\n\n3\n欣赏\nxīn shǎng - v. to enjoy, to admire\n\n\n4\n布\nbù - n. cloth\n\n\n5\n规则\nguī zé -adj./n. regular; rule\n\n\n6\n派\npài - n. faction, school, group of people sharing identical ideas, style or tastes\n\n\n7\n作品\nzuòpǐn - n. work (of art or literature)\n\n\n8\n洒\nsǎ - v. to sprinkle, to spray\n\n\n9\n极其\njí qí - adv. extremely\n\n\n10\n神秘\nshén mì - adj. mysterious, mystical\n\n\n11\n丑（陋）\nchǒu (lòu) - adj. ugly\n\n\n12\n自由\nzìyóu - n./adj. freedom; free\n\n\n13\n设计\nshè jì - v./n. to design; design\n\n\n14\n组\nzǔ - n./m. group, team, set\n\n\n15\n幅\nfú - m. used for paintings and scrolls etc.\n\n\n16\n出自\nchūzì - v. to come from, to stem from\n\n\n17\n业余\nyèyú - adj. amateur\n\n\n18\n婴儿\nyīng’ér - n. infant, baby\n\n\n19\n黑猩猩\nhēixīngxīng - n. chimpanzee\n\n\n20\n涂鸦\ntúyā - v. to scrawl, to scribble, graffiti\n\n\n21\n签\nqiān - v. to sign; to autograph\n\n\n22\n其余\nqí yú - pron. the rest, the others\n\n\n23\n身份\nshēn fèn - n. identity\n\n\n24\n确认\nquè rèn - v. to confirm, to affirm\n\n\n25\n随手\nsuíshǒu - adv. randomly, without extra trouble\n\n\n26\n分辨\nfēnbiàn - v. to distinguish, to differentiate\n\n\n27\n挥\nhuī - v. to wave, to wield\n\n\n28\n可见\nkějiàn - conj. it can be seen that..\n\n\n29\n哪怕\nnǎ pà - conj. even if\n\n\n30\n元素\nyuánsù - n. element\n\n\n31\n调整\ntiáo zhěng - v./n. to adjust, to revise; adjustment\n\n\n32\n位置\nwèizhì - n. position, location\n\n\n33\n含意\nhányì - n. implied meaning, implication\n\n\n34\n区域\nqūyù - n. region, district\n\n\n35\n活跃\nhuó yuè - adj./v. active, dynamic; to activate\n\n\n36\n布局\nbù jú - n. composition (of a piece of art or writing)\n\n\n37\n事实\nshì shí - n. fact\n\n\n38\n目前\nmù qián - n. now, present\n\n\n39\n证据\nzhèng jù - n. proof, evidence\n\n\n40\n话题\nhuà tí - n. topic"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第18课-抽象艺术美不美/index.html#听力",
    "href": "学汉语的日记/HSK5上-第18课-抽象艺术美不美/index.html#听力",
    "title": "HSK5上 | 第18课：抽象艺术美不美",
    "section": "2.1. 听力",
    "text": "2.1. 听力"
  },
  {
    "objectID": "学汉语的日记/HSK5上-第18课-抽象艺术美不美/index.html#阅读",
    "href": "学汉语的日记/HSK5上-第18课-抽象艺术美不美/index.html#阅读",
    "title": "HSK5上 | 第18课：抽象艺术美不美",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n19.\n20.\n21.\n22.\n23-25.\n23.\n24.\n25.\n26-28.\n26.\n27.\n28."
  },
  {
    "objectID": "学汉语的日记/HSK5上-第18课-抽象艺术美不美/index.html#书写",
    "href": "学汉语的日记/HSK5上-第18课-抽象艺术美不美/index.html#书写",
    "title": "HSK5上 | 第18课：抽象艺术美不美",
    "section": "2.3. 书写",
    "text": "2.3. 书写"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第20课-小人书摊/index.html",
    "href": "学汉语的日记/HSK5下-第20课-小人书摊/index.html",
    "title": "HSK5下 | 第20课： 小人书摊",
    "section": "",
    "text": "小人书摊 | Picture-story book stalls\n小人书，是一种以书的形式出版的连环画。在二十世纪五六十年代，那时候生活很单调，没有网络，没有动画片，读小人书是儿童最主要的娱乐之一。不仅小孩子爱看，还有无数的青少年和大人也爱看。\n随着小人书的流行，出现了从事租书业务的小人书摊，这对于那些想看又买不起书的人来说，只用很少的钱就能看一本，毫无疑问是件大好事。\n记得小时候，我家附近就有个小人书摊，就是一进街口靠墙的一个小棚子，里面用几块砖头支着粗糙的木头板子供人们坐着看书。棚子里有一张床板摆着各种题材的小人书，墙边还拉了几根绳子，一本本书翻开搭在上面，五颜六色的很好看。为了减少损坏程度，每本小小人书都用牛皮纸加了层封皮，封皮上用毛笔写上书名，整齐漂亮的毛笔宇能充分地显示出书摊主人的文化水平。摊主是位上了年纪、身材瘦小的老人，总是穿着一件交色长衫，静静地坐在一边，陪着看书的人们。\n在这里看书的人大部分是附近住户的孩子，也有一些喜欢小人书的成人。租借小人书很便宜，在摊里看，每册1分钱，选好书坐下就看，看完连书带钱交给摊主；假如借走回家看，则每本每天2分钱，桃好书后交给摊主，摊主仔细地将租书人的姓名、地址和所借小人书的的书名登记在本子上，收了租金就可以拿走了，第二天还书时再把记一个一个地画掉，还书手续就算是办理好了。印象中似乎没有什么押金，全凭信用。我每天放学回家总要经过这家书摊，都要进去看看。\n然而，这种影响了数代人的小人书，如今只能在北京的潘家园、护国寺等地的旧书摊上找到，一些印刷精美、有特色的作品则身价大涨，成了收藏品，甚至进了博物馆。小人书和小人书摊已成为历史的记忆。\n改编自《中国电视报》，作者：俞万林\n\n\n\n小人书摊\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n摊\ntān - n. stall, stand (vendor)\n\n\n2\n出版\nchūbǎn - v. to publish\n\n\n3\n连环画\nlián​ huán​ huà​ - n. picture-story book\n\n\n4\n年代\nnián dài - n. decade of a century\n\n\n5\n单调\ndān diào - adj. monotonous, dull\n\n\n6\n网络\nwǎngluò - n. network, web\n\n\n7\n动画片\ndòng huà piàn - n. animated cartoon\n\n\n8\n娱乐\nyú lè - n./v. entertainment; to amuse, to entertain\n\n\n9\n无数\nwú shù - adj. countless, innumerable\n\n\n10\n青少年\nqīngshàonián - n. youngsters, teenagers\n\n\n11\n从事\ncóng shì - v. to engage in\n\n\n12\n毫无\nháo wú - not in the least\n\n\n13\n疑问\nyí wèn - n. question, doubt\n\n\n14\n棚子\npéngzi - n. shed, shack\n\n\n15\n砖头\nzhuān tóu - n. brick\n\n\n16\n支\nzhī - v. to prop up, to support\n\n\n17\n粗糙\ncūcāo - adj. rough, crude\n\n\n18\n木头\nmù tou - n. wood, timber\n\n\n19\n题材\ntícái - n. theme, subject matter\n\n\n20\n翻\nfān - v. to turn (over)\n\n\n21\n搭\ndā - v. to hang over, to lay over\n\n\n22\n整齐\nzhěng qí - adj. tidy, orderly\n\n\n23\n年纪\nnián jì - n. age\n\n\n24\n身材\nshēncái - n. stature, figure\n\n\n25\n成人\nchéngrén - n. adult\n\n\n26\n册\ncè - m. volume\n\n\n27\n假如\njiǎrú - conj. if, in case\n\n\n28\n登记\ndēng jì - v. to register, to enter one’s name\n\n\n29\n记录\njì lù - n./v. record, note; to record\n\n\n30\n手续\nshǒu xù - n. procedure\n\n\n31\n办理\nbàn lǐ - v. to handle, to deal with\n\n\n32\n押金\nyājīn - n. cash pledge, deposit\n\n\n33\n凭\npíng - v./prep. to rely on; on the basis of\n\n\n34\n印刷\nyìnshuā - v. to print (books, newspapers, etc.)\n\n\n35\n涨\nzhǎng - v. to rise, to go up\n\n\n36\n收藏\nshōucáng - v. to collect, to store up\n\n\n\n\n\n\n\n动词+得/不+起\n\nBiểu thị về mặt chủ quan có (không có) năng lực và điều kiện thực hiện (hoặc tiếp nhận) động tác nào đó. Ví dụ:\n\n·····这对于那些想看又买不起(không mua nổi)书的人来说，只用很少的钱就能看一本，毫无疑问是件大好事。\n古时候，有个十分好学的年轻人，但他家里很穷，买不起灯，一到晚上就不能读书。\n只有经得起困难和时间考验的朋友才算是真正的朋友。\n\n\n支\n\n“支” (chống) động từ, biểu thị dùng đồ vật chống cho vật thể không bị đổ xuống. Ví dụ:\n\n他的两只手放在桌上，支着脑袋，正在想事情。\n·····我家附近就有个小人书摊，就是一进街口靠墙的一个小棚子，里面用几块砖头支着粗糙的木头板子供人们做着看书。\n\n“支” ,(cây/đội/bản) có thể làm lượng từ, dùng trong tác phẩm âm nhạc, đội ngũ, hoặc đồ vật có hình cán. Ví dụ\n\n他弹第二支曲子时引起了牛的注意。\n给他十支枪，他就能拉起一支军队来。\n\n\n凭\n\n“凭” Động từ, có nghĩa là nhờ vào, dựa vào. Ví dụ:\n\n干工作不能光凭经验，还要有创新。\n印象中似乎没有什么押金，全凭信用。\n\n“凭” Còn có thể làm giới từ, thường dùng cách thức:“ 凭+ tân ngữ + động từ”, biểu thị căn cứ, dựa vào. Ví dụ:\n\n请旅客们准备好车票，凭票进站。\n你凭什么怀疑我偷了东西?\n\n\nPhân biệt 记录 và 纪录\n\n共同点：没有\n不同点：\n\n不同点\n\n\n\n\n\n\n\n\n记录\n纪录\n\n\n\n\n1\nCó thể làm động từ, chỉ đem lời nói nghe thấy hoặc việc xảy ra ghi chép lại.\n如：我已经把这次会议的内容详细地记录下来了。\nDanh từ, chỉ thành tích tốt nhất trong khoảng thời gian nhất định, trong phạm vi nhất định.\n如：他在本次比赛中打破了世界纪录。\n\n\n2\nCũng có thể làm danh từ, chỉ tài liệu được ghi chép lại hoặc người ghi chép.\n如：\n\n第二天还书时再把记录一个一个地画掉。\n小刘，你来做这次会议的记录。\n\nDanh từ, cũng có thể chỉ việc ghi lại những sự kiện có giá trị tin tức.\n如：学校带孩子们看了一部有教育意义的纪录片。\n\n\n\n\n\n\n\n单位、场所和生产\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n俱乐部\nju4le4bu4 - club, câu lạc bộ\n\n\n2\n幼儿园\nyou4er2yuan2 - kindergarten, nhà trẻ, trường mẫu giáo\n\n\n3\n博物馆\nbo2wu4guan3 - museum, viện bảo tàng\n\n\n4\n酒吧\njiu3ba1 - bar\n\n\n5\n法院\nfa3yuan4 - court, tòa\n\n\n6\n海关\nhai3guan4 - customs, hải quan\n\n\n7\n生产\nsheng1chan3 - sản xuất, produce\n\n\n8\n发明\nfa1ming2 - invent, phát minh\n\n\n9\n设计\nshe4ji4 - design, thiết kế\n\n\n10\n业务\nye4wu4 - service offered, dịch vụ, lĩnh vực\n\n\n11\n项目\nxiang4mu4 - dự án, chương trình, project\n\n\n12\n效率\nxiao4lv4 - efficiency, hiệu quả\n\n\n13\n合法\nhe2fa3 - hợp pháp, legal\n\n\n14\n公平\ngong1ping2 - fair, công bằng\n\n\n\n\n\n\n我的童年\n我在乡下出生和长大，十几年前那儿的经济条件没有现在那么好，所以孩子们的娱乐方式也有点单调。我的朋友们一般喜欢在田野玩游戏，比如放风筝、在河里游泳。但我没有他们那么好动，我记得我的小学有一家小图书馆，里面有小人书、小说、历史和科学书等很多种类。我就变成了图书管的常客，几乎每次下课都留在学校看书。没有很多人读书，所以我还可以借回家。这些页面为我描绘了一个与我出生之处完全不同的新世界。在我记忆中留下最深刻的书是马克·吐温的《汤姆·索亚历险记》。Tom Sawyer\n\n\n\n1.  你的童年是在什么的地方度过的？\n我的童年是在一个山区农村度过的。十几年前那儿的经济条件没有现在那么好，所以孩子们的娱乐方式也有点单调。我的朋友们一般喜欢在田野玩游戏，比如又放牛又放风筝、在河里游泳。比较安静和内向的我却不太喜欢那样的娱乐活动。放学后，我大部分时间都在学校的图书馆或家里看电视。所以我从小都曾经读过很多小人书、小说、科学和历史等这样的书，也看过许多动画片、韩国和中国电视剧。我的童年就这样过的。\n2.  哪些人、哪些是或哪些东西给你留下的印象最深刻？\n在同年的所有娱乐中，个我留下最深刻的印象就是小说的那些页面。我度过了小说，有短篇的、也有长篇的，有越南、也有外国作家的小说。它们给我描绘了一个与我出生之处完全不同的新世界，集中让我到现在还很清楚记得是马克·吐温的《汤姆·索亚历险记》。\n3.  为什么？\n《汤姆·索亚历险记》是马克·吐温的经典小说，以南方小城为背景，描绘了主人公汤姆·索亚的儿时冒险故事。汤姆是个调皮捣蛋的男孩，他和好友哈克·费恩一同探险、捉弄村里的孩子。故事通过对冒险、友谊和成长的描写，深刻反映了美国南方社会的风土人情，成为美国文学的经典之一。这部小说是马克·吐温对美丽童年世界的赞美。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第20课-小人书摊/index.html#生词",
    "href": "学汉语的日记/HSK5下-第20课-小人书摊/index.html#生词",
    "title": "HSK5下 | 第20课： 小人书摊",
    "section": "",
    "text": "小人书摊\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n摊\ntān - n. stall, stand (vendor)\n\n\n2\n出版\nchūbǎn - v. to publish\n\n\n3\n连环画\nlián​ huán​ huà​ - n. picture-story book\n\n\n4\n年代\nnián dài - n. decade of a century\n\n\n5\n单调\ndān diào - adj. monotonous, dull\n\n\n6\n网络\nwǎngluò - n. network, web\n\n\n7\n动画片\ndòng huà piàn - n. animated cartoon\n\n\n8\n娱乐\nyú lè - n./v. entertainment; to amuse, to entertain\n\n\n9\n无数\nwú shù - adj. countless, innumerable\n\n\n10\n青少年\nqīngshàonián - n. youngsters, teenagers\n\n\n11\n从事\ncóng shì - v. to engage in\n\n\n12\n毫无\nháo wú - not in the least\n\n\n13\n疑问\nyí wèn - n. question, doubt\n\n\n14\n棚子\npéngzi - n. shed, shack\n\n\n15\n砖头\nzhuān tóu - n. brick\n\n\n16\n支\nzhī - v. to prop up, to support\n\n\n17\n粗糙\ncūcāo - adj. rough, crude\n\n\n18\n木头\nmù tou - n. wood, timber\n\n\n19\n题材\ntícái - n. theme, subject matter\n\n\n20\n翻\nfān - v. to turn (over)\n\n\n21\n搭\ndā - v. to hang over, to lay over\n\n\n22\n整齐\nzhěng qí - adj. tidy, orderly\n\n\n23\n年纪\nnián jì - n. age\n\n\n24\n身材\nshēncái - n. stature, figure\n\n\n25\n成人\nchéngrén - n. adult\n\n\n26\n册\ncè - m. volume\n\n\n27\n假如\njiǎrú - conj. if, in case\n\n\n28\n登记\ndēng jì - v. to register, to enter one’s name\n\n\n29\n记录\njì lù - n./v. record, note; to record\n\n\n30\n手续\nshǒu xù - n. procedure\n\n\n31\n办理\nbàn lǐ - v. to handle, to deal with\n\n\n32\n押金\nyājīn - n. cash pledge, deposit\n\n\n33\n凭\npíng - v./prep. to rely on; on the basis of\n\n\n34\n印刷\nyìnshuā - v. to print (books, newspapers, etc.)\n\n\n35\n涨\nzhǎng - v. to rise, to go up\n\n\n36\n收藏\nshōucáng - v. to collect, to store up"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第20课-小人书摊/index.html#注释",
    "href": "学汉语的日记/HSK5下-第20课-小人书摊/index.html#注释",
    "title": "HSK5下 | 第20课： 小人书摊",
    "section": "",
    "text": "动词+得/不+起\n\nBiểu thị về mặt chủ quan có (không có) năng lực và điều kiện thực hiện (hoặc tiếp nhận) động tác nào đó. Ví dụ:\n\n·····这对于那些想看又买不起(không mua nổi)书的人来说，只用很少的钱就能看一本，毫无疑问是件大好事。\n古时候，有个十分好学的年轻人，但他家里很穷，买不起灯，一到晚上就不能读书。\n只有经得起困难和时间考验的朋友才算是真正的朋友。\n\n\n支\n\n“支” (chống) động từ, biểu thị dùng đồ vật chống cho vật thể không bị đổ xuống. Ví dụ:\n\n他的两只手放在桌上，支着脑袋，正在想事情。\n·····我家附近就有个小人书摊，就是一进街口靠墙的一个小棚子，里面用几块砖头支着粗糙的木头板子供人们做着看书。\n\n“支” ,(cây/đội/bản) có thể làm lượng từ, dùng trong tác phẩm âm nhạc, đội ngũ, hoặc đồ vật có hình cán. Ví dụ\n\n他弹第二支曲子时引起了牛的注意。\n给他十支枪，他就能拉起一支军队来。\n\n\n凭\n\n“凭” Động từ, có nghĩa là nhờ vào, dựa vào. Ví dụ:\n\n干工作不能光凭经验，还要有创新。\n印象中似乎没有什么押金，全凭信用。\n\n“凭” Còn có thể làm giới từ, thường dùng cách thức:“ 凭+ tân ngữ + động từ”, biểu thị căn cứ, dựa vào. Ví dụ:\n\n请旅客们准备好车票，凭票进站。\n你凭什么怀疑我偷了东西?\n\n\nPhân biệt 记录 và 纪录\n\n共同点：没有\n不同点：\n\n不同点\n\n\n\n\n\n\n\n\n记录\n纪录\n\n\n\n\n1\nCó thể làm động từ, chỉ đem lời nói nghe thấy hoặc việc xảy ra ghi chép lại.\n如：我已经把这次会议的内容详细地记录下来了。\nDanh từ, chỉ thành tích tốt nhất trong khoảng thời gian nhất định, trong phạm vi nhất định.\n如：他在本次比赛中打破了世界纪录。\n\n\n2\nCũng có thể làm danh từ, chỉ tài liệu được ghi chép lại hoặc người ghi chép.\n如：\n\n第二天还书时再把记录一个一个地画掉。\n小刘，你来做这次会议的记录。\n\nDanh từ, cũng có thể chỉ việc ghi lại những sự kiện có giá trị tin tức.\n如：学校带孩子们看了一部有教育意义的纪录片。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第20课-小人书摊/index.html#扩展",
    "href": "学汉语的日记/HSK5下-第20课-小人书摊/index.html#扩展",
    "title": "HSK5下 | 第20课： 小人书摊",
    "section": "",
    "text": "单位、场所和生产\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n俱乐部\nju4le4bu4 - club, câu lạc bộ\n\n\n2\n幼儿园\nyou4er2yuan2 - kindergarten, nhà trẻ, trường mẫu giáo\n\n\n3\n博物馆\nbo2wu4guan3 - museum, viện bảo tàng\n\n\n4\n酒吧\njiu3ba1 - bar\n\n\n5\n法院\nfa3yuan4 - court, tòa\n\n\n6\n海关\nhai3guan4 - customs, hải quan\n\n\n7\n生产\nsheng1chan3 - sản xuất, produce\n\n\n8\n发明\nfa1ming2 - invent, phát minh\n\n\n9\n设计\nshe4ji4 - design, thiết kế\n\n\n10\n业务\nye4wu4 - service offered, dịch vụ, lĩnh vực\n\n\n11\n项目\nxiang4mu4 - dự án, chương trình, project\n\n\n12\n效率\nxiao4lv4 - efficiency, hiệu quả\n\n\n13\n合法\nhe2fa3 - hợp pháp, legal\n\n\n14\n公平\ngong1ping2 - fair, công bằng"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第20课-小人书摊/index.html#运用",
    "href": "学汉语的日记/HSK5下-第20课-小人书摊/index.html#运用",
    "title": "HSK5下 | 第20课： 小人书摊",
    "section": "",
    "text": "我的童年\n我在乡下出生和长大，十几年前那儿的经济条件没有现在那么好，所以孩子们的娱乐方式也有点单调。我的朋友们一般喜欢在田野玩游戏，比如放风筝、在河里游泳。但我没有他们那么好动，我记得我的小学有一家小图书馆，里面有小人书、小说、历史和科学书等很多种类。我就变成了图书管的常客，几乎每次下课都留在学校看书。没有很多人读书，所以我还可以借回家。这些页面为我描绘了一个与我出生之处完全不同的新世界。在我记忆中留下最深刻的书是马克·吐温的《汤姆·索亚历险记》。Tom Sawyer"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第20课-小人书摊/index.html#口语",
    "href": "学汉语的日记/HSK5下-第20课-小人书摊/index.html#口语",
    "title": "HSK5下 | 第20课： 小人书摊",
    "section": "",
    "text": "1.  你的童年是在什么的地方度过的？\n我的童年是在一个山区农村度过的。十几年前那儿的经济条件没有现在那么好，所以孩子们的娱乐方式也有点单调。我的朋友们一般喜欢在田野玩游戏，比如又放牛又放风筝、在河里游泳。比较安静和内向的我却不太喜欢那样的娱乐活动。放学后，我大部分时间都在学校的图书馆或家里看电视。所以我从小都曾经读过很多小人书、小说、科学和历史等这样的书，也看过许多动画片、韩国和中国电视剧。我的童年就这样过的。\n2.  哪些人、哪些是或哪些东西给你留下的印象最深刻？\n在同年的所有娱乐中，个我留下最深刻的印象就是小说的那些页面。我度过了小说，有短篇的、也有长篇的，有越南、也有外国作家的小说。它们给我描绘了一个与我出生之处完全不同的新世界，集中让我到现在还很清楚记得是马克·吐温的《汤姆·索亚历险记》。\n3.  为什么？\n《汤姆·索亚历险记》是马克·吐温的经典小说，以南方小城为背景，描绘了主人公汤姆·索亚的儿时冒险故事。汤姆是个调皮捣蛋的男孩，他和好友哈克·费恩一同探险、捉弄村里的孩子。故事通过对冒险、友谊和成长的描写，深刻反映了美国南方社会的风土人情，成为美国文学的经典之一。这部小说是马克·吐温对美丽童年世界的赞美。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第20课-小人书摊/index.html#听力",
    "href": "学汉语的日记/HSK5下-第20课-小人书摊/index.html#听力",
    "title": "HSK5下 | 第20课： 小人书摊",
    "section": "2.1. 听力",
    "text": "2.1. 听力\n1.  女：爸爸，阳阳可爱看您的那些旧小人书了，特别是那套《西游记》。\n男：你可得保留好，别弄坏了，现在那都是收藏品了。\n问：男的认为那些小人书怎么样？（A很有收藏价值）\n2.  女：这次没能进决赛，是我太骄傲了，对对手估计不足，准备不够充分。\n男：不要紧，回去好好总结经验，明年再来。\n问：男的最可能是谁？（A教练）\n3.  女：刘芳岁数不小了，也没个对象，你们学校有合适的吗？给她介绍一个。\n男：教体育的行吗？我们那儿的赵老师曾经得过武术冠军呢。\n问：关于刘芳，下列哪项正确？（D还没男朋友）\n4.  女：你怎么了？一直打喷嚏、流鼻涕的，感冒了吧？\n男：一到春天，我这花粉过敏的老毛病就又来了。\n问：男的怎么了？（D过敏了）\n5.  男：你们单位新来的小李怎么样？\n女：他平时话不多，干起活儿来却很卖力，多累都不抱怨。\n问：关于小李的工作表现，可以知道什么？（C工作很勤奋qin2fen）\n6.  男：你怎么这么快就回来了？借书证办好了吗？\n女：没有，手续倒是挺简单，登记时才知道要交押金，我没带那么多钱。\n问：女的为什么没办成借书证？（A没带狗押金）\n7.  男：最近找不到我的学生证了。\n女： 那你赶快补一个吧，马上就要毕业了，没有学生证到时候怎么办手续呀？\n男：这个有什么关系吗？\n女：按规定，办理离校手续时，如果交不出学生证，押金就不退还了。\n问：关于学生证，女的希望男的做什么？（A早点儿补办）\n8.  男：你帮我拿个主意，这两部手机你觉得买哪个好？\n女： 我不太喜欢大屏的，大的这部显得有点儿笨，而且处理速度也没小的快。\n男：大小我倒不在乎，但这部机身做工比较粗糙。\n女：那还是买这部小的吧。\n问：男的为什么不买大屏的那部手机？（D机身做工粗糙）\n9.  女：你身材保持这么好，天天去健身房吧？\n男： 那倒也不是，反正每周都去游两回泳。游泳馆离家比较近，还挺方便的。\n女：真羡慕你，我们家那儿想找个跑步的公园都得到五六公里以外。\n男：那就赶紧搬家吧。\n女：你说得倒容易。\n问：女的羡慕男的什么？（B游泳馆离家近）\n10.男：我记得以前在你家看见过一套《三国演义》的小人书，是不是？\n女：没错，那是我小时候爷爷给我买的。\n男：现在还有吗？听说那套书很有收藏价值，价钱都涨疯了。\n女：多贵我也不会卖的，那套书有纪念意义。\n问：关于《三国演义》的小人书，女的是什么意思？（B留作纪念）\n11-12.\n我家附近的医院有位老中医把脉特别准，不用病人自己介绍，他经过把脉之后，就能知道病人得的是什么病。\n那天，我感觉胃有些不舒服，打算去找老中医看看。正在做作业的儿子说：“爸爸，我刚好做完作业，我要和您一起去，您几天前就答应给我买玩具的，今天一定要买。”\n到了医院，我一字没说，老中医为我把脉后说：“你这病在胃上。”我点点头，然后说：“医生，请您也给我儿子看看吧。”儿子一听，急忙往后退，说：“算了，我还是回去把作业做完吧。”\n原来没做完作业的儿子担心老中医把脉看出他说了假话。\n11.这位老中医看病怎么样？（A判断病情很正确）\n12.儿子为什么要跟着一起去医院？（C顺便让爸爸买玩具）\n13-14.\n有个精力旺盛的老婆婆去乘公交车。上了车，一个彬彬有礼的小男孩儿起身给老婆婆让座，老婆婆说：“你坐好，我还很年轻，不需要你给我让座的！”\n过了一会儿小男孩儿又站了起来，老婆婆拍拍他的肩膀，说：“没有关系的啦，你不用给我让座，我没那么老，我还年轻！”\n就这样经过三四次后，小男孩儿哭了！\n他哭着说：“老奶奶，我家已经过了好几站了，你为什么不让我下车？”\n13.公交车上，老婆婆不坐下的原因是什么？（C觉得自己不需要做）\n14.关于小男孩儿， 可以知道什么？（D下不了车急哭了）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第20课-小人书摊/index.html#阅读",
    "href": "学汉语的日记/HSK5下-第20课-小人书摊/index.html#阅读",
    "title": "HSK5下 | 第20课： 小人书摊",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n世界球王贝利在20多年的足球生涯里,15A人参加过1364场比赛，共踢进1282个球，并创造了一个队员在一场比赛中射进8个球的16B纪录。他高超的球技不仅令万千观众心醉,就连球场上的对手17D也都为他鼓掌喝彩。不仅如此，他的谈吐也非同一般。当他个人进球达到1000个时，有人问他：“您哪个球踢得最好？”\n贝利的回答含蓄幽默，意味深长，如同他的球技一样精彩。他笑着说：“下一个。”\n在迈向成功的道路上，每当实现了一个近期18C目标时,我们不应自满，而应像贝利那样，把原来的成功当成是新的成功的起点，要有一种归零的心态，这样我们才能获得更多成功的乐趣。\n\nC科技的传播不受语言的影响\n\n科技的广泛传播逐渐打破了语言的障碍。电脑、因特网和各种电子通信设备使用的是一种没有国界的通用语言。令这里的孩子们着迷的电子游戏，也许在地球的另一端也同样受到其他孩子们的喜爱。\n\nD裤脚宽大有利于空气流动\n\n夏天的衣服，面料以外表面光滑、内表面粗糙的最好。因为平滑的面料对光的反射率高，吸收的热能较少，而粗糙的内表面能够增加衣服与皮肤之间的空气流通，有利于散热。同时，敞开的衣领及宽大的袖子和裤脚，在人走动时有明显的鼓风作用，能促进空气流动，而比较紧的衣服会阻碍空气流动，影响散热。\n\nB北京保存着很多皇家建筑\n\n提起“宫廷建筑”，一般人都不陌生。尤其是北京人，就生活在这宫廷建筑云集的城市之中。这里有中国现存最大、最完整的古建筑群——紫禁城（故宫），以及众多的皇家宫苑和园林，让人感到满眼皆风景，到处是古迹。它们述说着历史，凝聚着智慧，是中华民族建筑史上的瑰宝。\n\nC关心家人的孩子更有责任感\n\n培养孩子的责任感，就要让孩子学会关心别人、热爱生活。孩子是家庭的一分子，家里有事应该及时告知，让其承担自己的一份责任。要教育孩子关心亲人，要求孩子主动关心家里的老人、病人和兄弟姐妹。要让孩子做一些力所能及的家务劳动，使其在家庭生活的磨炼中形成责任感，进而上升为对父母、对家庭、对社会负责。\n23-25.\n一只蜜蜂无法度过严寒的冬天，一群蜜蜂则不同。据说蜂箱中的蜜蜂在过冬的时候,往往要抱成一团。最外面的一层是工蜂，它们拼命地扇动翅膀，像厚厚的衣服一样阻挡外面的寒冷。在这样严严实实的“包裹”之下，里边的温度恒定在130C左右，舒适如春。\n被工蜂“包裹”在里边的不仅有蜂王和雄蜂，还有其他的工蜂。饿了，它们依靠夏天采集来的蜜获得足够的能量。但里面的工蜂并不总在里面待着，一段时间后，它们需要到外面来“换岗”。就这样，蜜蜂的家族可以顺利度过寒冬。\n事实上，这个世界上没有人能孤立地活下来。有了别人的帮助，我们才容易度过人生的某个冬天，或者走出一段刻骨铭心的苦难，或者摆脱一场意外的困境。然而,我们如果能像蜜蜂过冬那样，反过来去”换岗”般地回报别人，或许，人生留给我们的所有最”冷”的路，我们都能从容走过。\n\nD阻止冷空气进入\nC接替外层工蜂的工作\nA懂得互帮互助\n\n26-28.\n你肯定听过卖橘子、卖房子，但是你相信世界上有人能“卖日子”吗？日子也可以当作商品，进行买卖吗？还真有这么一位先生，居然靠“卖日子”发了财呢。他就是法国的贝利。\n贝利是一个集报迷，家里收集了许多旧报纸。有一天，一个朋友来访，在他的报纸堆里随便翻着。突然，朋友满脸惊喜地叫出声来。原来，朋友手上拿的报纸正好是他女儿出生那天出版的。朋友最后要走了那份报纸，临出门还忙不停地说：“贝利，谢谢你，我把这份和我女儿同一天诞生的报纸送给女儿做生日礼物，她一定会非常高兴的。”\n通过这件事，贝利突然来了灵感：何不卖日子赚钱？也就是说，把家里的旧报纸当作商品，卖给跟报纸出版日期同一天出生的人。说干就干，贝利把家里的旧报纸一一整理好，成立了历史报纸档案公司，专营生日礼品报纸。\n每个人都有探源的心理，对自己生日那一天出版的报纸有一种微妙的关切之情。因此，贝利的奇特礼品一推出，就受到了顾客的欢迎，不出几日，贝利的报纸就销售一空了。于是，他遍访各地的图书馆，请求他们把准备丢弃的旧报纸卖给他。不久，法国国家图书馆等报纸馆藏单位与贝利签约，答应一旦图书馆把旧报纸制成显微胶片后，贝利可优先拥有购买权。\n有了丰富的报源之后，贝利就花大力气做推广。他在报纸、杂志、广播里大做广告,塑造新型“生日礼品”的形象，并且在礼品店、文具店建立销售点。他凭借自己的经验训练店员，教他们如何向顾客推荐这种“礼品”以提高销量。\n功夫不负有心人。如今，贝利历史报纸档案公司每年可卖出25万份旧报纸，平均每天能销出近700份。历史上一个个泛黄的“日子”，在他的经营之下都变成了白花花的银子。\n\nD卖光了家里的旧报纸\nD朋友的想法启发了他\nB要靠自己创造"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第20课-小人书摊/index.html#书写",
    "href": "学汉语的日记/HSK5下-第20课-小人书摊/index.html#书写",
    "title": "HSK5下 | 第20课： 小人书摊",
    "section": "2.3. 书写",
    "text": "2.3. 书写\n手续，押金，纪录，办理，登记。\n办理图书馆会员的手续非常简单。首先，你要登记在场阅读和借书回家的权利，填写相关表格，提交个人身份证。接着，你要付一定金额的押金，以确保书的安全使用。工作人员会认真记录个人信息，整个办理过程迅速高效。完成手续后，即可得到图书馆的会员卡，这张卡成为借阅丰富资源的通行证，让读者在知识的海洋中尽兴徜徉。\n缴纳（jiao3na4 - pay）；金额（jin1e2 - amount）；尽情（jin4qing2 – as much as one likes）；徜徉（chang2yang2 - wander）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第22课-阅读与思考/index.html",
    "href": "学汉语的日记/HSK5下-第22课-阅读与思考/index.html",
    "title": "HSK5下 | 第22课： 阅读与思考",
    "section": "",
    "text": "阅读与思考 | Reading and thinking\n很多家长可能过分强调阅读的作用，觉得多读书就能够把作文写得特别好。这个观点是不客观、不全面的，我们需要转变自己的观念。\n我曾经听一位美国的小学老师说，他们十分重视和学生一起讨论问题。比如，他们讨论过《卖火柴的小女孩儿》是写给谁看的，还讨论过灰姑娘的故事。老师讲完故事之后，问同学们：灰姑娘一旦进了这个王宫，成为王子的心上人，她的梦想实现了，一切幸福都属于她之后，这时她应该怎样对待她的继母，应该怎样对待她的两个姐姐？\n为什么要讨论这个问题呢？因为这是一种情感交换。老师和学生通过讨论得出的结论是：一个已经拥有巨大幸福的人，应该原谅和理解那些伤害过自己的人。另外还要承认人性中一些先天的不完美，就是说作为-个母亲，在自己的亲生女儿和不是亲生的灰姑娘之间，难免会更疼爱自己亲生的女儿，很难完全平等地对待她们。可能你觉得继母很自私，但这种行为有自然倾向的理由，与道德没有必然的关系。\n自从我听说了这件事，就开始思考应该如何阅读，除了阅读还应该做什么。你看这个老师在讲童话的时候，已经在有意识地把这种情感影响，甚至把人性的价值判断，都给了孩子们。如果只是单纯地主张阅读而不强调思考，那是片面的。\n“知识”两个字我始终认为它是要分开来谈的，“知”就是知感，“识”就是认识。所谓“知感”就是别人告诉你、 说给你听、要求你记住的那一部分。但只有这一部分是不够的，还要有认识、思考。\n改编自《北京青年报》，作者：梁晓声\n\n\n\n阅读与思考\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n过分\nguò fèn - adj. excessive\n\n\n2\n强调\nqiáng diào - v. to emphasize\n\n\n3\n作文\nzuòwén - n. essay, composition\n\n\n4\n观点\nguān diǎn - n. idea, opinion, point of view\n\n\n5\n客观\nkè guān - adj. objective\n\n\n6\n全面\nquán miàn - adj. all-round, comprehensive\n\n\n7\n转变\nzhuǎnbiàn - v. to change, to transform\n\n\n8\n观念\nguān niàn - n. mentality, concept\n\n\n9\n火柴\nhuǒ chái - n. match (for producing a flame)\n\n\n10\n灰\nhuī - n./adj. dust; gray\n\n\n11\n一旦\nyí dàn - adv. once, when\n\n\n12\n王宫\nwánggōng - n. royal palace\n\n\n13\n王子\nwángzǐ - n. prince\n\n\n14\n属于\nshǔ yú - v. to belong to\n\n\n15\n对待\nduì dài - v. to treat, to adopt a certain attitude towards\n\n\n16\n交换\njiāo huàn - v. to exchange, to interchange\n\n\n17\n拥有\nyōngyǒu - v. to own, to possess\n\n\n18\n巨大\njù dà - adj. huge, tremendous\n\n\n19\n承认\nchéng rèn - v. to admit, to acknowledge\n\n\n20\n人性\nrén xìng - n. human nature\n\n\n21\n完美\nwán měi - adj. perfect, flawless\n\n\n22\n难免\nnán miǎn - adj. hard to avoid\n\n\n23\n疼爱\nténg ài - v. to love dearly\n\n\n24\n平等\npíng děng - adj. equal\n\n\n25\n自私\nzì sī - adj. selfish\n\n\n26\n倾向\nqīng xiàng - v./n. to be inclined to; tendency, inclination\n\n\n27\n理由\nlǐyóu - n. reason, ground\n\n\n28\n道德\ndào dé - n. morality, ethics\n\n\n29\n自从\nzì cóng - prep. ever since\n\n\n30\n童话\ntónghuà - n. fairly tale\n\n\n31\n价值\njià zhí - n. value\n\n\n32\n单纯\ndān chún - adj. simple, mere\n\n\n33\n主张\nzhǔ zhāng - v./n. to hold, to advocate; view, stand\n\n\n34\n知感\nzhī gǎn - n. sense perception\n\n\n\n\n\n\n\n一旦\n\nphó từ, biểu thị thời gian không xác định, ngày đó bỗng nhiên tới hoặc giả dụ có ngày đó. Ví dụ:\n\n长大后，我终于明白了这个道理：女人一旦做了母亲，就变得矛盾了。\n灰姑娘一旦进了这个王宫，······应该怎样对待她的继母，应该怎样对待她的两个姐姐。\n所谓私人空间，是指我们身体周围的一定的空间，一旦有人闯入这个空间，我们就会感觉不舒服、不自在。\n\n\n难免\n\ntính từ, có nghĩa là khó tránh khỏi, không thể tránh khỏi. Ví dụ:\n\n刚开始工作，这样的错误是难免的。\n朋友间难免会发生矛盾、误会甚至是伤害。\n·····作为一个母亲，在自己的亲生女儿和不是亲生的灰姑娘之间，难免会更疼爱自己亲生的女儿，很难完全平等地对待她们。\n\n\n自从\n\ngiới từ, biểu thị bắt đầu từ mốc thời gian nào đó trong quá khứ. Ví dụ:\n\n自从城市出现后，它就成为人类生活的中心。\n自从有了长大后成为作家这个理想之后，他每天都坚持写作。\n自从我听了这件事，就开始思考应该如何阅读，除了阅读还应该做什么。\n\n\nPhân biệt 平等 và 公平\n\n共同点：Đều là tính từ, ý nghĩa tương tự nhau, có lúc có thể thay thế nhau.\n\n如：作为一个母亲，在自己的亲生女儿和不是亲生的灰姑娘之间，难免会更疼爱自己亲生的女儿，很难完全平等/公平地对待她们。\n\n不同点：\n\n不同点\n\n\n\n\n\n\n\nNo.\n平等\n公平\n\n\n\n\n1\nNhấn mạnh quyền lợi hoặc sự đãi ngộ giống nhau giữa người với người trong xã hội.\n如：法律面前人人平等。\nNhấn mạnh xử lí vấn đề hợp tình hợp lí , không thiên về một bên.\n如：我们应当公平竞争。\n\n\n2\nThường dùng với tình huống thường gặp, tính phố biến.\n如：现实社会中，女人与男人有时并不平等。\nThường dùng với người hoặc vật cụ thể.\n如：我认为公司对这次事情的处理不够公平。\n\n\n\n\n\n\n\n写作表达\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n作文\nzuo4wen2 - sáng tác, composition\n\n\n2\n论文\nlun4wen2 - luận văn, paper\n\n\n3\n主题\nzhu3ti2 - chủ đề, theme\n\n\n4\n题目\nti2mu4 - nội dung, chủ đề, subject\n\n\n5\n话题\nhua4ti2 - chủ đề, theme, topic\n\n\n6\n目录\nmu4lu4 - mục lục, catalog\n\n\n7\n提纲\nti2gang1 - outline, dàn ý\n\n\n8\n标点\nbiao1dian3 - punctuation, dấu chấm câu\n\n\n9\n废话\nfei4hua4 - tào lao, crap\n\n\n10\n胡说\nhu2shuo1 - nonsense, ăn nói hồ đồ\n\n\n\n\n\n\n假如我是灰姑娘\n（跟口语2同样）\n\n\n\n1.  简单地讲讲《灰姑娘》\n《灰姑娘》是一则广受欢迎的童话故事，描绘了一个经过不公待遇的女孩的成长之旅。故事中，灰姑娘受到仙女（xian1nv3）的帮助，得以参加王子的舞会。尽管遭受（zao1shou4）继母和继姐的刁难（diao1nan4），她依然保持着善良和美好的心灵。在舞会上，她与王子相遇，两人坠入爱河。尽管途中有一些波折（bo1zhe2），最终他们战胜了一切，过上了幸福的生活。这个故事传递出善良、美德和真爱的积极信息，深受读者和观众的喜爱。\n2.  。。。怎么对待继母和姐姐？\n如果我是灰姑娘，进入王宫后，我会选择以善待的态度面对继母和姐姐。尽管她们曾对我不友好，但我将以宽容的心情对待过去的不愉快。过去就过去了，我不会再计较他们。我会努力把以前不愉快的回忆都忘记，这样才能享受幸福。我认为人生中有的一些先天的事情不完美，最重要的是最后我能战胜一切，拿出来我值得享受的幸福。\n3.  有人承认人生中一些先天的不完美，你同意他的看法吗？\n是的，我同意这个观点。人生中存在各种各样的因素，有些是我们无法选择或改变的，即所谓的先天因素。这可能包括身体的某些特征、家庭背景、遗传等。人们生来就有各种各样的不完美之处，而这些不完美并不意味着人们无法获得幸福或成功。关键在于我们如何面对和接受这些不完美，并通过自己的努力和积极的态度去创造更美好的人生。接纳自己的不完美，并学会从中汲取力量，是迈向更充实人生的一部分。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第22课-阅读与思考/index.html#生词",
    "href": "学汉语的日记/HSK5下-第22课-阅读与思考/index.html#生词",
    "title": "HSK5下 | 第22课： 阅读与思考",
    "section": "",
    "text": "阅读与思考\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n过分\nguò fèn - adj. excessive\n\n\n2\n强调\nqiáng diào - v. to emphasize\n\n\n3\n作文\nzuòwén - n. essay, composition\n\n\n4\n观点\nguān diǎn - n. idea, opinion, point of view\n\n\n5\n客观\nkè guān - adj. objective\n\n\n6\n全面\nquán miàn - adj. all-round, comprehensive\n\n\n7\n转变\nzhuǎnbiàn - v. to change, to transform\n\n\n8\n观念\nguān niàn - n. mentality, concept\n\n\n9\n火柴\nhuǒ chái - n. match (for producing a flame)\n\n\n10\n灰\nhuī - n./adj. dust; gray\n\n\n11\n一旦\nyí dàn - adv. once, when\n\n\n12\n王宫\nwánggōng - n. royal palace\n\n\n13\n王子\nwángzǐ - n. prince\n\n\n14\n属于\nshǔ yú - v. to belong to\n\n\n15\n对待\nduì dài - v. to treat, to adopt a certain attitude towards\n\n\n16\n交换\njiāo huàn - v. to exchange, to interchange\n\n\n17\n拥有\nyōngyǒu - v. to own, to possess\n\n\n18\n巨大\njù dà - adj. huge, tremendous\n\n\n19\n承认\nchéng rèn - v. to admit, to acknowledge\n\n\n20\n人性\nrén xìng - n. human nature\n\n\n21\n完美\nwán měi - adj. perfect, flawless\n\n\n22\n难免\nnán miǎn - adj. hard to avoid\n\n\n23\n疼爱\nténg ài - v. to love dearly\n\n\n24\n平等\npíng děng - adj. equal\n\n\n25\n自私\nzì sī - adj. selfish\n\n\n26\n倾向\nqīng xiàng - v./n. to be inclined to; tendency, inclination\n\n\n27\n理由\nlǐyóu - n. reason, ground\n\n\n28\n道德\ndào dé - n. morality, ethics\n\n\n29\n自从\nzì cóng - prep. ever since\n\n\n30\n童话\ntónghuà - n. fairly tale\n\n\n31\n价值\njià zhí - n. value\n\n\n32\n单纯\ndān chún - adj. simple, mere\n\n\n33\n主张\nzhǔ zhāng - v./n. to hold, to advocate; view, stand\n\n\n34\n知感\nzhī gǎn - n. sense perception"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第22课-阅读与思考/index.html#注释",
    "href": "学汉语的日记/HSK5下-第22课-阅读与思考/index.html#注释",
    "title": "HSK5下 | 第22课： 阅读与思考",
    "section": "",
    "text": "一旦\n\nphó từ, biểu thị thời gian không xác định, ngày đó bỗng nhiên tới hoặc giả dụ có ngày đó. Ví dụ:\n\n长大后，我终于明白了这个道理：女人一旦做了母亲，就变得矛盾了。\n灰姑娘一旦进了这个王宫，······应该怎样对待她的继母，应该怎样对待她的两个姐姐。\n所谓私人空间，是指我们身体周围的一定的空间，一旦有人闯入这个空间，我们就会感觉不舒服、不自在。\n\n\n难免\n\ntính từ, có nghĩa là khó tránh khỏi, không thể tránh khỏi. Ví dụ:\n\n刚开始工作，这样的错误是难免的。\n朋友间难免会发生矛盾、误会甚至是伤害。\n·····作为一个母亲，在自己的亲生女儿和不是亲生的灰姑娘之间，难免会更疼爱自己亲生的女儿，很难完全平等地对待她们。\n\n\n自从\n\ngiới từ, biểu thị bắt đầu từ mốc thời gian nào đó trong quá khứ. Ví dụ:\n\n自从城市出现后，它就成为人类生活的中心。\n自从有了长大后成为作家这个理想之后，他每天都坚持写作。\n自从我听了这件事，就开始思考应该如何阅读，除了阅读还应该做什么。\n\n\nPhân biệt 平等 và 公平\n\n共同点：Đều là tính từ, ý nghĩa tương tự nhau, có lúc có thể thay thế nhau.\n\n如：作为一个母亲，在自己的亲生女儿和不是亲生的灰姑娘之间，难免会更疼爱自己亲生的女儿，很难完全平等/公平地对待她们。\n\n不同点：\n\n不同点\n\n\n\n\n\n\n\nNo.\n平等\n公平\n\n\n\n\n1\nNhấn mạnh quyền lợi hoặc sự đãi ngộ giống nhau giữa người với người trong xã hội.\n如：法律面前人人平等。\nNhấn mạnh xử lí vấn đề hợp tình hợp lí , không thiên về một bên.\n如：我们应当公平竞争。\n\n\n2\nThường dùng với tình huống thường gặp, tính phố biến.\n如：现实社会中，女人与男人有时并不平等。\nThường dùng với người hoặc vật cụ thể.\n如：我认为公司对这次事情的处理不够公平。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第22课-阅读与思考/index.html#扩展",
    "href": "学汉语的日记/HSK5下-第22课-阅读与思考/index.html#扩展",
    "title": "HSK5下 | 第22课： 阅读与思考",
    "section": "",
    "text": "写作表达\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n作文\nzuo4wen2 - sáng tác, composition\n\n\n2\n论文\nlun4wen2 - luận văn, paper\n\n\n3\n主题\nzhu3ti2 - chủ đề, theme\n\n\n4\n题目\nti2mu4 - nội dung, chủ đề, subject\n\n\n5\n话题\nhua4ti2 - chủ đề, theme, topic\n\n\n6\n目录\nmu4lu4 - mục lục, catalog\n\n\n7\n提纲\nti2gang1 - outline, dàn ý\n\n\n8\n标点\nbiao1dian3 - punctuation, dấu chấm câu\n\n\n9\n废话\nfei4hua4 - tào lao, crap\n\n\n10\n胡说\nhu2shuo1 - nonsense, ăn nói hồ đồ"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第22课-阅读与思考/index.html#运用",
    "href": "学汉语的日记/HSK5下-第22课-阅读与思考/index.html#运用",
    "title": "HSK5下 | 第22课： 阅读与思考",
    "section": "",
    "text": "假如我是灰姑娘\n（跟口语2同样）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第22课-阅读与思考/index.html#口语",
    "href": "学汉语的日记/HSK5下-第22课-阅读与思考/index.html#口语",
    "title": "HSK5下 | 第22课： 阅读与思考",
    "section": "",
    "text": "1.  简单地讲讲《灰姑娘》\n《灰姑娘》是一则广受欢迎的童话故事，描绘了一个经过不公待遇的女孩的成长之旅。故事中，灰姑娘受到仙女（xian1nv3）的帮助，得以参加王子的舞会。尽管遭受（zao1shou4）继母和继姐的刁难（diao1nan4），她依然保持着善良和美好的心灵。在舞会上，她与王子相遇，两人坠入爱河。尽管途中有一些波折（bo1zhe2），最终他们战胜了一切，过上了幸福的生活。这个故事传递出善良、美德和真爱的积极信息，深受读者和观众的喜爱。\n2.  。。。怎么对待继母和姐姐？\n如果我是灰姑娘，进入王宫后，我会选择以善待的态度面对继母和姐姐。尽管她们曾对我不友好，但我将以宽容的心情对待过去的不愉快。过去就过去了，我不会再计较他们。我会努力把以前不愉快的回忆都忘记，这样才能享受幸福。我认为人生中有的一些先天的事情不完美，最重要的是最后我能战胜一切，拿出来我值得享受的幸福。\n3.  有人承认人生中一些先天的不完美，你同意他的看法吗？\n是的，我同意这个观点。人生中存在各种各样的因素，有些是我们无法选择或改变的，即所谓的先天因素。这可能包括身体的某些特征、家庭背景、遗传等。人们生来就有各种各样的不完美之处，而这些不完美并不意味着人们无法获得幸福或成功。关键在于我们如何面对和接受这些不完美，并通过自己的努力和积极的态度去创造更美好的人生。接纳自己的不完美，并学会从中汲取力量，是迈向更充实人生的一部分。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第22课-阅读与思考/index.html#听力",
    "href": "学汉语的日记/HSK5下-第22课-阅读与思考/index.html#听力",
    "title": "HSK5下 | 第22课： 阅读与思考",
    "section": "2.1. 听力",
    "text": "2.1. 听力\n1.  女：老师，我们孩子平时很爱看书，为什么作文还是写不好？\n男：认为只要多读书就能把作文写好，这个观点是不全面的。\n问：老师的看法是什么？\n2.  男：网上报名的资料，你得仔细看看。要知道，一旦提交，就不能更改了。\n女：放心吧，我看了好几遍了。\n问：男的为什么让女的多看看资料？\n3.  女： 儿子，你的毕业论文写得怎么样了？\n男： 论文好说，我已经找了资料，写了一大部分，再修改修改就差不多完了。\n问：男的的论文现在是什么情况？\n4.  男：你不是不喜欢儿子找的那个女孩儿吗？\n女：孩子大了，有自己的主张了，我有什么办法？\n问：女的是什么语气？\n5.  女：这计划我都做了四次了，还是觉得不满意。\n男：没有真正完美的计划，先干着吧。\n问：男的对这个计划有什么看法？\n6.  男：这本书的主题是你感兴趣的吧？\n女：对，但是我翻了翻目录，觉得没什么意思，还不如去看几篇论文。\n问：他们在谈论什么？\n7.  女：对不起，小说不在这儿，你可以去二层借。\n男：二层借书处的老师告诉我，新书都在一层的阅览室。\n女： 哦，一层有两个阅览室，我们这边只有理科方面的书，文科的在那一头。\n男：好的，谢谢。\n问：说话人现在在哪儿？\n8.  男：你这次考试怎么考得这么差？是不是考前没复习，还是身体不好？\n女：都不是，我就是考试时太紧张了。\n男：是吗？考试时紧张是难免的，但没想到影响会这么大。\n女：我下次想办法调整。\n问：女的为什么没考好？\n9.  女：这次的事情你做得太过分了！\n男：你先听我解释，我这么做是有原因的。\n女：你最好给我一个好的理由。\n男：当然，你别急，听我慢慢说……\n问：女的是什么语气？\n10.男： 你觉得灰姑娘进入王宫以后，应该怎么对待她的继母？\n女： 一个已经拥有巨大幸福的人，应该原谅和理解别人。\n男： 但是继母毕竟伤害过她啊？\n女： 虽然继母很自私，但作为一个母亲，在自己的亲生女儿和继女之间，\n很难做到完全的平等，这也是人的天性。\n问：对于灰姑娘的继母，女的有什么看法？\n11-12.\n法国作家杜马在俄国旅行时,来到一个城市,决定参观城里最大的书店。书店老板听说作家要来,便把所有书架都摆上了杜马的书,想让他高兴。杜马走进书店，发现书架上全是自己的著作，非常吃惊。他问道：“别人的书呢？”老板对这个问题完全没有思想准备，于是紧张地回答：“别人的书，都……都卖完了。”\n11.书店里为什么没有别人的书？\n12.这个故事想告诉我们什么？\n13-14.\n据说有位大作家，写作时有个古怪的习惯——他写文章从来不坐着。有人问他原因，他说：“坐着太舒服，这样，文章一写起来就没完没了。站久了容易腿疼，这样，你就想快点儿写完，快点儿坐下。”这个故事可能只是个玩笑，文章的长短与坐着写还是站着写当然没有太大关系，但是，它却让我们思考一个问题：如果我们都像他一样站着写作，是不是文章都会更简短、更有可读性呢？\n13.这位作家为什么要站着写文章？\n14.对于文章的长短，说话人是什么态度？"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第22课-阅读与思考/index.html#阅读",
    "href": "学汉语的日记/HSK5下-第22课-阅读与思考/index.html#阅读",
    "title": "HSK5下 | 第22课： 阅读与思考",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n各持己见往往是人与人之间矛盾冲突的重要原因。人们在生活中15B难免会与家人、朋友、同事、同学或者领导产生这样那样的矛盾。这些矛盾会使人伤心气愤。如果矛盾双方都各持己见，互不相让，可能还会导致过激的言行，使人际关系恶化，而这是非常有害于心理健康和办事结果的。要避免这种情况出现，需要心理换位—16D交换位置,试着站到对方立场上去思考，你就会发现其实对方的17C观点也不无道理。比如，我的父母总是对我提出各种各样的过高要求：学习上要努力，工作上要出色，生活上要俭朴勤快,最好所有家务都能干……我烦死了，有时候18B真想跟他们大吵一架。但是从父母的角度想一想，他们也是为我好，假如有一天我自己做了父母，我肯定也会对自己的孩子提出这样那样的要求。这样，我心中的怒气便消了很多。\n\nB这套书可以用来识字 Đọc viêt\n\n这是一套非常有趣的儿童读物，一套8本，包含8个世界著名的童话故事。故事情节易懂，语言简单，适合6〜8岁的孩子。它很适合在睡前读给孩子听；还配有图画和拼音，所以也适合孩子在家长的带领下自己阅读，有利于识字。\n\nC语言文字有很重要的作用\n\n教育学家认为，儿童需要阅读与他们的年龄、兴趣和能力相符的书，同时，他们也希望阅读题材丰富多样。所以专家建议，可以让儿童多接触不同方面的阅读材料，包括报纸、杂志，甚至商品包装、广告标语等。通过这些，儿童会越来越认识到语言文字的重要性。\n\nD发展个性与全面发展并不矛盾\n\n现在有一种观点认为，坚持全面发展，就不该突出发展个性；主张发展个性，就是否定全面发展，把发展个性与全面发展放在对立的位置上。我不同意这种观点。我们应该大胆地、理直气壮地承认学生的个体存在价值，我认为对个性教育的种种顾虑是不必要的。\n\nB诚实可以使大家受益\n\n人们通常认为诚实是一种道德。如果每个人都遵守诚实的道德标准，对任何事情都能实事求是地处理，社会中每个人都能受益。反过来如果社会中有人是不诚实的，某个人为了自己的利益而不诚实，损害了其他人的利益，那么事情就不能被正确解决和处理，社会也就不会向前发展，这样对社会中每个人都不利。人们认识到不诚实对社会中的每一个人都没有好处，而诚实能给每个人带来好处，诚实就成了一种道德。\n23-25.\n《卖火柴的小女孩》是19世纪中叶丹麦作家安徒生的著名童话作品，发表于1846年。主要讲了一个卖火柴的小女孩在合家欢乐的大年夜冻死在街头的悲剧故事。小女孩死了,嘴角却带着微笑，擦燃火柴的美好幻想与她饥寒交迫的现实生活形成了鲜明的对比。\n因为没能卖出一根火柴，小女孩整天都没有东西吃。她又冷又饿，于是点燃了自己没有卖掉的火柴。擦亮第一根火柴，她看见了大火炉；擦亮第二根火柴，她看见了香喷喷的烤鹅；擦亮第二根火柴,她看见了美丽的圣诞树；而第四根火柴擦亮时，她看见了久违的奶奶，她想让奶奶留在自己身边，于是点燃了剩下的一整把火柴。然而，当火柴熄灭的时候，这所有的一切都不见了，小女孩就这样在圣诞之夜悲惨地死去，没有人知道她在生前最后一刻看到的美好情景。\n安徒生童话的题材很广，在他众多的童话中，悲剧性故事占有相当大的比例。带着微笑和温柔的情感进行悲剧叙事是安徒生童话的一个显著特点。从他创作中期的《海的女儿》到其晚期创作的《老约翰尼讲的故事》，悲剧无所不在。安徒生在其中寄托了自己对人物的同情、理解、挚爱和尊重。当然，他也有《拇指姑娘》这样轻松愉快的作品。\n23. C童话故事\n24. D奶奶\n25. B《拇指姑娘》Thumbelina\n26-28.\n无论学习什么学科，都该预先认清楚为什么要学习它。认清楚了，一切努力才有目标、有方向，不至于盲目地胡搅一阵。\n学生为什么要学习语文呢？这个问题，读者如果没有思考过，请仔细地思考一下。如果已经思考过了，请把思考的结果和后面所说的对照一下，看从中能不能得到些补充或修正。\n学习语文就是学习本国的语言文字。语言文字的学习，就理解方面说，是得到一种知识；就运用方面说，是养成一种习惯。这两方面必须联成一体。就是说，理解是必要的，但是理解之后必须能够运用；知识是必要的，但是这种知识必须成为习惯。语言文字的学习，出发点在“知”，而终极点在“行二到能够“行”的地步才算具有这种生活的能力。这是每一个学习语文的人应该记住的。\n学习语文，我们将得到什么知识，养成什么习惯呢？简括地说，只有两项，一项是阅读，另一项是写作。要从语文得到阅读和写作的知识，养成阅读和写作的习惯。阅读是“吸收”的事情，通过阅读可以领受人家的经验，接触人家的心情；写作是“发表”的事情，写作可以显示自己的经验，吐露自己的心情。在人群中间，经验的授受和心情的沟通是最切要的，所以阅读和写作两项也最重要。这两项的知识和习惯，其他学科是不负授予和训练的责任的，这是语文的专责。每一个学习语文的人应该认清楚：得到阅读和写作的知识，从而养成阅读和写作的习惯，就是学习语文的目标。\n26. A“知”是“行”的目标\n27. D在阅读和写作方面教授知识、训练习惯\n28. C为什么要学习语文"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第22课-阅读与思考/index.html#书写",
    "href": "学汉语的日记/HSK5下-第22课-阅读与思考/index.html#书写",
    "title": "HSK5下 | 第22课： 阅读与思考",
    "section": "2.3. 书写",
    "text": "2.3. 书写\n平等，观念，对待，一旦，自私\n平等是现代社会的核心观念之一。人们应该以平等的观念对待男女，不论性别差异（cha1yi4），都应该给予（ji3yu3）平等的机会和尊重。一旦我们树立起这样的观念，社会就能摆脱（bai3tuo4）偏见（pian1jian4），真正实现性别平等。对待男女要摒弃（bing3qi4）过去的偏见，不让自私的观念左右我们的看法。只有在平等的基础上，社会才能更加公正，每个人都能充分发挥潜力（qian1li4），共同建设一个更加和谐的世界。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第24课-支教行动/index.html",
    "href": "学汉语的日记/HSK5下-第24课-支教行动/index.html",
    "title": "HSK5下 | 第24课： 支教行动",
    "section": "",
    "text": "支教行动 | Volunteer teaching\n来云南支教一年多，郝琳硕老师自己也记不清有多少次家访了。刚到时，一位叫赵福根的男生引起了她的注意。他上课从不发言，很多课不及格，平时也几乎不和同学交往。\n郝老师家访后得知，赵福根的父亲去世了，姐姐在外打工，他家里很穷，还得帮着妈妈做家务，是个体贴孝顺的孩子。“和他妈妈聊天才知道他很喜欢跳舞，”郝老师说，“我觉得这是个机会！”\n她鼓励福根在学校艺术节上表演，每周二带着他一起去找音乐老师排练。表演时，福根的蝴蝶舞得了舞蹈组的冠军，台下的同学们鼓起掌来，齐声地喊着“福根”的名字……\n之后，赵福根学习用功了，成绩也逐渐进步。他写了一篇题目为《那天的舞蹈和掌声》的作文，得了全班最高分，他朗读了自己的作文：“郝老师来到我家，那是第一次有老师来。她非常温柔……\n我永远都忘不了那热烈的掌声和同学们送我的糖，甜甜的。我感觉在学校也有人爱我了，我开始有勇气……”\n郝老师发现，山里的青壮年都出去闯世界，只有老人、孩子留守，“他们出去了还回来吗？大山以后谁来负责”？于是，郝老师组织了一个8周的研究型学习活动，主题是“让家乡的明天更美好”。她鼓励学生寻找村子的问题，通过了解历史地理情况采访村里的老人、小组讨论等，最终提出解决方案。\n她和其他志愿者利用午休、周末等空闲时间给学生们指导和培训。学生们说：“以前，我们总认为建设家乡是大人的事，用不着我们操心。不过，现在我们明白了，建设家乡，人人有责，我们也要承担这个义务。这个任务很艰巨，我们要尽自己最大的力量。”\n郝琳硕觉得自己的收获远多于给孩子们的。“不管以后在哪儿，我都会继续用我的力量影响山里的孩子们，因为他们是国家的未来与希望。”\n改编自《美丽中国》官网，提米自拟\n\n\n\n支教行动\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n支教\nzhījiào - v. to volunteer to teach in a backward region\n\n\n2\n行动\nxíng dòng - n. action, activity\n\n\n3\n家访\njiāfǎng - v. to visit the parents of schoolchildren\n\n\n4\n发言\nfā yán - v. to speak, to make a speech\n\n\n5\n及格\njí gé - v. to pass an exam\n\n\n6\n交往\njiāo wǎng - v. to associate, to contact\n\n\n7\n家务\njiā wù - n. household duties\n\n\n8\n体贴\ntǐ tiē - adj. thoughtful, considerate\n\n\n9\n排练\npáiliàn - v. to rehearse\n\n\n10\n蝴蝶\nhú dié - n. butterfly\n\n\n11\n冠军\nguàn jūn - n. champion, first-prize winner\n\n\n12\n鼓掌\ngǔ zhǎng - v. to applaud\n\n\n13\n用功\nyònggōng - adj. hardworking, diligent\n\n\n14\n进步\njìn bù - v. to make progress, to improve\n\n\n15\n题目\ntí mù - n. title, subject, topic, problem (in exam)\n\n\n16\n朗读\nlǎngdú - v. to read aloud\n\n\n17\n温柔\nwēn róu - adj. gentle\n\n\n18\n热烈\nrè liè - adj. enthusiastic, ardent\n\n\n19\n勇气\nyǒng qì - n. courage\n\n\n20\n青壮年\nqīng zhuàngnián - young and middle-aged adults\n\n\n21\n闯\nchuǎng - v. to go around (to accomplish certain goals)\n\n\n22\n留守\nliúshǒu - v. to stay behind to take care of things\n\n\n23\n主题\nzhǔ tí - n. theme, subject\n\n\n24\n地理\ndì lǐ - n. geography\n\n\n25\n采访\ncǎi fǎng - v. to interview\n\n\n26\n利用\nlì yòng - v. to utilize, to make use of\n\n\n27\n空闲\nkòng xián - adj. leisurely, free\n\n\n28\n指导\nzhǐdǎo - v. to guide, to instruct\n\n\n29\n培训\npéi xùn - v. to train\n\n\n30\n建设\njiàn shè - v. to build, to construct\n\n\n31\n操心\ncāo xīn - v. to worry about, to be concerned about\n\n\n32\n承担\nchéng dān - v. to undertake, to shoulder\n\n\n33\n义务\nyìwù - n. duty, obligation\n\n\n34\n艰巨\njiān jù - adj. arduous, difficult\n\n\n35\n力量\nlì liàng - n. strength, capability\n\n\n36\n收获\nshōu huò - n. gain, to harvest\n\n\n\n\n\n\n\n行动\n\n“行动” Động từ, biểu thị đi lại, hoạt động cơ thể. Ví dụ:\n\n他运动时受伤了，行动不便。\n有些鸟类喜欢单独行动。\n\nLàm động từ, cũng có thể biểu thị tiến hành hành động nào đó vì mục đích nào đó . Ví dụ:\n\n做什么事他都喜欢提前行动，早做准备。\n有的人总是怀疑计划不够准确而迟迟不能开始行动。\n\nCũng có thể làm danh từ, biểu thị hoạt động, hành vi. Ví dụ:\n\n郝老师到云南参加支教行动。\n我们应该勇敢面对困难，迅速采取行动，主动承担责任。\n\n\n义务\n\n“义务”Danh từ, biểu thị trách nhiệm về mặt pháp luật, đạo đức nên gánh vác, đảm nhiệm. Ví dụ:\n\n不过，现在我们明白了，建设家乡，人人有责，我们也要承担这个义务。\n参与社会事务和促进社会进步是每个人的权利，也是每个人的义务和责任。\n\nCũng có thể làm tính từ, biểu thị không nhận thù lao. Ví dụ:\n\n我们每个学期都要至少参加三次义务劳动。\n中国有关于九年制义务教育的法律。\n\n\nPhân biệt 发言 và 发表\n\n共同点：Đều có thể làm động từ, đều liên quan đến biểu đạt ý kiến。\n不同点：\n\n不同点\n\n\n\n\n\n\n\nNo.\n发言\n发表\n\n\n\n\n1\nChỉ lời nói khi ở trong hội nghị, trong lớp.\n如：他上课从不发言，很多课不及格，平时也几乎不和同学交往。\nChỉ hướng về tập thể, xã hội nói ra ý kiến của mình hoặc đăng bài trên báo.\n如：总统发表了有关两国关系的讲话。\n\n\n2\nCó thể làm danh từ, chỉ những ý kiến đã phát biểu.\n如：他今天在会上的发言很精彩。\nKhông thể làm danh từ.\n\n\n3\nLà từ li hợp, ở giữa có thể xen các thành phần khác, đằng sau không thể mang thêm tân ngữ.\n如：你已经发过言了吗？\nKhông phải từ li hợp.\n如：你发表过这篇文章吗？\n\n\n\n\n\n\n\n教学\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n测验\nce4yan4 - test, kiểm tra\n\n\n2\n实验\nshi2yan4 - experiment, thực nghiệm\n\n\n3\n抄\nchao1 - to copy\n\n\n4\n试卷\nshi4juan4 - exam paper\n\n\n5\n夏令营\nxia4ling4ying2 - summer camp, trại hè\n\n\n6\n操场\ncao1chang3 - playground, nhà thi đấu (??)\n\n\n7\n用功\nyong4gong1 - study hard, diligent\n\n\n8\n辅导\nfu3dao3 - phụ đạo to tutor\n\n\n9\n收获\nshou1huo4 - thu hoạch, to harvest\n\n\n10\n铃\nling2 - bell, chuông\n\n\n11\n退步\ntui4bu4 - to regress, thoái lui\n\n\n12\n改正\ngai3zheng4 - to amend, cải chính, to correct\n\n\n\n\n\n\n大山的未来谁负责\n大山的未来建设是每个人的责任。政府部门的责任是更多地扶助住在农村的人，帮助他们的家，包括提供电源、清洁水、建设当地流到中心的交通系统和孩子的学习条件。老人的责任是教导孩子们如何建设他们的家乡，如何保留文化价值和风俗习惯。成年人的责任是积极地保护祖先留给他们的土地、历史，互相帮助发展经济，培养和教育新一代的孩子。（156字）\n\n\n\n1.  青壮年外出打工对农村可能产生什么影响？\n出生率高但教育水平还有限的贫困农村和山区对劳动力市场提供了大量蓝领工人。对这里的青壮年来说，在家乡，交通不好、水电短缺、农业工作没有带来良好的收入，所以他们应该选择去工业化和劳动力需求非常高的大城市。如果一段时间后，他们积累了金钱和技能，然后回到家乡定居，这将是有益的。但事实上，大多数这样的工人会选择在城市边缘定居。从长远来看，这不仅导致人口分布不平衡，而对农村经济发展没有帮助，这里的历史和文化价值也会随着时间的推移消失。\n2.  青壮年外出打工对城市可能产生什么影响？\n如上所述，情壮年外出打工会增加城市人口密度。这导致了一系列其他的问题，比如环境污染、缺乏规划的住房、降低生活质量、儿童教育供应不足。\n3.  你认为，政府应该怎么帮助这些老人、孩子和农民工？\n我认为，政府部门的责任是更多地扶助住在农村的人，帮助他们的家，包括提供电源、清洁水、建设当地流到中心的交通系统和孩子的学习条件。政府需要更加积极地在城市和农村之间分配资源，吸引更多企业在当地开发劳动力，并通过技术提高农村农业效率。关键在于通过教育政策提高劳动力质量，并创造足够的就业机会来吸引这些劳动力。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第24课-支教行动/index.html#生词",
    "href": "学汉语的日记/HSK5下-第24课-支教行动/index.html#生词",
    "title": "HSK5下 | 第24课： 支教行动",
    "section": "",
    "text": "支教行动\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n支教\nzhījiào - v. to volunteer to teach in a backward region\n\n\n2\n行动\nxíng dòng - n. action, activity\n\n\n3\n家访\njiāfǎng - v. to visit the parents of schoolchildren\n\n\n4\n发言\nfā yán - v. to speak, to make a speech\n\n\n5\n及格\njí gé - v. to pass an exam\n\n\n6\n交往\njiāo wǎng - v. to associate, to contact\n\n\n7\n家务\njiā wù - n. household duties\n\n\n8\n体贴\ntǐ tiē - adj. thoughtful, considerate\n\n\n9\n排练\npáiliàn - v. to rehearse\n\n\n10\n蝴蝶\nhú dié - n. butterfly\n\n\n11\n冠军\nguàn jūn - n. champion, first-prize winner\n\n\n12\n鼓掌\ngǔ zhǎng - v. to applaud\n\n\n13\n用功\nyònggōng - adj. hardworking, diligent\n\n\n14\n进步\njìn bù - v. to make progress, to improve\n\n\n15\n题目\ntí mù - n. title, subject, topic, problem (in exam)\n\n\n16\n朗读\nlǎngdú - v. to read aloud\n\n\n17\n温柔\nwēn róu - adj. gentle\n\n\n18\n热烈\nrè liè - adj. enthusiastic, ardent\n\n\n19\n勇气\nyǒng qì - n. courage\n\n\n20\n青壮年\nqīng zhuàngnián - young and middle-aged adults\n\n\n21\n闯\nchuǎng - v. to go around (to accomplish certain goals)\n\n\n22\n留守\nliúshǒu - v. to stay behind to take care of things\n\n\n23\n主题\nzhǔ tí - n. theme, subject\n\n\n24\n地理\ndì lǐ - n. geography\n\n\n25\n采访\ncǎi fǎng - v. to interview\n\n\n26\n利用\nlì yòng - v. to utilize, to make use of\n\n\n27\n空闲\nkòng xián - adj. leisurely, free\n\n\n28\n指导\nzhǐdǎo - v. to guide, to instruct\n\n\n29\n培训\npéi xùn - v. to train\n\n\n30\n建设\njiàn shè - v. to build, to construct\n\n\n31\n操心\ncāo xīn - v. to worry about, to be concerned about\n\n\n32\n承担\nchéng dān - v. to undertake, to shoulder\n\n\n33\n义务\nyìwù - n. duty, obligation\n\n\n34\n艰巨\njiān jù - adj. arduous, difficult\n\n\n35\n力量\nlì liàng - n. strength, capability\n\n\n36\n收获\nshōu huò - n. gain, to harvest"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第24课-支教行动/index.html#注释",
    "href": "学汉语的日记/HSK5下-第24课-支教行动/index.html#注释",
    "title": "HSK5下 | 第24课： 支教行动",
    "section": "",
    "text": "行动\n\n“行动” Động từ, biểu thị đi lại, hoạt động cơ thể. Ví dụ:\n\n他运动时受伤了，行动不便。\n有些鸟类喜欢单独行动。\n\nLàm động từ, cũng có thể biểu thị tiến hành hành động nào đó vì mục đích nào đó . Ví dụ:\n\n做什么事他都喜欢提前行动，早做准备。\n有的人总是怀疑计划不够准确而迟迟不能开始行动。\n\nCũng có thể làm danh từ, biểu thị hoạt động, hành vi. Ví dụ:\n\n郝老师到云南参加支教行动。\n我们应该勇敢面对困难，迅速采取行动，主动承担责任。\n\n\n义务\n\n“义务”Danh từ, biểu thị trách nhiệm về mặt pháp luật, đạo đức nên gánh vác, đảm nhiệm. Ví dụ:\n\n不过，现在我们明白了，建设家乡，人人有责，我们也要承担这个义务。\n参与社会事务和促进社会进步是每个人的权利，也是每个人的义务和责任。\n\nCũng có thể làm tính từ, biểu thị không nhận thù lao. Ví dụ:\n\n我们每个学期都要至少参加三次义务劳动。\n中国有关于九年制义务教育的法律。\n\n\nPhân biệt 发言 và 发表\n\n共同点：Đều có thể làm động từ, đều liên quan đến biểu đạt ý kiến。\n不同点：\n\n不同点\n\n\n\n\n\n\n\nNo.\n发言\n发表\n\n\n\n\n1\nChỉ lời nói khi ở trong hội nghị, trong lớp.\n如：他上课从不发言，很多课不及格，平时也几乎不和同学交往。\nChỉ hướng về tập thể, xã hội nói ra ý kiến của mình hoặc đăng bài trên báo.\n如：总统发表了有关两国关系的讲话。\n\n\n2\nCó thể làm danh từ, chỉ những ý kiến đã phát biểu.\n如：他今天在会上的发言很精彩。\nKhông thể làm danh từ.\n\n\n3\nLà từ li hợp, ở giữa có thể xen các thành phần khác, đằng sau không thể mang thêm tân ngữ.\n如：你已经发过言了吗？\nKhông phải từ li hợp.\n如：你发表过这篇文章吗？"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第24课-支教行动/index.html#扩展",
    "href": "学汉语的日记/HSK5下-第24课-支教行动/index.html#扩展",
    "title": "HSK5下 | 第24课： 支教行动",
    "section": "",
    "text": "教学\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n测验\nce4yan4 - test, kiểm tra\n\n\n2\n实验\nshi2yan4 - experiment, thực nghiệm\n\n\n3\n抄\nchao1 - to copy\n\n\n4\n试卷\nshi4juan4 - exam paper\n\n\n5\n夏令营\nxia4ling4ying2 - summer camp, trại hè\n\n\n6\n操场\ncao1chang3 - playground, nhà thi đấu (??)\n\n\n7\n用功\nyong4gong1 - study hard, diligent\n\n\n8\n辅导\nfu3dao3 - phụ đạo to tutor\n\n\n9\n收获\nshou1huo4 - thu hoạch, to harvest\n\n\n10\n铃\nling2 - bell, chuông\n\n\n11\n退步\ntui4bu4 - to regress, thoái lui\n\n\n12\n改正\ngai3zheng4 - to amend, cải chính, to correct"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第24课-支教行动/index.html#运用",
    "href": "学汉语的日记/HSK5下-第24课-支教行动/index.html#运用",
    "title": "HSK5下 | 第24课： 支教行动",
    "section": "",
    "text": "大山的未来谁负责\n大山的未来建设是每个人的责任。政府部门的责任是更多地扶助住在农村的人，帮助他们的家，包括提供电源、清洁水、建设当地流到中心的交通系统和孩子的学习条件。老人的责任是教导孩子们如何建设他们的家乡，如何保留文化价值和风俗习惯。成年人的责任是积极地保护祖先留给他们的土地、历史，互相帮助发展经济，培养和教育新一代的孩子。（156字）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第24课-支教行动/index.html#口语",
    "href": "学汉语的日记/HSK5下-第24课-支教行动/index.html#口语",
    "title": "HSK5下 | 第24课： 支教行动",
    "section": "",
    "text": "1.  青壮年外出打工对农村可能产生什么影响？\n出生率高但教育水平还有限的贫困农村和山区对劳动力市场提供了大量蓝领工人。对这里的青壮年来说，在家乡，交通不好、水电短缺、农业工作没有带来良好的收入，所以他们应该选择去工业化和劳动力需求非常高的大城市。如果一段时间后，他们积累了金钱和技能，然后回到家乡定居，这将是有益的。但事实上，大多数这样的工人会选择在城市边缘定居。从长远来看，这不仅导致人口分布不平衡，而对农村经济发展没有帮助，这里的历史和文化价值也会随着时间的推移消失。\n2.  青壮年外出打工对城市可能产生什么影响？\n如上所述，情壮年外出打工会增加城市人口密度。这导致了一系列其他的问题，比如环境污染、缺乏规划的住房、降低生活质量、儿童教育供应不足。\n3.  你认为，政府应该怎么帮助这些老人、孩子和农民工？\n我认为，政府部门的责任是更多地扶助住在农村的人，帮助他们的家，包括提供电源、清洁水、建设当地流到中心的交通系统和孩子的学习条件。政府需要更加积极地在城市和农村之间分配资源，吸引更多企业在当地开发劳动力，并通过技术提高农村农业效率。关键在于通过教育政策提高劳动力质量，并创造足够的就业机会来吸引这些劳动力。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第24课-支教行动/index.html#听力",
    "href": "学汉语的日记/HSK5下-第24课-支教行动/index.html#听力",
    "title": "HSK5下 | 第24课： 支教行动",
    "section": "2.1. 听力",
    "text": "2.1. 听力\n1.  女：班里有哪些同学特别需要注意吗？\n男：你多留意赵福根。他上课从不发言，很多课不及格，平时也几乎不和同学交往。\n问：下列哪项是赵福根的问题？（B不爱发言）\n2.  男：听说你在读一个网络课程，感觉怎么样？\n女：还不错。你要不要也考虑考虑？可以免费试听两周，我就是试听了以后觉得好，才正式报的名。到现在已经听了两个月了。\n问：女的这段时间在干什么？（D正在上网课）\n3.  女：他这次比赛的成绩怎么样？\n男：很可惜，银牌Yínpái，差一点儿就能拿冠军了。（金牌，银牌，铜牌）\n问：他这次比赛得了第几名？（B第二名）\n4.  男：真没想到，计划得这么好，还会出这样的事！\n女：出事不怕，现在的问题是我们要主动承担责任，迅速采取行动。\n问：女的认为应该怎么做？（C主动负责）\n5.  女：你到底想找个什么样的女朋友？\n男：漂不漂亮、工作好不好、有没有钱都不要紧，我就想要个温柔体贴的。\n问：男的找女朋友最重视什么？（A性格）\n6.  男：他们俩不是挺好的吗，怎么突然就离婚了？\n女：家家有本难念的经，你就别替人家操心了。\n问：女的是什么意思？（D别人的情况我们不了解）\n7.  女：爸爸，老师明天要来我们家做家访。\n男：明天我有事，跟你妈妈说吧。\n女：老师特意说了，每次家访都是见的妈妈，家长会也是妈妈去参加，希望这次能跟您见见面。\n男：好吧，我明天先去公司安排一下。\n问：明天老师和爸爸会在什么地方见面？（A家里）\n8.  男：刘老师，能不能请您利用空闲时间给我做一下辅导？\n女：对不起，恐怕不行。\n男：我可以另付费用的。\n女：我们学校有规定，不允许老师给自己的学生做收费的辅导。\n问：女的为什么不辅导男的？（B学校不同意）\n9.  女：真不明白，儿子怎么老想出去闯世界，咱们家又不缺钱花！\n男：男孩子，出去闯闯也好。\n女： 哪儿那么容易？\n男：你不让他试试，怎么知道他不行呢？\n问：男的是什么意思？（C应该让孩子去试试）\n10.男：这么晚了，你怎么还不睡啊？\n女：明天轮到我发言，我还要再练两遍。\n男：差不多就行了，不必这么认真吧。\n女：要是我自己的事就算了，但这是要算小组成绩的。\n问：女的为什么还不睡？（C她要代表小组发言）\n11-12.\n男：郝老师，你来支教一年多，觉得辛苦吗？\n女：辛苦是辛苦，但我来这里，本来就不是为了来享受的。\n男：这里条件这么差，刚来时是不是很不习惯？\n女：条件虽然艰苦，但我觉得我的收获远远大于付出。\n男：以后你还会做志愿者吗？\n女：不管以后在哪儿，我都会继续用我的力量帮助山里的孩子们，因为孩子是我们国家的未来与希望。\n11.关于支教，郝老师有什么看法？（D很有收获）\n12.郝老师将来有什么打算？（C她要帮助孩子们）\n13-14.\n今天，我们这个大学校园里迎来了众多中学生朋友。来自全国各地的 2500多名高中生齐聚清华园，共同参加 2016 年青少年高校科学营全国开营式。首先，我代表清华大学，向各位同学和老师的到来表示热烈的欢迎！ 2012 年青少年高校科学营开办， 4 年来共吸引了 37000 多名营员参加，承办高校也由最初的 41 所增加到 51 所，科学营已成为传播科学知识、科学思想、科学方法和提高青少年科学素质的重要平台。清华大学是科学营的最初参与者和积极的承办者， 2013 年全国开营式就在清华园举行，我们非常愿意再次承办科学营活动，为所有有科学梦想的同学搭建更加宽广的舞台。\n13.这是什么活动上的讲话？（B科学营开营）\n关于这次活动，下列哪项正确？（A在大学举行）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第24课-支教行动/index.html#阅读",
    "href": "学汉语的日记/HSK5下-第24课-支教行动/index.html#阅读",
    "title": "HSK5下 | 第24课： 支教行动",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n各位老师，我院五年前曾经公开征集听力考试试题，进行试题库15C建设，当时工作取得了很好的效果。本学期，学院计划开展新题库的有关工作。具体包括：一、进一步扩大题库规模。原题库包括单句题、对话题和短文题各200道。现计划增加16A题目的数量，将三种题型各增加100道。二、因时代发展，社会发生变化，原题库中部分试题内容过时，不再符合当前的社会情况，需要进行17C修改或替换。这项工作需要大家的力量，18D欢迎各位老师积极参与！\n\nC每个人都应该承担建设家乡的责任\n\n这次，老师组织了一项8周的研究型学习活动，主题是”让家乡的明天更美好”。学生们参加以后说：“以前，我们总认为建设家乡是大人的事，用不着我们操心。不过,现在我们明白了，建设家乡，人人有责，我们也要承担这个义务。这个任务很艰巨,我们要尽自己最大的力量去完成。”\n\nB研究生可以参加支教活动\n\n为了能够保证活动顺利进行，请务必确保以下所有条件与您的实际情况相符：年满二十二周岁；大专或以上学历；有充足的时间，必须能完成最短一个学期的教学任务；取得家人对支教的理解和支持；有一定的经济能力，能承担支教期间所产生的相关费用，包括交通及日常生活用品和其他私人支出。\n\nD为取得好的成绩，应提前调整作息时间\n\n按照自己的特点制定作息时间表固然有道理，但有时却与考试要求的作息时间不一致。而人体的生物钟具有惯性，很难一下子完全调整过来。所以，在重大考试之前,必须提前行动，使自己各方面的情况，在考前调节到最理想的状态。\n\nD实施”素质教育”应该因地而异\n\n中国从80年代开始开展普及义务教育的工作，但至今仍未完成。因此，“素质教育”的改革不能只是一句简单的口号，它在各个地区所面临的情况和需完成的任务是不一样的。北京、上海这样的大城市，可以侧重培养学生的创造性等，但对贫困地区来说，首先需要的还是完全普及义务教育。\n23-25.\n每周三是女儿畅畅班里的图书馆日。这一天，他们要把上周借的书还回去，再把新的书借回来。这周二晚上，我帮畅畅收拾书包，发现她从学校图书馆借的书不见了，在家里找遍了也没找到。一想到把学校图书馆的书弄丢了不知道有什么后果，也不知道这一周畅畅还能不能借书，我就很着急。\n我慢慢回想，我们放书的地方基本是固定的，出门也不会带书出去，那么只有一个可能，就是我把书还错了——上周畅畅从学校图书馆借了一本书，我在附近的公共图书馆也给她借了一本同样的。大概是去公共图书馆还书的时候我没注意，两本放在一起还了。\n第二天，我忐忑不安地去公共图书馆咨询。当我说明了情况，工作人员告诉我，不用担心，凡是错还到这里的书，他们都会送回去的。“这种事经常发生。”工作人员这样说。放学的时候到学校接畅畅，我跟老师说了这件事。她的第一句话也是：“没关系，这种事经常发生。”原来，不止公共图书馆会收到学校图书馆的书，学校图书馆也会收到不属于自己的书。同样的，他们也会送回去。这是通行的规则。当我问及我是否需要跟学校图书馆说明，她说她会去跟图书馆核对，假如这本书他们没有收到，她再告诉我。所以，我什么也不用做。\n至此，我一颗悬着的心才放了下来。我以为是多么严重的问题，他们却都显得很轻松。我想，这就是信任。\n\nB她把图书馆的书弄丢了\nC图书馆和老师都没有批评妈妈\nD信任\n\n26-28.\n在中国贫困山区，师资非常紧缺，很多学校都是一个老师负责上多种文化课，所以 每年都会招募大量的支教志愿者前往山区进行支教，改良山区的教学水平。支教志愿者 招募条件有哪些呢？我们一起来看看吧！\n首先呢，一定要有爱心。作为一名支教志愿者，每天相处最多的就是孩子，所以一 定要有爱心，热爱这份事业！有很多人可能是为了体验一下山村生活而选择前往，这是 不负责任的行为！\n其次呢，一定要有充足的时间。给自己多留 一点儿时间，多陪陪这些孩子，多让他们体会一下 你看到的外面的世界。这对于他们来说是难能可贵 的。不要刚刚前往就选择离开，这对于山区孩子来 说是一种伤害。\n然后呢，还要有一定的学识。当然，需要支教 的山区孩子大多数都是小学生，支教老师不需要有 多么高的学历，但是学识还是一定要有的。不只是文化课方面，其他方面的知识储备也很必要，比如说一些医学常识。\n最后，承受能力、吃苦能力要强。山区生活条件艰苦，前来支教的志愿者要有有较 好的承受能力，而不能因为生活条件差就匆匆离开。\n如果大致符合以上条件的话，爱心人士基本就可以选择前往山区支教了，当然，不 同的组织在招募不同地区的支教老师时，条件可能会有所不同，这是根据当地的实际情 况来考量的。\n\nB有很高的学历\nA孩子们需要陪伴\nC最好具备一些医学常识"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第24课-支教行动/index.html#书写",
    "href": "学汉语的日记/HSK5下-第24课-支教行动/index.html#书写",
    "title": "HSK5下 | 第24课： 支教行动",
    "section": "2.3. 书写",
    "text": "2.3. 书写\n义务、勇气、利用、承担、行动。\n教育孩子是一门艺术，需要父母和老师的耐心。我们需要关注孩子们的行为，但也需要给他们自由的空间。跟孩子交流的过程中，当他们有良好的行动时，我们要及时表扬，利用他们的好胜性格，鼓励孩子不断进步。当他们有错误时，父母应该严格地批评，但千万不要强烈地指责他们，下次他们会因为害怕而隐藏错误。最重要的是要教孩子有勇气承担责任。儿童是社会的未来，所以养育他们是每个人的义务。（182字）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第26课-你属于哪种忙/index.html",
    "href": "学汉语的日记/HSK5下-第26课-你属于哪种忙/index.html",
    "title": "HSK5下 | 第26课： 你属于哪种“忙”",
    "section": "",
    "text": "你属于哪一种 “忙”？| Which kind of “busy” person are you?\n工作中的忙碌大概可分为三种：第一种忙，忙得很被动，总是被事情追着、赶着，人几乎成了工作的奴隶；第二种忙，忙得很主动，乱，人是工作的主人；第三种忙，忙得有些虚伪，忙而不因为在他们的思想中，已经把忙与成功、闲与失败联系到一起，所以，这样的人总是想办法让自己忙。\n你属于哪种忙呢？\n我有一个体会：现实中，我们不一定知道正确的道路是什么，但时时反省、总结，却可以使我们不会在错误的道路上走得太远。\n据说，曾经有一位很有个性、极爱冒险的大导演到南美丛林拍有关古代印加文明的纪录片。他雇了20来个当地人为他带路和搬运行李。这批当地人个个都表现出色，尽管他们背着重重的行李，但他们的脚力过人，健步如飞。一连三天，他们都很顺利地实现了原定的计划。到了第四天大导演一早醒来就催着大家上路。然而，当地人却拒绝行动。大导演非常着急，一来，耽误了时间，日程就得重新安排；二来，会因为费用增加而让投资人不高兴。\n至于这部影片的投资人，可是一位大人物，他可不敢得罪。经过沟通，大导演总算搞明白了，当地人自古就有一种习俗：在赶路时，用尽全力地向前冲，但每走上三天，便要休息一天。当大导演进一步询问原因时，当地人的回答令他受益终生。\n“那是为了让我们的灵魂，能够追得上我们赶了三天路的疲劳的身体。”\n多么富有哲理的话！\n改编自《青年参考》，作者：毛尧飞，杨立军\n\n\n\n你属于哪种“忙”\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n忙碌\nmánglù - adj. busy, fully occupied\n\n\n2\n被动\nbèidòng - adj. passive\n\n\n3\n奴隶\nnúlì - n. slave\n\n\n4\n虚伪\nxū wěi - adj. hypocritical\n\n\n5\n思想\nsī xiǎng - n. thought, thinking\n\n\n6\n反省\nfǎnxǐng - v. to reflect on oneself, to self-examine\n\n\n7\n据说\njù shuō - v. it is said, reputedly\n\n\n8\n个性\ngè xìng - n. individual character, personality\n\n\n9\n冒险\nmào xiǎn - v. to venture, to have an adventure\n\n\n10\n丛林\ncóng lín - n. jungle, forest\n\n\n11\n文明\nwénmíng - n. civilization\n\n\n12\n纪录\njì lù - n./v. record, note; to record, to note down\n\n\n13\n雇\ngù - v. to employ, to hire\n\n\n14\n来\nlái - part. used after round numbers to indicate approximation\n\n\n15\n批\npī - m. group, batch\n\n\n16\n出色\nchū sè - adj. remarkable, outstanding\n\n\n17\n健步如飞\njiànbùrúfēi - to walk as if on wings\n\n\n18\n一连\nyī lián - adv. in a row, in succession\n\n\n19\n耽误\ndānwù - v. to delay, to spoil through delay\n\n\n20\n至于\nzhìyú - prep./v. as for, as to; to go as far as\n\n\n21\n投资\ntóu zī - v. to invest, to fund\n\n\n22\n人物\nrén wù - n. figure, personage\n\n\n23\n得罪\ndézuì - v. to offend, to displease\n\n\n24\n总算\nzǒng suàn - adv. at last, finally\n\n\n25\n搞\ngǎo - v. to produce a certain effect or result\n\n\n26\n习俗\nxísú - n. custom, convention\n\n\n27\n灵魂\nlíng hún - n. soul, spirit\n\n\n28\n疲劳\npí láo - adj. tired, fatigued\n\n\n29\n哲理\nzhélǐ - n. philosophy\n\n\n30\n提倡\ntí chàng - v. to advocate, to encourage\n\n\n31\n步骤\nbù zhòu - n. step, procedure\n\n\n32\n闭关\nbìguān - v. to stay secluded (a Taoist Practice)\n\n\n33\n一律\nyílǜ - adv. all, without exception\n\n\n34\n寂寞\njì mò - adj. lonely\n\n\n35\n效率\nxiào lǜ - n. efficiency\n\n\n\n\n\n\n\n来\n\n“来” trợ từ, đứng sau các từ chỉ số “十、百、千”… hoặc từ chỉ số lượng biểu thị ước lượng. Ví dụ :\n\n他雇了20来个当地人为他带路和搬运行李。\n按照老人教的方法，他几乎每天都能钓到5斤来重的大鱼。\n\n“来” còn có thể dùng sau các từ chỉ số như “一、二、三” tạo thành kết cấu “一来······，二来······” biểu thị liệt kê lí do. Ví dụ:\n\n今天是大年三十，我们来看看大家，一来是给大家送水果，二来是看看大家过节还有什么难处。\n我对上海很有感情，一来上大学时在那里住过几年，二来我太太也是上海人。\n\n\n至于\n\n“至于” động từ, biểu thị đạt đến một loại trình độ nào đó., thường dùng trong câu phản vấn.\n\n我只是和你开个玩笑，你至于生那么大的气吗？\n什么？一件衬衫要一千来块钱。哪至于那么贵呢？\n\n“至于” cũng có thể làm giới từ, dùng trong kết cấu “（A）·····，至于（B）······” biểu thị nhắc đến một việc khác nữa.\n\n·····至于这部电影片的投资人，可是一位大人物，他可不敢得罪。\n我只知道他是六班的学生，至于住在哪儿，我就不清楚了。\n\n\n总算\n\n“总算” (cuối cùng cũng) phó từ, biểu thị nguyện vọng cuối cùng cũng đã được thực hiện sau khi trải qua một khoảng thời gian dài. Ví dụ:\n\n经过沟通，大导演总算搞明白了。\n总算把活儿干完了，可把我累坏了。\n\n“总算” (nhìn chung/nói chung ) còn biểu thị về mặt cơ bản vẫn vượt qua được.Ví dụ:\n\n虽然我对这家宾馆不太满意，但总算有个睡觉的地方了。\n临走前能和你见上一面，这趟总算没有白来！\n\n\nPhân biệt 总算 và 终于\n\n共同点：\n不同点：\n\n\n\n\n社会关系和婚恋\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n演讲\nyan3jing3 - speech, diễn thuyết\n\n\n2\n发言\nfa1yan2 - to give a speech, comment, phát ngôn\n\n\n3\n宴会\nyan4hui4 - banquet, bữa tiệc\n\n\n4\n嘉宾\njia1bin1 - distinguished guest, khách mời\n\n\n5\n证件\nzheng4jian4 - ID\n\n\n6\n名片\nming2pian4 - business card\n\n\n7\n嫁\njia4 - to marry (a husband), gả chồng\n\n\n8\n娶\nqu3 - to marry (a wife), dựng vợ\n\n\n9\n分手\nfen1shou3 - chia tay, break up\n\n\n10\n怀孕\nhuai2yun4 - pregnant, mang thai\n\n\n11\n吻\nwen3 - kiss, hôn\n\n\n\n\n\n\n认真思考，轻松生活\n在快节奏的现代社会中，人们往往被各种压力和烦恼所困扰，难以获得真正的放松。我认为要想轻松生活，关键在于认真思考。认真思考可以帮助我们有理性的思绪（思维），找到问题的根源，从而制定有效的解决方案。当我们不再为未知而焦虑，不再为无措而彷徨时，自然就会感到轻松自在。认真思考，还可以帮助我们提升自我，不断进步。通过深入思考，我们可以学习新知识，掌握新技能，拓宽眼界，提升格局（作风）。所以我认为，养成每两三个月就花时间思考的习惯，对我们的生活很有价值。\n\n\n\n1.  你是一个平时爱思考的人吗？当你遇到不顺利的情况时，你通常会怎么做？\n我是一个平时爱思考的人。每几个月，我会一个人坐下来，自我反思。在短期内，通常是半年或一年，我通常会提前设定我需要完成的目标或任务。思考的时候，我会评估什么做到的、什么还没完成、什么不再必须、或者刚出现的目标。我也会看看过去的时间我的健康是否有问题。从此，在生活中做出适当的调整。当我遇到不顺利的情况时，我也会冷静地坐下来，想一想那件不顺利的事情是否严重—值不值得我用心解决，原因是什么—主观还是客观，我能不能自己处理。有时我也会找一些好朋友或者亲人，请他们给我建议。他们总会从许多角度说明观点，帮助我克服困难。在繁忙的生活中，我们常常被突然的事情和暂时的情绪所迷惑，并忘记自己想要什么，为什么而活。我觉得思考会帮助我们减轻烦恼，调整生活方向，走向幸福。\n2.  请以学习或工作中一次经历为例，说说思考对你有哪些帮助？\n以工作为例，刚开始工作的那段时期，我总是感到忙碌和疲惫，每天工作到晚上9、10点、每周工作7天。但几个月后，我觉得结果和其他人一样，他们却看起来总是很开心。然后一位前辈看到我那么担忧，就让我做点小事，把我过去三个月所作的写在纸上。我花了30分钟，但只能写了几行。我意识到我只满足了基本的工作要求，我做的大多数事情都是小的或者同事要我帮助，微不足道的，而且非常耗时。我还记得有些任务很有价值，但太忙了，还没开始做。我的时间和工作管理技能真是很差。他笑着跟我说：“如果上个月你做了这个小事，这个月的工作可能会有所不同。如果每个月你都这样思考，你一定会看到积极的发展”。他还告诉我不要花那么多时间为工作，还有比工作更重要的事情。\n从那刻起，我养成了每隔几个月重新评估工作和生活的习惯。这帮我工作有效，轻松生活，一步一步地发展。\n3.  思考和行动，两者哪个更重要？你怎么看这个问题？\n我觉得思考和行动都很重要。如果我们不采取行动，就无法实现想法。但不假思索地行动会让我们走上错误的道路，或者做无用的事情。思考是帮助我们确定应该做什么的策略，而行动是带我们走向目标的工具。我认为最有效的是我们缩小思维和行动的循环，一边走一边调整自己的方向。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第26课-你属于哪种忙/index.html#生词",
    "href": "学汉语的日记/HSK5下-第26课-你属于哪种忙/index.html#生词",
    "title": "HSK5下 | 第26课： 你属于哪种“忙”",
    "section": "",
    "text": "你属于哪种“忙”\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n忙碌\nmánglù - adj. busy, fully occupied\n\n\n2\n被动\nbèidòng - adj. passive\n\n\n3\n奴隶\nnúlì - n. slave\n\n\n4\n虚伪\nxū wěi - adj. hypocritical\n\n\n5\n思想\nsī xiǎng - n. thought, thinking\n\n\n6\n反省\nfǎnxǐng - v. to reflect on oneself, to self-examine\n\n\n7\n据说\njù shuō - v. it is said, reputedly\n\n\n8\n个性\ngè xìng - n. individual character, personality\n\n\n9\n冒险\nmào xiǎn - v. to venture, to have an adventure\n\n\n10\n丛林\ncóng lín - n. jungle, forest\n\n\n11\n文明\nwénmíng - n. civilization\n\n\n12\n纪录\njì lù - n./v. record, note; to record, to note down\n\n\n13\n雇\ngù - v. to employ, to hire\n\n\n14\n来\nlái - part. used after round numbers to indicate approximation\n\n\n15\n批\npī - m. group, batch\n\n\n16\n出色\nchū sè - adj. remarkable, outstanding\n\n\n17\n健步如飞\njiànbùrúfēi - to walk as if on wings\n\n\n18\n一连\nyī lián - adv. in a row, in succession\n\n\n19\n耽误\ndānwù - v. to delay, to spoil through delay\n\n\n20\n至于\nzhìyú - prep./v. as for, as to; to go as far as\n\n\n21\n投资\ntóu zī - v. to invest, to fund\n\n\n22\n人物\nrén wù - n. figure, personage\n\n\n23\n得罪\ndézuì - v. to offend, to displease\n\n\n24\n总算\nzǒng suàn - adv. at last, finally\n\n\n25\n搞\ngǎo - v. to produce a certain effect or result\n\n\n26\n习俗\nxísú - n. custom, convention\n\n\n27\n灵魂\nlíng hún - n. soul, spirit\n\n\n28\n疲劳\npí láo - adj. tired, fatigued\n\n\n29\n哲理\nzhélǐ - n. philosophy\n\n\n30\n提倡\ntí chàng - v. to advocate, to encourage\n\n\n31\n步骤\nbù zhòu - n. step, procedure\n\n\n32\n闭关\nbìguān - v. to stay secluded (a Taoist Practice)\n\n\n33\n一律\nyílǜ - adv. all, without exception\n\n\n34\n寂寞\njì mò - adj. lonely\n\n\n35\n效率\nxiào lǜ - n. efficiency"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第26课-你属于哪种忙/index.html#注释",
    "href": "学汉语的日记/HSK5下-第26课-你属于哪种忙/index.html#注释",
    "title": "HSK5下 | 第26课： 你属于哪种“忙”",
    "section": "",
    "text": "来\n\n“来” trợ từ, đứng sau các từ chỉ số “十、百、千”… hoặc từ chỉ số lượng biểu thị ước lượng. Ví dụ :\n\n他雇了20来个当地人为他带路和搬运行李。\n按照老人教的方法，他几乎每天都能钓到5斤来重的大鱼。\n\n“来” còn có thể dùng sau các từ chỉ số như “一、二、三” tạo thành kết cấu “一来······，二来······” biểu thị liệt kê lí do. Ví dụ:\n\n今天是大年三十，我们来看看大家，一来是给大家送水果，二来是看看大家过节还有什么难处。\n我对上海很有感情，一来上大学时在那里住过几年，二来我太太也是上海人。\n\n\n至于\n\n“至于” động từ, biểu thị đạt đến một loại trình độ nào đó., thường dùng trong câu phản vấn.\n\n我只是和你开个玩笑，你至于生那么大的气吗？\n什么？一件衬衫要一千来块钱。哪至于那么贵呢？\n\n“至于” cũng có thể làm giới từ, dùng trong kết cấu “（A）·····，至于（B）······” biểu thị nhắc đến một việc khác nữa.\n\n·····至于这部电影片的投资人，可是一位大人物，他可不敢得罪。\n我只知道他是六班的学生，至于住在哪儿，我就不清楚了。\n\n\n总算\n\n“总算” (cuối cùng cũng) phó từ, biểu thị nguyện vọng cuối cùng cũng đã được thực hiện sau khi trải qua một khoảng thời gian dài. Ví dụ:\n\n经过沟通，大导演总算搞明白了。\n总算把活儿干完了，可把我累坏了。\n\n“总算” (nhìn chung/nói chung ) còn biểu thị về mặt cơ bản vẫn vượt qua được.Ví dụ:\n\n虽然我对这家宾馆不太满意，但总算有个睡觉的地方了。\n临走前能和你见上一面，这趟总算没有白来！\n\n\nPhân biệt 总算 và 终于\n\n共同点：\n不同点："
  },
  {
    "objectID": "学汉语的日记/HSK5下-第26课-你属于哪种忙/index.html#扩展",
    "href": "学汉语的日记/HSK5下-第26课-你属于哪种忙/index.html#扩展",
    "title": "HSK5下 | 第26课： 你属于哪种“忙”",
    "section": "",
    "text": "社会关系和婚恋\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n演讲\nyan3jing3 - speech, diễn thuyết\n\n\n2\n发言\nfa1yan2 - to give a speech, comment, phát ngôn\n\n\n3\n宴会\nyan4hui4 - banquet, bữa tiệc\n\n\n4\n嘉宾\njia1bin1 - distinguished guest, khách mời\n\n\n5\n证件\nzheng4jian4 - ID\n\n\n6\n名片\nming2pian4 - business card\n\n\n7\n嫁\njia4 - to marry (a husband), gả chồng\n\n\n8\n娶\nqu3 - to marry (a wife), dựng vợ\n\n\n9\n分手\nfen1shou3 - chia tay, break up\n\n\n10\n怀孕\nhuai2yun4 - pregnant, mang thai\n\n\n11\n吻\nwen3 - kiss, hôn"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第26课-你属于哪种忙/index.html#运用",
    "href": "学汉语的日记/HSK5下-第26课-你属于哪种忙/index.html#运用",
    "title": "HSK5下 | 第26课： 你属于哪种“忙”",
    "section": "",
    "text": "认真思考，轻松生活\n在快节奏的现代社会中，人们往往被各种压力和烦恼所困扰，难以获得真正的放松。我认为要想轻松生活，关键在于认真思考。认真思考可以帮助我们有理性的思绪（思维），找到问题的根源，从而制定有效的解决方案。当我们不再为未知而焦虑，不再为无措而彷徨时，自然就会感到轻松自在。认真思考，还可以帮助我们提升自我，不断进步。通过深入思考，我们可以学习新知识，掌握新技能，拓宽眼界，提升格局（作风）。所以我认为，养成每两三个月就花时间思考的习惯，对我们的生活很有价值。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第26课-你属于哪种忙/index.html#口语",
    "href": "学汉语的日记/HSK5下-第26课-你属于哪种忙/index.html#口语",
    "title": "HSK5下 | 第26课： 你属于哪种“忙”",
    "section": "",
    "text": "1.  你是一个平时爱思考的人吗？当你遇到不顺利的情况时，你通常会怎么做？\n我是一个平时爱思考的人。每几个月，我会一个人坐下来，自我反思。在短期内，通常是半年或一年，我通常会提前设定我需要完成的目标或任务。思考的时候，我会评估什么做到的、什么还没完成、什么不再必须、或者刚出现的目标。我也会看看过去的时间我的健康是否有问题。从此，在生活中做出适当的调整。当我遇到不顺利的情况时，我也会冷静地坐下来，想一想那件不顺利的事情是否严重—值不值得我用心解决，原因是什么—主观还是客观，我能不能自己处理。有时我也会找一些好朋友或者亲人，请他们给我建议。他们总会从许多角度说明观点，帮助我克服困难。在繁忙的生活中，我们常常被突然的事情和暂时的情绪所迷惑，并忘记自己想要什么，为什么而活。我觉得思考会帮助我们减轻烦恼，调整生活方向，走向幸福。\n2.  请以学习或工作中一次经历为例，说说思考对你有哪些帮助？\n以工作为例，刚开始工作的那段时期，我总是感到忙碌和疲惫，每天工作到晚上9、10点、每周工作7天。但几个月后，我觉得结果和其他人一样，他们却看起来总是很开心。然后一位前辈看到我那么担忧，就让我做点小事，把我过去三个月所作的写在纸上。我花了30分钟，但只能写了几行。我意识到我只满足了基本的工作要求，我做的大多数事情都是小的或者同事要我帮助，微不足道的，而且非常耗时。我还记得有些任务很有价值，但太忙了，还没开始做。我的时间和工作管理技能真是很差。他笑着跟我说：“如果上个月你做了这个小事，这个月的工作可能会有所不同。如果每个月你都这样思考，你一定会看到积极的发展”。他还告诉我不要花那么多时间为工作，还有比工作更重要的事情。\n从那刻起，我养成了每隔几个月重新评估工作和生活的习惯。这帮我工作有效，轻松生活，一步一步地发展。\n3.  思考和行动，两者哪个更重要？你怎么看这个问题？\n我觉得思考和行动都很重要。如果我们不采取行动，就无法实现想法。但不假思索地行动会让我们走上错误的道路，或者做无用的事情。思考是帮助我们确定应该做什么的策略，而行动是带我们走向目标的工具。我认为最有效的是我们缩小思维和行动的循环，一边走一边调整自己的方向。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第26课-你属于哪种忙/index.html#听力",
    "href": "学汉语的日记/HSK5下-第26课-你属于哪种忙/index.html#听力",
    "title": "HSK5下 | 第26课： 你属于哪种“忙”",
    "section": "2.1. 听力",
    "text": "2.1. 听力\n1.  男：上海的那批货准备得怎么样了？时间可别耽误了。\n女：您放心，小李他们这两天都在加班，保证按合同要求的日期发货。\n问：关于那批货，可以知道什么？（C能按时发货）\n2.  男：喂，你现在在宿舍吗？\n女：我刚下课，正要去图书馆查一份资料，有什么事儿吗？\n问：女的现在准备做什么？（A去查找资料）\n3.  女：平时你要是能多关心一下孩子， 他肯定也不至于考不及格吧？\n男：这怎么能都怪我呢？ 你这当妈的也有责任啊。\n问：从他们的谈话中，可以知道什么？（C孩子成绩不好）\n4.  男：总算弄好了，真把我累坏了。\n女：快休息休息，没想到这么个小桌子安装起来还挺麻烦。\n问：关于男的，可以知道什么？（D干活儿累了）\n5. 女：欢迎光临， 本店五周年店庆， 所有商品一律 5 折销售。\n男：我想给孩子买双网球鞋，这种有 42 号的吗？\n问：从对话中可以知道什么？（A网球鞋半价优惠）\n6.  女：王总，这是您的机票，是往返的。\n男：好的，谢谢。这两份合同我签过字了，你给销售部的刘经理送去吧。\n问：从对话中可以知道什么？（B王总批准了合同）\n7.  男：这个书架我怎么装不上呀？\n女：你是按说明书上的步骤安装的吗？\n男：是啊，有个零件我看不懂它装在哪儿，说明书上说得不清楚。\n女：那你给卖家打个电话问问。\n问：男的为什么装不上书架？（C说明书不详细）\n8.  男：你们家小雪的作业要做到几点？\n女：吃晚饭前就完成了呀。现在学校不让留太多作业。\n男：我们家小明每天都要做到十点钟。\n女：那可不对，你要注意培养他的专注力，要提高效率。\n问：小明作业做到很晚，女的觉得是什么原因？（D做作业不专心）\n9.  女：真不好意思，耽误了您这么长时间。\n男：别这么说，亮亮的学习也离不开家长的关心和督促。\n女：非常感谢！让您费心了。\n男：您太客气了，感谢您对我们学校工作的支持和配合。\n问：男的最可能是亮亮的什么人？（B老师）\n10.男：最近眼睛看东西总感觉模模糊糊的，是不是老花眼了？\n女：你才多大就老花眼？\n男：我们同事刚 45，眼睛就花了。\n女：我觉得你最近写东西弄得太晚了，少看会儿电脑就好了。\n问：女的觉得男的的眼睛是什么问题？（A疲劳了）\n11-12.\n有一位酿酒师，经过长期的努力，酿制出了一种口感独特的葡萄酒，起名叫“赤玉”。但他做不起广告，酒的销路非常不好。情急之下，他想了一个办法，就是每天晚上到各个卖酒的商店去买赤玉葡萄酒。得到的回答当然是“没有”，这时，他热情地说：“没关系，等你们进了货，我再来买。”随着询问次数的增多，许多店家开始关心这种酒，并打听哪里能进到货。赤玉葡萄酒由此打开了销路。这则故事告诉我们一个道理：当你无法解决一个问题时，积极变换思路，从其他角度去想，眼前就会豁然开朗。\n11.赤玉这种酒为什么卖得不好？（B宣传不够）\n12.酿酒师想出了一个什么办法？(D反复询问店家)\n13-14.\n只有自己经历过创业之苦，才懂得节省的道理，才懂得创造的艰辛和财富的来之不易。约翰就是这样的人。他虽然非常富有，可在日常开支方面却很节省。\n一天，他在纽约一家宾馆办理入住，要求住一间最便宜的房间。宾馆的服务生说：“先生，你为什么要住便宜的小房间？你儿子来住宿时，总是挑最豪华的房间呀！”“没错，”约翰说，“我儿子有个百万富翁的父亲，可我没有呀！”\n13.对话中的两个人是什么关系？（B客人和店员）\n14.关于约翰，可以知道什么？（C从不浪费）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第26课-你属于哪种忙/index.html#阅读",
    "href": "学汉语的日记/HSK5下-第26课-你属于哪种忙/index.html#阅读",
    "title": "HSK5下 | 第26课： 你属于哪种“忙”",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n某人在沙漠中行进了大半天，口渴得直冒烟。在他快要走出沙漠时，遇到了一位推销员，劝他买一条领带。他说：“你行行好吧，我渴得连衬衣都想撕开了，15A还买什么领带!”推销员讨了个没趣便走开了。这个可怜人16D总算在沙漠边上的一个小镇上找到了一家酒吧，他急不可待地要冲进去。“快给我点儿什么喝的吧！”他对门口的服务员说.他的嗓子都快说不出话了。“对不起，先生，不17C系领带者是不许进入的。”这个服务员很有礼貌地拒绝了他的要求。\n生活中的我们都会遇到领带的问题，不过不是这么极端而已。短期效应和长期需求本身就是一个需要18B平衡的问题。其实，夏卖冬装，冬卖空调，精明的商家一直在挖掘我们对未来的预期。\n\nD帮助别人会得到满足感\n\n请求别人帮忙，这是一种获得别人信任和支持的非常有效的方法。因为你使自己处于一种弱势的地位，这样就等于尊重了别人，使别人感到某种心理上的满足和自豪，从而对你心存善意。\n\nC小男孩儿想得到财富\n\n一个小男孩儿问上帝：“一万年对你来说有多长？”上帝回答说：“像一分钟。”小男孩儿又问上帝：“100万元对你来说有多少？”上帝回答说：“像一元。”小男孩儿再问上帝：“那你能给我100万元吗？”上帝回答说：“当然可以，只要你给我一分钟。”天下没有免费的午餐，创造财富要凭自己的毅力、耐心。\n\nD干工作，团队合作很关键\n\n在这个竞争的社会里，什么人都不能忽视。的确，在一个大集体里，干好一项工作,占主导地位的往往不是一个人的能力，关键是各成员间的团结合作。团结大家就是提升自己，因为别人会心甘情愿地教会你很多有用的东西。\n\nD两节的大拇指和四指配合最好\n\n人有五只手指，长度各有不同。但是，你有没有注意到，其他手指都有三节，而唯独大拇指只有两节，这是为什么呢？原来，它的节数正好配合其他四指。要是三节的话，大拇指会显得没有力气，不能提起较重的物品；要是只有一节，它便不能灵活自如地与其他四指配合抓紧东西。\n23-25.\n8年前，朋友开始涉足化妆品销售行业，短短几年时间，就积累了数千万的财富。她买下了小城里公交线路的经营权，又投资了一家上档次、上规模的超市。小日子过得有滋有味。近年来，有人邀她投资房地产生意，她拒绝了。她说，现在的日子已经过得很好了，她也不想再发太大的财。再说了，她也没那么多精力去做其他的事情。\n朋友认为，不要为追求太多的财富而去做超出自己能力范围的事，或者浪费掉自己太多的精力，生活，快乐就好，要知足。更何况，任何生意都有风险。\n还有一个中年丧偶的女人，在织布厂里打工，辛辛苦苦抚养着孩子。有好心人给她找了好几份兼职工作，被她一一拒绝了。她说，自己身体不好，穷就穷着点儿吧，还得留着命等着儿子考大学、娶媳妇，自己抱孙子呢！北极有一种企鹅，每当它们兴奋时.就会群聚“跳舞”。说是跳舞，其实就是在一起转圈。有考察队员因为好奇而细心地观察，结果发现，无论企鹅高兴到什么程度，它们最多只转十二圈。研究表明，企鹅在转圈的时候，会给身体带来一定的负荷，而在转到第十二圈的时候，也是身体负荷接近承受不了的时刻。企鹅知道，转到第十二圈停下来，是为了以后还能继续“跳舞”。\n人也应该这样，在漫漫人生路上，知道什么叫适可而止才是硬道理，明白什么叫量力而行才是大智慧。\n23. C满足现状\n24. B常是为表达兴奋\n25. D做人要量力而行\n26-28.\n有个女孩儿没考上大学，被安排在本村的小学教书，由于讲不清数学题，不到一周就被学生赶下了台。母亲安慰她说：“满肚子的东西，有人倒得出来，有人倒不出来，没必要为这个伤心，也许有更适合你的事情等着你去做。”\n后来，她随本村的伙伴一起外出打工，不幸的是，她又被老板解雇了，原因是剪裁衣服的时候，手脚太慢，质量也不过关。母亲对女儿说：“手脚总是有快有慢，别人已经干了很多年了，而你一直在念书，怎么快得了呢？”\n女儿先后当过纺织工，干过市场管理员，做过会计,但一律以失败告终。然而每次女儿失望地回来时，母亲总会安慰她，从没有抱怨。30来岁时，女儿总算凭着出色的手语才能，做了聋哑学校的辅导员，后来她又开办了一家残疾人学校，再后来，她在许多城市投资的残疾人用品连锁店生意红火，使她成为拥有几千万元资产的老板。\n一天，成功后的女儿凑到已经年迈的母亲面前，她想得到一个一直以来想知道的答案，那就是前些年她连连失败，自己都觉得前途暗淡的时候，是什么原因让母亲对她那么有信心呢？母亲的回答简单而富有哲理：“一块地，不适合种麦子，可以试试种豆子，豆子也长不好的话，可以种瓜果，瓜果也不济的话，撒上一些葬麦种子一定能开花，因为一块地，总会有一粒种子适合它，也终会有属于它的一片收成。”\n26. C不受学生欢迎\n27. B工作效率太低\n28. D抓住自己出色的一面"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第26课-你属于哪种忙/index.html#书写",
    "href": "学汉语的日记/HSK5下-第26课-你属于哪种忙/index.html#书写",
    "title": "HSK5下 | 第26课： 你属于哪种“忙”",
    "section": "2.3. 书写",
    "text": "2.3. 书写\n效率、提倡、步骤、耽误、出色。\n沟通是提高工作效率的关键。当我们在工作中遇到了问题，向同事或领导提出问题是解决问题的最快方法。当然你可以自己处理问题，但这样可能耽误了时间，严重的话，可能会影响工作目标。在进行项目时，如果利益相关者能够通过沟通从一开始就了解目标、方向和困难，则项目已经迈过了重要第一步骤。表现出色的人员总能很好地沟通并将团队中的每个人联系起来。大多数的现代公司也提倡在工作环境中进行公开沟通。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第28课-最受欢迎的毕业生/index.html",
    "href": "学汉语的日记/HSK5下-第28课-最受欢迎的毕业生/index.html",
    "title": "HSK5下 | 第28课： 最受欢迎的毕业生",
    "section": "",
    "text": "最受欢迎的毕业生 | The most popular graduate\n他叫刘辰，是一个年仅23岁的应届本科毕业生，再过一个月就要毕业了。面对并不乐观的就业形势，他压力很大：“说实话，我觉得自己实在没什么优势。”就在他为工作发愁时，机会来了。天津卫视的《非你莫属》节目组看了他的简历，接受了他的申请，他可以到节目现场去求职。来到现场，他发现，果然有一家公司有适合他的职位——旅游体验师。\n\n因为小学六年级的时候，他迷上了公交车，从此，就一直关注公交线路，北京市范围内所有的公交线路他都了如指掌。从上初中起，他就是同学们的出行顾问，无论谁想去哪里，他都能很快地回答出最方便的路线，提供给同学们参考。在他的成长过程中，公交就是他最好的伙伴。节目制作时，电视台问他有什么才艺，他便说:“我是个公交迷，对北京市的公交、地铁线路都有一些研究。”\n\n主持人现场考他：“假设我要从国贸到鼓楼大街，该怎么乘车？”他反应得非常快，马上回答说：“在国贸坐1路车，到天安门东，换乘82路，就可以到达。”他的回答让台上的12位老板都兴奋了起来，他们开始陆续向他提问。他有问必答，不但准确无误地按顺序报了一大堆公交车、地铁站的名字，而且还给一对情侣制订了北京休闲一日游的具体方案。\n他对公交的这种专注显然为他的求职打开了大门。老总们向他发出了热情的邀请，给他非常好的职位和待遇，甚至要专门为他成立有关的部门，只为留住这个人才。最终，他选择了一家他感兴趣的单位。\n主持人问这家公司的老总：“你给的工资是不是太高了？”这个老总回答：“专业的、执着的、优秀的人才是无价的，这样的人一定会有光明的前途。”是的，无论在哪个行业，最缺乏的永远都是专注的人，专注的人永远不缺机会！\n改编自《年经人》，作者：张宏生\n\n\n\n最受欢迎的毕业生\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n届\njiè - m. session, year, class\n\n\n2\n本科\nběn kē - n. undergraduate education\n\n\n3\n乐观\nlè guān - adj. optimistic\n\n\n4\n就业\njiù yè - v. to find employment\n\n\n5\n实话\nshí huà - n. truth, true words\n\n\n6\n优势\nyōu shì - n. superiority, advantage\n\n\n7\n简历\njiǎn lì - n. CV, resume\n\n\n8\n现场\nxiànchǎng - n. site, sport\n\n\n9\n职位\nzhíwèi - n. position, post\n\n\n10\n体验\ntǐ yàn - v. to feel and experience\n\n\n11\n从此\ncóng cǐ - adv. from then on, since then\n\n\n12\n范围\nfàn wéi - n. scope, range\n\n\n13\n初（级）中（学）\nchū (jí) zhōng (xué) - n. junior high school\n\n\n14\n顾问\ngù wèn - n. consultant, adviser\n\n\n15\n参考\ncānkǎo - v. to consult, to refer to\n\n\n16\n成长\nchéng zhǎng - v. to grow up\n\n\n17\n制作\nzhì zuò - v. to make, to produce\n\n\n18\n才艺\ncáiyì - n. talent and skill\n\n\n19\n假设\njiǎshè - v. to suppose, to assume\n\n\n20\n乘\nchéng - v. to ride, to travel by\n\n\n21\n到达\ndào dá - v. to reach, to arrive\n\n\n22\n老板\nlǎo bǎn - n. boss, employer\n\n\n23\n陆续\nlù xù - adv. one after another, in succession\n\n\n24\n提问\ntí wèn - v. to ask a question\n\n\n25\n堆\nduī - m. heap, pack, pile\n\n\n26\n情侣\nqíng lǚ - n. couple, lovers\n\n\n27\n制订\nzhìdìng - v. to make, to draw up, to work out\n\n\n28\n休闲\nxiū xián - v. to have leisure, to relax\n\n\n29\n具体\njù tǐ - adj. specific, detailed\n\n\n30\n专注\nzhuānzhù - adj. concentrated, engrossed\n\n\n31\n显然\nxiǎn rán - adj. obvious, evident\n\n\n32\n成立\nchéng lì - v. to establish, to set up\n\n\n33\n部门\nbù mén - n. department, section\n\n\n34\n执着\nzhízhuó - adj. persistent, persevering\n\n\n35\n光明\nguāng míng - adj. bright, promising\n\n\n36\n前途\nqiántú - n. future, prospect\n\n\n37\n行业\nháng yè - n. trade, profession, industry\n\n\n\n\n\n\n\n从此\n\nphó từ, biểu thị từ thời điểm này hoặc từ thời điểm nói trở đi. Ví dụ:\n\n李白听了老婆婆的话，很受感动，从此他刻苦用功，最后成了一位伟大的诗人。\n因为小学六年级的时候，他迷上了公交车，从此，他就一直关注公交线路，·······\n嫦娥自己吃下了不死药，结果她飞到了月亮上，从此与后羿分离。\n\n\n假设\n\n“假设”, động từ, biểu thị coi một số tình huống là thật. Ví dụ:\n\n假设我要从国贸到鼓楼大街，该怎么乘车？\n假设汽水两块钱一瓶，两个空瓶可以换一瓶汽水，如果给你6块钱，你最多能喝几瓶汽水？\n\n“假设” , cũng có thể làm danh từ, biểu thị tình huống giả thiết, tưởng tượng. Ví dụ:\n\n你当年的假设已经被证明是对的。\n这是一种大胆的假设，但不一定是科学的。\n\n\n堆 dui1\n\n“堆” (đống/đám), lượng từ, dùng cho đám người hoặc đống đồ vật (không thể dùng với những người đáng kính) .Ví dụ:\n\n他有问必答，不但准确无误地按顺序报了一大堆公交车、地铁站的名字，而且还给一对情侣制订了北京休息一日游的具体方案。\n一个小师弟结婚才半年，就跑过来找我诉苦，说妻子几乎每天都要挑出他一大堆毛病：饭后不洗碗、睡前不洗脚······\n\n“堆” , cũng có thể làm động từ, biểu thị dùng tay hoặc dụng cụ để xếp, chất lên nhau. Ví dụ：\n\n这些零件怎么都堆在这儿啊？\n今年真不错！你看这粮食，都堆成山了。\n\n“堆” , còn có thể làm danh từ, biểu thị đồ vật xếp cùng nhau. Ví dụ:\n\n工厂旁边有一个建筑材料堆。\n叔叔把手指上的金戒指取了下来，扔到石头堆里。\n\n\nPhân biệt 反应 và 反映\n\n共同点：Đồng âm, đều vừa có thể làm động từ vừa có thể làm danh từ.\n不同点：\n\n不同点\n\n\n\n\n\n\n\nNo.\n反应\n反映\n\n\n\n\n1\nLà động từ chỉ chịu kích thích của ngoại cảnh mà dẫn đến hành động hoặc biến hóa ；là danh từ chỉ những hành động này , những sự thay đổi này.\n如：这时人体精力下降，反应减慢，情绪地下，利于人体进入甜美的梦乡。\nĐem tình hình hoặc ý kiến báo cáo với cấp trên.\n\n\n2\nKhông còn ý nghĩa nào khác.\nCòn có thể chỉ phô bày ra bản chất của sự vât.\n如：谈话可以反映一个人的职业特点。\n\n\n3\nKhông thể kết hợp với tân ngữ.\n如：他反应得非常快，一点儿也不用思考。\nCó thể kết hợp với tân ngữ.\n如：这个电影反映了中国年轻一代的新变化。\n\n\n\n\n\n\n\n职业和求职\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n模特\nmo2te4 - model, mẫu\n\n\n2\n会计\nkuai4ji4 - accounting, kế toán\n\n\n3\n秘书\nmi4shu1 - secretary, thư ký\n\n\n4\n农民\nnong2min2 - farmer, nông dân\n\n\n5\n工程师\ngong1cheng2shi1, engineer, kỹ sư\n\n\n6\n工人\ngong1ren2 - công nhân, worker\n\n\n7\n员工\nyuan2gong1 - staff, nhân viên\n\n\n8\n人事\nren2shi4 - nhân sự, HR, human resources\n\n\n9\n报道\nbao4dao4 - báo cáo, report\n\n\n10\n失业\nshi1ye4 - thất nghiệp, unemployed\n\n\n11\n手续\nshou3xu4 - thủ tục, procedure\n\n\n12\n待遇\ndai4yu4 - đãi ngộ, compensation\n\n\n13\n兼职\njian1zhi2 - part-time job, công việc bán thời gian\n\n\n14\n简历\njian3li4 - sơ yếu lí lịch, CV\n\n\n\n\n\n\n寻找自己的优势\n人在世上将花一半以上的时间工作，为自己、家庭和社会创造价值。因此，我认为只有做一份自己有优势的工作，我们才能发挥自己的优点，创造更多的价值，从而过上幸福的生活。优势是我们可以轻松地做某事的技能、并对它充满热情，但这对其他人来说却是一件困难的事情。找到优势并不容易，它也可能是一种你以为很正常的能力。对我来说，找到自己的优势其实不太难，下几种方法相信会对你们有所帮助：\n首先，在平常的生活中自己的优势很有可能是自己的兴趣爱好，所以自己喜欢做的事情就一定要把它做好，这样和别人相比的时候就是属于自己的一大优势之一。\n其次，在生活中也需要尝试一些以前不敢尝试的事情，一定要学会突破自己，突破自己之后也能够快速找到自己的优势。\n找到让我们的生活更轻松的优势，以及让人们更加重视我们，是幸福生活的关键。\n\n\n\n1.  你喜欢“分配工作”还是“双向选择”？为什么？\n我当然喜欢“双方选择”。对我来说，招聘是工人和雇主之间平等选择和谈判的过程。雇主寻找有能力的人担任职位，并为公司带来价值。员工寻找公司，为他们提供收入来源和长期发展机会。\n作为一名工人，我也希望如此。“分配工作”不会利用工人的优势，而是取决于“分配者”的主观评估。在双方选择的过程中，通过考试和面试，员工将了解公司招聘的职位要求、他们是否满足要求以及他们是否对工作感兴趣。此外，他们还将了解公司的发展方向，并了解工作文化。另一方面，公司还将对候选人的技能、经验和态度进行全面评估。有了这样的相互理解，双方可以通过谈判就最合适的头衔和待遇达成最终协议。“分配工作”不会带来这样理解的方法，便造成员工无法完成工作或开始工作后感觉不到兴奋的风险，导致他们很快离开公司。然后公司又开始招聘，成本和时间都很高。\n我相信，如果市场上的所有员工和雇主都有“双向选择”这个思想，双方都将受益，工人的权利将得到更大保障。\n2.  一般的用人单位对员工可能有什么样的要求？\n我认为一般用人单位对员工有三点主要的要求：技能、经验和态度。\n技能是一份工作的特定知识和技术。例如，销售人员需要说服客户购买产品的技能，程序员需要编程技能，应用算法解决问题并将其开发成软件。技能是可以教的，随着时间的推移而发展。\n经验是通过工作时间积累的对行业的理解。销售人员可能原来具有说服力，但客户群体的口味特征或市场情况需要积累经验，这也有助于提高销售成功率。\n公司对员工的态度始终是认真、正直、进取精神和对工作的热情。 态度是一个难以评估的因素，需要很长时间才能得出结论。公司将始终监控您的态度，并决定您是被提升还是被解雇。\n我认为这三个因素都很重要，它们部分相互依赖，也可以相互抵消。根据工作的不同，雇主对这三个方面有不同的要求。你可能错过了这三个方面中的一个或两个，但在你的职业生涯中始终专注于构建它们。\n3.  你认为自己有什么优秀？面对找工作的问题，你应该做哪些准备？\n我认为我有两个主要的优势。\n首先，解决问题的能力。一旦我确定了要解决的问题或任务，我就能够集中精力，从多个方面综合现有知识、经验和见解，提出解决方案。在我的工作环境中，我总是关注整个公司的业务和运营，而不仅仅是我自己的任务。举个例子，为公司数据工作者，我始终主动与大多数部门沟通，以了解流程和数据需求，从而始终能够快速提出解决方案。\n其次，我很善于处理数据。这来自两个因素，良好的数学背景和对理解商业问题和指标之间实际联系的兴趣。商业知识使我能够识别数字的合理性，并提出适当的指标。数学和统计知识使我能够从分析结果中提出意见和建议。这就是我在数据行业工作的原因。\n面对找工作的问题，我认为面试官需要做好两方面的准备。\n首先，要仔细阅读工作要求，看看自己达到了哪些要求，没有达到任何要求。不要对自己的能力撒谎，因为经验丰富的招聘人员很容易识别。对于缺点，建议一种替代技能或表现出你的学习精神。\n其次，你还需要先事了解你面试的公司。一方面表明你在申请公司时的严肃性，另一方面让你有基本的理解，从中向雇主提问，以确定工作、文化和待遇是否适合你。\n面试的目的是说服雇主选择你，并弄清楚这份工作是否适合你，所以要做好准备。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第28课-最受欢迎的毕业生/index.html#生词",
    "href": "学汉语的日记/HSK5下-第28课-最受欢迎的毕业生/index.html#生词",
    "title": "HSK5下 | 第28课： 最受欢迎的毕业生",
    "section": "",
    "text": "最受欢迎的毕业生\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n届\njiè - m. session, year, class\n\n\n2\n本科\nběn kē - n. undergraduate education\n\n\n3\n乐观\nlè guān - adj. optimistic\n\n\n4\n就业\njiù yè - v. to find employment\n\n\n5\n实话\nshí huà - n. truth, true words\n\n\n6\n优势\nyōu shì - n. superiority, advantage\n\n\n7\n简历\njiǎn lì - n. CV, resume\n\n\n8\n现场\nxiànchǎng - n. site, sport\n\n\n9\n职位\nzhíwèi - n. position, post\n\n\n10\n体验\ntǐ yàn - v. to feel and experience\n\n\n11\n从此\ncóng cǐ - adv. from then on, since then\n\n\n12\n范围\nfàn wéi - n. scope, range\n\n\n13\n初（级）中（学）\nchū (jí) zhōng (xué) - n. junior high school\n\n\n14\n顾问\ngù wèn - n. consultant, adviser\n\n\n15\n参考\ncānkǎo - v. to consult, to refer to\n\n\n16\n成长\nchéng zhǎng - v. to grow up\n\n\n17\n制作\nzhì zuò - v. to make, to produce\n\n\n18\n才艺\ncáiyì - n. talent and skill\n\n\n19\n假设\njiǎshè - v. to suppose, to assume\n\n\n20\n乘\nchéng - v. to ride, to travel by\n\n\n21\n到达\ndào dá - v. to reach, to arrive\n\n\n22\n老板\nlǎo bǎn - n. boss, employer\n\n\n23\n陆续\nlù xù - adv. one after another, in succession\n\n\n24\n提问\ntí wèn - v. to ask a question\n\n\n25\n堆\nduī - m. heap, pack, pile\n\n\n26\n情侣\nqíng lǚ - n. couple, lovers\n\n\n27\n制订\nzhìdìng - v. to make, to draw up, to work out\n\n\n28\n休闲\nxiū xián - v. to have leisure, to relax\n\n\n29\n具体\njù tǐ - adj. specific, detailed\n\n\n30\n专注\nzhuānzhù - adj. concentrated, engrossed\n\n\n31\n显然\nxiǎn rán - adj. obvious, evident\n\n\n32\n成立\nchéng lì - v. to establish, to set up\n\n\n33\n部门\nbù mén - n. department, section\n\n\n34\n执着\nzhízhuó - adj. persistent, persevering\n\n\n35\n光明\nguāng míng - adj. bright, promising\n\n\n36\n前途\nqiántú - n. future, prospect\n\n\n37\n行业\nháng yè - n. trade, profession, industry"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第28课-最受欢迎的毕业生/index.html#注释",
    "href": "学汉语的日记/HSK5下-第28课-最受欢迎的毕业生/index.html#注释",
    "title": "HSK5下 | 第28课： 最受欢迎的毕业生",
    "section": "",
    "text": "从此\n\nphó từ, biểu thị từ thời điểm này hoặc từ thời điểm nói trở đi. Ví dụ:\n\n李白听了老婆婆的话，很受感动，从此他刻苦用功，最后成了一位伟大的诗人。\n因为小学六年级的时候，他迷上了公交车，从此，他就一直关注公交线路，·······\n嫦娥自己吃下了不死药，结果她飞到了月亮上，从此与后羿分离。\n\n\n假设\n\n“假设”, động từ, biểu thị coi một số tình huống là thật. Ví dụ:\n\n假设我要从国贸到鼓楼大街，该怎么乘车？\n假设汽水两块钱一瓶，两个空瓶可以换一瓶汽水，如果给你6块钱，你最多能喝几瓶汽水？\n\n“假设” , cũng có thể làm danh từ, biểu thị tình huống giả thiết, tưởng tượng. Ví dụ:\n\n你当年的假设已经被证明是对的。\n这是一种大胆的假设，但不一定是科学的。\n\n\n堆 dui1\n\n“堆” (đống/đám), lượng từ, dùng cho đám người hoặc đống đồ vật (không thể dùng với những người đáng kính) .Ví dụ:\n\n他有问必答，不但准确无误地按顺序报了一大堆公交车、地铁站的名字，而且还给一对情侣制订了北京休息一日游的具体方案。\n一个小师弟结婚才半年，就跑过来找我诉苦，说妻子几乎每天都要挑出他一大堆毛病：饭后不洗碗、睡前不洗脚······\n\n“堆” , cũng có thể làm động từ, biểu thị dùng tay hoặc dụng cụ để xếp, chất lên nhau. Ví dụ：\n\n这些零件怎么都堆在这儿啊？\n今年真不错！你看这粮食，都堆成山了。\n\n“堆” , còn có thể làm danh từ, biểu thị đồ vật xếp cùng nhau. Ví dụ:\n\n工厂旁边有一个建筑材料堆。\n叔叔把手指上的金戒指取了下来，扔到石头堆里。\n\n\nPhân biệt 反应 và 反映\n\n共同点：Đồng âm, đều vừa có thể làm động từ vừa có thể làm danh từ.\n不同点：\n\n不同点\n\n\n\n\n\n\n\nNo.\n反应\n反映\n\n\n\n\n1\nLà động từ chỉ chịu kích thích của ngoại cảnh mà dẫn đến hành động hoặc biến hóa ；là danh từ chỉ những hành động này , những sự thay đổi này.\n如：这时人体精力下降，反应减慢，情绪地下，利于人体进入甜美的梦乡。\nĐem tình hình hoặc ý kiến báo cáo với cấp trên.\n\n\n2\nKhông còn ý nghĩa nào khác.\nCòn có thể chỉ phô bày ra bản chất của sự vât.\n如：谈话可以反映一个人的职业特点。\n\n\n3\nKhông thể kết hợp với tân ngữ.\n如：他反应得非常快，一点儿也不用思考。\nCó thể kết hợp với tân ngữ.\n如：这个电影反映了中国年轻一代的新变化。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第28课-最受欢迎的毕业生/index.html#扩展",
    "href": "学汉语的日记/HSK5下-第28课-最受欢迎的毕业生/index.html#扩展",
    "title": "HSK5下 | 第28课： 最受欢迎的毕业生",
    "section": "",
    "text": "职业和求职\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n模特\nmo2te4 - model, mẫu\n\n\n2\n会计\nkuai4ji4 - accounting, kế toán\n\n\n3\n秘书\nmi4shu1 - secretary, thư ký\n\n\n4\n农民\nnong2min2 - farmer, nông dân\n\n\n5\n工程师\ngong1cheng2shi1, engineer, kỹ sư\n\n\n6\n工人\ngong1ren2 - công nhân, worker\n\n\n7\n员工\nyuan2gong1 - staff, nhân viên\n\n\n8\n人事\nren2shi4 - nhân sự, HR, human resources\n\n\n9\n报道\nbao4dao4 - báo cáo, report\n\n\n10\n失业\nshi1ye4 - thất nghiệp, unemployed\n\n\n11\n手续\nshou3xu4 - thủ tục, procedure\n\n\n12\n待遇\ndai4yu4 - đãi ngộ, compensation\n\n\n13\n兼职\njian1zhi2 - part-time job, công việc bán thời gian\n\n\n14\n简历\njian3li4 - sơ yếu lí lịch, CV"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第28课-最受欢迎的毕业生/index.html#运用",
    "href": "学汉语的日记/HSK5下-第28课-最受欢迎的毕业生/index.html#运用",
    "title": "HSK5下 | 第28课： 最受欢迎的毕业生",
    "section": "",
    "text": "寻找自己的优势\n人在世上将花一半以上的时间工作，为自己、家庭和社会创造价值。因此，我认为只有做一份自己有优势的工作，我们才能发挥自己的优点，创造更多的价值，从而过上幸福的生活。优势是我们可以轻松地做某事的技能、并对它充满热情，但这对其他人来说却是一件困难的事情。找到优势并不容易，它也可能是一种你以为很正常的能力。对我来说，找到自己的优势其实不太难，下几种方法相信会对你们有所帮助：\n首先，在平常的生活中自己的优势很有可能是自己的兴趣爱好，所以自己喜欢做的事情就一定要把它做好，这样和别人相比的时候就是属于自己的一大优势之一。\n其次，在生活中也需要尝试一些以前不敢尝试的事情，一定要学会突破自己，突破自己之后也能够快速找到自己的优势。\n找到让我们的生活更轻松的优势，以及让人们更加重视我们，是幸福生活的关键。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第28课-最受欢迎的毕业生/index.html#口语",
    "href": "学汉语的日记/HSK5下-第28课-最受欢迎的毕业生/index.html#口语",
    "title": "HSK5下 | 第28课： 最受欢迎的毕业生",
    "section": "",
    "text": "1.  你喜欢“分配工作”还是“双向选择”？为什么？\n我当然喜欢“双方选择”。对我来说，招聘是工人和雇主之间平等选择和谈判的过程。雇主寻找有能力的人担任职位，并为公司带来价值。员工寻找公司，为他们提供收入来源和长期发展机会。\n作为一名工人，我也希望如此。“分配工作”不会利用工人的优势，而是取决于“分配者”的主观评估。在双方选择的过程中，通过考试和面试，员工将了解公司招聘的职位要求、他们是否满足要求以及他们是否对工作感兴趣。此外，他们还将了解公司的发展方向，并了解工作文化。另一方面，公司还将对候选人的技能、经验和态度进行全面评估。有了这样的相互理解，双方可以通过谈判就最合适的头衔和待遇达成最终协议。“分配工作”不会带来这样理解的方法，便造成员工无法完成工作或开始工作后感觉不到兴奋的风险，导致他们很快离开公司。然后公司又开始招聘，成本和时间都很高。\n我相信，如果市场上的所有员工和雇主都有“双向选择”这个思想，双方都将受益，工人的权利将得到更大保障。\n2.  一般的用人单位对员工可能有什么样的要求？\n我认为一般用人单位对员工有三点主要的要求：技能、经验和态度。\n技能是一份工作的特定知识和技术。例如，销售人员需要说服客户购买产品的技能，程序员需要编程技能，应用算法解决问题并将其开发成软件。技能是可以教的，随着时间的推移而发展。\n经验是通过工作时间积累的对行业的理解。销售人员可能原来具有说服力，但客户群体的口味特征或市场情况需要积累经验，这也有助于提高销售成功率。\n公司对员工的态度始终是认真、正直、进取精神和对工作的热情。 态度是一个难以评估的因素，需要很长时间才能得出结论。公司将始终监控您的态度，并决定您是被提升还是被解雇。\n我认为这三个因素都很重要，它们部分相互依赖，也可以相互抵消。根据工作的不同，雇主对这三个方面有不同的要求。你可能错过了这三个方面中的一个或两个，但在你的职业生涯中始终专注于构建它们。\n3.  你认为自己有什么优秀？面对找工作的问题，你应该做哪些准备？\n我认为我有两个主要的优势。\n首先，解决问题的能力。一旦我确定了要解决的问题或任务，我就能够集中精力，从多个方面综合现有知识、经验和见解，提出解决方案。在我的工作环境中，我总是关注整个公司的业务和运营，而不仅仅是我自己的任务。举个例子，为公司数据工作者，我始终主动与大多数部门沟通，以了解流程和数据需求，从而始终能够快速提出解决方案。\n其次，我很善于处理数据。这来自两个因素，良好的数学背景和对理解商业问题和指标之间实际联系的兴趣。商业知识使我能够识别数字的合理性，并提出适当的指标。数学和统计知识使我能够从分析结果中提出意见和建议。这就是我在数据行业工作的原因。\n面对找工作的问题，我认为面试官需要做好两方面的准备。\n首先，要仔细阅读工作要求，看看自己达到了哪些要求，没有达到任何要求。不要对自己的能力撒谎，因为经验丰富的招聘人员很容易识别。对于缺点，建议一种替代技能或表现出你的学习精神。\n其次，你还需要先事了解你面试的公司。一方面表明你在申请公司时的严肃性，另一方面让你有基本的理解，从中向雇主提问，以确定工作、文化和待遇是否适合你。\n面试的目的是说服雇主选择你，并弄清楚这份工作是否适合你，所以要做好准备。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第28课-最受欢迎的毕业生/index.html#听力",
    "href": "学汉语的日记/HSK5下-第28课-最受欢迎的毕业生/index.html#听力",
    "title": "HSK5下 | 第28课： 最受欢迎的毕业生",
    "section": "2.1. 听力",
    "text": "2.1. 听力\n1.  女：小刘，你今年就要毕业了，对找工作有什么想法？\n男：说实话，我觉得从专业看，自己实在没有什么优势。\n问：男的觉得自己怎么样？（D就业形势不乐观）\n2.  男：请问，我想去鼓楼大街，应该怎么坐车？\n女：在前面坐1路，到天安门东，然后换乘3路。\n问：从这里到鼓楼大街要换乘几次车？（A 1次）\n3.  女：来应聘我们网站的编辑职位，你觉得自己有什么优势？\n男：我的专业是计算机，电脑软件用得很熟练；另外，我也很爱好文学。\n问：男的觉得自己哪方面最强？（C电脑）\n4.  男：昨天我才听说，你跟王林是同学。你们熟吗？\n女：不算熟，同届不同班，就是在楼里见面打个招呼的交情。\n问：女的跟王林是什么关系？（B同年级同学）\n5.  女：这次比赛让小李去参加怎么样？他反应比较快。\n男：小李反应虽然快，但基本功没有小张好。\n问：男的希望派谁去参加比赛？（A小张）\n6.  男：你们给的工资是不是太高了？\n女：专业的、执着的、优秀的人才是无价的。\n问：女的是什么意思？（C为人才付钱是值得的）\n7.  女：明天就要开项目会了，你真的打算这次让小刘负责？\n男：我们不是都说好了吗？小刘有什么不好？\n女：她可还是个新人。\n男：你总是对别人缺乏信心。\n问：男的是什么意思？（B要相信小刘）\n8.  男：今年来找工作的毕业生里，有适合干办公室的人吗？\n女：怎么了？小高不是挺好的吗？\n男：她下个月就要辞职了，说是跟别人一起去开公司。\n女：我帮你看看。\n问：关于小高，下列哪项正确？（B准备辞职）\n9.  女：老师，我的论文怎么样？\n男：你论文里这个结论是怎么来的？我完全看不出科学性。\n女：您不是说要大胆假设吗？\n男：“大胆假设”后面还有一句是“小心求证”呢！\n问：老师认为这篇论文怎么样？（C没有证据）\n10.男：你们今天怎么回事？8：30上课，只来了十几个人，现在9：00了，还有三个没来。\n女：老师，他们三个病了，去了医院，马上就到。\n男：马上马上，已经迟到半小时了！\n女：您看，已经来了！\n问：今天学生来上课的情况怎么样？(C是陆陆续续来的)\n11-12.\n女：之前公司并没有录用你，你为什么还要写感谢信给我们呢？\n男：虽然公司没有录用我，但认真看过我的简历，也使我收获了面试经验，我觉得也应该表示感谢。\n女：你做得好！正是因为你写来的感谢信，让我们觉得你是个很有礼貌的人，对我们公司也很有诚意，所以这次招聘时我们就优先考虑了你。\n男：非常感谢您和公司给我这个机会。\n女：这个机会是你自己创造的。\n11. 说话的两个人是什么关系？（D面试官和求职者）\n12. 这次公司招聘为什么优先考虑这个男的？（A他写过感谢信）\n13-14.\n希望国际大酒店，是本市唯一一家五星级酒店，占地面积 5.5 万平方米，共有客房 288 套。现酒店财务部需要招聘财务经理一名，成本主管一名， IT 主管一名。以上职位都要求相关专业研究生毕业，需有两年以上酒店工作经验。酒店可提供住宿，薪酬面议。我们将为你提供一个和谐的工作环境和良好的学习氛围，传授五星级酒店管理的技能，助你快速成长。\n13．酒店对应聘者有什么要求？（B研究生学历）\n14．关于酒店，下列哪项正确？（C有员工宿舍）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第28课-最受欢迎的毕业生/index.html#阅读",
    "href": "学汉语的日记/HSK5下-第28课-最受欢迎的毕业生/index.html#阅读",
    "title": "HSK5下 | 第28课： 最受欢迎的毕业生",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n生活就像一面镜子。你对它哭，15C它也对你哭。如果你想要它对你微笑，那么只有一种办法，就是也对它微笑。微笑，是最美好、最迷人的一种表情。人生有成功也有失败，谁都不可能一帆风顺。真正大的智慧，往往源于失败的教训。古今中外，大多数成功者都经历过失败，可贵的是他们的勇气，他们能微笑着16A面对失败。就像马克ㅤ吐温经商失意，便弃商从文，结果一举成名。我们不能单单停留在失败中，要从失败中吸取经验教训，把这些经验教训作为以后行动的17C参考，这样，我们才会变得成熟，最终18B到达成功的彼岸。正如拿破仑所说的：“避免失败的最好方法，就是决心获得下一次成功。”\n\nB想要录用他的公司不止一家\n\n他对公交的这种专注显然为他求职打开了大门。老总们向他发出了热情的邀请，给他非常好的职位和待遇，甚至要专门为他成立有关的部门，只为留住这个人才。最终,他选择了一家他最感兴趣的单位。\n\nC求职者要表现自己的优势\n\n任何一家公司在招聘时，都会注意一个人的综合能力。然而在短暂的面试时间里，无论准备得如何充分，都无法让个人才能全方位地展示出来。作为求职者，应该做的是,针对所应聘岗位强调个人的能力和专长，针对这项工作详细阐述自己的优点与长处。\n\nC“名片效应”可以缩小人之间的距离 （shrink, thu ngắn lại)\n\n“名片效应”是指两个人在交往时，如果首先表明自己与对方的态度和价值观相同,就会使对方感到你与他有很多的相似性，从而很快地缩小与你的心理距离，更愿意同你接近，结成良好的人际关系。在这里，有意识、有目的地向对方表明态度和观点,就如同名片一样，可以把自己介绍给对方。\n\nD这个职位可能常常需要出差\n\n本职位任职要求：一、从事电视编导、新闻采编工作3年以上，有丰富的外拍经验；二、集体荣誉感强，能很好地与团队其他工作人员进行工作对接；三、能适应出差的工作节奏；四、有超强的抗压能力和工作主动性。\n23-25.\n年轻人选择职业时，常常把收人水平当作最重要的、甚至是唯一的标准。实际上,我们需要思考的问题还有很多。\n在选择工作时，首先应该考虑行业。因为行业决定了企业业务发展趋势，大多数企业的业务发展，都会受到行业整体的影响。如果行业不景气，企业要取得好的发展就比较难；如果行业竞争过于激烈，企业的发展也必然受到影响，不容易获得较大收益。所以，选择一个好的行业、处于上升阶段的行业，是选择好工作的前提。\n其次，即使是好行业，也可能有不那么好的企业。请注意，好企业不等于大企业，有些企业虽然规模大，但未必好。例如，有些企业，行业一出现问题，便立刻裁员,这至少说明企业没有长远规划，经营不稳。所以，在选择工作时，要尽量避免选择看上去大实际上弱的企业，与其如此，不如在小企业待着，至少有比较大的发展空间。\n第三个要考虑的就是个人发展空间的问题。对于毕业时间不长的人而言，职业发展是关键，这会决定你未来若干年的位置和走向，所以，在选择工作时，职业发展空间应作为关注焦点。职业发展空间包括两方面：职位发展与能力发展。因此，对于你所申请的工作，必须明确岗位职责，思考该岗位是否能够提高自身能力，能否得到职位上的提升。\n\nA工资\nD经营稳定的企业\nC发展空间大，未来才能提升\n\n26-28.\n与其说应聘像是通过一项考试，倒不如说像是一个生动活泼的商业谈判。为什么？参加考试时，对你的沟通能力和技巧的挑战不大；而应聘是一个商业行为，只有出色的沟通能力才能确保你成功地把自己包装和销售出去。很多自视甚高的人，甚至已在职场打拼多年的人经常抱怨：“我为什么总是怀才不遇啊？”其实我们更应该问问自己：“我是一个别人感兴趣的人吗？我有没有能力让招聘官喜欢自己？”\n一家著名的4人广告公司到一所高校进行校园宣讲会。会后，大学生们投递的简历堆得像山一样。小张一直渴望能进这家广告公司，但看着一摞摞简历，她对自己能不能脱颖而出实在没有信心。她反复思考着，怎样才能在简历中突出一个广告人应有的素质——有创意呢？突然，她看见了校园里卖圣诞贺卡的小商店，不由灵光一闪：何不利用圣诞卡写简历？\n小张马上买下一张精美的音乐卡，在首页用漂亮——费最受欢迎的毕业生的钢笔字写下对招聘官的问候，翻开来，伴随着动听的音乐，呈现的是小张贴得工工整整的简历。小张亲自把这份”祝福简历”送到招聘官手里时，招聘官脸上流露出明显的惊讶和欣喜。就这样，小张进入了第一轮面试。\n沟通力说白了就是让别人了解你，进而接受你，更高境界就是让他们喜欢你，让他们觉得没有你不行。从应聘沟通来讲，一个明显的挑战是要在极短的时间里为自己树立一个自信可靠的形象。\n\nB谈判\nD她的简历很有创意\nC应聘中沟通的重要性"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第28课-最受欢迎的毕业生/index.html#书写",
    "href": "学汉语的日记/HSK5下-第28课-最受欢迎的毕业生/index.html#书写",
    "title": "HSK5下 | 第28课： 最受欢迎的毕业生",
    "section": "2.3. 书写",
    "text": "2.3. 书写\n前途、行业、体验、缺乏、显然。\n对于年轻人而言，选择一个有前途的行业至关重要。然而，在缺乏了解的情况下，贸然进入一个行业，显然会导致迷茫和挫折。因此，积累相关经验，获取行业体验就显得尤为必要。通过实践和体验，我们可以深入了解某个行业的现状、发展趋势和自身是否适合从事该行业"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第30课-竞争让市场更高效/index.html",
    "href": "学汉语的日记/HSK5下-第30课-竞争让市场更高效/index.html",
    "title": "HSK5下 | 第30课： 竞争让市场更高效",
    "section": "",
    "text": "竞争让市场更高效 - Competition makes the market more efficient\n西班牙人特别喜欢吃沙丁鱼。但沙丁鱼对离开大海后的环境极不适应，运输就成了问题。鱼上岸后，过不了多久就会死去。而死掉的沙丁鱼口感很差，作为商品销售，价格就会便宜很多。如果上岸时沙丁鱼还活着，鱼的卖价可以涨很多倍。为了延长沙丁鱼的存活期，减少经济损失，渔民们想了很多办法，但情况仍然没有得到太大的改善。\n后来一位渔民无意中发现了一种巧妙而实用的方法：把几条沙丁鱼的天敌鲇鱼放进装鱼的设备中。因为鲇鱼是食肉鱼，无法和沙丁鱼和平共处，它会四处游动寻找小鱼吃，对沙丁鱼构成威胁。为了逃避天敌，沙丁鱼自然会不断地加速 游动，从而保持了旺盛的生命力，存活的比例大大提高。看到这里，你有什么感想和体会呢？其实，这在经济学上被称为“鲇鱼效应”。\n鲇鱼效应对于市场经济以及现代企业管理都有着重要的启发作用。这个概念的核心是：一个市场如果能采取一种措施，刺激企业活跃起来，就能使企业获得足够的活力，在市场中积极参与竞争而不至于落后，同时这样反过来又能促使市场更为高效。\n从本质上说，“鲇鱼效应”使得企业和员工产生一种危机感，其实就是一种压力效应。\n很多研究发现，适度的压力有利于我们保持良好的状态，更加有助于挖掘我们的潜力，从而提高个人的工作效率。比如运动员每到参加比赛，尤其是决赛时，一定要将自己调整到接近最佳状态，让自己感到适度的压力，如果他不紧张、没压力感，则不利于出成绩。因此，“鲇鱼效应”的确对挖掘员工潜力、提高企业活力具有积极的意义\n\n\n\n\n\n\n\n\n\n鲇鱼 Catfish, photo credit to Wiki\n\n\n\n\n\n\n\n\n竞争让市场更高效\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n沙丁鱼\nshādīngyú - n. sardine\n\n\n2\n运输\nyùnshū - v. to transport, to convey\n\n\n3\n岸\nàn - n. bank (of a river, lake, etc.), shore, coast\n\n\n4\n商品\nshāngpǐn - n. goods, commodity\n\n\n5\n延长\nyán cháng - v. to prolong, to lengthen\n\n\n6\n存活\ncún huó - v. to survive, to exist\n\n\n7\n改善\ngǎi shàn - v. to improve, to make sth. better\n\n\n8\n无意\nwúyì - adv. accidentally, inadvertently\n\n\n9\n巧妙\nqiǎo miào - adj. ingenious, clever\n\n\n10\n实用\nshí yòng - adj. practical\n\n\n11\n天敌\ntiān dí - n. natural enemy\n\n\n12\n鲇鱼\nniányú - n. catfish\n\n\n13\n设备\nshè bèi - n. equipment, device\n\n\n14\n和平\nhépíng - adj. peaceful\n\n\n15\n构成\ngòu chéng - v. to compose, to form, to pose\n\n\n16\n逃避\ntáo bì - v. to escape, to evade\n\n\n17\n不断\nbú duàn - adv. continuously, unceasingly\n\n\n18\n旺盛\nwàng shèng - adj. exuberant, vibrant\n\n\n19\n比例\nbǐ lì - n. proportion, scale\n\n\n20\n感想\ngǎnxiǎng - n. impressions, thoughts\n\n\n21\n概念\ngàiniàn - n. concept, notion\n\n\n22\n核心\nhé xīn - n. core, kernel\n\n\n23\n刺激\ncì jī - v. to stimulate, to excite\n\n\n24\n活力\nhuólì - n. vigor, vitality\n\n\n25\n落后\nluò hòu - v. to fall behind, to lag behind\n\n\n26\n本质\nběn zhì - n. essence, nature, intrinsic quality\n\n\n27\n员工\nyuángōng - n. staff, employee\n\n\n28\n危机\nwēijī - n. crisis\n\n\n29\n有利\nyǒu lì - adj. beneficial, advantageous\n\n\n30\n挖掘\nwājué - v. to dig, to unearth\n\n\n31\n潜力\nqián lì - n. potential\n\n\n32\n决赛\njué sài - v. final, final match\n\n\n33\n接近\njiē jìn - v. to approach, to be close to\n\n\n34\n佳\njiā - adj. good, fine\n\n\n35\n的确\ndíquè - adv. indeed, really\n\n\n\n\n\n\n\n无意：\n“无意” , động từ, có nghĩa là không muốn, không có ý định. Ví dụ：\n\n他无意伤害任何人。\n我无意打扰您，不过我可以跟您谈一会儿吗？\n\n“无意”, có thể làm phó từ, có nghĩa là không cố ý, thường nói, “无意中·····”. Ví dụ:\n\n后来一位渔民无意中发现了一种巧妙而实用的方法······\n他在收拾花园时，无意地找到了这只耳环。\n\n有利：\nTính từ, có nghĩa là có lợi, có ích. Thường dùng “有利于” để biểu thị có lợi cho người hoặc vật nào đó. Phủ định là “不利”.\n\n高高的个子，漂亮的外表，都是他的有利条件。\n很多研究发现，适度的压力有利于我们保持良好的状态，······\n笑能促进心肺活动，改善肌肉紧张状况，对睡眠也是有利的。\n\n的确：\nPhó từ, ý nghĩa là hoàn toàn chính xác, chân thực. Có thể trùng điệp “的的确确”。Ví dụ:\n\n因此，“鮎鱼效应”的确对挖掘员工潜力，提高企业活力具有积极的意义。\n他的确我所教过的学生中最聪明的。\n咱们总裁选择李阳负责的的确确有些冒险，因为他太年轻了。\n\nPhân biệt 接近 và 靠近：\n共同点：Đều là động từ, đều có nghĩa là khoảng cách giữa hai đối tượng rất gần hoặc chuyển động về mục tiêu nhất định, làm khoảng cách giữa cả hai trở nên nhỏ, có lúc có thể dùng thay thế cho nhau. Ví dụ:\n\n这个地方接近/靠近北极地区，夏季白天很长，天亮得也很早。\n\n不同点：\n\n\n\n\n\n\n\n\n\n\n接近\n靠近\n\n\n\n\n1\nTừ được kết hợp có thể biểu thị người, sự vật, thời gian, địa điểm và số lượng cụ thể. Ví dụ\n\n接近下午一点时，救护车终于赶到了。\n\nTừ kết hợp có thể biểu thị người, sự vật, địa điểm cụ thể nhưng thông thường không thể dùng với thời gian, số lượng. Ví dụ:\n\n他们挤在靠近车窗的地方，脸对脸离得很近。\n\n\n\n2\nCòn có thể kết hợp với từ ngữ biểu thị sự vật trừu tượng. Ví dụ:\n\n经过努力，现在我们已越来越接近年初定下的销售目标了。\n\nThông thường không thể kết hợp với những từ ngữ biểu thị sự vật trừu tượng.\n\n\n3\nCòn biểu thị khoảng cách/ sự chênh lệch không lớn lắm. Ví dụ:\n\n他们俩的水平非常接近，这场比赛真不好说谁会赢。\n\nKhông có ý nghĩa này.\n\n\n\n\n\n\n问题：经济2\n\n经济2\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n出口\nchu1kou3 - to export (abroad)\n\n\n2\n进口\njin4kou3 - to import (from abroad)\n\n\n3\n贸易\nmao4yi4 - trading\n\n\n4\n谈判\ntan2pan4 - to negotiate\n\n\n5\n合同\nhe2tong - contract\n\n\n6\n中介\nzhong1jie4 - go-between, real estate agent\n\n\n7\n破产\npo4chan3 - bankruptcy\n\n\n8\n资金\nzi1jin1 - capital\n\n\n9\n利润\nli4run4 - profit\n\n\n10\n股票\ngu3piao4 - stock, shares\n\n\n11\n账户\nzhang4hu4 - account\n\n\n12\n利息\nli4xi1 - interest\n\n\n13\n贷款\ndai4kuan3 - loan\n\n\n14\n汇率\nhui4lv4 - exchange rate\n\n\n15\n押金\nya1jin1 - deposit\n\n\n\n\n\n\n竞争利与弊\n对我来说，竞争是一种有利有弊的状态。\n有利的是，竞争赋予个体压力和动力，提高学习和工作效率、客观地评价个人、提高水平，还能让集体更富有生气，丰富生活、增添学习和生活的乐趣。\n有弊的是，竞争使某些成功者滋长骄傲情绪，使某些失败者产生自卑感，引起心情的过分紧张和焦虑、产生忌妒心理。竞争使人更匆忙地做出决定，那些仓促决策的结果有时是可观的，但如果不是这样的，则可能会造成糟糕的后果。\n我认为竞争是社会个体之间自然形成的行为，是不可避免的。然而，如果我们保持竞争的健康，我相信竞争始终是利多弊少的。\n🚀 Từ mới: 赋予 - fu4yu3 - entrust/confer; 集体 - ji2ti3 - collective (i.e 个人); 富有 - fu4you3 - be rich in/be full of; 增添 - zeng1tian - add/increase; 滋长 - zi1zh ang3 - grow/develop; 自卑感 - zi4bei1gan3 - sense of inferiority; 焦虑 - jiao1lv4 - anxiety; 忌妒 (or 嫉妒) - ji4du - jealousy; 匆忙 - cong1mang2; 仓促 - cang1cu4 - hasty/hurried; 决策 - jue2ce4 - policy decision.\n\n\n\n1. 请列举一些生活中的实例，说明竞争给我们带来的好处？\n竞争让市场更高效，的确如此。 举个例子，智能手机的繁荣(fán róng - prosperous)始于2010年。在过去的十年里，市场规模迅速(xùn sù - rapid)增长，智能手机的智能水平也相应提高。一些公司，比如苹果(Apple)、三星(Samsung)，即使是在这一时期成立的，比如小米(Xiaomi)，华为(Huawei)，都已成为该行业的巨头(ju4tou2 - tycoon)。另一方面，没有竞争的公司已经从市场上消失了，比如Nokia（诺基亚）、Sony（索尼）、HTC。\n这个市场的严酷(yán kù - hash)之处在于，用户的口味变化非常快，公司必须灵活地赢得用户的喜爱。此外，由于它是“智能”手机，用户无法自行(zì xíng - self imposed)定义(dìng yì - define)智能功能，公司需要研究和开发这些功能来将其集成(jí chéng - intergrate)到产品中。任何有能力创造趋势(qū sh - trend)的公司都将具有巨大的竞争优势。例如，苹果的虚拟(xū nǐ - fictitious)助理(zhù lǐ - assistant)是最好的、三星的人工智能功能是最先进的、华为是第一家拥有卫星(wèi xīng - satellite)电话功能的公司、小米的摄像头(shè xiàng tóu - camera)非常受欢迎。在价格的方面上，公司还必须优化产品的开发和生产，以降低成本。\n结果，我们现在拥有一个技术快速发展、型号多样、价格具有竞争力的智能手机市场，对消费者利多弊少。\n2. 你有过在竞争中失败的经历吗？说说他对你有何影响？\n我有过很多竞争中失败的经历了，但最让我想起的是我上高中的时候。 在那之前，我一直是家乡初中学的顶尖(dǐng jiān - top tier)学生。直到上了高中，我很幸运地上了本省高中的专数学班，我才真正感受到来自同学们的压力。班主任会挑选(tiāo xuǎn - pick)一批优秀的学生培养，一两年后组成学校团队参加全国比赛。因此，我们班每个人的最高目标是，成为这群优秀学生中的一员。\n因为好胜，我也参加了这场比赛。但这并不像我想象的那么容易。我班的大多数同学都是从初中数学专学的，他们的数学背景和思维都比我好。此外，因为和几个朋友住在宿舍里，我很快意识到他们的努力对我有压倒性(yā dǎo xìnɡ - overwhelming)。一些重要考试的糟糕成绩使我被淘汰，非常失望。\n在那次失败之后，我意识到我没有足够的逻辑智慧(zhì huì)来解决困难的数学问题。我擅长的是记忆，将记忆中的知识联系起解决问题。在数学上竞争不了，我把目标转移到大学考试。这个群体也有竞争，但压力比较少。我们不会相互排斥(pái chì - exclude)，而是支持共同发展。\n最后我考上了很著名的大学，在一定程度上仍然擅长数学。另外，我现在仍然在工作中使用数学。那失败让我看到了自己的优势和劣势，选择一条适合自己发展的道路。\n3. 如果竞争是不可避免的，你认为应该如何面对？\n如果必须面对竞争，首先，要树立(shù lì - establish)积极乐观的心态。竞争不可避免，但并非洪水猛兽(hóng shuǐ měng shòu - A fierce beast in floods)。只要我们保持积极乐观的心态，将竞争视为挑战和机遇(jī yù - opportunity)，就能在竞争中不断激励自己，超越自我。\n其次，要明确目标，制定计划。清楚目标让我找到前进的方向和动力。只有树立合适的目标，才能明确努力的方向，集中精力，全力以赴(quán lì yǐ fù - go to all lengths)。制定科学合理的计划，并脚踏实地(jiǎo tà shí dì - down-to-earth)地执行(zhí xíng - implement)，才能逐步实现目标。\n最后，要不断学习，提升自身实力。竞争的关键在于实力的强弱。只有不断提升自己的知识、技能、经验和能力，才能获胜。要善于学习新知识、新技能，不断提高自身的综合素质(zōnɡ hé sù zhì - comprehensive quality)。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第30课-竞争让市场更高效/index.html#生词",
    "href": "学汉语的日记/HSK5下-第30课-竞争让市场更高效/index.html#生词",
    "title": "HSK5下 | 第30课： 竞争让市场更高效",
    "section": "",
    "text": "竞争让市场更高效\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n沙丁鱼\nshādīngyú - n. sardine\n\n\n2\n运输\nyùnshū - v. to transport, to convey\n\n\n3\n岸\nàn - n. bank (of a river, lake, etc.), shore, coast\n\n\n4\n商品\nshāngpǐn - n. goods, commodity\n\n\n5\n延长\nyán cháng - v. to prolong, to lengthen\n\n\n6\n存活\ncún huó - v. to survive, to exist\n\n\n7\n改善\ngǎi shàn - v. to improve, to make sth. better\n\n\n8\n无意\nwúyì - adv. accidentally, inadvertently\n\n\n9\n巧妙\nqiǎo miào - adj. ingenious, clever\n\n\n10\n实用\nshí yòng - adj. practical\n\n\n11\n天敌\ntiān dí - n. natural enemy\n\n\n12\n鲇鱼\nniányú - n. catfish\n\n\n13\n设备\nshè bèi - n. equipment, device\n\n\n14\n和平\nhépíng - adj. peaceful\n\n\n15\n构成\ngòu chéng - v. to compose, to form, to pose\n\n\n16\n逃避\ntáo bì - v. to escape, to evade\n\n\n17\n不断\nbú duàn - adv. continuously, unceasingly\n\n\n18\n旺盛\nwàng shèng - adj. exuberant, vibrant\n\n\n19\n比例\nbǐ lì - n. proportion, scale\n\n\n20\n感想\ngǎnxiǎng - n. impressions, thoughts\n\n\n21\n概念\ngàiniàn - n. concept, notion\n\n\n22\n核心\nhé xīn - n. core, kernel\n\n\n23\n刺激\ncì jī - v. to stimulate, to excite\n\n\n24\n活力\nhuólì - n. vigor, vitality\n\n\n25\n落后\nluò hòu - v. to fall behind, to lag behind\n\n\n26\n本质\nběn zhì - n. essence, nature, intrinsic quality\n\n\n27\n员工\nyuángōng - n. staff, employee\n\n\n28\n危机\nwēijī - n. crisis\n\n\n29\n有利\nyǒu lì - adj. beneficial, advantageous\n\n\n30\n挖掘\nwājué - v. to dig, to unearth\n\n\n31\n潜力\nqián lì - n. potential\n\n\n32\n决赛\njué sài - v. final, final match\n\n\n33\n接近\njiē jìn - v. to approach, to be close to\n\n\n34\n佳\njiā - adj. good, fine\n\n\n35\n的确\ndíquè - adv. indeed, really"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第30课-竞争让市场更高效/index.html#注释",
    "href": "学汉语的日记/HSK5下-第30课-竞争让市场更高效/index.html#注释",
    "title": "HSK5下 | 第30课： 竞争让市场更高效",
    "section": "",
    "text": "无意：\n“无意” , động từ, có nghĩa là không muốn, không có ý định. Ví dụ：\n\n他无意伤害任何人。\n我无意打扰您，不过我可以跟您谈一会儿吗？\n\n“无意”, có thể làm phó từ, có nghĩa là không cố ý, thường nói, “无意中·····”. Ví dụ:\n\n后来一位渔民无意中发现了一种巧妙而实用的方法······\n他在收拾花园时，无意地找到了这只耳环。\n\n有利：\nTính từ, có nghĩa là có lợi, có ích. Thường dùng “有利于” để biểu thị có lợi cho người hoặc vật nào đó. Phủ định là “不利”.\n\n高高的个子，漂亮的外表，都是他的有利条件。\n很多研究发现，适度的压力有利于我们保持良好的状态，······\n笑能促进心肺活动，改善肌肉紧张状况，对睡眠也是有利的。\n\n的确：\nPhó từ, ý nghĩa là hoàn toàn chính xác, chân thực. Có thể trùng điệp “的的确确”。Ví dụ:\n\n因此，“鮎鱼效应”的确对挖掘员工潜力，提高企业活力具有积极的意义。\n他的确我所教过的学生中最聪明的。\n咱们总裁选择李阳负责的的确确有些冒险，因为他太年轻了。\n\nPhân biệt 接近 và 靠近：\n共同点：Đều là động từ, đều có nghĩa là khoảng cách giữa hai đối tượng rất gần hoặc chuyển động về mục tiêu nhất định, làm khoảng cách giữa cả hai trở nên nhỏ, có lúc có thể dùng thay thế cho nhau. Ví dụ:\n\n这个地方接近/靠近北极地区，夏季白天很长，天亮得也很早。\n\n不同点：\n\n\n\n\n\n\n\n\n\n\n接近\n靠近\n\n\n\n\n1\nTừ được kết hợp có thể biểu thị người, sự vật, thời gian, địa điểm và số lượng cụ thể. Ví dụ\n\n接近下午一点时，救护车终于赶到了。\n\nTừ kết hợp có thể biểu thị người, sự vật, địa điểm cụ thể nhưng thông thường không thể dùng với thời gian, số lượng. Ví dụ:\n\n他们挤在靠近车窗的地方，脸对脸离得很近。\n\n\n\n2\nCòn có thể kết hợp với từ ngữ biểu thị sự vật trừu tượng. Ví dụ:\n\n经过努力，现在我们已越来越接近年初定下的销售目标了。\n\nThông thường không thể kết hợp với những từ ngữ biểu thị sự vật trừu tượng.\n\n\n3\nCòn biểu thị khoảng cách/ sự chênh lệch không lớn lắm. Ví dụ:\n\n他们俩的水平非常接近，这场比赛真不好说谁会赢。\n\nKhông có ý nghĩa này."
  },
  {
    "objectID": "学汉语的日记/HSK5下-第30课-竞争让市场更高效/index.html#扩展",
    "href": "学汉语的日记/HSK5下-第30课-竞争让市场更高效/index.html#扩展",
    "title": "HSK5下 | 第30课： 竞争让市场更高效",
    "section": "",
    "text": "问题：经济2\n\n经济2\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n出口\nchu1kou3 - to export (abroad)\n\n\n2\n进口\njin4kou3 - to import (from abroad)\n\n\n3\n贸易\nmao4yi4 - trading\n\n\n4\n谈判\ntan2pan4 - to negotiate\n\n\n5\n合同\nhe2tong - contract\n\n\n6\n中介\nzhong1jie4 - go-between, real estate agent\n\n\n7\n破产\npo4chan3 - bankruptcy\n\n\n8\n资金\nzi1jin1 - capital\n\n\n9\n利润\nli4run4 - profit\n\n\n10\n股票\ngu3piao4 - stock, shares\n\n\n11\n账户\nzhang4hu4 - account\n\n\n12\n利息\nli4xi1 - interest\n\n\n13\n贷款\ndai4kuan3 - loan\n\n\n14\n汇率\nhui4lv4 - exchange rate\n\n\n15\n押金\nya1jin1 - deposit"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第30课-竞争让市场更高效/index.html#运用",
    "href": "学汉语的日记/HSK5下-第30课-竞争让市场更高效/index.html#运用",
    "title": "HSK5下 | 第30课： 竞争让市场更高效",
    "section": "",
    "text": "竞争利与弊\n对我来说，竞争是一种有利有弊的状态。\n有利的是，竞争赋予个体压力和动力，提高学习和工作效率、客观地评价个人、提高水平，还能让集体更富有生气，丰富生活、增添学习和生活的乐趣。\n有弊的是，竞争使某些成功者滋长骄傲情绪，使某些失败者产生自卑感，引起心情的过分紧张和焦虑、产生忌妒心理。竞争使人更匆忙地做出决定，那些仓促决策的结果有时是可观的，但如果不是这样的，则可能会造成糟糕的后果。\n我认为竞争是社会个体之间自然形成的行为，是不可避免的。然而，如果我们保持竞争的健康，我相信竞争始终是利多弊少的。\n🚀 Từ mới: 赋予 - fu4yu3 - entrust/confer; 集体 - ji2ti3 - collective (i.e 个人); 富有 - fu4you3 - be rich in/be full of; 增添 - zeng1tian - add/increase; 滋长 - zi1zh ang3 - grow/develop; 自卑感 - zi4bei1gan3 - sense of inferiority; 焦虑 - jiao1lv4 - anxiety; 忌妒 (or 嫉妒) - ji4du - jealousy; 匆忙 - cong1mang2; 仓促 - cang1cu4 - hasty/hurried; 决策 - jue2ce4 - policy decision."
  },
  {
    "objectID": "学汉语的日记/HSK5下-第30课-竞争让市场更高效/index.html#口语",
    "href": "学汉语的日记/HSK5下-第30课-竞争让市场更高效/index.html#口语",
    "title": "HSK5下 | 第30课： 竞争让市场更高效",
    "section": "",
    "text": "1. 请列举一些生活中的实例，说明竞争给我们带来的好处？\n竞争让市场更高效，的确如此。 举个例子，智能手机的繁荣(fán róng - prosperous)始于2010年。在过去的十年里，市场规模迅速(xùn sù - rapid)增长，智能手机的智能水平也相应提高。一些公司，比如苹果(Apple)、三星(Samsung)，即使是在这一时期成立的，比如小米(Xiaomi)，华为(Huawei)，都已成为该行业的巨头(ju4tou2 - tycoon)。另一方面，没有竞争的公司已经从市场上消失了，比如Nokia（诺基亚）、Sony（索尼）、HTC。\n这个市场的严酷(yán kù - hash)之处在于，用户的口味变化非常快，公司必须灵活地赢得用户的喜爱。此外，由于它是“智能”手机，用户无法自行(zì xíng - self imposed)定义(dìng yì - define)智能功能，公司需要研究和开发这些功能来将其集成(jí chéng - intergrate)到产品中。任何有能力创造趋势(qū sh - trend)的公司都将具有巨大的竞争优势。例如，苹果的虚拟(xū nǐ - fictitious)助理(zhù lǐ - assistant)是最好的、三星的人工智能功能是最先进的、华为是第一家拥有卫星(wèi xīng - satellite)电话功能的公司、小米的摄像头(shè xiàng tóu - camera)非常受欢迎。在价格的方面上，公司还必须优化产品的开发和生产，以降低成本。\n结果，我们现在拥有一个技术快速发展、型号多样、价格具有竞争力的智能手机市场，对消费者利多弊少。\n2. 你有过在竞争中失败的经历吗？说说他对你有何影响？\n我有过很多竞争中失败的经历了，但最让我想起的是我上高中的时候。 在那之前，我一直是家乡初中学的顶尖(dǐng jiān - top tier)学生。直到上了高中，我很幸运地上了本省高中的专数学班，我才真正感受到来自同学们的压力。班主任会挑选(tiāo xuǎn - pick)一批优秀的学生培养，一两年后组成学校团队参加全国比赛。因此，我们班每个人的最高目标是，成为这群优秀学生中的一员。\n因为好胜，我也参加了这场比赛。但这并不像我想象的那么容易。我班的大多数同学都是从初中数学专学的，他们的数学背景和思维都比我好。此外，因为和几个朋友住在宿舍里，我很快意识到他们的努力对我有压倒性(yā dǎo xìnɡ - overwhelming)。一些重要考试的糟糕成绩使我被淘汰，非常失望。\n在那次失败之后，我意识到我没有足够的逻辑智慧(zhì huì)来解决困难的数学问题。我擅长的是记忆，将记忆中的知识联系起解决问题。在数学上竞争不了，我把目标转移到大学考试。这个群体也有竞争，但压力比较少。我们不会相互排斥(pái chì - exclude)，而是支持共同发展。\n最后我考上了很著名的大学，在一定程度上仍然擅长数学。另外，我现在仍然在工作中使用数学。那失败让我看到了自己的优势和劣势，选择一条适合自己发展的道路。\n3. 如果竞争是不可避免的，你认为应该如何面对？\n如果必须面对竞争，首先，要树立(shù lì - establish)积极乐观的心态。竞争不可避免，但并非洪水猛兽(hóng shuǐ měng shòu - A fierce beast in floods)。只要我们保持积极乐观的心态，将竞争视为挑战和机遇(jī yù - opportunity)，就能在竞争中不断激励自己，超越自我。\n其次，要明确目标，制定计划。清楚目标让我找到前进的方向和动力。只有树立合适的目标，才能明确努力的方向，集中精力，全力以赴(quán lì yǐ fù - go to all lengths)。制定科学合理的计划，并脚踏实地(jiǎo tà shí dì - down-to-earth)地执行(zhí xíng - implement)，才能逐步实现目标。\n最后，要不断学习，提升自身实力。竞争的关键在于实力的强弱。只有不断提升自己的知识、技能、经验和能力，才能获胜。要善于学习新知识、新技能，不断提高自身的综合素质(zōnɡ hé sù zhì - comprehensive quality)。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第30课-竞争让市场更高效/index.html#听力",
    "href": "学汉语的日记/HSK5下-第30课-竞争让市场更高效/index.html#听力",
    "title": "HSK5下 | 第30课： 竞争让市场更高效",
    "section": "2.1. 听力",
    "text": "2.1. 听力\n\n女：这么多家卖沙丁鱼的店，怎么只有那家特别贵？\n男：只有他们知道怎么延长沙丁鱼的存活期。\n问：从对话中可以知道什么？（A让沙丁鱼存活很困难）\n男：你昨天的飞机几点到的？\n女：别提了，快12点才到，晚点了4个小时。\n问：飞机本来应该几点到？（B快八点）\n女：听说对方有个队员非常强，你有把握吗？\n男：我不觉得他可以对我构成威胁。\n问：男的是什么意思？（D对手不如他）\n男：你们今年不招聘男老师吗？\n女：我们是理工科学校，男老师占的比例太大了。\n问：女的是什么意思？（C他们希望那女比例平衡）\n女：你觉得小张能办好这件事吗？\n男：我听他说了他的想法，我觉得的确很巧妙。\n问：男的是什么态度？（A信任）\n男：那里是山区，条件那么差，你怎么会想到要去支教呢？\n女：正因为那里是落后地区，才需要大家去建设。\n问：关于山区，下列哪项正确？（C非常落后）\n女：我已经两个月没有跟他见面了，也不接他的电话。\n男：你总这么逃避也不是办法。\n女：可是我不知道该怎么跟他说……\n男：不知道怎么说也得说啊，时间越久越麻烦。\n问：男的是什么意思？（C要面对这个问题）\n男：来中国一年多了，你有什么感想？\n女：我最大的体会就是中国太大了，要学的、要看的、要吃的太多了，只待两年远远不够。\n男：那你打算延长留学时间吗？\n女：我正在考虑这个问题。\n问：关于女的，下列哪项正确？（B她正在中国学习）\n女：这批来应聘的人，你对谁比较满意？\n男：我觉得那位刘先生不错，很会做商业谈判。\n女：但他原来是做进出口贸易的，跟我们离得有点儿远，年龄也大了些。\n男：专业知识不是问题，可以学嘛。\n问：刘先生的优势是什么？（C善于商业谈判）\n男：最近股票市场很火，你不打算试试吗？(stock market)\n女：股市风险太大了。\n男：现在形势好，我们楼里的大爷大妈都赚钱了。\n女：正因为连大爷大妈都开始炒股了，我才害怕呢。(chǎo ɡǔ - stock trading)\n问：女的对炒股是什么态度？（B反对）\n女：爸爸，今天生物课上，老师教了我们达尔文的“自然选择学说”。 (natural selection theory)\n男：是吗？那你说说是什么意思。\n女：简单地说，“自然选择”就是生物之间存在着竞争，适应者才能够生存下来，不适应的则会被淘汰。\n男：说得真清楚！\n女：我们人类也是通过这样的竞争，躲开(duǒ kai - dodge)了很多天敌的威胁，才生存到今天的。\n男：是的。不过，自然选择的范围也没有当年达尔文所说的那么广泛。以后这些你也会慢慢学到的。(Darwin)\n“自然选择学说”是谁提出来的？（C达尔文）\n根据这段对话，下列哪项正确？（D动物之间存在着竞争）\n一只兔子(tù zi - rabbit)被猎人(liè rén - hunter)打伤了，但它竟然带着枪伤(qiāng shāng - gunshot injury)成功地逃过了猎狗的追捕，回到了家里。同伴们都围过来惊讶地问它：“那只猎狗很凶呀，你又带着伤，是怎么甩掉它的呢？”兔子说：“它是跑得快，但我用尽了全力呀！它没追上我，最多挨一顿骂，而我如果不用尽全力地跑，就没命了！”每个人都有很大的潜能。正如心理学家所指出的，一般人的潜能只开发了百分之2到百分之8左右。这就是说，我们还有百分之90多的潜能处于沉睡状态。谁要想成功，必须用尽全力才行。\n谁被打伤了？（C兔子）\n兔子为什么可以逃跑？（D他用尽全力地跑）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第30课-竞争让市场更高效/index.html#阅读",
    "href": "学汉语的日记/HSK5下-第30课-竞争让市场更高效/index.html#阅读",
    "title": "HSK5下 | 第30课： 竞争让市场更高效",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n1888年，美国银行家莫尔Moore当选副总统。可是，在当银行家、当总统以前，他曾经只是个卖布的小商人。莫尔说：“我做布匹生意真的很成功。可有一天，我读了一本文学家爱默尔Emerson的书，书中的一段话深深地15刺激了我。它的16核心意思是：一个人如果拥有一种人家需要的才能和特长，那么，不管他处在什么环境、什么角落，17终有一天会被人发现。这段话让我怦然心动，我觉得自己应该走向更广阔的空间去发展。这使我想到了当时最重要的金融业，于是，我不顾别人的反对，放弃布匹生意，改营银行。我18不断地努力，在稳妥可靠的条件下进行运作，许多人和企业都愿意找我，因此我经营银行十分成功，最终成为金融巨头。”\n\n\n\n很多研究发现，适度的压力有利于我们保持良好的状态，更加有助于挖掘我们的潜力，从而提高个人的工作效率。比如运动员每到参加比赛，尤其是决赛时，一定要将自己凋整到接近最佳状态，感到适度的压力，让自己兴奋，如果他不紧张、没压力感，则不利于出成绩。适度的压力对挖掘自身的潜力，是有正面意义的。（B给人的压力应适度）\n\n\n\n美国有个43岁的妇女，为她姨妈向政府申请到了一个免责的轮椅。她所做的不过是准备了一些必要的文件，并填写了一些表格。为此，她写了一篇如何向政府申请免费轮椅的报告。然后，她在网上卖她的报告，售价仅仅2美元，后来她每月可赚3万!简直难以置信，这么简单的事会有市场，会有如此的潜在利益! （B申请免费轮椅的手续非常简单）\n\n\n\n一家森林公园曾养了几百只梅花鹿。尽管环境幽静，水草丰美，又没有天敌，但几年以后，鹿群非但没有发展，反而病的病，死的死，竟然出现了负增长。后来公园买回几只狼放置在园内。在狼的追赶捕食下，鹿群只得紧张地奔跑逃命。这祥一来，除了那些老弱病残者被狼捕食外，其他鹿的体质日益增强，数量也迅速地增长起来。（D狼的追赶使梅花鹿体质增强）\n\n\n\n市场竞争是指商品生产者或者商品经营者为争夺有利的生产或流通条件、地位而进行的斗争。竟争是商品经济的一般规律，它是商品本身内在矛盾的产物，只要存在商品生产和商品交换，竟争规律就起作用。商品经济还有一个重要的价值规律。价值规律存在，必然产生竟争;同时，价值规律的作用，也只有在竟争中才能实现。（C只要有商品生产和交换，就有竞争）\n23-25.\n有一句老话，叫“一个和尚挑水吃，两个和尚抬水吃，三个和尚没水吃”。如今，这个观点过时了。现在的观点是“一个和尚没水吃，三个和尚水多得吃不完”。\n有三个庙，这三个庙离河边都比较远。怎么角旱决吃水问题呢?第一个庙，由于挑水的路比较长，如果一个人挑一会儿就累了。于是三个和尚商量，咱们来个接力\n赛吧，每人挑一段路。第一个和尚从河边挑到半路停下来休息，第二个和尚继续挑，之后再转给第三个和尚，挑到缸里灌进去，空桶回来再接着挑，大家都不累，水很快就挑满了。这是协作的办法，也叫“机制创新”。\n第二个庙，老和尚把三个徒弟都叫来，说我们立下了新的庙规，要引进竟争机制。三个和尚都去挑水，谁水挑得多，晚上吃饭加一道莱;谁水挑得少，吃白饭，没菜。三个和尚拼命去挑，一会儿水就挑满了。这个办法叫“管理创新”。\n第三个庙，三个小和尚商量，天天挑水太累，咱们想想办法。山上有竹子，把竹子砍下来连在一起，竹子中心是空的，然后买一个辊鲈。第一个和尚把一桶水摇上去，第二个和尚专管倒水，第三个和尚在地上休息。三个人轮流换班，一会儿水就灌满了。这叫“技术创新”。\n23。第一个庙用的办法是：C三个人接力挑水\n24。第二个庙的“管理者”是：A老和尚\n25。第三个庙跟前两个的区别是：D使用了新的工具\n26-28.\n世界著名的立顿公司为了使自己的产品迅速打进市场，在开业伊始别出心裁地举办了一次精彩的表演。他们买来几头小猪，用绸带给它们精心打扮，并挂上写有“立屯页家的孤儿”的横幅，然后赶着它们穿过闹市，引起众人的注意，达到了让商品家喻户晓的目的。\n做广告通常需要花重金，但若匠心独运，也能四两拨千斤，用最少的钱让广告有声有色。茶叶公司与猪，风马牛不相及，但经公司公关人员策划，小猪成了促销功臣，企业也借此（thereby）腾飞（fly swiftly upward）。\n相比之下，我们有些企业至今仍固守着传统的营销模式，促销方式习愤跟着感觉走，以致推出的促销方式不是步人后尘，就是偏离了市场，结果总是感到竟争激烈，生意难做。\n在当今的市场竟争中，除了商品质量和销售价格的竟争之外，营销策略也是一种竞争手段。如何以较少的投人获得轰动效果，已成为许多商家参与竟争、吸引顾客的又一热点。聪明的经营者不妨从立顿公司促销的成功经验中寻找一些启示，针对不同层次的消费需求，搞一些别出心裁的促销妙招，从而达到迅速销售的目的。\n26。立顿公司为什么要买小猪?： C为了宣传他们的产品\n27。根据上文，立顿公司主营的商品应该是：D茶叶\n28。上文主要想告诉我们：C促销方式要有特点"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第30课-竞争让市场更高效/index.html#书写",
    "href": "学汉语的日记/HSK5下-第30课-竞争让市场更高效/index.html#书写",
    "title": "HSK5下 | 第30课： 竞争让市场更高效",
    "section": "2.3. 书写",
    "text": "2.3. 书写\n写短文：决赛、感想、刺激、延长、落后\n上周末的决赛给我内心留下了无比激动的感想。球员们全力以赴，为观众献上了一场精彩绝伦的比赛。90分钟后，评分*使比赛延长到加时赛，双方依然势均力敌，比分始终焦灼。落后的队伍没有放弃，在最后时刻逆袭获胜，令全场观众沸腾。这场比赛不仅让人们看到了竞技体育的魅力，也刺激着我们在面对困难时永不放弃、勇往直前。\n🚀Có thể sử dụng: 扳平的比分"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第32课-身边的环保/index.html",
    "href": "学汉语的日记/HSK5下-第32课-身边的环保/index.html",
    "title": "HSK5下 | 第32课： 身边的环保",
    "section": "",
    "text": "身边的环保 - Protecting the environment around us\n如果你觉得地球上的一点儿小污染没什么关系，那你可就大错特错了！环境污染会危害动物、植物以及人类自身。\n有些生活在过去的动物，你今天再也看不到了，植物也面临着同样的危险。动植物的消失，部分原因是由于自然界的变化，比如洪水、地震等改变了它们生活的环境，但更大的原因则是人类对自然的破坏——有些地区的森林已经几乎被人砍光了，很多河流被污染，不再适合鱼类生存，有的地区原本是草原，如今已变为沙漠……从国际环保组织公布的数据可知，地球上一半以上的动植物正在消失，这是真实的情况，一点儿也不夸张。\n人类自身也饱受污染的危害。有些地区地表水已污染，地下水又被过量使用，水资源短缺问题就连科学家们也不知道该如何解決，目前世界上有17%的人无法享用干净的饮用水，而每年死于与空气污染有关的疾病的人比死于车祸的还要多。这些数据确实令人不安。\n一部分环境污染是由工业农业生产活动造成的，例如，大型工厂生产过程中，有的会产生大量废水；有的要大量燃烧煤炭，从而产生大量废气和废物。还有一部分污染和我们的日常生活密切相关，汽车尾气就是其中之一。此外，垃圾也会对环境造成严重的损害。\n幸运的是，越来越多的人敏感地认识到了环境问题的严重，并自觉地投入到了保护地球的行动中。生产中，增加环保设施，减少污染物排放，调整能源消费结构，逐步向可再生能源转变；而在日常生活中，政变生活习惯，尽量减少生活垃圾，做到垃圾分类。同时，尽量多骑自行车，多选择公共交通，少使用私家车。这些为此付出努力的人们令人尊敬，取得的成绩也令人鼓舞。\n地球是人类共同的家园，我们应该把它看作一个属于自己的大房间。房间脏了，消极的逃避和不符合实际的幻想都不能解决问题，为保持它的卫生每一个人都应付出行动，做出贡献。人类的命远由我们自己掌握，政变要靠我们自己。\n\n\n\n\n\n\n\n\n\nVolunteers clean up nation on Earth Day, photo credit to vietnamnews\n\n\n\n\n\n\n\n\n身边的环保\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n消失\nxiāoshī - v. to disappear, to vanish\n\n\n2\n洪水\nhóngshuǐ - n. flood\n\n\n3\n地震\ndìzhèn - v. earthquake\n\n\n4\n破坏\npò huài - v. to destroy, to damage\n\n\n5\n砍\nkǎn - v. to cut, to chop, to fell\n\n\n6\n生存\nshēng cún - v. to live, to subsist\n\n\n7\n沙漠\nshāmò - n. desert\n\n\n8\n公布\ngōng bù - v. to announce, to make public\n\n\n9\n数据\nshù jù - n. data\n\n\n10\n真实\nzhēn shí - adj. real, actual\n\n\n11\n夸张\nkuā zhāng - adj. exaggerated\n\n\n12\n资源\nzī yuán - n. resource\n\n\n13\n车祸\nchēhuò - n. traffic accident\n\n\n14\n不安\nbù’ān - adj. upset, disturbed\n\n\n15\n工业\ngōng yè - n. industry\n\n\n16\n农业\nnóng yè - n. agriculture\n\n\n17\n生产\nshēng chǎn - v. to produce, to manufacture\n\n\n18\n大型\ndàxíng - adj. large-scale\n\n\n19\n工厂\ngōng chǎng - n. factory\n\n\n20\n废\nfèi - adj. waste, useless\n\n\n21\n燃烧\nrán shāo - v. to burn, to combust\n\n\n22\n煤炭\nméi tàn - n. coal\n\n\n23\n密切\nmì qiè - adj. close, intimate\n\n\n24\n尾气\nwěi qì - n. exhaust gas, vehicle emission\n\n\n25\n幸运\nxìng yùn - adj. lucky, fortunate\n\n\n26\n敏感\nmǐn gǎn - adj. sensitive, susceptible\n\n\n27\n自觉\nzì jué - adj. conscious, on one’s own initiative\n\n\n28\n设施\nshè shī - n. installation, facilities\n\n\n29\n能源\nnéngyuán - n. energy resource\n\n\n30\n逐步\nzhú bù - adv. gradually, step by step\n\n\n31\n尽量\njǐnliàng - adv. to the best of one’s abilities, to the greatest extent\n\n\n32\n私（人）\nsī (rén) - n. private\n\n\n33\n尊敬\nzūnjìng - v. to respect, to esteem\n\n\n34\n鼓舞\ngǔ wǔ - v. to encourage, to inspire\n\n\n35\n消极\nxiāo jí - adj. passive, inactive\n\n\n36\n幻想\nhuàn xiǎng - n. fantasy, illusion\n\n\n37\n贡献\ngòng xiàn - n./v. contribution; to contribute, to devote\n\n\n38\n命运\nmìng yùn - n. fate, destiny\n\n\n39\n掌握\nzhǎng wò - v. to take charge of, to control\n\n\n\n\n\n\n\n密切\n“密切”, tính từ, có thể biểu thị quan hệ gần gũi, mật thiết. Ví dụ:\n\n还有一部分污染和我们的日常生活密切相关，汽车尾气就是其中之一。\n参加了这次环保活动后，两人便有了共同语言，来往也比先前密切了。\n\n“密切” , còn có thể biểu thị（đối với các vấn đề）chú trọng, tỉ mỉ, thấu đáo. Ví dụ:\n\n刘医生密切地观察着李妈妈病情的发展。\n家长应和老师密切配合，形成合力，保持教育的一致性。\n\n“密切” ,còn có thể làm động từ, có nghĩa làm cho quan hệ trở nên gần gũi mật thiết hơn. Ví dụ:\n\n这条铁路的建成，大大密切了西南地区与首都的关系。\n友好城市之间的交往密切了两国人民之间的友谊。\n\n\n（1）。。。需要密切地控制。\n（2）。。。密切了各国之间的贸易关系。\n（3）。。。对，我们的关系现在不太密切。\n\n尽量\nphó từ, biểu thị trong một phạm vi nhất định nỗ lực đạt đến mức độ cao nhất. Ví dụ:\n\n同时，尽量多骑自行车，多选择公共交通，少使用私家车。\n老年人要尽量少吃油炸食品。\n为了节约能源，请大家都尽量使用节能电器。\n\n\n（1）。。。父母尽量让孩子学会做好自己的事情。\n（2）。。。但我会尽量完成工作，早早回家。\n（3）。。。你尽量做吧，答不上以两个问题无所谓。\n\n逐步\nbiểu thị từng bước từng bước một, dùng trong những tình huống do còn người làm ra, thông thường không thể bổ sung cho từ ngữ có tính chất là tính từ. Ví dụ:\n\n云计算应用市场规模正在逐步扩大。\n·····调整能源消费结构，逐步向可再生能源转变。\n记者了解到，现在受灾群众已逐步恢复了正常的生产生活。\n\n\n（1）。。。平均温度将逐步升高。\n（2）。。。逐步成为日常的习惯。\n（3）。。。对的，学费会逐步降低。\n\nPhân biệt 鼓励 và 鼓舞:\n\n共同点：Đều là động từ, đều có ý nghĩa làm cho người khác phấn khới, tăng thêm tự tin. Ví dụ:\n\n这次谈话，使刘洋受到极大的鼓励/鼓舞。\n\n不同点：\n\n\n\n\n\n\n\n\n\n鼓励\n鼓舞\n\n\n\n\n1\nTừ trung tính, có thể dùng ở những phương diện không tốt.\n如：吸烟有害健康，你不阻止他，怎么还鼓励呢？\nTừ mang nghĩa tốt\n如：新产品的研制成功极大地鼓舞了科技人员。\n\n\n2\nNgữ nghĩa thiên về khích lệ đối phương tham gia vào một hoạt động nào đó. Chủ ngữ thường là người hoặc tổ chức.\nThường dùng hình thức kiêm ngữ “鼓励某人做某事”\n如：近些年，国家越来越鼓励大学生毕业后开办自己的公司。\nNgữ nghĩa thiên về tinh thần phấn chấn do sự ảnh hưởng nào đó. Chủ ngữ thường là sự vật\n如：新的胜利给了全体队员很大的鼓舞。\n\n\n3\nKhông có ý nghĩa và cách dùng này.\nCó thể làm tính từ, mô tả sự hưng phấn, phấn khởi.\n如：年初制定的目标顺利实现，取得的成绩令人十分鼓舞。\n\n\n\n\n\n\n问题：资源\n\n资源\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n金属\njin1shu3 - kim loại metal\n\n\n2\n黄金\nhuang2jin1 - vàng, gold\n\n\n3\n银\nyin2 - bạc, silver\n\n\n4\n钢铁\ngang1tie3 - sắt thép, steel\n\n\n5\n煤炭\nmei2tan4 - coal, than đá\n\n\n6\n能源\nneng2yuan2 - năng lượng, energy\n\n\n7\n原料\nyuan2liao4 - nguyên liệu thô, raw material\n\n\n8\n资源\nzi1yuan2 - tài nguyên, resource\n\n\n\n\n\n\n环保，可以这样开始\n每个人都是地球的守护者，每个人都可以为保护环境贡献自己的力量。我们不需要做惊天动地的大事，只要从身边小事做起，也能为地球带来巨大的改变。比如：节约用水—洗漱时关闭水龙头，洗澡时缩短时间，尽量使用节水龙头；节约用电—离开房间时关灯，使用节能电器，拔掉不使用的插头；减少浪费—购物时自带购物袋，尽量少使用一次性塑料制品，选择可再使用的物品；低碳出行—尽量步行、骑自行车或乘坐公共交通工具，减少开车；植树绿化—在家里或社区种植树木，净化空气，美化环境；宣传环保理念—积极参与环保活动，让更多人加入到保护环境的队伍。\n\n\n\n\n说说你每天从身边观察到不环保的行为。\n\n我住在胡志明市—一座越南人口最稠密的城市。越南也是一个发展中国家，城市周围都是大型工业区。因此，我很容易从日常生活中看到不环保的行为。\n每天上班的时候，我都要使用摩托车，这种出行方式在我国非常流行。这种类型的私人车辆只能搭载一到两个人，但产生的排放量很大。减少交通拥堵或限制排放的效果都不如公共交通工具。但这里的公共交通不发达，非常不方便，所以我和很多人一样，每天都在骑摩托车。\n路上我可以看到街道两边很多垃圾。人们随手乱扔垃圾，有的甚至将垃圾丢弃在河道、湖泊等水域中，严重污染了环境。我会买早餐和一杯咖啡，通常都装在塑料杯和塑料袋里。这些东西只能使用一次，城市每天都会产生大量这样的废物。\n这些只是最明显的日常行为，只要走出城市，我们就可以看到更多的地区受到人类工业活动的严重污染。\n\n结合自己的经历，谈谈你对环保的认识。\n\n越南是一个自然资源丰富的国家，拥有美丽的山川、河流和森林。然而，近年来，越南的环境问题也日益严重，比如空气污染、水污染和白色污染等。越南正在用许多环境和自然价值换取经济发展，许多森林被种植园取代，工厂的建设没有控制外出气体和水的排放。人们在生活中也缺乏环保意识，使用野生动物产品的习惯导致许多生物正在逐渐消失。\n作为一名越南人，我认为保护环境是每个人的责任。我们要从自身做起，养成良好的环保习惯，比如节约水电、减少垃圾、使用可再使用的物品等。我们还可以积极参加环保活动，比如植树、捡垃圾、宣传环保知识等。保护环境就是保护我们的家园，让我们携手共创美丽的越南。\n\n请介绍几件生活中我们可以做到的环保事情。\n\n《跟运用同样》"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第32课-身边的环保/index.html#生词",
    "href": "学汉语的日记/HSK5下-第32课-身边的环保/index.html#生词",
    "title": "HSK5下 | 第32课： 身边的环保",
    "section": "",
    "text": "身边的环保\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n消失\nxiāoshī - v. to disappear, to vanish\n\n\n2\n洪水\nhóngshuǐ - n. flood\n\n\n3\n地震\ndìzhèn - v. earthquake\n\n\n4\n破坏\npò huài - v. to destroy, to damage\n\n\n5\n砍\nkǎn - v. to cut, to chop, to fell\n\n\n6\n生存\nshēng cún - v. to live, to subsist\n\n\n7\n沙漠\nshāmò - n. desert\n\n\n8\n公布\ngōng bù - v. to announce, to make public\n\n\n9\n数据\nshù jù - n. data\n\n\n10\n真实\nzhēn shí - adj. real, actual\n\n\n11\n夸张\nkuā zhāng - adj. exaggerated\n\n\n12\n资源\nzī yuán - n. resource\n\n\n13\n车祸\nchēhuò - n. traffic accident\n\n\n14\n不安\nbù’ān - adj. upset, disturbed\n\n\n15\n工业\ngōng yè - n. industry\n\n\n16\n农业\nnóng yè - n. agriculture\n\n\n17\n生产\nshēng chǎn - v. to produce, to manufacture\n\n\n18\n大型\ndàxíng - adj. large-scale\n\n\n19\n工厂\ngōng chǎng - n. factory\n\n\n20\n废\nfèi - adj. waste, useless\n\n\n21\n燃烧\nrán shāo - v. to burn, to combust\n\n\n22\n煤炭\nméi tàn - n. coal\n\n\n23\n密切\nmì qiè - adj. close, intimate\n\n\n24\n尾气\nwěi qì - n. exhaust gas, vehicle emission\n\n\n25\n幸运\nxìng yùn - adj. lucky, fortunate\n\n\n26\n敏感\nmǐn gǎn - adj. sensitive, susceptible\n\n\n27\n自觉\nzì jué - adj. conscious, on one’s own initiative\n\n\n28\n设施\nshè shī - n. installation, facilities\n\n\n29\n能源\nnéngyuán - n. energy resource\n\n\n30\n逐步\nzhú bù - adv. gradually, step by step\n\n\n31\n尽量\njǐnliàng - adv. to the best of one’s abilities, to the greatest extent\n\n\n32\n私（人）\nsī (rén) - n. private\n\n\n33\n尊敬\nzūnjìng - v. to respect, to esteem\n\n\n34\n鼓舞\ngǔ wǔ - v. to encourage, to inspire\n\n\n35\n消极\nxiāo jí - adj. passive, inactive\n\n\n36\n幻想\nhuàn xiǎng - n. fantasy, illusion\n\n\n37\n贡献\ngòng xiàn - n./v. contribution; to contribute, to devote\n\n\n38\n命运\nmìng yùn - n. fate, destiny\n\n\n39\n掌握\nzhǎng wò - v. to take charge of, to control"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第32课-身边的环保/index.html#注释",
    "href": "学汉语的日记/HSK5下-第32课-身边的环保/index.html#注释",
    "title": "HSK5下 | 第32课： 身边的环保",
    "section": "",
    "text": "密切\n“密切”, tính từ, có thể biểu thị quan hệ gần gũi, mật thiết. Ví dụ:\n\n还有一部分污染和我们的日常生活密切相关，汽车尾气就是其中之一。\n参加了这次环保活动后，两人便有了共同语言，来往也比先前密切了。\n\n“密切” , còn có thể biểu thị（đối với các vấn đề）chú trọng, tỉ mỉ, thấu đáo. Ví dụ:\n\n刘医生密切地观察着李妈妈病情的发展。\n家长应和老师密切配合，形成合力，保持教育的一致性。\n\n“密切” ,còn có thể làm động từ, có nghĩa làm cho quan hệ trở nên gần gũi mật thiết hơn. Ví dụ:\n\n这条铁路的建成，大大密切了西南地区与首都的关系。\n友好城市之间的交往密切了两国人民之间的友谊。\n\n\n（1）。。。需要密切地控制。\n（2）。。。密切了各国之间的贸易关系。\n（3）。。。对，我们的关系现在不太密切。\n\n尽量\nphó từ, biểu thị trong một phạm vi nhất định nỗ lực đạt đến mức độ cao nhất. Ví dụ:\n\n同时，尽量多骑自行车，多选择公共交通，少使用私家车。\n老年人要尽量少吃油炸食品。\n为了节约能源，请大家都尽量使用节能电器。\n\n\n（1）。。。父母尽量让孩子学会做好自己的事情。\n（2）。。。但我会尽量完成工作，早早回家。\n（3）。。。你尽量做吧，答不上以两个问题无所谓。\n\n逐步\nbiểu thị từng bước từng bước một, dùng trong những tình huống do còn người làm ra, thông thường không thể bổ sung cho từ ngữ có tính chất là tính từ. Ví dụ:\n\n云计算应用市场规模正在逐步扩大。\n·····调整能源消费结构，逐步向可再生能源转变。\n记者了解到，现在受灾群众已逐步恢复了正常的生产生活。\n\n\n（1）。。。平均温度将逐步升高。\n（2）。。。逐步成为日常的习惯。\n（3）。。。对的，学费会逐步降低。\n\nPhân biệt 鼓励 và 鼓舞:\n\n共同点：Đều là động từ, đều có ý nghĩa làm cho người khác phấn khới, tăng thêm tự tin. Ví dụ:\n\n这次谈话，使刘洋受到极大的鼓励/鼓舞。\n\n不同点：\n\n\n\n\n\n\n\n\n\n鼓励\n鼓舞\n\n\n\n\n1\nTừ trung tính, có thể dùng ở những phương diện không tốt.\n如：吸烟有害健康，你不阻止他，怎么还鼓励呢？\nTừ mang nghĩa tốt\n如：新产品的研制成功极大地鼓舞了科技人员。\n\n\n2\nNgữ nghĩa thiên về khích lệ đối phương tham gia vào một hoạt động nào đó. Chủ ngữ thường là người hoặc tổ chức.\nThường dùng hình thức kiêm ngữ “鼓励某人做某事”\n如：近些年，国家越来越鼓励大学生毕业后开办自己的公司。\nNgữ nghĩa thiên về tinh thần phấn chấn do sự ảnh hưởng nào đó. Chủ ngữ thường là sự vật\n如：新的胜利给了全体队员很大的鼓舞。\n\n\n3\nKhông có ý nghĩa và cách dùng này.\nCó thể làm tính từ, mô tả sự hưng phấn, phấn khởi.\n如：年初制定的目标顺利实现，取得的成绩令人十分鼓舞。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第32课-身边的环保/index.html#扩展",
    "href": "学汉语的日记/HSK5下-第32课-身边的环保/index.html#扩展",
    "title": "HSK5下 | 第32课： 身边的环保",
    "section": "",
    "text": "问题：资源\n\n资源\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n金属\njin1shu3 - kim loại metal\n\n\n2\n黄金\nhuang2jin1 - vàng, gold\n\n\n3\n银\nyin2 - bạc, silver\n\n\n4\n钢铁\ngang1tie3 - sắt thép, steel\n\n\n5\n煤炭\nmei2tan4 - coal, than đá\n\n\n6\n能源\nneng2yuan2 - năng lượng, energy\n\n\n7\n原料\nyuan2liao4 - nguyên liệu thô, raw material\n\n\n8\n资源\nzi1yuan2 - tài nguyên, resource"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第32课-身边的环保/index.html#运用",
    "href": "学汉语的日记/HSK5下-第32课-身边的环保/index.html#运用",
    "title": "HSK5下 | 第32课： 身边的环保",
    "section": "",
    "text": "环保，可以这样开始\n每个人都是地球的守护者，每个人都可以为保护环境贡献自己的力量。我们不需要做惊天动地的大事，只要从身边小事做起，也能为地球带来巨大的改变。比如：节约用水—洗漱时关闭水龙头，洗澡时缩短时间，尽量使用节水龙头；节约用电—离开房间时关灯，使用节能电器，拔掉不使用的插头；减少浪费—购物时自带购物袋，尽量少使用一次性塑料制品，选择可再使用的物品；低碳出行—尽量步行、骑自行车或乘坐公共交通工具，减少开车；植树绿化—在家里或社区种植树木，净化空气，美化环境；宣传环保理念—积极参与环保活动，让更多人加入到保护环境的队伍。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第32课-身边的环保/index.html#口语",
    "href": "学汉语的日记/HSK5下-第32课-身边的环保/index.html#口语",
    "title": "HSK5下 | 第32课： 身边的环保",
    "section": "",
    "text": "说说你每天从身边观察到不环保的行为。\n\n我住在胡志明市—一座越南人口最稠密的城市。越南也是一个发展中国家，城市周围都是大型工业区。因此，我很容易从日常生活中看到不环保的行为。\n每天上班的时候，我都要使用摩托车，这种出行方式在我国非常流行。这种类型的私人车辆只能搭载一到两个人，但产生的排放量很大。减少交通拥堵或限制排放的效果都不如公共交通工具。但这里的公共交通不发达，非常不方便，所以我和很多人一样，每天都在骑摩托车。\n路上我可以看到街道两边很多垃圾。人们随手乱扔垃圾，有的甚至将垃圾丢弃在河道、湖泊等水域中，严重污染了环境。我会买早餐和一杯咖啡，通常都装在塑料杯和塑料袋里。这些东西只能使用一次，城市每天都会产生大量这样的废物。\n这些只是最明显的日常行为，只要走出城市，我们就可以看到更多的地区受到人类工业活动的严重污染。\n\n结合自己的经历，谈谈你对环保的认识。\n\n越南是一个自然资源丰富的国家，拥有美丽的山川、河流和森林。然而，近年来，越南的环境问题也日益严重，比如空气污染、水污染和白色污染等。越南正在用许多环境和自然价值换取经济发展，许多森林被种植园取代，工厂的建设没有控制外出气体和水的排放。人们在生活中也缺乏环保意识，使用野生动物产品的习惯导致许多生物正在逐渐消失。\n作为一名越南人，我认为保护环境是每个人的责任。我们要从自身做起，养成良好的环保习惯，比如节约水电、减少垃圾、使用可再使用的物品等。我们还可以积极参加环保活动，比如植树、捡垃圾、宣传环保知识等。保护环境就是保护我们的家园，让我们携手共创美丽的越南。\n\n请介绍几件生活中我们可以做到的环保事情。\n\n《跟运用同样》"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第32课-身边的环保/index.html#听力",
    "href": "学汉语的日记/HSK5下-第32课-身边的环保/index.html#听力",
    "title": "HSK5下 | 第32课： 身边的环保",
    "section": "2.1. 听力",
    "text": "2.1. 听力\n1.  男：你能教教我怎么用这个程序吗？（cheng2xu4 - program, chương trình, phần mềm）\n女：这个没你想象的那么难，我这儿有本书，你一看就懂了。\n问：女的是什么意思？（C很容易掌握）\n2.  女：明天去给老师拜年，是开车去吗？\n男：老师家离地铁站很近，咱们也为绿色出行做点儿贡献吧。（đóng góp cho phát triển xanh）\n问：男的想怎么去老师家？（C乘地铁）\n3.  男：明天学校组织春游，让我们带个塑料袋。\n女：对，把吃剩下的垃圾都装好，不要随地乱丢啊。（vứt lung tung, tùy tiện）\n问：关于男的，下列哪项正确？（B明天要去春游）\n4.  男：你说莉莉还会同意和我和好吗？(莉莉 li4li4 Lily)\n女：我劝你死了这条心吧，别再抱什么幻想了。死了这条心吧, 断念头 - từ bỏ ý định đó đi, 抱什么幻想 - ôm mộng tưởng làm gì\n问：对于莉莉，女的建议男的怎么做？（C放弃努力）\n5.  女：这几天空气质量比较差，雾霾浓度很高。（wu4mai2 - khói bụi, nong2du4 - nồng độ）\n男：家里的老人对空气污染比较敏感，最近最好少出门。（min3gan3 - nhạy cảm）\n问：男的建议老人怎么做？（C减少外出）\n6.  男：李老先生的那台手术，尽量安排在上午做吧。\n女：我也是这么想的，早上精神好，就排在明天第一台吧。\n问：关于李先生，从对话中可以知道什么？（B明天要手术）\n7.  男：我跟刘方打过招呼了，他会带你的。\n女：谢谢领导关心，不明白的地方我一定请教。\n男：这是公司四季度的销售报告，你拿回去看看。（ji4du4 - quarter, quý）\n女：好的，我好好学习学习，先熟悉一下业务。\n问：关于女的，下列哪项正确？（C接手了新工作）\n8.  女：复赛结果什么时候公布？\n男：说是下个月5号前在比赛官方网站上查询。（cha2xun2 - tra cứu）\n女：还早呢，这两天可以放松一下了。\n男：是的，耐心等待吧。\n问：复赛结果将通过什么方式公布？（D比赛官网）website của đơn vị chính thức\n9.  男：你今天怎么无精打采的，昨晚失眠啦？wújīngdǎcǎi - không có tinh thần, bơ phờ\n女：还说呢，下月的订单一半还没完成呢，愁死我了。\n男：现在生意都不好做，你也别太着急。\n女：下午要见个客户，成不成就看他了。\n问：女的现在的心情怎么样？（D紧张不安）\n\n女：明天是10号，有一些重要的经济数据将要公布。\n\n男：感觉经济的压力还是比较大呀。\n女：我现在最担心的是我的股票明天会怎么样。\n男：我觉得说不定还涨呢，房价最近不就涨了嘛。\n问：根据对话，女的现在担心什么？（A股票）\n女：你说这空气中的污染物都是从哪儿来的呢？\n11-12.\n男：大部分是城市周边地区工厂排放的废气，另外，汽车尾气也是一大污染源。\n女：我每天都密切注意天气预报中公布的 PM2.5数据。\n男：现在治理力度不断加强，今年的情况已有所改善。\n女：虽说好了一些，但离满意还差得远呢。我家已经买了两台空气净化器了。\n男：老人、小孩是敏感人群，要特别注意防护，外出最好戴口罩。（phòng hộ）\n11．对城市现在的空气污染，女的有何看法？（C治理效果还不满意）\n12．面对空气污染，男的提到应该怎么做？（B外出戴口罩）\n13-14.\n有研究显示，平均每个家庭每天要开冰箱门22次。如果这个家里有个充满好奇心的孩子，那么次数会更多。每开一次门，冰箱中的冷空气涌（yong3 - surge）出，外面的热空气灌（guan4 - irrigation, tràn vào）入，都会使得冰箱要消耗（xiao1hao4 - tiêu hao, consume）更多的电力来重新制冷（re-cooling）。\n因此，不要等到打开冰箱门以后再考虑自己要拿什么。先想好，再开门。取出东西后，立刻把门关上，减少冰箱中冷空气的流出和外界热空气的流入。此外，别忘了跟你的父母确认冰箱的温度设置是否合理，顺便告诉他们：保持冰箱蒸发（zheng1fa1 - bay hơi, evaporation）器的清洁，也能提高它的工作效率。\n13．关于家中的冰箱，这段话提到哪种情况？（C孩子爱开冰箱）\n14．这段话建议冰箱省电的方法是什么？（A想好拿什么再开门）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第32课-身边的环保/index.html#阅读",
    "href": "学汉语的日记/HSK5下-第32课-身边的环保/index.html#阅读",
    "title": "HSK5下 | 第32课： 身边的环保",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18\n有这样一个笑话：一个十分胆小的人去医院拔牙（ba2ya2 - extract a tooth, nhổ răng），医生想15 D尽量让他镇定（trấn tĩnh），就递给他一杯酒。病人很听话，16 C接过杯子一次而尽。医生以为他酒量不错，怕没有效果，就又递给了他一杯，这个病人又喝光了。\n“好了，”医生说，“现在，你有勇气拔牙了吧?”\n可没想到，这个原本胆小的病人却大声地叫道：“哼（heng1 - hừ）!我倒要看看，有谁敢来碰（peng1 - động vào）我的牙!”\n企业激励员工的目的是为了17 B鼓舞员工的士气和提高他们的工作效率，但很多管理者却犯（fan4 - phạm）了和前面医生一样的错误，他们常常鼓励过了头，这容易使有些员工产生骄傲自大（tự cao tự đại）的心理，不但对他们的工作无益，还会对别的员工产生18 A消极影响。\n19 A许多人自觉参与环保行动\n越来越多的人敏感地认识到了环境污染问题的严重，并自觉地投入到了保护地球的行动中。生产中，增加环保设施减少污染物排放，凋整能源消费结构，逐步向可再生能源转变。而在日常生活中，改变生活习惯，尽量减少生活垃圾，做到垃圾分类；同时，尽量多骑自行车，多选择公共交通，少使用私人汽车。为此付出努力的人们令人尊敬，取得的成绩也令人鼓舞。\n20 D冰岛的自然景观种类丰富\n冰岛（bing1dao3 - Iceland）是一片净土，有最干净的空气最纯净（chúnjìng - thuần khiết）的水、最活跃的火山，以及最洁白（jie2pai2 - pure white）的冰川（glacier - sông băng）。地下有火，地上有冰，所以冰岛被称为冰火之国。这里的四季更以完全不同的风貌展现在世人面前，无论是壮观（zhuang4guan1 - spectacular, tuyệt vời）的瀑布（pu4bu4 - waterfall, thác nước）、宁静（ning2jing4 - quiet）的湖泊，还是成片的浮冰（fu2bing1 - ice floe tảng băng），更或是雪山、荒原、海岸（an4 - bank, bờ），都会让你惊喜得忘记呼吸。\n21 B废气中的二氧化碳是海洋酸化\n废气排放的二氧化碳（er4yang3 hua4tan4 - carbon dioxide, CO2）中，大约有25%被海洋吸收并转化（chuyển hóa）成碳酸（tan4suan1 - axid carbonic），这造成了海洋中酸碱度（suan1jian3du4 - độ PH, axit và kiềm）的改变，使贝壳（bei4ke2 - shell, vỏ sỏ）类生物数量减少，同时威胁到珊瑚礁（shan1hu2jiao1 - rạng san hô, coral reef）及其（ji2qi2 - and its, and their）周边鱼类的健康。据统计，全球有大约10亿人以海洋鱼类作为摄入蛋白质的主要途径（tu2jing4 - way, channel）。因而海洋酸化（suan1hua4 - axid hóa）在影响海洋生物的同时，也严重威胁到人类的食品安全。\n22 D绿色沙漠的生态环境是脆弱的\n“绿色沙漠”是指大面积（miànjī - area）种类单一的绿色树林，其年龄、高矮一致，且十分密集（mi4ji2 - compressed, crowded together, dày đặc）。密集单一的树冠（shu1guan1 - treetop）层完全遮挡（zhe1dang3 - to shelter, chắn mất, che khuất）了阳光，使下层植被（zhi2bei4 - plant cover, thảm thực vật）无法生长，林下缺乏中间的灌木（guan4mu4 - bush, cây bụi）层和地面的植被。单一的树种导致生物多样（đa dạng sinh học）性差，保持水土能力也不强，如果遇到病虫害（bing4chong2hai4 - plant diseases, sâu bệnh），就会大面积死亡，导致生态环境迅速恶化 (xấu đi nhanh chóng)。\n23-25\n面面相觑 - mian4mian4xiang1qu4, nhìn nhau im lặng sững sờ bàng hoàng, to look at each other at dismay\n一天，一位医生将一群爱酒如命的酒鬼召集到一起，在他们面前做了这样一个实验：\n医生将两只杯子放到了桌上，一杯装满了清水，另一杯装满了白酒。他把一只毛毛虫先丢进装满清水的杯子，大家看着虫子在清水里游动，慢慢地又爬了出来。然后，医生又将毛毛虫抓了起来，投进装白酒的杯子，虫子在酒里挣扎了一会儿就死去了。\n看了这个实验后，酒鬼们面面相觑（），你看看我，我看看你。屋子里沉默了好长一段时间。正当医生准备对他们说明酒精对人体有害的时候，在屋子的最后排传来一个声音：“医生，我明白了，只要我们多喝酒，那我们肚子里就决不会生虫子!”\n你们看，即使是一种正确的观念，也总有人站在相反的角度去理解。对这些人来讲，过多的劝说是没有太大作用的。所以不要把你的时间花赉在“无用功”上。\n23 B饮酒对健康的危害\n24 D沉默无语\n25 C付出要用对地方\n26-28       内蒙古自治区 - nèiménggǔ zìzhìqū, khu tự trị Nội Mông Cổ, Inner Mongolia Autonomous Region阿拉善盟 - ālā shàn méng, Liên minh A Lạp Thiện, Alxa League肉苁蓉 - Ròucōngróng, Nhục thung dung, Cistanche deserticola梭梭 - Suo1suo1, Toa Toa, một loài cây bụi họ dền Haloxylon ammodendron防风固沙 - fángfēng gùshā, chống gió giữ cát双峰驼 - shuāng fēng tuó， lạc đà hai bướu退牧还林 - tui4mu4huan2lin2 trồng cây trả lại cho rừng\n位于内蒙古自治区西部的阿拉善盟，生长着一种具有补气（bu3qi4 - bổ khí）、补血（bổ máu）功能的名贵中药材—肉苁蓉，人们称赞它为“沙漠人参”（nhân sâm sa mạc）。梭梭，是生长在沙漠边缘（biên viễn, rìa）的小乔木（qiao2mu4 - cây nhỏ），它的根系（gen1xi4 - root system, hệ rễ）非常发达，极耐干旱（gan1han4 - hạn），是防风固沙的优良树种。如果人们要找肉苁蓉就必须先找到梭梭，这是因为肉苁蓉的生长完全依靠梭梭，它是生长在梭梭根部的一种寄生（ji4sheng1 - ký sinh）植物，靠梭梭的根系供给（gong1ji3 - cung cấp, supply）菅养、水分，完成生长发育（fa1yu4 - to growth）。\n在阿拉善，梭梭林是保护沙漠生态的重要植被。可是从20世纪90年代开始，这里的梭梭林开始大面积退化（tui4hua4 - thoái hóa, degenerate）。原因是梭梭不仅是肉苁蓉的寄主，还是阿拉善双峰驼最钟爱的食物。牧民（mu4min2 - người chăn nuôi, herdsman）长期超载（chao1zai4 - overload, quá mức）放牧（fang4mu4 - to graze, to herd, chăn thả）造成了梭梭林面积的减少，除此之外，当地人大面积砍伐（kan3fa2 - to cut down, chạt phá）梭梭林来满足冬季取暖（giữ ấm）的生活方式，同样带来了不良的后果。\n近些年，当地政府在实行退牧还林政策（zheng1ce4 - chính sách, policy）的同时，也在大力推广新型（xin1xing2 - new type）能源的普及（pu3ji2）利用，帮助牧民改善传统的生活方式。现在牧民的家里都用上了煤气灶（mei2qi4zao4 - coal gas, bếp ga）、电、太阳能。可是梭梭林的数量依然越来越少。这是因为野生（ye3sheng1 - wild, hoang dã）肉苁蓉的价格近年持续走高，从而吸引了大批外来人员涌入（yong3ru4 - to come pouring in）阿拉善，疯狂（feng1kuang2 - điên cuồng）盗挖（dao4wa1 - đào）野生肉苁蓉。他们不仅毁坏（hui3huai4 - hủy hoại, damage）了肉苁蓉资源，更造成梭梭林持续退化，破坏了阿拉善的生态防线（fang2xian4 - defensive of perimeter, phòng tuyến）。\n科研人员经过几年潜心（qian2xin1 - tập trung vào, concentrate）研究，经历无数次的失败之后，终于取得了人工培育（pei2yu4 - cultivate, cày cấy）肉苁蓉的成功，从而制止（zhi4zhi3 - ngăn chặn, to put a stop）了对梭梭林的破坏。现在，阿拉善已经封（feng1 - to seal）育梭梭林接近200万亩（mu3 - mẫu），人工造林40万亩，在大漠边缘重新筑（zhu4 - to build, construct, xây dựng）起了一条绿色的生态防线。\n26 A药用价值很高\n27 D盗挖肉苁蓉时的破坏\n28 C沙漠生态防线的重建"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第32课-身边的环保/index.html#书写",
    "href": "学汉语的日记/HSK5下-第32课-身边的环保/index.html#书写",
    "title": "HSK5下 | 第32课： 身边的环保",
    "section": "2.3. 书写",
    "text": "2.3. 书写\n环保、掌握、自觉、破坏、贡献\n保护环境，人人有责。我们要树立环保意识，掌握环保知识，自觉养成良好的环保行为，共同守护我们的家园。我们不能做破坏环境的事情，要节约资源，减少污染，为环保贡献自己的力量。只有每个人都行动起来，才能创造一个更加美丽、健康的环境。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第34课-鸟儿的护肤术/index.html",
    "href": "学汉语的日记/HSK5下-第34课-鸟儿的护肤术/index.html",
    "title": "HSK5下 | 第34课： 鸟儿的护肤术",
    "section": "",
    "text": "公鸟 - chim đực, 母鸟 - chim cái, 生产期 - mùa sinh sản, 下蛋 - đẻ trứng, 筑巢 - xây tổ, 幼崽 - con non."
  },
  {
    "objectID": "学汉语的日记/HSK5下-第34课-鸟儿的护肤术/index.html#生词",
    "href": "学汉语的日记/HSK5下-第34课-鸟儿的护肤术/index.html#生词",
    "title": "HSK5下 | 第34课： 鸟儿的护肤术",
    "section": "1.1. 生词",
    "text": "1.1. 生词\n\n鸟儿的护肤术\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n接触\njiēchù - v. to contact, to get in touch with\n\n\n2\n特征\ntè zhēng - n. feature, characteristic\n\n\n3\n翅膀\nchì bǎng - n. wing\n\n\n4\n昆虫\nkūnchóng - n. insect\n\n\n5\n天空\ntiān kōng - n. sky\n\n\n6\n区分\nqūfēn - v. to distinguish, to differentiate\n\n\n7\n唯一\nwéi yī - adj. only, sole\n\n\n8\n斑\nbān - n. spot, speckle, stripe\n\n\n9\n充当\nchōng dāng - v. to serve as, to play the part of\n\n\n10\n总之\nzǒng zhī - conj. in short, in brief\n\n\n11\n角色\njué sè - n. role, part\n\n\n12\n爱惜\nàixī - v. to cherish, to treasure\n\n\n13\n保养\nbǎo yǎng - v. to take good care of, to maintain\n\n\n14\n反复\nfǎn fù - adv. repeatedly, over and over again\n\n\n15\n啄\nzhuó - v. to peck\n\n\n16\n随身\nsuíshēn - adj. (to carry/take…) with one, personally\n\n\n17\n梳子\nshū zi - n. comb\n\n\n18\n光滑\nguāng huá - adj. smooth, glossy\n\n\n19\n寄生\njìshēng - v. to live on another animal or plant, to be parasitic\n\n\n20\n肥皂\nféi zào - n. soap\n\n\n21\n种类\nzhǒnglèi - n. kind, category\n\n\n22\n概括\ngài kuò - adj./v. brief and to the point; to summarize, to sum up\n\n\n23\n岛屿\ndǎo yǔ - n. island\n\n\n24\n知更鸟\nzhī gēng niǎo - n. robin, redbreast\n\n\n25\n坑\nkēng - n. pit, hollow\n\n\n26\n池塘\nchí táng - n. pond\n\n\n27\n老鹰\nlǎo yīng - n. eagle, hawk\n\n\n28\n痛快\ntòng kuài - adj. to one’s heart’s content\n\n\n29\n洗礼\nxǐlǐ - n. baptism, washing ceremony\n\n\n30\n沙子\nshā zi - n. sand\n\n\n31\n干燥\ngān zào - adj. dry, arid\n\n\n32\n秘密\nmì mì - adj./n. secret"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第34课-鸟儿的护肤术/index.html#注释",
    "href": "学汉语的日记/HSK5下-第34课-鸟儿的护肤术/index.html#注释",
    "title": "HSK5下 | 第34课： 鸟儿的护肤术",
    "section": "1.2. 注释",
    "text": "1.2. 注释\n\n总之\n\nLiên từ, khái quát tình huống trước đó, nói tóm lại. Ví dụ:\n\n暑假我可能去上海、南京，还有杭州，总之，想去南方几个城市转转。\n总之，网络的确带给我们以前无法想象的方便，但同时它也带来了一定的危害。\n总之，在鸟儿的生活中，羽毛充当着十分重要的角色。\n\n（1）不管你去不去，我们都会参加这次比赛。总之，去不去你自己决定吧。\n（2）A：你怎么能把汉语学得这么好？\nB：我每天都会练习写汉字，如果学到新的东西，我会立即记在笔记本上，我也会尝试更多地使用中文进行交流。总之，需要大量学习。\n（3）一条鱼，你可以做成几个菜：煎鱼，蒸鱼，鱼汤。总之，中国菜有很多做法。\n\n动词+过\n\nBiểu thị người hoặc vật thể thông qua động tác để thay đổi phương hướng. Ví dụ:\n\n他转过身，一句话也不说。\n·····它们只要有时间，就会情不自禁地背过头去，反复地啄着羽毛，······\n\nBiểu thị người hoặc vật thể thông qua động tác để di chuyển vị trí. Ví dụ:\n\n接过书的那一刻，老王突然明白了自己失败的原因。\n短短的几分钟里，我的脑子里闪过了很多想法。\n\n（1）你回过头就可以看见我了。\n（2）他递过一块毛巾给我擦汗。\n（3）青年走到门口，转过身说：“我们会再见的。”\n\n动词+开\n\nBiểu thị mở ra, giãn ra. Ví dụ:\n\n猴子突然站了起来，张开手臂，抱住了管理员。\n《清明上河图》在我们的面前慢慢展开。\n而老鹰的洗澡方式更是直接，它们会在雨中张开双翅痛快地迎接洗礼！\n\n（1）回家时，妈妈张开双臂迎接我\n（2）他把纸铺开，笔拿好，准备练习书法。\n（3）那件事情已经都传开了，大家都知道了。\n☢ Phân biệt: - 传 chuan2 (v) truyền đi - 转 zhuan3 (v) chuyển hướng - 转 zhuan4 (v) đi vòng quanh, loanh quanh (đi dạo) - 传 zhuan4 (n) truyện, ví dụ tự truyện là 自传\n\nPhân biệt 反复 và 重复\n\n共同点： Đều có ý nghĩa là hơn 1 lần.\n\n如：这件事情你已经反复/重复说过好几遍了。\n\n不同点：\n\n\n\n\n\n\n\n\n\n反复\n重复\n\n\n\n\n1\nPhó từ, nhiều lần\n如：他们只要一有时间，就会情不自禁的背过头去，反复地啄着羽毛。\nĐộng từ, chỉ lại một lần nữa làm điều tương tự\n如：我没听清，请你再重复一遍。\n\n\n2\nĐộng từ, tình huống bất lợi, điều gì đó không tốt lại một lần nữa xuất hiện.\n如：这种病容易反复。\nĐộng từ, đồ vật tương tự lại lần nữa xuất hiện.\n如：这两个练习题重复了。\n\n\n3\nDanh từ, tình huống xấu lại lần nữa xuất hiện.\n如：对这个问题，他思想上可能还有反复。\nKhông có ý nghĩa này.。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第34课-鸟儿的护肤术/index.html#扩展",
    "href": "学汉语的日记/HSK5下-第34课-鸟儿的护肤术/index.html#扩展",
    "title": "HSK5下 | 第34课： 鸟儿的护肤术",
    "section": "1.3. 扩展",
    "text": "1.3. 扩展\n问题：地理环境\n\n地理环境\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n天空\ntian1kong1 - sky, bầu trời\n\n\n2\n陆地\nlu4di4 - land (opp to sea)\n\n\n3\n土地\ntu4di4 - land (properties)\n\n\n4\n池塘\nchi2tang2 - pond, cái ao\n\n\n5\n沙漠\nsha1mo1 - sa mạc, desert\n\n\n6\n沙滩\nsha1tan1 - beach, bãi biển\n\n\n7\n岛屿\ndao3yu3 - đảo, island\n\n\n8\n岸\nan4 - bờ, river or sea bank\n\n\n9\n洞\ndong4 - hole, lỗ, hang động\n\n\n10\n木头\nmu4tou - wood, gỗ\n\n\n11\n石头\nshi2tou - stone, đá\n\n\n12\n灰尘\nhui1chen2 - dust, bụi"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第34课-鸟儿的护肤术/index.html#运用",
    "href": "学汉语的日记/HSK5下-第34课-鸟儿的护肤术/index.html#运用",
    "title": "HSK5下 | 第34课： 鸟儿的护肤术",
    "section": "1.4. 运用",
    "text": "1.4. 运用\n我与/看宠物\n作为一个生活在城市中的现代人,我深刻体会到养宠物给主人带来的巨大精神价值。在繁忙的城市生活中,一只忠诚的狗或温顺的猫咪能成为我们忠实的朋友，为我们驱散孤独感，缓解工作压力。每天下班回家,看到它们摇尾迎接或蹭腿撒娇,仿佛瞬间卸下了一天的疲惫。然而,城市生活的现实也给养宠物带来了挑战。狭小的居住空间可能限制了宠物的活动范围,高昂的生活成本也增加了饲养的经济负担。因此,在决定养宠物之前,我们需要深思熟虑,确保能为它们提供一个健康自由的生活环境。同时,我们还要考虑到邻里关系,尽量不影响他人的生活。总的来说,只要我们做好准备,养宠物依然是一种能带来快乐和慰藉的生活方式。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第34课-鸟儿的护肤术/index.html#口语",
    "href": "学汉语的日记/HSK5下-第34课-鸟儿的护肤术/index.html#口语",
    "title": "HSK5下 | 第34课： 鸟儿的护肤术",
    "section": "1.5. 口语",
    "text": "1.5. 口语\n1.  你养过宠物吗？你对养宠物有什么看法？\n我还没有养过宠物，但对养宠物这件事有着复杂的看法。虽然我很喜欢动物，但养宠物确实存在许多挑战。首先是时间问题，作为全职员工，我担心没有足够的时间陪伴和照顾宠物。其次是卫生问题，宠物掉毛和清理排泄物等可能影响家居环境，需要额外的清洁工作。\n财务方面也是重要考虑的因素。宠物食品的日常开销不小，尤其是高质量的食品价格不菲。更大的挑战是医疗费用，宠物生病或需要定期体检时，兽医费用可能会很高。\n我也担心会影响到邻居。狗叫声可能会打扰他们，猫咪可能会在公共区域造成麻烦。在胡志明市这样的大城市里，维护好邻里关系很重要。\n最后，宠物寿命问题也让我犹豫。大多数宠物寿命在10-15年左右，我害怕建立深厚感情后面对它们离开的痛苦。有些人正是因为无法承受这种分离之痛，而选择不养宠物。\n尽管存在这些顾虑，我仍然对养宠物充满向往。我相信只要充分地准备和规划，这些挑战都是可以克服的。养宠物带来的快乐和陪伴可能会远远超过这些困难。\n2.  如果你养过或正在养宠物，请说说你和宠物的故事？\n虽然我还没有养过宠物，但我常常幻想自己有一只可爱的小狗或猫咪。我想象它们会在我下班回家时热情地迎接我，陪我一起度过闲暇时光。这些想象让我对未来充满期待。\n3.  如果你对养宠物没有兴趣，请说明原因？\n我对养宠物非常有兴趣，所以没有不感兴趣的原因。我现在还没有养宠物主要是因为想要做好充分的准备，包括时间、经济和居住环境等方面。我相信在适当的时机，我会成为一个负责任的宠物主人。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第34课-鸟儿的护肤术/index.html#听力",
    "href": "学汉语的日记/HSK5下-第34课-鸟儿的护肤术/index.html#听力",
    "title": "HSK5下 | 第34课： 鸟儿的护肤术",
    "section": "2.1. 听力",
    "text": "2.1. 听力\n1.  女：鸟儿最重要的特征是什么？是有翅膀会飞？还是吃昆虫？\n男：区分鸟儿和其他动物的唯一特征就是羽毛。\n问：鸟儿最重要的特征是什么？（C有羽毛）\n2.  男：妈，我今天晚上又得加班，估计11点以后才能到家。\n女：哎呀，老这样下去怎么行啊……你得爱惜身体！Ai da, Mẹ sao có thể như này mãi, con phải học cách quý trọng cơ thể\n问：女的是什么语气？（B担心）\n3.  女：年纪轻轻的，总这么把自己关在家里，不接触社会，怎么行？\n男：可我就是不愿意出去工作，家里又不是没钱养我？\n问：关于男的，下列哪项正确？（D不想工作）\n4.  男：我觉得他挺好的，你怎么就不喜欢他呢？\n女：一个大男人，天天随身带着把梳子，碰到镜子就梳来梳去的，我看不惯。Nhìn không có quen\n问：女的为什么不喜欢那个人？（C太女性化） Nữ tính hóa 臭美 chou1mei3 làm điệu, điệu đà\n5.  女：电视电影里常常看到鲨鱼，都说它们是“海洋杀手”，是真的吗？鲨鱼 sha1yu2 cá mập, shark\n男：没那么可怕，地球上大约有370多种鲨鱼，大部分鲨鱼对人类无害，只有少数种类才会伤害人类。\n问：关于鲨鱼，下列哪项正确？（D大部分不伤害人类）\n6.  男：这个杯子真不错，是石头的还是金属的？\n女：你看走眼了，这是木头的。Nhìn nhầm\n问：这个杯子是什么材料做的？（A木头）\n7.  女：爸爸，鸟儿是不是不用睡觉？\n男：睡啊！大多数鸟1天大约睡8小时，有些鸟差不多要睡20个小时，当然，也有一些鸟几乎一点儿觉也不用睡。\n女：那为什么我们很少看到睡眠中的鸟呢？\n男：因为它们通常会寻找一处秘密的地方休息。\n问：关于鸟儿的睡眠，下列哪项正确？（D经常躲起来） Trốn đi\n8.  男：今天学的鸟儿沙浴，就是鸟儿用沙子洗澡，很有意思。\n女：真是很难想象，用沙子怎么能洗澡呢？\n男：因为它们生活在沙漠等干燥的环境里。\n女：我倒是听说过，在一些沙漠地区，有人用沙疗的办法来健身治病。\n问：沙疗有什么作用？（A强身健体）\n9.  女：你相信吗，鲸以前是生活在陆地上的。\n男：怎么可能呢？它不是海洋中最大的动物之一吗？\n女：科学家们曾经在高山上发现过五十万年前古代鲸的骨头。研究后发现，鲸以前生活在海边，后来因为陆地上的食物越来越少，而海里鱼类丰富，所以它们慢慢地离开了陆地。\nCác nhà khoa học từng phát hiện là xương của các loài cá voi cổ đại ở trên núi cao…\n问：鲸为什么离开了陆地？（C寻找食物）\n10. 男：你们宿舍新来的那个小王，办事真是太不痛快了！\n女：怎么了？\n男：她昨天说去超市，我请她帮我带块肥皂回来，她问了半天，又问要多少钱的，又问要洗衣服的还是洗澡的，又问要什么颜色的……有这工夫我自己都买回来了。\n女：那是人家态度认真。\n问：对于小王，男的怎么看？（C不干脆） 办事痛快 Làm việc một cách nhanh gọn dứt khoát, lúc này gần nghĩa với 干脆\n一只老鼠向狮子挑战，想要同它一决高低，被狮子拒绝了。老鼠问：“你害怕了吗？”“非常害怕，但我害怕的不是你。”狮子说，“如果我答应你，你就能得到曾与狮子比武的荣誉；而我呢，则会被所有动物嘲笑，说我竟然和老鼠打架。”毫无疑问，这只狮子是非常聪明的。因为它清楚与老鼠比赛的麻烦在于：即使赢了，对手也只是一只老鼠而已。一般情况下，大人物是没兴趣与低层次的人竞争的，他们更乐于面对与自己旗鼓相当（ngang tài ngang sức）甚至远高于自己的对手。狮子 Sư tử, Lion\n11-12.\n11．狮子为什么拒绝老鼠？（D对手不够资格）\n12．作者对狮子的做法持什么态度？（A赞成）\n13-14.\n通过长期的观察和研究，动物学家为我们展现了动物睡眠方面千姿百态（muôn hình muôn vẻ）的特点：兔子是个胆小鬼，一天只打三次盹，每次只有几秒钟，全天一共只睡大约两分钟。牛不停地吃草和反刍，一天最多睡半小时。大象是站着睡觉的，为了避免小虫子和老鼠钻进鼻子里，它在睡觉时要把长鼻子弯起来，卷进嘴里轻轻含着。狮子只要吃饱喝足了，可以一直睡上16个小时。\n13．睡觉时间最短的是哪种动物？（B兔子）\n14．大象睡觉时为什么要把鼻子弯起来？（D预防虫子，老鼠进入）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第34课-鸟儿的护肤术/index.html#阅读",
    "href": "学汉语的日记/HSK5下-第34课-鸟儿的护肤术/index.html#阅读",
    "title": "HSK5下 | 第34课： 鸟儿的护肤术",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n有一天，老虎抓住一只狐狸，心想，今天可以美美地享受一顿午餐了。可是，狐狸很15A狡猾，它骗老虎说：“我是天帝派到山林中来做百兽之王的，你要是吃了我，天帝是不会16D原谅你的。”老虎不相信。狐狸17B连忙说：“你如栗不相信我的话，可以跟我到山林中去走一走，我让你亲眼看看百兽害怕我的样子。”老虎想这倒也是个办法，于是就让狐猩在前面带路，自己尾随其后，一起向山林深处走去。森林中的动物们远远地看见老虎来了，都纷纷逃命。老虎不知道动物们是害怕自己而逃跑的，还以为18B他们是害怕狐狸才逃走的。\n19. B鸟儿扇动翅膀产生向上的力量\n鸟儿飞行时，主要起作用的是飞羽和尾羽。飞羽是长在翅膀上的，侬靠扇动气流产生向上的力量。尾羽是长在尾巴上的，主要用来控制方向，起到“舵”的作用。而它们身上其他的羽毛，在飞行过程中主要是起到减小空气阻力的作用。\n20. D即使失败也可能有收获\n赵老板运了一船鲜蚌在海上航行，阻干风浪，误了归期，满船的蚌肉都腐烂了。赵老板见血本无归，急得要跳海自杀。船长劝他：“等一等，也许你还剩下什么东西。”他率领水手清理船舱，从满船烂肉中找到一粒明珠，它的价值足以弥补货价运费而有余。“失败”同样会给我们留下一些宝贵的东西，比如说经验，它比珍珠还可贵。\n21. C每个物种都有不一样的特点\n一只小鸡看到一只老鹰在高高的蓝天上飞过，十分羡慕。于是它问母鸡：“妈妈，我们也有一双翅膀，为什么不能像鹰那样高高地在蓝天上飞呢？”母鸡回答说：“飞得高对我们来说没什么用。蓝天上没有谷粒，也没有虫子。”每个人都有自己的生存技能和与之相适应的环境（mỗi người đều có kĩ năng sinh tồn và môi trường phụ hợp với họ），我们在不断追求更高目标的同时，也要知道什么才是最适合自己的。\n22. D教材里写的内容是错误的\n百余年来，全世界所有的中学教材都告诉孩子们：鸟类最早的祖先是始祖鸟。始祖鸟（Archaeopteryx）生活在晚侏罗纪（Kỷ Jura）肘期，形象与现在的鸟儿不完全相同，它们虽然有羽毛，但前面两只翅膀上长着爪子，嘴里有尖尖的牙齿，尾巴很长。但是，历史，尤其是远古时期的历史，往往是在后人对前人的否定中曰渐接近真实的。   \n23-25.\n长颈鹿母亲刚生下小长颈鹿后，会做出一件很不合常理的事——她抬起长长的脚，踢向她的孩子，使它翻了一个跟斗后，四肢摊开。长颈鹿 chang2jing3lu4 - hươu cao cổ\n如果小长颈鹿不能站起身来，这个粗暴的动作就会被长颈鹿妈妈不断地重复。小长颈鹿为站起来，必须拼命努力。疲倦时，小长颈鹿有时停止努力，母亲看到，就会再次踢向它，迫使它继续奋斗，直到小长颈鹿终于第一次用它颤动（run rẩy）的双脚站起身来。\n这时，长颈鹿母亲会做出更不合常理的举动。她会再次把小长颈鹿踢倒。为什么？她想让小长颈鹿记住自己是怎么站起来的，在头脑中形成长久的记忆。因为荒野里的狮子、土狼等野兽都喜欢猎食小长颈鹿，如果长颈鹿母亲不教会她的孩子尽快站起来，与大部队保持一致，那么它就会成为这些野兽口中的食物。\n已故（quá cố）著名作家欧文·斯通毕生研究伟人，为许多人写过传记。斯通曾经被问及是否发现了贯穿这些杰出人物生命的线索。他说：“我写的这些人，都曾遭遐当头一击，一度被彻底打倒，然后在接下来的许多年里，他们走投无路。但是每次被击倒后，他们总会站起来。你不能摧毁这些人。”sợi dây xuyên suốt cuộc đời của những người này\n23. C他让孩子联系站起来\n24. A可能是群居动物\n25. D要学会摔倒后爬起来\n26-28.\n台湾宽尾凤蝶，为昆虫纲（lớp côn trùng）、鳞翅目（bộ vẩy cánh）、凤蝶科（họ bướm phượng）、宽尾凤蝶属物种（loài bướm đuôi rộng），是台湾特有的一种大型蝴蝶。Bướm đuôi rộng Đài LoanLepidoptera (鳞翅目)Swallowtail butterfly (凤蝶科)\n它的翅膀展开可达90~120mm。前翅底色黑而略带褐色，后翅在中室附近有白色大纹，在外沿则有一排红色弦月型纹。雌(cái)蝶较雄(đực)蝶略大，翅面更宽圆。台湾尾凤蝶具有特别宽大的尾状突起，同时二支翅脉贯穿其间，这是其他凤蝶所没有的特征，因此举世闻名。成虫栖息时常平放翅膀，飞行缓慢，常作滑翔，喜欢在向阳的崩塌地活动，幼虫寄生在擦树(cây Sát thụ Đài Loan)上。该种在乌来(Ô Lai)和台东(Đài Đông)都有采集记录。在学术上有很高的价值。\n它的首次发现是在1932年的宜兰(Nghi Lan)乌帽子河滩。过去中国台湾没有发现宽尾凤蝶的记载时，学术界普遍认为宽尾凤蝶属可能仅分布于中国大陆，然而在1932年7月，宜兰农林学校的曰本籍教师铃木利一采集到了宽尾凤蝶标本。这引起了当时台北大学教授素木得一的兴趣，当他初次见到铃木利一采集的标本时，表示难以置信。1933年5月，素木得一带着他的助手中条道夫前往该地点，历经数天辛苦等候，终于采集到第二只台湾宽尾凤蝶。\n台湾宽尾凤蝶喜欢出没于海拔1000到2000米的山区，早期这些地点都是交通不便、人迹罕至的地方，加上其发生期仅限于每年5到8月间，因此当时的采集记录非常稀少，而被称为“梦幻之蝶”。但却也因为声名大噪，一对宽尾凤蝶往往可以卖到上万元。在金钱的驱使下，宽尾凤蝶开始面临前所未有的生存压力。\n26. C昆虫\n27. D一般雄蝶比蝴蝶小\n28. B人们为赚钱大量捕捉宽尾凤蝶"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第34课-鸟儿的护肤术/index.html#书写",
    "href": "学汉语的日记/HSK5下-第34课-鸟儿的护肤术/index.html#书写",
    "title": "HSK5下 | 第34课： 鸟儿的护肤术",
    "section": "2.3. 书写",
    "text": "2.3. 书写\n秘密、唯一、痛快、爱惜、总之\n可持续发展的秘密其实并不复杂，但实现起来却并非易事。星球是人类共同生活的唯一家园，所以我们应当痛快地接受环保责任。爱惜资源、减少浪费是基本之道。从日常生活的小事做起，如垃圾分类、节约用水，到支持环保企业，每一步都至关重要。总之，可持续发展不是一蹴而就的目标，而是需要我们不断努力的长期承诺。只有携手共进，我们才能为后代创造一个更美好的未来。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第36课-老舍与养花/index.html",
    "href": "学汉语的日记/HSK5下-第36课-老舍与养花/index.html",
    "title": "HSK5下 | 第36课： 老舍与养花",
    "section": "",
    "text": "老舍与养花 | Lao She and his flowers\n作家老舍先生爱花，他养的花很多，满满摆了一院子。可除非是那些好种易活、自己会奋斗的花草，否则他是不养的。因为他知道北京的气候对养花来说，不算很好，想把南方的名花养活并非易事。\n老舍把养花当作一种生活乐趣。他不在乎花开得大小好坏，只要开花，他就高兴。每天老舍像好朋友似的照管着花草。工作的时候，经常写几十个宇，就到院中去转转，瞧瞧这棵，看看那朵，有时拿起剪刀给它们剪剪枝，有时蹲下检几块小石头放在花盆里做点儿装饰，然后回到屋中再写一会儿，然后再出去，就这样脑力和体力很好地结合，身心也得到放松。\n写作是件艰苦的工作，养花也是如此。有时赶上狂风暴雨，情况紧急，他就得劳驾全家人抢救花草。几百盆花，要很快地抢到屋里去，累得腰酸腿疼，热汗直流。第二天，天气好了，又得一盒盆地搬出去。可是他并不抱怨，在他看来，任何事都要有付出，不然怎么会有回报？这是生活的真理。\n一来二去，他慢慢地总结出些养花的经验：有的花喜干，就别多浇水；有的花喜欢潮湿的环境，就别放在太阳地里。给花换盆剪枝施肥的活儿他越做越熟练，花生病长虫他也知道如何应付了。看着院子里那鲜艳的花朵，老舍自豪地说，“不是乱吹，这就是知识啊！多得些知识，定不是坏事。”\n老舍很有爱心，更懂得快乐要分享。每到昙花开放的时候，他就约上几位朋友来家里赏花庆祝。花分根了，一棵分为几棵，他会毫无保留地送给朋友们。看着友人高兴地拿走自己的劳动果实，老舍心里十分欢喜。有一次，送牛奶的小伙子进门就夸“好香”！这让老舍先生感到格外高兴。\n当然，也有伤心的时候。年夏天，下了暴雨，邻居家的墙倒了，菊花被砸死了一百多棵，这下可把老舍难受坏了，一连几天人们都看不到他脸上的笑容。\n“有喜有悲，有笑有沮” 这是老舍对养花、对生活的体验。“我不知道花草们受我的照顾，感谢我不感谢，反正我要感谢它们。〞老舍在自己的文章中这样写道。从中我们不难看出老舍先生对大自然的热爱，对生活的热爱。\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n\n\n\n\n老舍与养花\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n养\nyǎng - v. to raise, to keep, to grow\n\n\n2\n除非\nchú fēi - conj. only if, unless\n\n\n3\n奋斗\nfèn dòu - v. to fight, to struggle, to strive\n\n\n4\n乐趣\nlè qù - n. joy, pleasure\n\n\n5\n在乎\nzàihu - v. to care, to mind\n\n\n6\n朵\nduǒ - m. used for flowers and clouds\n\n\n7\n剪刀\njiǎn dāo - n. scissors\n\n\n8\n捡\njiǎn - v. to pick up, to collect\n\n\n9\n装饰\nzhuāng shì - n. decoration\n\n\n10\n结合\njié hé - v. to combine, to integrate\n\n\n11\n暴雨\nbào yǔ - n. rainstorm\n\n\n12\n紧急\njǐn jí - adj. urgent, emergent\n\n\n13\n劳驾\nláo jià - v. to trouble sb. (to do sth.)\n\n\n14\n抢救\nqiǎngjiù - v. to rescue, to save\n\n\n15\n腰\nyāo - n. waist\n\n\n16\n直\nzhí - adv. continuously, straight\n\n\n17\n不然\nbùrán - conj. or else, otherwise\n\n\n18\n回报\nhuí bào - v. to repay, to requite\n\n\n19\n真理\nzhēnlǐ - n. truth\n\n\n20\n浇\njiāo - v. to water, to pour (liquid on sth.)\n\n\n21\n潮湿\ncháoshī - adj. wet, moist\n\n\n22\n施肥\nshī féi - v. to apply fertilizer\n\n\n23\n熟练\nshú liàn - adj. skilled, practiced\n\n\n24\n应付\nyìng fu - v. to handle, to cope with\n\n\n25\n鲜艳\nxiān yàn - adj. bright-colored\n\n\n26\n自豪\nzìháo - adj. proud\n\n\n27\n吹\nchuī - v. to boast, to brag, to blow\n\n\n28\n爱心\naì xīn - n. love, compassion\n\n\n29\n分享\nfēnxiǎng - v. to share (good things such as joy and rights)\n\n\n30\n昙花\ntánhuā - n. broad-leaved epiphyllum (type of plant)\n\n\n31\n庆祝\nqìng zhù - v. to celebrate\n\n\n32\n保留\nbǎoliú - v. to reserve, to save\n\n\n33\n菊花\njúhuā - n. chrysanthemum (type of flower)\n\n\n34\n砸\nzá - v. to crush, to smash\n\n\n35\n悲（伤）\nbēi (shāng) - adj. sad, sorrowful\n\n\n36\n反正\nfǎn zhèng - adv. (used to indicate the same result despite different circumstances) anyway, no matter what\n\n\n37\n热爱\nrè ài - v. to love ardently\n\n\n\n\n\n\n\n除非\n\n“除非”, liên từ, biểu thị điều kiện duy nhất, giống với “只有”, đằng sau thường đi với “才、否则、不然”. Ví dụ：\n\n可除非是那些好种易活、自己会奋斗的花草，否则他是不养的。\n除非急需一大笔钱，我才会考虑卖了这房子。\n\n“除非”,cũng là giới từ, biểu thị không bao gồm tính toán, giống với “除了”. Ví dụ:\n\n这种机器，除非李阳，没人修得好。\n日常工作他从来不过问，除非极特殊的问题。\n\n\n他工作时不喜欢别人打扰，除非很重要的事，别人的电话她都不接。\n除非偶尔跟家庭在客厅一边聊天一边看新闻，我平时一般都不看电视。\nA：这个周末你陪我去看场电影，行吗？\n\nB：想让我答应你，除非这次你能付票。\n\n\n直\n\n“直”，làm phó từ có thể biểu thị luôn luôn, thẳng tuốt, trực tiếp, đằng sau là động từ đơn âm tiết. Ví dụ:\n\n这趟车可以直达北京，非常方便。\n直到今天，我也不明白他当时为什么发那么大脾气。\n\n“直”, còn có thể biểu thị liên tiếp, không ngừng nghỉ (một động tác,hành vi). Ví dụ:\n\n父亲听说儿子卖了房子，气得直发抖。\n几百盆花，要很快地抢到屋里去，累得腰酸腿疼，热汗直流。\n\n\nB\nA\nB\nA\n\n\n\n反正\n\n“反正”, phó từ, biểu thị tình huống tuy không giống nhưng kết quả lại như nhau. Ví dụ:\n\n不管你们谁去，反正我不会去。\n我不知道花草们受我的照顾， 感谢我不感谢，反正我要感谢它们。\n\n“反正”, còn biểu thị ngữ khí kiên quyết khẳng định. Ví dụ:\n\n你别再说了，反正我是会考虑的。\n算了，反正不是什么要紧事，还是别打扰他们了。\n\n\n你别问那么多了，反正你也不敢问她的电话到吗。\n反正事情也那样发生了，信不信，你随便吧。\nA：这是今年最流行的颜色，你真没眼力。\n\nB：我反正只能穿黑色的。\n\n\nPhân biệt 应付 và 处理\n\n共同点：Đều là động từ, đề có nghĩa là áp dụng phương pháp, biện pháp đối với người, sự việc.\n\n如：依我看，以他现在有的经验应付/处理不了目前的工作。\n\n不同点：\n\n\n\n\n\n\n\n\n\n应付\n处理\n\n\n\n\n1\nThiên về biểu thị áp dụng phương pháp phù hợp với người và sự việc.\n如：他们会想方设法说服你，你准备好怎么应付他们了吗?\nThiên về nhấn mạnh giải quyết vấn đề.\n如：严重的环境污染使人们深思该如何处理好人与自然的关系。\n\n\n2\nCòn có ý nghĩa là làm việc không chăm chỉ, không có trách nhiệm, chỉ thể hiện tốt mặt bề ngoài.\n如：小林学习不认真，完全是在应付父母和老师。\nKhông có ý nghĩa này.\n\n\n3\nKhông có ý nghĩa này.\nCòn có nghĩa là sắp xếp, xứ lý sự vật.\n如：洗衣机的包装纸箱，既占地方又没什么用，快处理了吧。\n\n\n4\nKhông có ý nghĩa này.\nCòn có nghĩa thanh lí.\n如：这批过季的衣服尽快减价处理吧。\n\n\n\n\n\n\n问题：行动\n\n行动\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n拆\nchai1 - to dismantle，tháo dỡ, dỡ ra\n\n\n2\n撕\nsi1 - to tear, xé\n\n\n3\n摸\nmo1 - to touch, feel, stroke, sờ, chạm\n\n\n4\n拍\npai1 - to beat with the palm, vỗ, đập bằng lòng bàn tay\n\n\n5\n抓\nzhua1 - to grab, túm, nắm, chộp, bắt\n\n\n6\n捡\njian3 - to pick up and object, nhặt (một vật)\n\n\n7\n摘\nzhai1 - to pick a fruit, hái (trái cây)\n\n\n8\n披\npi1 - to put on a coat, cloak, or cape, khoác (1 cái áo)\n\n\n9\n偷\ntou1 - to steal, trộm, cắp\n\n\n10\n抢\nqiang3 - to rob, cướp\n\n\n11\n捐\njuan1 - to donate, quyên góp, hiến tặng\n\n\n12\n扶\nfu2 - to support (and prevent from failing), đỡ, nâng (tránh ngã)\n\n\n13\n挡\ndang3 - to block, to get in the way of, chặn, cản trở\n\n\n14\n栏\nlan2 - to block, to hold back, chắn, ngăn cản\n\n\n15\n退\ntui4 - to retreat, rút lui, lùi lại\n\n\n\n\n\n\n人类自古以来就与大自然密不可分。我们呼吸着树木净化的氧气，饮用着山泉流淌的清水，享受着阳光带来的温暖。然而，随着科技的发展，人类逐渐远离了自然，甚至肆意破坏生态环境。我们忘记了自己也是自然的一部分。唯有重新认识人与自然的关系，珍惜地球资源，保护生态平衡，才能实现可持续发展。让我们携手共建人与自然和谐共处的美好家园。\n\n\n\n1.  描述一个给你留下深刻印象的自然景观，说说你的感受？\n去年年底，我参加了一次前往越南南部森林的徒步旅行。两天约35公里的行程给我留下了深刻的印象。一方面，我亲眼目睹了许多奇妙的景象：绿色的草坡与神秘的热带森林交织在一起，雨后云雾缭绕着山坡，随后是美丽的日落，接着是布满成千上万颗星星的夜空——漆黑的森林夜里只有一丝微光。另一方面，我走过的路几乎布满了人的脚印和轮胎痕迹，前后都有其他旅行团。我的前辈告诉我，十年前他们走的也是同样的路，那时几乎没有人为痕迹。我突然开始担心人类对自然的侵扰越来越严重。或许再过几年，旅游项目就会如雨后春笋般在这片森林中涌现，而下一代人将无法再像我一样享受这片野性之美。\n2.  有人说，大自然和人类就像母子一样，你同意这种说法吗？\n我同意大自然和人类像母子一样的说法。就如母亲孕育、哺育孩子，大自然为人类提供了生存所需的一切：清新的空气、干净的水源、肥沃的土地。我们依赖自然而生，从自然中汲取养分成长。然而，这个比喻也提醒我们：作为”子女”，我们有责任善待、保护我们的”母亲”。只有尊重自然、维护生态平衡，我们才能确保这份珍贵的母子关系永续存在。\n3.  有人认为人类应该尊重大自然，也有人认为人类可以征服大自然，你同意哪种观点？\n我更倾向于人类应该尊重大自然的观点。历史告诉我们，盲目征服自然往往导致灾难性后果，如沙漠化、物种灭绝等生态危机。相反，以尊重和理解的态度与自然和谐共处，才能实现真正的可持续发展。这并不意味着我们要放弃科技进步，而是要在发展过程中充分考虑生态影响，寻求人与自然的平衡。只有这样，我们才能为子孙后代创造一个更美好的家园。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第36课-老舍与养花/index.html#生词",
    "href": "学汉语的日记/HSK5下-第36课-老舍与养花/index.html#生词",
    "title": "HSK5下 | 第36课： 老舍与养花",
    "section": "",
    "text": "老舍与养花\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n养\nyǎng - v. to raise, to keep, to grow\n\n\n2\n除非\nchú fēi - conj. only if, unless\n\n\n3\n奋斗\nfèn dòu - v. to fight, to struggle, to strive\n\n\n4\n乐趣\nlè qù - n. joy, pleasure\n\n\n5\n在乎\nzàihu - v. to care, to mind\n\n\n6\n朵\nduǒ - m. used for flowers and clouds\n\n\n7\n剪刀\njiǎn dāo - n. scissors\n\n\n8\n捡\njiǎn - v. to pick up, to collect\n\n\n9\n装饰\nzhuāng shì - n. decoration\n\n\n10\n结合\njié hé - v. to combine, to integrate\n\n\n11\n暴雨\nbào yǔ - n. rainstorm\n\n\n12\n紧急\njǐn jí - adj. urgent, emergent\n\n\n13\n劳驾\nláo jià - v. to trouble sb. (to do sth.)\n\n\n14\n抢救\nqiǎngjiù - v. to rescue, to save\n\n\n15\n腰\nyāo - n. waist\n\n\n16\n直\nzhí - adv. continuously, straight\n\n\n17\n不然\nbùrán - conj. or else, otherwise\n\n\n18\n回报\nhuí bào - v. to repay, to requite\n\n\n19\n真理\nzhēnlǐ - n. truth\n\n\n20\n浇\njiāo - v. to water, to pour (liquid on sth.)\n\n\n21\n潮湿\ncháoshī - adj. wet, moist\n\n\n22\n施肥\nshī féi - v. to apply fertilizer\n\n\n23\n熟练\nshú liàn - adj. skilled, practiced\n\n\n24\n应付\nyìng fu - v. to handle, to cope with\n\n\n25\n鲜艳\nxiān yàn - adj. bright-colored\n\n\n26\n自豪\nzìháo - adj. proud\n\n\n27\n吹\nchuī - v. to boast, to brag, to blow\n\n\n28\n爱心\naì xīn - n. love, compassion\n\n\n29\n分享\nfēnxiǎng - v. to share (good things such as joy and rights)\n\n\n30\n昙花\ntánhuā - n. broad-leaved epiphyllum (type of plant)\n\n\n31\n庆祝\nqìng zhù - v. to celebrate\n\n\n32\n保留\nbǎoliú - v. to reserve, to save\n\n\n33\n菊花\njúhuā - n. chrysanthemum (type of flower)\n\n\n34\n砸\nzá - v. to crush, to smash\n\n\n35\n悲（伤）\nbēi (shāng) - adj. sad, sorrowful\n\n\n36\n反正\nfǎn zhèng - adv. (used to indicate the same result despite different circumstances) anyway, no matter what\n\n\n37\n热爱\nrè ài - v. to love ardently"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第36课-老舍与养花/index.html#注释",
    "href": "学汉语的日记/HSK5下-第36课-老舍与养花/index.html#注释",
    "title": "HSK5下 | 第36课： 老舍与养花",
    "section": "",
    "text": "除非\n\n“除非”, liên từ, biểu thị điều kiện duy nhất, giống với “只有”, đằng sau thường đi với “才、否则、不然”. Ví dụ：\n\n可除非是那些好种易活、自己会奋斗的花草，否则他是不养的。\n除非急需一大笔钱，我才会考虑卖了这房子。\n\n“除非”,cũng là giới từ, biểu thị không bao gồm tính toán, giống với “除了”. Ví dụ:\n\n这种机器，除非李阳，没人修得好。\n日常工作他从来不过问，除非极特殊的问题。\n\n\n他工作时不喜欢别人打扰，除非很重要的事，别人的电话她都不接。\n除非偶尔跟家庭在客厅一边聊天一边看新闻，我平时一般都不看电视。\nA：这个周末你陪我去看场电影，行吗？\n\nB：想让我答应你，除非这次你能付票。\n\n\n直\n\n“直”，làm phó từ có thể biểu thị luôn luôn, thẳng tuốt, trực tiếp, đằng sau là động từ đơn âm tiết. Ví dụ:\n\n这趟车可以直达北京，非常方便。\n直到今天，我也不明白他当时为什么发那么大脾气。\n\n“直”, còn có thể biểu thị liên tiếp, không ngừng nghỉ (một động tác,hành vi). Ví dụ:\n\n父亲听说儿子卖了房子，气得直发抖。\n几百盆花，要很快地抢到屋里去，累得腰酸腿疼，热汗直流。\n\n\nB\nA\nB\nA\n\n\n\n反正\n\n“反正”, phó từ, biểu thị tình huống tuy không giống nhưng kết quả lại như nhau. Ví dụ:\n\n不管你们谁去，反正我不会去。\n我不知道花草们受我的照顾， 感谢我不感谢，反正我要感谢它们。\n\n“反正”, còn biểu thị ngữ khí kiên quyết khẳng định. Ví dụ:\n\n你别再说了，反正我是会考虑的。\n算了，反正不是什么要紧事，还是别打扰他们了。\n\n\n你别问那么多了，反正你也不敢问她的电话到吗。\n反正事情也那样发生了，信不信，你随便吧。\nA：这是今年最流行的颜色，你真没眼力。\n\nB：我反正只能穿黑色的。\n\n\nPhân biệt 应付 và 处理\n\n共同点：Đều là động từ, đề có nghĩa là áp dụng phương pháp, biện pháp đối với người, sự việc.\n\n如：依我看，以他现在有的经验应付/处理不了目前的工作。\n\n不同点：\n\n\n\n\n\n\n\n\n\n应付\n处理\n\n\n\n\n1\nThiên về biểu thị áp dụng phương pháp phù hợp với người và sự việc.\n如：他们会想方设法说服你，你准备好怎么应付他们了吗?\nThiên về nhấn mạnh giải quyết vấn đề.\n如：严重的环境污染使人们深思该如何处理好人与自然的关系。\n\n\n2\nCòn có ý nghĩa là làm việc không chăm chỉ, không có trách nhiệm, chỉ thể hiện tốt mặt bề ngoài.\n如：小林学习不认真，完全是在应付父母和老师。\nKhông có ý nghĩa này.\n\n\n3\nKhông có ý nghĩa này.\nCòn có nghĩa là sắp xếp, xứ lý sự vật.\n如：洗衣机的包装纸箱，既占地方又没什么用，快处理了吧。\n\n\n4\nKhông có ý nghĩa này.\nCòn có nghĩa thanh lí.\n如：这批过季的衣服尽快减价处理吧。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第36课-老舍与养花/index.html#扩展",
    "href": "学汉语的日记/HSK5下-第36课-老舍与养花/index.html#扩展",
    "title": "HSK5下 | 第36课： 老舍与养花",
    "section": "",
    "text": "问题：行动\n\n行动\n\n\n\n\n\n\n\nNo.\nWord\nMeaning\n\n\n\n\n1\n拆\nchai1 - to dismantle，tháo dỡ, dỡ ra\n\n\n2\n撕\nsi1 - to tear, xé\n\n\n3\n摸\nmo1 - to touch, feel, stroke, sờ, chạm\n\n\n4\n拍\npai1 - to beat with the palm, vỗ, đập bằng lòng bàn tay\n\n\n5\n抓\nzhua1 - to grab, túm, nắm, chộp, bắt\n\n\n6\n捡\njian3 - to pick up and object, nhặt (một vật)\n\n\n7\n摘\nzhai1 - to pick a fruit, hái (trái cây)\n\n\n8\n披\npi1 - to put on a coat, cloak, or cape, khoác (1 cái áo)\n\n\n9\n偷\ntou1 - to steal, trộm, cắp\n\n\n10\n抢\nqiang3 - to rob, cướp\n\n\n11\n捐\njuan1 - to donate, quyên góp, hiến tặng\n\n\n12\n扶\nfu2 - to support (and prevent from failing), đỡ, nâng (tránh ngã)\n\n\n13\n挡\ndang3 - to block, to get in the way of, chặn, cản trở\n\n\n14\n栏\nlan2 - to block, to hold back, chắn, ngăn cản\n\n\n15\n退\ntui4 - to retreat, rút lui, lùi lại"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第36课-老舍与养花/index.html#运用",
    "href": "学汉语的日记/HSK5下-第36课-老舍与养花/index.html#运用",
    "title": "HSK5下 | 第36课： 老舍与养花",
    "section": "",
    "text": "人类自古以来就与大自然密不可分。我们呼吸着树木净化的氧气，饮用着山泉流淌的清水，享受着阳光带来的温暖。然而，随着科技的发展，人类逐渐远离了自然，甚至肆意破坏生态环境。我们忘记了自己也是自然的一部分。唯有重新认识人与自然的关系，珍惜地球资源，保护生态平衡，才能实现可持续发展。让我们携手共建人与自然和谐共处的美好家园。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第36课-老舍与养花/index.html#口语",
    "href": "学汉语的日记/HSK5下-第36课-老舍与养花/index.html#口语",
    "title": "HSK5下 | 第36课： 老舍与养花",
    "section": "",
    "text": "1.  描述一个给你留下深刻印象的自然景观，说说你的感受？\n去年年底，我参加了一次前往越南南部森林的徒步旅行。两天约35公里的行程给我留下了深刻的印象。一方面，我亲眼目睹了许多奇妙的景象：绿色的草坡与神秘的热带森林交织在一起，雨后云雾缭绕着山坡，随后是美丽的日落，接着是布满成千上万颗星星的夜空——漆黑的森林夜里只有一丝微光。另一方面，我走过的路几乎布满了人的脚印和轮胎痕迹，前后都有其他旅行团。我的前辈告诉我，十年前他们走的也是同样的路，那时几乎没有人为痕迹。我突然开始担心人类对自然的侵扰越来越严重。或许再过几年，旅游项目就会如雨后春笋般在这片森林中涌现，而下一代人将无法再像我一样享受这片野性之美。\n2.  有人说，大自然和人类就像母子一样，你同意这种说法吗？\n我同意大自然和人类像母子一样的说法。就如母亲孕育、哺育孩子，大自然为人类提供了生存所需的一切：清新的空气、干净的水源、肥沃的土地。我们依赖自然而生，从自然中汲取养分成长。然而，这个比喻也提醒我们：作为”子女”，我们有责任善待、保护我们的”母亲”。只有尊重自然、维护生态平衡，我们才能确保这份珍贵的母子关系永续存在。\n3.  有人认为人类应该尊重大自然，也有人认为人类可以征服大自然，你同意哪种观点？\n我更倾向于人类应该尊重大自然的观点。历史告诉我们，盲目征服自然往往导致灾难性后果，如沙漠化、物种灭绝等生态危机。相反，以尊重和理解的态度与自然和谐共处，才能实现真正的可持续发展。这并不意味着我们要放弃科技进步，而是要在发展过程中充分考虑生态影响，寻求人与自然的平衡。只有这样，我们才能为子孙后代创造一个更美好的家园。"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第36课-老舍与养花/index.html#听力",
    "href": "学汉语的日记/HSK5下-第36课-老舍与养花/index.html#听力",
    "title": "HSK5下 | 第36课： 老舍与养花",
    "section": "2.1. 听力",
    "text": "2.1. 听力\n1.  女：您写作的过程中会休息吗？\n男：会。我经常写一会儿就到院子里转转，给花草剪剪枝，捡几块漂亮的小石头。这样脑力和体力可以很好地结合，身心得到放松。\n问：男的为什么要给花草剪枝？（D为了放松身心）\n2.  男：你的花养得这么好，总结过经验吗？\n女：有的花喜干，就别多浇水；有的花喜欢潮湿的环境，就别放在太阳地里。\n问：女的认为养花最重要的是什么？（C了解花的习性）\n3.  女：你怎么看起来这么累啊？\n男：马上快下班的时候来了个病人，情况紧急，又做了个两小时的手术，累得我腰都直不起来了。\n问：男的怎么了？（B加班了）\n4.  男：这次真是搞砸了！\n女：生活总是有成功也有失败，有悲伤也有幸福，别这么灰心。Làm hỏng chuyện\n问：女的是什么语气？（B安慰）\n5.  女：明天的比赛你准备好了吗？\n男：熟练工种，天天干，我闭着眼睛都能应付，放心吧！\n问：男的是什么意思？（A很有把握）\n6.  男：你靠扫大街挣钱，不觉得丢脸吗？\n女：我不偷不抢，靠自己劳动养活自己，有什么丢脸的？\n问：女的的钱是怎么来的？（C自己挣得）\n7.  女：听说老舍先生特别爱花\n男：对，他养的花很多，满满摆了一院子。\n女：他都喜欢什么花？\n男：好种易活、自己会奋斗的。因为他说北京的气候不适合养花，想把南方的名花养活并非易事。\n问：老舍先生喜欢什么花？（B容易养的花）\n8.  男：北京有个老舍茶馆，是不是老舍先生开的店\n女：不是，是以他的名字命名的。\n男：你去过吗？怎么样？\n女：很好，有便宜的大碗茶、各种北京传统风味小吃，还有京剧、相声表演什么的。Tướng Thanh ~ kịch nói, kể chuyện\n问：关于老舍茶馆，下列哪项正确？（B用老舍的名字命名）\n9.  女：怎么，又跟你们家亲爱的吵架了\n男：没吵架，她乱发脾气，我懒得理她，就自己出来了。\n女：这可不行，除非你以后都不打算回去了，不然还是早点 儿 回家的好。\n男：先各自冷静一下吧。\n问：男的是什么意思？（A暂时不回去）\n绿茶味如 Mùi trà xanh, 白莲花 Quần xì trắng , 黄脸婆 Người vợ tần tảo〈俚〉〈讽〉形容人披上清純的外表，但股子裡卻仍然保有腐敗的思想\n10. 男：这是我前天买的衣服，有点儿问题，麻烦您帮我退了。\n女：您这已经拆了，我们退不了。\n男：这衣服是质量问题，必须得退。\n女：那您也别把价签儿剪了啊！\n问：女的为什么不给他退货？（B价签被剪了）\n11-12.\n人们都爱吃营养丰富、味道鲜美的新鲜蔬菜和瓜果。然而，生活在城市中的人们，只能购买超市或菜场里出售的蔬果商品。这些商品，不但在新鲜度和品种上不能完全满足人们的需求，而且价格也比较贵，尤其在冬天更是如此。为了节省开支，吃到自己喜欢的蔬菜，在英国，许多居民开始自己用营养土在窗台上种植蔬菜。安排设计好的话，不仅可以吃到自己喜欢的蔬果，家里还多了一些可供观赏的植物。\n11．根据这段话，许多英国人在哪里种植蔬果？（C家中）\n12．为什么有些人不愿意买超市的蔬果商品？（B不够新鲜）\n13-14.\n据新闻报道，一个19岁的北京女孩儿在10年间有过将近40个拓麻歌子，她一共在这种迷你游戏机上花去了一万多元。你可能不知道“拓麻歌子”这个名字，但是你一定记得一种叫作“电子宠物”的东西。电子宠物现在听起来可能是个有点儿年代感的说法了，不过它可是许多人，尤其是90后的童年记忆。甚至到现在，这种迷你的电子宠物游戏机依旧吸引着一众忠实的爱好者。\n13．这段话主要谈论什么？（D一种电子游戏机）\n14．关于电子宠物，下列哪项正确？（B受90后欢迎）"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第36课-老舍与养花/index.html#阅读",
    "href": "学汉语的日记/HSK5下-第36课-老舍与养花/index.html#阅读",
    "title": "HSK5下 | 第36课： 老舍与养花",
    "section": "2.2. 阅读",
    "text": "2.2. 阅读\n15-18.\n老舍先生是一位语言大师。他写作的时候，习惯边写边读，自己大声地15D念给自己听。16B凡是耳朵通不过的，就马上修改。有一次，一家出版社准备把他的儿童文学作品《宝船》收入汉语课本中。其中有一句“开船喽”，17A编辑问老舍，为什么不用“开船啦”，虽然都是语气词，但显然“啦”平时更为常见。先生回答，我朗读的时候，感觉“喽”是对大伙儿说的，如果一个人说“开船喽”，表示他在招呼大家；如果说“开船啦”，意思18B则没有这么明确。两个词的区别也许就在这里。\n19. A老舍会邀请朋友来家赏花\n老舍很有爱心，吏懂得快乐要分享。每到昙花开放的时候，他就约上几位朋友来家里赏花庆祝。花分根了，一棵分为几棵，他会毫无保留地送给朋友们。看着友人高兴地拿走自己的劳动果实，老舍心里十分欢喜。\n20. D老舍没有为舞会买新衣服\n梅花香自苦寒来（Điển cố, vẻ đẹp đến từ sự cực khổ, không được coi là thành ngữ tục ngữ）。老舍小时候家庭条件不好，在艰苦的生活环境中培养了勤俭节约的精神。当他功成名就之后，仍然保持着勤俭的好习愤。一次，朋友邀请他参加一个舞会。可是老舍只有两套灰布中山装，洗过几次后，都显得旧了，穿在身上像个清洁工。老舍就穿着这样的衣服进了舞会，他对投来不解目光的朋友说：“对不起了，这已经是我最好的衣服了。”\n21. B宠物可以给主人一种精神安慰\n弗洛伊德认为，真正的心理咨询是咨询师把自己当作“会听故事的小男孩儿”，换句话说，咨询师就是倾听，而不做任何评价。而宠物就是良好的聆听者。从这个角度说，它们是天生的心理咨询师，可以给予主人一种牛寺有的精神支持，安慰他们的心灵。\n22. B鲜艳的颜色容易吸引人的注意\n鲜艳的颜色往往是人们的视觉中心，它是一个环境中最突出的部分。利用好鲜艳的颜色，就给人的视觉带来了主题，就像一首优美的歌曲，不仅仅在曲凋上有丰富的变化，而且能在听众的心中产生共鸣，达到高潮。所以，在室内装饰中，利用好鲜艳的颜色，就会使居室既雅致漂亮，又具有鲜明的个性，给人带来丰富的情感享受，对人的心理活动产生积极的影响。\n23-25.\n在第四纪更新世时期，小熊猫曾广泛分布于欧亚大陆，欧洲中部和英国都有化石纪录。现代小熊猫主要生活在中国的西藏、云南和四川，也见于印度、尼泊尔、不丹和缅甸北部。它们喜欢凉爽潮湿的高山林区或竹林，栖居在树洞或石洞中，凌晨和黄昏出洞觅食。成年的小熊猫十分害羞，行动缓慢，但很善于爬树，整曰待在枝叶茂密的树上。所以，即使是专业考察人员，也难得一见。\n刚出生的小熊猫很小，但三天后体重就可以达到184克左右；满月时，体重就到了400多克；一个半月后，它的形体就跟成年的小熊猫基本一样了，只是稍微小一点儿。当然，成年的小熊猫也没有多大，比家猫大不了多少。它的外形也和家猫有相似之处，猫脸熊身，似猫非猫，似熊非熊，还拖着一条粗大带彩色环纹的尾巴，显然并非短尾大熊猫的亲族。\n跟大熊猫一样，小熊猫已适应杂食，且以植物为主，多食嫩叶、果实，尤其是竹子，有时也捕食小鸟和鸟蛋。中国竹林种植面积广，本来可以为小熊猫提供良好的生存环境。但是，近年来，竹林砍伐严重，小熊猫觅食越来越困难，它们的生存状况受到了威肋。\n23. C潮湿的\n24. A曾在欧洲生活过\n25. D担心\n26-28.\n因为人的生理结构更接近草食动物，用这个理由来宣传素食的说法很常见。其实，单从生理结构上看，人类更适合杂食。人体需要的营养成分有的在植物性食物中含得多，有的在动物性食物中含得多，合理的杂食食谱能够方便有效地实现营养均衡。纯素食也可以实现营养全面均衡，但难度比较大。\n要判断人类是不是更适合吃素，还是应该去探索吃素对人体健康的影响。\n“素食者更加健康长寿”的说法流传甚广，似乎也符合人们的直观感觉。为了查证这种说法是否正确，英美等国科学家进行了几项大规模、长时间的跟踪调查。结果发现，与社会平均水平相比，素食者的平均预期寿命确实更高。这个结果当然让素食者很高兴。不过，素食者往往还伴随着其他的生活方式，比如：素食者中抽烟、喝酒的人更少，他们一般饮食比较节制，甚至生活方式的其他方面——比如锻炼、心态等——也“更为健康”。科学上有很充分的证据表明这些“混杂因素”有助于健康长寿。要知道素食到底对健康长寿有什么样的影响，就要排除这些因素的影响。随机双盲试验很难进行，不过可以用统计工具对大样本的调查数据进行回归，把“混杂因素”对结果的贡献剔除出去。结果发现，素食这个因素对健康长寿其实没有明显的影响。也就是说，素食者健康长寿的原因，主要是他们生活方式的其他方面，而不是素食本身。Bài 26-28 lúc nãy có từ 随机双盲试验 - Randomized Double-blind Experiment, làm data mà giờ mới biết term này, đi thi gặp mấy từ này chắc chớt :-dig, source\n不过我们也要承认，以植物性食品为主的均衡饮食有利于健康，也有利于环境。\n26. C动物性和植物性食物都吃\n27. D生活习惯良好\n28. B有保留的赞同"
  },
  {
    "objectID": "学汉语的日记/HSK5下-第36课-老舍与养花/index.html#书写",
    "href": "学汉语的日记/HSK5下-第36课-老舍与养花/index.html#书写",
    "title": "HSK5下 | 第36课： 老舍与养花",
    "section": "2.3. 书写",
    "text": "2.3. 书写\n奋斗、热爱、熟练、庆祝、不然。\n人生如戏，舞台由我们自己搭建。热爱是起点，激发我们追逐梦想的勇气。奋斗是过程，磨砺我们攀登高峰的毅力。熟练是结果，彰显我们不懈努力的成果。成功庆祝不仅是对成就的肯定，更是继续前行的动力。不然，我们可能迷失在未来的挑战中，忘记了曾经的成长。珍惜一点一滴的进步，方能在人生的舞台上演绎精彩。"
  },
  {
    "objectID": "blog/2024-11-26-nn-z2h-p4/index.html#the-fully-linear-case-of-no-non-linearity",
    "href": "blog/2024-11-26-nn-z2h-p4/index.html#the-fully-linear-case-of-no-non-linearity",
    "title": "NN-Z2H Lesson 4: Building makemore part 3 - Activations & Gradients, BatchNorm",
    "section": "the fully linear case of no non-linearity",
    "text": "the fully linear case of no non-linearity\nNow imagine if we remove the tanh from all layers, the recommend gain now for Linear is 1.\n\n\nShow the code\nlayers = [\n  Linear(n_embd * block_size, n_hidden), #Tanh(),\n  Linear(           n_hidden, n_hidden), #Tanh(),\n  Linear(           n_hidden, n_hidden), #Tanh(),\n  Linear(           n_hidden, n_hidden), #Tanh(),\n  Linear(           n_hidden, n_hidden), #Tanh(),\n  Linear(           n_hidden, vocab_size),\n]\n\n\nBut you’ll end up getting a pure linear network. No matter of how many Linear Layers you stacked up, it just the combination of all layers to a massive linear function \\(y = xA^T + b\\), which will greatly limit the capacity of the neural nets."
  },
  {
    "objectID": "blog/2024-12-02-nn-z2h-p5/index.html",
    "href": "blog/2024-12-02-nn-z2h-p5/index.html",
    "title": "NN-Z2H Lesson 5: Building makemore part 4 - Becoming a Backprop Ninja",
    "section": "",
    "text": "This is not orginal content!\n\n\n\nThis is my study notes / codes along with Andrej Karpathy’s “Neural Networks: Zero to Hero” series.\n\n\n\nintro: why you should care\n\n\n\nswole doge style backpropagation, image credit to this video\n\n\nIn the previous lecture, we’re introduced to some common issues with our “shallow” (and of course for deep as well) neural network and how to fix with the initialization setting and Batch Normalization. We’re also learned some diagnostic tools to observe forward pass activations, backward pass gradients, and weights update, to calibrate the training loop. In this lecture, we aim to replace this line of code:\n\n\nShow the code\n... loss.backward() ...\n\n\nwith from-scratch-code. It’s basically identical to MicroGrad, but on Tensor rather than Scalar. But why we should care?\n\nThe problem with Backpropagation is that it is a leaky abstraction. – Andrej Karpathy\n\nreadmore: - https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b, - and https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html.\ninshort, to effectively debug neural network, we should be deeply understanding how back propagation work under the hood. Let’s do it!\n\n\nstarter code\n\nimport libraries:\n\n\nShow the code\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n\n\nread data:\n\n\nShow the code\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt\"\nwords = pd.read_csv(url, header=None).iloc[:, 0].tolist()\nwords[:8]\n\n\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n\n\n\n\nbuild vocab:\n\n\nShow the code\n# build the vocabulary of characters and mapping to/from integer\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i, s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i: s for s, i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n\n\n{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n\n\n\n\nbuild dataset splits (identical to previous so I folded it):\n\n\nShow the code\nblock_size = 3 # context length: how many characters do we take to predict the next one.\n# build the dataset\ndef buid_dataset(words):\n    X, Y = [], []\n\n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix]\n\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\nXtr, Ytr = buid_dataset(words[:n1])        # 80#\nXdev, Ydev = buid_dataset(words[n1:n2])    # 10%\nXte, Yte = buid_dataset(words[n2:])        # 10%\n\n\ntorch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n\n\n\n\nutility function we will use later when comparing manual gradients to PyTorch gradients.\n\n\nShow the code\ndef cmp(s, dt, t):\n  ex = torch.all(dt == t.grad).item()\n  app = torch.allclose(dt, t.grad)\n  maxdiff = (dt - t.grad).abs().max().item()\n  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n\n\n\n\nnetwork construction:\n\n\nShow the code\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 64 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden))*0.1 + 1.0\nbnbias = torch.randn((1, n_hidden))*0.1\n\n# Note: I am initializating many of these parameters in non-standard ways\n# because sometimes initializating with e.g. all zeros could mask an incorrect\n# implementation of the backward pass.\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n\n\n4137\n\n\n\n\nmini-batch construction:\n\n\nShow the code\nbatch_size = 32\nn = batch_size # a shorter variable also, for convenience\n# construct a minibatch\nix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\nXb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n\n\n\nforward pass, “chunkated” into smaller steps that are possible to backward one at a time.\n\n\nShow the code\nemb = C[Xb] # embed the characters into vectors\nembcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n\n# Linear layer 1\nhprebn = embcat @ W1 + b1 # hidden layer pre-activation\n\n# BatchNorm layer\nbnmeani = 1/n*hprebn.sum(0, keepdim=True)\nbndiff = hprebn - bnmeani\nbndiff2 = bndiff**2\nbnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\nbnvar_inv = (bnvar + 1e-5)**-0.5\nbnraw = bndiff * bnvar_inv\nhpreact = bngain * bnraw + bnbias\n\n# Non-linearity\nh = torch.tanh(hpreact) # hidden layer\n\n# Linear layer 2\nlogits = h @ W2 + b2 # output layer\n\n# cross entropy loss (same as F.cross_entropy(logits, Yb)), Kullback–Leibler divergence\nlogit_maxes = logits.max(1, keepdim=True).values\nnorm_logits = logits - logit_maxes # subtract max for numerical stability\ncounts = norm_logits.exp()\ncounts_sum = counts.sum(1, keepdims=True)\ncounts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\nprobs = counts * counts_sum_inv\nlogprobs = probs.log()\nloss = -logprobs[range(n), Yb].mean()\n\n# PyTorch backward pass\nfor p in parameters:\n  p.grad = None\nfor t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n         embcat, emb]:\n  t.retain_grad()\n\nhprebn.retain_grad() # Tuan added this line since code above does not ensure hprebn's grad to be retained.\n\nloss.backward()\nloss\n\n\ntensor(3.3267, grad_fn=&lt;NegBackward0&gt;)\n\n\n\n\n\nexercise 1: backproping the atomic compute graph\nI can do 60, 70% of this myself ~ for popular mathematics operations. What I need to remember after these excercises are backwards of:\n\nelements in matrix mult;\nmax operator;\nindexing operator; and\nbroadcasting behavior.\n\n\n\n\nAndrej inductive reasoning, explains how to get a derivative from matrix multiplication.\n\n\nGiven the matrix mult expression \\(d = a @ b + c\\), we have:\n\n\\(\\frac{\\partial L}{\\partial \\mathbf{a}} = \\frac{\\partial L}{\\partial \\mathbf{d}} @ \\mathbf{b}^T\\)\n\\(\\frac{\\partial L}{\\partial \\mathbf{b}} = \\mathbf{a}^T @ \\frac{\\partial L}{\\partial \\mathbf{d}}\\)\n\\(\\frac{\\partial L}{\\partial \\mathbf{c}} = \\frac{\\partial L}{\\partial \\mathbf{d}}.sum(0)\\)\n\n\n\nShow the code\n# Exercise 1: backprop through the whole thing manually, \n# backpropagating through exactly all of the variables \n# as they are defined in the forward pass above, one by one\nprint(\"Cross entropy loss\".upper())\ndlogprobs = torch.zeros_like(logprobs) # create zeros tensor with same size\n1dlogprobs[range(n), Yb] = - 1.0 / n\ndprobs = torch.zeros_like(probs) \n2dprobs = (1.0 / probs) * dlogprobs\n3dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\ndcounts_sum = (-1 * counts_sum**-2) * dcounts_sum_inv\n4dcounts = torch.ones_like(counts) * dcounts_sum + counts_sum_inv * dprobs\ndnorm_logits = (norm_logits.exp()) * dcounts # = counts * dcounts\ndlogit_maxes = (- dnorm_logits).sum(1, keepdim=True) # broadcasting, again\n\ncmp('logprobs', dlogprobs, logprobs)\ncmp('probs', dprobs, probs)\ncmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\ncmp('counts_sum', dcounts_sum, counts_sum)\ncmp('counts', dcounts, counts)\ncmp('norm_logits', dnorm_logits, norm_logits)\ncmp('logit_maxes', dlogit_maxes, logit_maxes)\n\n\ndlogits = (dnorm_logits.clone() \n          + F.one_hot( logits.max(1).indices, # broadcasting, again\n                        num_classes=logits.shape[1]\n5                        ) * dlogit_maxes)\n\nprint(\"Layer 2 with Linear and Non-linearity tanh\".upper())\ndh =  dlogits @ W2.T\ndW2 = h.T @ dlogits\ndb2 = dlogits.sum(0) # sum by column, vertical, to eliminate the 0-index dim\n\ncmp('logits', dlogits, logits)\ncmp('h', dh, h)\ncmp('W2', dW2, W2)\ncmp('b2', db2, b2)\n\nprint(\"Batch Norm layer\".upper())\ndhpreact = (1 - h**2) * dh # output of the tanh, square\n6dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\ndbnbias = dhpreact.sum(0, keepdim=True)\ndbnraw = bngain * dhpreact\n7dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\ndbnvar = (-0.5 * (bnvar + 1e-5)**-1.5) * dbnvar_inv\ndbndiff2 = 1/(n-1) * torch.ones_like(bndiff2) * dbnvar\ndbndiff = bnvar_inv * dbnraw + 2 * bndiff * dbndiff2\ndbnmeani = (- torch.ones_like(dbndiff) * dbndiff).sum(0)\n\ncmp('hpreact', dhpreact, hpreact)\ncmp('bngain', dbngain, bngain)\ncmp('bnbias', dbnbias, bnbias)\ncmp('bnraw', dbnraw, bnraw)\ncmp('bnvar_inv', dbnvar_inv, bnvar_inv)\ncmp('bnvar', dbnvar, bnvar)\ncmp('bndiff2', dbndiff2, bndiff2)\ncmp('bndiff', dbndiff, bndiff)\ncmp('bnmeani', dbnmeani, bnmeani)\n\nprint(\"Linear layer 1\".upper())\ndhprebn = dbndiff.clone() + (1.0/n * (torch.ones_like(hprebn) * dbnmeani))\ndembcat = dhprebn @ W1.T\ndW1 = embcat.T @ dhprebn\ndb1 = dhprebn.sum(0, keepdim=True)\n8demb = dembcat.view(emb.shape)\n9dC = torch.zeros_like(C)\nfor k in range(Xb.shape[0]): # iterate all elements of the Xb\n  for j in range(Xb.shape[1]):\n    ix = Xb[k,j]\n    dC[ix] += demb[k,j]\n# Method 2\ndC = torch.zeros_like(C)\ndC.index_add_(0, Xb.view(-1), demb.view(-1, 10))\n\ncmp('hprebn', dhprebn, hprebn)\ncmp('embcat', dembcat, embcat)\ncmp('W1', dW1, W1)\ncmp('b1', db1, b1)\ncmp('emb', demb, emb)\ncmp('C', dC, C)\n\n\n\n1\n\ndlogprobs: logprobs is an (32, 27) array - contains log probabilities of every character in 27 vocab for 32 output. loss just indexes ((range(n), Yb)) to pick out the corresponding numbers of the ground trues, then do mean (1 / n) and get negative (-). So the grad for each element that had been picked out is -1/n, while for others is 0;\n\n2\n\ndprobs: d/dx log(x) is just 1/x - local derivative, then multiply by next leaf grad dlogprobs, element wise;\n\n3\n\ndcounts_sum_inv: should be counts * dprobs according to chainrule. But remember counts.shape is (32, 27), and counts_sum_inv.shape is (32, 1), then there is broadcasting to 27 columns. counts_sum_inv is being used multiple times in the topo/diagram -&gt; we need to sum them (grad) up. We do it by columns so Keepdim=True;\n\n4\n\ndcount: was used in 2 expression, so it would be the sum of (1) counts_sum_inv * dprobs - same with dcounts_sum_inv by symmetry, and (2) torch.ones_like(counts) * dcounts_sum - the gradient flow from dcounts_sum equally, and equal to 1;\n\n5\n\ndlogits: sum of 2 flows, the 2nd one for max() operations -&gt; gradient should be the 1 for those max elements, the remain would be 0;\n\n6\n\nNotice the vertical broadcasting of bngain and bnbias;\n\n7\n\nNeed to sum up vertically because bnvar_inv is (1, 64) while 2 multipliers are (32, 64);\n\n8\n\nUndo the concatenation;\n\n9\n\nUndo the indexing: emb.shape = (32, 3, 10), C.shape = (27, 10), Xb.shape = (32, 3).\n\n\n\n\nCROSS ENTROPY LOSS\nlogprobs        | exact: True  | approximate: True  | maxdiff: 0.0\nprobs           | exact: True  | approximate: True  | maxdiff: 0.0\ncounts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\ncounts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\ncounts          | exact: True  | approximate: True  | maxdiff: 0.0\nnorm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\nlogit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\nLAYER 2 WITH LINEAR AND NON-LINEARITY TANH\nlogits          | exact: True  | approximate: True  | maxdiff: 0.0\nh               | exact: True  | approximate: True  | maxdiff: 0.0\nW2              | exact: True  | approximate: True  | maxdiff: 0.0\nb2              | exact: True  | approximate: True  | maxdiff: 0.0\nBATCH NORM LAYER\nhpreact         | exact: True  | approximate: True  | maxdiff: 0.0\nbngain          | exact: True  | approximate: True  | maxdiff: 0.0\nbnbias          | exact: True  | approximate: True  | maxdiff: 0.0\nbnraw           | exact: True  | approximate: True  | maxdiff: 0.0\nbnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\nbnvar           | exact: True  | approximate: True  | maxdiff: 0.0\nbndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\nbndiff          | exact: True  | approximate: True  | maxdiff: 0.0\nbnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\nLINEAR LAYER 1\nhprebn          | exact: True  | approximate: True  | maxdiff: 0.0\nembcat          | exact: True  | approximate: True  | maxdiff: 0.0\nW1              | exact: True  | approximate: True  | maxdiff: 0.0\nb1              | exact: True  | approximate: True  | maxdiff: 0.0\nemb             | exact: True  | approximate: True  | maxdiff: 0.0\nC               | exact: True  | approximate: True  | maxdiff: 0.0\n\n\n\n\nbrief digression: bessel’s correction in batchnorm\nThe paper of Batch Norm inconsistantly mentioned that:\n\nthey used biased variance for training;\nand used un-biased variance for inference.\n\nWe train on small mini-batch, so should be using un-biased variance. PyTorch, since implemented what exactly paper wrote, has this discrepancy.\n\n\nexercise 2: cross entropy loss backward pass\nWe are realizing that we doing too much work since we need to break the loss calculation from logits into too many steps that (1) they are easy enough for us to do backpropagation, but (2) most of them can cancel each other out. So in exercise two, we need to convert those bunch of atomic pieces of calculation to a shorter formula of cross entropy that can facilitate the backpropating.\n\n\nShow the code\n# Exercise 2: backprop through cross_entropy but all in one go\n# to complete this challenge look at the mathematical expression of the loss,\n# take the derivative, simplify the expression, and just write it out\n\n# forward pass\n\n# before:\n# logit_maxes = logits.max(1, keepdim=True).values\n# norm_logits = logits - logit_maxes # subtract max for numerical stability\n# counts = norm_logits.exp()\n# counts_sum = counts.sum(1, keepdims=True)\n# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n# probs = counts * counts_sum_inv\n# logprobs = probs.log()\n# loss = -logprobs[range(n), Yb].mean()\n\n# now:\nloss_fast = F.cross_entropy(logits, Yb)\nprint(loss_fast.item(), 'diff:', (loss_fast - loss).item())\n\n\n3.3267292976379395 diff: 4.76837158203125e-07\n\n\nMathematically given the Cross-Entropy loss formula:\n\\(L = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{k} Y_{ij} \\log(\\hat{Y}_{ij})\\)\nwith \\(\\hat{Y}{ij}\\) is calculated by the softmax transformation:\n\\(\\hat{Y}{ij} = \\frac{\\exp(\\text{logits}{ij})}{\\sum_{c=1}^{k} \\exp(\\text{logits}_{ic})}\\)\nwhere:\n\n\\(n\\): batch size, in this case is 32 - n;\n\\(k\\): vocab size or number of classes, in this case is 27 - vocab_size;\n\\(Y \\in {0,1}^{n \\times k}\\): one-hot encoding maxtrix (ground truth) - Y;\n\\(\\text{logits} \\in \\mathbb{R}^{n \\times k}\\): raw logits, input of softmax layer - logits;\n\\(\\hat{Y} \\in [0,1]^{n \\times k}\\): probabilities after softmax layer - probs.\n\nI actually can not do this exercise so AK’s solution here:\n\n\nShow the code\n# backward pass\n\ndlogits = F.softmax(logits, 1)\ndlogits[range(n), Yb] -= 1.0\ndlogits *= n**-1.0\n\ncmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9\n\n\nlogits          | exact: False | approximate: True  | maxdiff: 6.752088665962219e-09\n\n\nI will comeback with another post on thisss: the backward for cross entropy loss.\n\nwhat is dlogits intuitively?\nNow let’s look how dlogits look like.\nThis is the first row of logits through softmax layer, it’s probabilities of every possible character in vocab size 27, they are all small and sum of them is 1.\n\n\nShow the code\nF.softmax(logits, 1)[0]\n\n\ntensor([0.0765, 0.0852, 0.0175, 0.0537, 0.0181, 0.0835, 0.0274, 0.0357, 0.0181,\n        0.0319, 0.0393, 0.0355, 0.0382, 0.0277, 0.0341, 0.0135, 0.0096, 0.0207,\n        0.0163, 0.0540, 0.0484, 0.0206, 0.0263, 0.0672, 0.0549, 0.0257, 0.0205],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\nAnd this is the first row of dlogits multiplied by n for comparision, it’s all identical excep the 8th probability (Xb[0] = 8). And sum of them is 0!\n\n\nShow the code\ndlogits[0] * n\n\n\ntensor([ 0.0765,  0.0852,  0.0175,  0.0537,  0.0181,  0.0835,  0.0274,  0.0357,\n        -0.9819,  0.0319,  0.0393,  0.0355,  0.0382,  0.0277,  0.0341,  0.0135,\n         0.0096,  0.0207,  0.0163,  0.0540,  0.0484,  0.0206,  0.0263,  0.0672,\n         0.0549,  0.0257,  0.0205], grad_fn=&lt;MulBackward0&gt;)\n\n\nSo for each data point in the batch of 32, we pushnish super hard the correct character, making the magnitude of it’s grad is so high (negative number), then the prob of that predict-character can change toward to correct one. This push and pull is somehow the way that network learn.\n\n\nShow the code\nplt.figure(figsize=(8, 8))\nplt.imshow(dlogits.detach(), cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nWhen the network perfectly predict, softmax will have a full row vector except the 8th prob, which is 1. Then the dlogits will be full of 0, the network stop learning.\n\n\n\nexercise 3: batch norm layer backward pass\n\n\nShow the code\n# Exercise 3: backprop through batchnorm but all in one go\n# to complete this challenge look at the mathematical expression of the output of batchnorm,\n# take the derivative w.r.t. its input, simplify the expression, and just write it out\n\n# forward pass\n\n# before:\n# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n# bndiff = hprebn - bnmeani\n# bndiff2 = bndiff**2\n# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n# bnvar_inv = (bnvar + 1e-5)**-0.5\n# bnraw = bndiff * bnvar_inv\n# hpreact = bngain * bnraw + bnbias\n\n# now:\nhpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\nprint('max diff:', (hpreact_fast - hpreact).abs().max())\n\n\nmax diff: tensor(4.7684e-07, grad_fn=&lt;MaxBackward1&gt;)\n\n\nBelow is the image of Batch Norm algorithm:\n\n\n\nBatch Normalization algo, source\n\n\nAnd this is flowchart\n\n\n\n\n\nflowchart LR\n\nInput(...) --&gt; X(x)\nX(x) -- \"(4) dL/dx\" --&gt; Mu(mu)\nX(x) -- \"(4) dL/dx\" --&gt; Si(\"sigma (square)\")\nX(x) -- \"(4) dL/dx\" --&gt; Xh(x_hat)\nSi(\"sigma (square)\") -- \"(2) dL/dsigma\" --&gt; Xh(x_hat)\nMu(mu) -- \"(3.1) dL/dmu\" --&gt; Xh(x_hat)\nMu(mu) -- \"(3.2) dL/dmu\" --&gt; Si(\"sigma (square)\")\nXh(x_hat) -- \"(1) dL/dx_hat\" --&gt; Y(y)\n\nG(gamma) --&gt; Y(y)\nB(beta) --&gt; Y(y)\n\n\n\n\n\n\nWe will need to calculate dL/dx given dL/dy, we will calculate by hand reversely:\n\nscale and shift: dL/dx_hat = gamma * dL/dy easy enough;\nnormalize: dL/dsigma = -1/2 * gamma * SUM[dL/dy * (x - mu) * (sigma^2 + eps)^(-3/2)];\nnormalize & mini-batch variance: dL/dmu = - SUM[dL/dy * gamma * (sigma^2 + eps)^(-1/2)], this is path (3.1), we can prove that path (3.2) equal to zero - since mu is average of x, we can think of change in mu will be eliminated by x itself w.r.t. L;\ngiven all the gradients above, we just write the expressions down and do some transformation, this is the final: dL/dx = gamma * (sigma^2 + eps)^(-1/2) / m { [m * dL/dy] - SUMj[dL/dy] - m/(m-1) * x_hat * SUMj[dL/dy * x_hat]}, where m is mini-batch size (in our data is n)\n\n\n\nShow the code\n# backward pass\n\n# before we had:\n# dbnraw = bngain * dhpreact\n# dbndiff = bnvar_inv * dbnraw\n# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n# dbndiff += (2*bndiff) * dbndiff2\n# dhprebn = dbndiff.clone()\n# dbnmeani = (-dbndiff).sum(0)\n# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n\n# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n# (you'll also need to use some of the variables from the forward pass up above)\n\ndhprebn = bngain * bnvar_inv / n * ( n * dhpreact - dhpreact.sum(0)  - n / (n-1) * bnraw * (dhpreact * bnraw).sum(0))\n\ncmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10\n\n\nhprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n\n\n\n\nexercise 4: putting it all together\n\n\nShow the code\n# Exercise 4: putting it all together!\n# Train the MLP neural net with your own backward pass\n\n# init\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden))*0.1 + 1.0\nbnbias = torch.randn((1, n_hidden))*0.1\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nn = batch_size # convenience\nlossi = []\n\n# use this context manager for efficiency once your backward pass is written (TODO)\nwith torch.no_grad():\n\n  # kick off optimization\n  for i in range(max_steps):\n\n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n    # Linear layer\n    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n    # BatchNorm layer\n    # -------------------------------------------------------------\n    bnmean = hprebn.mean(0, keepdim=True)\n    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n    bnvar_inv = (bnvar + 1e-5)**-0.5\n    bnraw = (hprebn - bnmean) * bnvar_inv\n    hpreact = bngain * bnraw + bnbias\n    # -------------------------------------------------------------\n    # Non-linearity\n    h = torch.tanh(hpreact) # hidden layer\n    logits = h @ W2 + b2 # output layer\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n      p.grad = None\n    # loss.backward() # use this for correctness comparisons, delete it later!\n\n    # manual backprop! #swole_doge_meme\n    # -----------------\n    dlogits = F.softmax(logits, 1)\n    dlogits[range(n), Yb] -= 1\n    dlogits /= n\n    # 2nd layer backprop\n    dh = dlogits @ W2.T\n    dW2 = h.T @ dlogits\n    db2 = dlogits.sum(0)\n    # tanh\n    dhpreact = (1.0 - h**2) * dh\n    # batchnorm backprop\n    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n    dbnbias = dhpreact.sum(0, keepdim=True)\n    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n    # 1st layer\n    dembcat = dhprebn @ W1.T\n    dW1 = embcat.T @ dhprebn\n    db1 = dhprebn.sum(0)\n    # embedding\n    demb = dembcat.view(emb.shape)\n    dC = torch.zeros_like(C)\n    for k in range(Xb.shape[0]):\n      for j in range(Xb.shape[1]):\n        ix = Xb[k,j]\n        dC[ix] += demb[k,j]\n    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n    # -----------------\n\n    # update\n    lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay\n    for p, grad in zip(parameters, grads):\n      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n      p.data += -lr * grad # new way of swole doge TODO: enable\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n\n    # if i &gt;= 100: # TODO: delete early breaking when you're ready to train the full net\n    #   break\n\n\n12297\n      0/ 200000: 3.8430\n  10000/ 200000: 2.1560\n  20000/ 200000: 2.4334\n  30000/ 200000: 2.4665\n  40000/ 200000: 1.9888\n  50000/ 200000: 2.3378\n  60000/ 200000: 2.4149\n  70000/ 200000: 2.0440\n  80000/ 200000: 2.3377\n  90000/ 200000: 2.1847\n 100000/ 200000: 1.9861\n 110000/ 200000: 2.2556\n 120000/ 200000: 2.0190\n 130000/ 200000: 2.4322\n 140000/ 200000: 2.3159\n 150000/ 200000: 2.2161\n 160000/ 200000: 1.8835\n 170000/ 200000: 1.8359\n 180000/ 200000: 2.0352\n 190000/ 200000: 1.8955\n\n\nWe can check gradients using this:\n\n\nShow the code\n# useful for checking your gradients\nfor p,g in zip(parameters, grads):\n  # g.requires_grad = True\n  # g.retain_grad()\n  cmp(str(tuple(p.shape)), g, p)\n\n\nCalibrate the batch norm at the end of training:\n\n\nShow the code\nwith torch.no_grad():\n  # pass the training set through\n  emb = C[Xtr]\n  embcat = emb.view(emb.shape[0], -1)\n  hpreact = embcat @ W1 + b1\n  # measure the mean/std over the entire training set\n  bnmean = hpreact.mean(0, keepdim=True)\n  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n\n\nEvaluate train and val loss:\n\n\nShow the code\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n  x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n  }[split]\n  emb = C[x] # (N, block_size, n_embd)\n  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n  hpreact = embcat @ W1 + b1\n  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n  h = torch.tanh(hpreact) # (N, n_hidden)\n  logits = h @ W2 + b2 # (N, vocab_size)\n  loss = F.cross_entropy(logits, y)\n  print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\n\ntrain 2.0704638957977295\nval 2.10821533203125\n\n\nSimilar to what we achieved before!\nSample from model:\n\n\nShow the code\n# sample from the model\ng = torch.Generator().manual_seed(2147483647 + 10)\n\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n      # ------------\n      # forward pass:\n      # Embedding\n      emb = C[torch.tensor([context])] # (1,block_size,d)      \n      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n      hpreact = embcat @ W1 + b1\n      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n      h = torch.tanh(hpreact) # (N, n_hidden)\n      logits = h @ W2 + b2 # (N, vocab_size)\n      # ------------\n      # Sample\n      probs = F.softmax(logits, dim=1)\n      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n      context = context[1:] + [ix]\n      out.append(ix)\n      if ix == 0:\n        break\n    \n    print(''.join(itos[i] for i in out))\n\n\nmora.\nmayah.\nsee.\nmadyn.\nalayethruthadra.\ngradelyn.\nelin.\nshi.\njenleigh.\nsana.\narleigh.\nmalaia.\nnoshubergshira.\nsten.\njoselle.\njose.\ncasun.\nmacder.\nyarulyeh.\nyuma.\n\n\n\n\noutro\nWe have gone through and learned how can we manually do the gradients in our networks, it’s just some line of code for each step and pretty simple (but not for the Batch Norm formula 😪). And I’ll also be back with another post to calculate how the grad of cross entropy is coming.\nIn next lesson we will build RNN, LSTM, GRU, etc. Interesting and happly leanring!\n\n\nresources\n\nNotebook: https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part4_backprop.ipynb;\nColab notebook: https://colab.research.google.com/drive/1WV2oi2fh9XXyldh02wupFQX0wh5ZC-z-?usp=sharing"
  },
  {
    "objectID": "blog/2024-06-17-nn-z2h-p1/index.html#walk-through-of-the-full-code-of-micrograd-on-github",
    "href": "blog/2024-06-17-nn-z2h-p1/index.html#walk-through-of-the-full-code-of-micrograd-on-github",
    "title": "NN-Z2H Lesson 1: The spelled-out intro to neural networks and backpropagation - building micrograd",
    "section": "walk through of the full code of micrograd on github",
    "text": "walk through of the full code of micrograd on github\nSame with which we built today:\n\nengine: Value\nnn: Neuron, Layer, MLP, and modulize the zero grad process to class Module\ntest: sanity check - compare the backward with torch, also for the forward pass\ndemo: a bit complicated example with sklearn dataset, using batch processing when the dataset come large, the loss is slightly different - SVM max-margin loss and using of auto L2 regularization\nlearning rate decay: is a scaled as a function of number of iterations, high at begin and low at the end"
  },
  {
    "objectID": "blog/2024-12-02-nn-z2h-p5/index.html#import-libraries",
    "href": "blog/2024-12-02-nn-z2h-p5/index.html#import-libraries",
    "title": "NN-Z2H Lesson 5: Building makemore part 4 - Becoming a Backprop Ninja",
    "section": "import libraries:",
    "text": "import libraries:\n\n\nShow the code\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n%matplotlib inline"
  },
  {
    "objectID": "blog/2024-12-02-nn-z2h-p5/index.html#read-data",
    "href": "blog/2024-12-02-nn-z2h-p5/index.html#read-data",
    "title": "NN-Z2H Lesson 5: Building makemore part 4 - Becoming a Backprop Ninja",
    "section": "read data:",
    "text": "read data:\n\n\nShow the code\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt\"\nwords = pd.read_csv(url, header=None).iloc[:, 0].tolist()\nwords[:8]\n\n\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
  },
  {
    "objectID": "blog/2024-12-02-nn-z2h-p5/index.html#build-vocab",
    "href": "blog/2024-12-02-nn-z2h-p5/index.html#build-vocab",
    "title": "NN-Z2H Lesson 5: Building makemore part 4 - Becoming a Backprop Ninja",
    "section": "build vocab:",
    "text": "build vocab:\n\n\nShow the code\n# build the vocabulary of characters and mapping to/from integer\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i, s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i: s for s, i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n\n\n{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27"
  },
  {
    "objectID": "blog/2024-12-02-nn-z2h-p5/index.html#build-dataset-splits-identical-to-previous-so-i-folded-it",
    "href": "blog/2024-12-02-nn-z2h-p5/index.html#build-dataset-splits-identical-to-previous-so-i-folded-it",
    "title": "NN-Z2H Lesson 5: Building makemore part 4 - Becoming a Backprop Ninja",
    "section": "build dataset splits (identical to previous so I folded it):",
    "text": "build dataset splits (identical to previous so I folded it):\n\n\nShow the code\nblock_size = 3 # context length: how many characters do we take to predict the next one.\n# build the dataset\ndef buid_dataset(words):\n    X, Y = [], []\n\n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix]\n\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\nXtr, Ytr = buid_dataset(words[:n1])        # 80#\nXdev, Ydev = buid_dataset(words[n1:n2])    # 10%\nXte, Yte = buid_dataset(words[n2:])        # 10%\n\n\ntorch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])"
  },
  {
    "objectID": "blog/2024-12-02-nn-z2h-p5/index.html#utility-function-we-will-use-later-when-comparing-manual-gradients-to-pytorch-gradients.",
    "href": "blog/2024-12-02-nn-z2h-p5/index.html#utility-function-we-will-use-later-when-comparing-manual-gradients-to-pytorch-gradients.",
    "title": "NN-Z2H Lesson 5: Building makemore part 4 - Becoming a Backprop Ninja",
    "section": "utility function we will use later when comparing manual gradients to PyTorch gradients.",
    "text": "utility function we will use later when comparing manual gradients to PyTorch gradients.\n\n\nShow the code\ndef cmp(s, dt, t):\n  ex = torch.all(dt == t.grad).item()\n  app = torch.allclose(dt, t.grad)\n  maxdiff = (dt - t.grad).abs().max().item()\n  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
  },
  {
    "objectID": "blog/2024-12-02-nn-z2h-p5/index.html#network-construction",
    "href": "blog/2024-12-02-nn-z2h-p5/index.html#network-construction",
    "title": "NN-Z2H Lesson 5: Building makemore part 4 - Becoming a Backprop Ninja",
    "section": "network construction:",
    "text": "network construction:\n\n\nShow the code\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 64 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden))*0.1 + 1.0\nbnbias = torch.randn((1, n_hidden))*0.1\n\n# Note: I am initializating many of these parameters in non-standard ways\n# because sometimes initializating with e.g. all zeros could mask an incorrect\n# implementation of the backward pass.\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n\n\n4137"
  },
  {
    "objectID": "blog/2024-12-02-nn-z2h-p5/index.html#mini-batch-construction",
    "href": "blog/2024-12-02-nn-z2h-p5/index.html#mini-batch-construction",
    "title": "NN-Z2H Lesson 5: Building makemore part 4 - Becoming a Backprop Ninja",
    "section": "mini-batch construction:",
    "text": "mini-batch construction:\n\n\nShow the code\nbatch_size = 32\nn = batch_size # a shorter variable also, for convenience\n# construct a minibatch\nix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\nXb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
  },
  {
    "objectID": "blog/2024-12-02-nn-z2h-p5/index.html#forward-pass-chunkated-into-smaller-steps-that-are-possible-to-backward-one-at-a-time.",
    "href": "blog/2024-12-02-nn-z2h-p5/index.html#forward-pass-chunkated-into-smaller-steps-that-are-possible-to-backward-one-at-a-time.",
    "title": "NN-Z2H Lesson 5: Building makemore part 4 - Becoming a Backprop Ninja",
    "section": "forward pass, “chunkated” into smaller steps that are possible to backward one at a time.",
    "text": "forward pass, “chunkated” into smaller steps that are possible to backward one at a time.\nI can do 60, 70% of this myself.\n\n\nShow the code\nemb = C[Xb] # embed the characters into vectors\nembcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n\n# Linear layer 1\nhprebn = embcat @ W1 + b1 # hidden layer pre-activation\n\n# BatchNorm layer\nbnmeani = 1/n*hprebn.sum(0, keepdim=True)\nbndiff = hprebn - bnmeani\nbndiff2 = bndiff**2\nbnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\nbnvar_inv = (bnvar + 1e-5)**-0.5\nbnraw = bndiff * bnvar_inv\nhpreact = bngain * bnraw + bnbias\n\n# Non-linearity\nh = torch.tanh(hpreact) # hidden layer\n\n# Linear layer 2\nlogits = h @ W2 + b2 # output layer\n\n# cross entropy loss (same as F.cross_entropy(logits, Yb)), Kullback–Leibler divergence\nlogit_maxes = logits.max(1, keepdim=True).values\nnorm_logits = logits - logit_maxes # subtract max for numerical stability\ncounts = norm_logits.exp()\ncounts_sum = counts.sum(1, keepdims=True)\ncounts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\nprobs = counts * counts_sum_inv\nlogprobs = probs.log()\nloss = -logprobs[range(n), Yb].mean()\n\n# PyTorch backward pass\nfor p in parameters:\n  p.grad = None\nfor t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n         embcat, emb]:\n  t.retain_grad()\n\nhprebn.retain_grad() # Tuan added this line since code above does not ensure hprebn's grad to be retained.\n\nloss.backward()\nloss\n\n\ntensor(3.3490, grad_fn=&lt;NegBackward0&gt;)"
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "",
    "text": "This is not orginal content!\n\n\n\nThis is my study notes / codes along with Andrej Karpathy’s “Neural Networks: Zero to Hero” series."
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html#starter-code-walkthrough",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html#starter-code-walkthrough",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "starter code walkthrough",
    "text": "starter code walkthrough\n\nimport libraries\n\n\nShow the code\nimport torch\nimport torch.nn.functional as F\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n\n\nreading data\n\n\nShow the code\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt\"\nwords = pd.read_csv(url, header=None).iloc[:, 0].tolist()\nwords[:8]\n\n\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n\n\n\n\nbuilding vocab\n\n\nShow the code\n# build the vocabulary of characters and mapping to/from integer\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i, s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i: s for s, i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n\n\n{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n\n\n\n\ninitializing randomnization\n\n\nShow the code\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\n\n\n\n\ncreate train/dev/test splits\n\n\nShow the code\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n# build the dataset\ndef buid_dataset(words):\n    X, Y = [], []\n\n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix]\n\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\nXtr, Ytr = buid_dataset(words[:n1])        # 80#\nXdev, Ydev = buid_dataset(words[n1:n2])    # 10%\nXte, Yte = buid_dataset(words[n2:])        # 10%\n\n\ntorch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n\n\n\n\n\n\n\nShow the code\nfor x, y in zip(Xtr[:20], Ytr[:20]):\n  print(''.join(itos[ix.item()] for ix in x), '---&gt;', itos[y.item()])\n\n\n... ---&gt; y\n..y ---&gt; u\n.yu ---&gt; h\nyuh ---&gt; e\nuhe ---&gt; n\nhen ---&gt; g\neng ---&gt; .\n... ---&gt; d\n..d ---&gt; i\n.di ---&gt; o\ndio ---&gt; n\nion ---&gt; d\nond ---&gt; r\nndr ---&gt; e\ndre ---&gt; .\n... ---&gt; x\n..x ---&gt; a\n.xa ---&gt; v\nxav ---&gt; i\navi ---&gt; e\n\n\n\n\ninitializing objects in networks\nNear copy paste of the layers we have developed in Part 3, I added some docstring to the classes.\n\nclass Linear\n\n\nShow the code\nclass Linear:\n  \"\"\"    \n  Applies an affine linear transformation to the incoming data: y = xA^T + b.\n\n  This class implements a linear (fully connected) layer, which performs a linear \n  transformation on the input tensor. It is typically used in neural network architectures \n  to transform input features between layers.\n\n  Args:\n      fan_in (int): Number of input features (input dimension).\n      fan_out (int): Number of output features (output dimension).\n      bias (bool, optional): Whether to include a learnable bias term. \n          Defaults to True.\n\n  Attributes:\n      weight (torch.Tensor): Weight matrix of shape (fan_in, fan_out), \n          initialized using Kaiming initialization.\n      bias (torch.Tensor or None): Bias vector of shape (fan_out), \n          initialized to zeros if bias is True, otherwise None.\n\n  Methods:\n      __call__(x): Applies the linear transformation to the input tensor x.\n      parameters(): Returns a list of trainable parameters (weight and bias).\n\n  Example:\n      &gt;&gt;&gt; layer = Linear(10, 5)  # Creates a linear layer with 10 input features and 5 output features\n      &gt;&gt;&gt; x = torch.randn(3, 10)  # Input tensor with batch size 3 and 10 features\n      &gt;&gt;&gt; output = layer(x)  # Applies linear transformation\n      &gt;&gt;&gt; output.shape\n      torch.Size([3, 5])\n  \"\"\"\n\n  def __init__(self, fan_in, fan_out, bias=True):\n    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n    self.bias = torch.zeros(fan_out) if bias else None\n  \n  def __call__(self, x):\n    self.out = x @ self.weight\n    if self.bias is not None:\n      self.out += self.bias\n    return self.out\n  \n  def parameters(self):\n    return [self.weight] + ([] if self.bias is None else [self.bias])\n\n\n\n\nclass BatchNorm1d\n\n\nShow the code\nclass BatchNorm1d:\n  \"\"\"\n  Applies Batch Normalization to the input tensor, a technique to improve \n  training stability and performance in deep neural networks.\n\n  Batch Normalization normalizes the input across the batch dimension, \n  reducing internal covariate shift and allowing higher learning rates. \n  This implementation supports both training and inference modes.\n\n  Args:\n      dim (int): Number of features or channels to be normalized.\n      eps (float, optional): A small constant added to the denominator for \n          numerical stability to prevent division by zero. \n          Defaults to 1e-5.\n      momentum (float, optional): Momentum for updating running mean and \n          variance during training. Controls the degree of exponential \n          moving average. Defaults to 0.1.\n\n  Attributes:\n      eps (float): Epsilon value for numerical stability.\n      momentum (float): Momentum for running statistics update.\n      training (bool): Indicates whether the layer is in training or inference mode.\n      gamma (torch.Tensor): Learnable scale parameter of shape (dim,).\n      beta (torch.Tensor): Learnable shift parameter of shape (dim,).\n      running_mean (torch.Tensor): Exponential moving average of batch means.\n      running_var (torch.Tensor): Exponential moving average of batch variances.\n\n  Methods:\n      __call__(x): Applies batch normalization to the input tensor.\n      parameters(): Returns learnable parameters (gamma and beta).\n\n  Key Normalization Steps:\n  1. Compute batch mean and variance (in training mode)\n  2. Normalize input by subtracting mean and dividing by standard deviation\n  3. Apply learnable scale (gamma) and shift (beta) parameters\n  4. Update running statistics during training\n\n  Example:\n      &gt;&gt;&gt; batch_norm = BatchNorm1d(64)  # For 64-channel input\n      &gt;&gt;&gt; x = torch.randn(32, 64)  # Batch of 32 samples with 64 features\n      &gt;&gt;&gt; normalized_x = batch_norm(x)  # Apply batch normalization\n      &gt;&gt;&gt; normalized_x.shape\n      torch.Size([32, 64])\n\n  Note:\n      - Supports both 2D (batch, features) and 3D (batch, channels, sequence) input tensors\n      - During inference, uses running statistics instead of batch statistics\n  \"\"\"\n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.momentum = momentum\n    self.training = True\n    # parameters (trained with backprop)\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n    # buffers (trained with a running 'momentum update')\n    self.running_mean = torch.zeros(dim)\n    self.running_var = torch.ones(dim)\n  \n  def __call__(self, x):\n    # calculate the forward pass\n    if self.training:\n      if x.ndim == 2:\n        dim = 0\n      elif x.ndim == 3:\n        dim = (0,1)\n      xmean = x.mean(dim, keepdim=True) # batch mean\n      xvar = x.var(dim, keepdim=True) # batch variance\n    else:\n      xmean = self.running_mean\n      xvar = self.running_var\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    # update the buffers\n    if self.training:\n      with torch.no_grad():\n        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n    return self.out\n  \n  def parameters(self):\n    return [self.gamma, self.beta]\n\n\n\n\n\nclass Tanh\n\n\nShow the code\nclass Tanh:\n    \"\"\"\n    Hyperbolic Tangent (Tanh) Activation Function\n\n    Applies the hyperbolic tangent activation function element-wise to the input tensor. \n    Tanh maps input values to the range [-1, 1], providing a symmetric and non-linear \n    transformation that helps neural networks learn complex patterns.\n\n    Mathematical Definition:\n    tanh(x) = (e^x - e^-x) / (e^x + e^-x)\n    \n    Key Characteristics:\n    - Output Range: [-1, 1]\n    - Symmetric around the origin\n    - Gradient is always less than 1, which helps mitigate the vanishing gradient problem\n    - Commonly used in recurrent neural networks and hidden layers\n\n    Methods:\n        __call__(x): Applies the Tanh activation to the input tensor.\n        parameters(): Returns an empty list, as Tanh has no learnable parameters.\n\n    Attributes:\n        out (torch.Tensor): Stores the output of the most recent forward pass.\n\n    Example:\n        &gt;&gt;&gt; activation = Tanh()\n        &gt;&gt;&gt; x = torch.tensor([-2.0, 0.0, 2.0])\n        &gt;&gt;&gt; y = activation(x)\n        &gt;&gt;&gt; y\n        tensor([-0.9640, 0.0000, 0.9640])\n\n    Note:\n        This implementation is stateless and does not modify the input tensor.\n        The activation is applied element-wise, preserving the input tensor's shape.\n    \"\"\"\n\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n\n    def parameters(self):\n        return []\n\n\n\nrandom number generator\n\n\nShow the code\ntorch.manual_seed(42); # seed rng for reproducibility\n\n\n\n\nnetwork architecture\n\n\nShow the code\n# original network\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 300 # the number of neurons in the hidden layer of the MLP\n# model = Sequential([\n#   Embedding(vocab_size, n_embd),\n#   FlattenConsecutive(8), Linear(n_embd * 8, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n#   Linear(n_hidden, vocab_size),\n# ])\n\n# parameter init\nwith torch.no_grad():\n  model.layers[-1].weight *= 0.1 # last layer make less confident\n\nparameters = model.parameters()\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n\n\n\n\noptimization\n\n\nShow the code\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n  # minibatch construct\n  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n  \n  # forward pass\n  logits = model(Xb)\n  loss = F.cross_entropy(logits, Yb) # loss function\n  \n  # backward pass\n  for p in parameters:\n    p.grad = None\n  loss.backward()\n  \n  # update: simple SGD\n  lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay\n  for p in parameters:\n    p.data += -lr * p.grad\n\n  # track stats\n  if i % 10000 == 0: # print every once in a while\n    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n  lossi.append(loss.log10().item())\n\n  break"
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html#lets-fix-the-learning-rate-plot",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html#lets-fix-the-learning-rate-plot",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "let’s fix the learning rate plot",
    "text": "let’s fix the learning rate plot"
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html#pytorchifying-our-code-layers-containers-torch.nn-fun-bugs",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html#pytorchifying-our-code-layers-containers-torch.nn-fun-bugs",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "pytorchifying our code: layers, containers, torch.nn, fun bugs",
    "text": "pytorchifying our code: layers, containers, torch.nn, fun bugs"
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html#overview-wavenet",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html#overview-wavenet",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "overview: WaveNet",
    "text": "overview: WaveNet"
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html#dataset-bump-the-context-size-to-8",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html#dataset-bump-the-context-size-to-8",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "dataset bump the context size to 8",
    "text": "dataset bump the context size to 8"
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html#re-running-baseline-code-on-block_size-8",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html#re-running-baseline-code-on-block_size-8",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "re-running baseline code on block_size 8",
    "text": "re-running baseline code on block_size 8"
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html#implementing-wavenet-1",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html#implementing-wavenet-1",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "implementing WaveNet",
    "text": "implementing WaveNet"
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html#training-the-wavenet-first-pass",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html#training-the-wavenet-first-pass",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "training the WaveNet: first pass",
    "text": "training the WaveNet: first pass"
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html#fixing-batchnorm1d-bug",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html#fixing-batchnorm1d-bug",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "fixing batchnorm1d bug",
    "text": "fixing batchnorm1d bug"
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html#re-training-wavenet-with-bug-fix",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html#re-training-wavenet-with-bug-fix",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "re-training WaveNet with bug fix",
    "text": "re-training WaveNet with bug fix"
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html#scaling-up-our-wavenet",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html#scaling-up-our-wavenet",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "scaling up our WaveNet",
    "text": "scaling up our WaveNet"
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html#experimental-harness",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html#experimental-harness",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "experimental harness",
    "text": "experimental harness"
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html#wavenet-but-with-dilated-causal-convolutions",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html#wavenet-but-with-dilated-causal-convolutions",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "WaveNet but with “dilated causal convolutions”",
    "text": "WaveNet but with “dilated causal convolutions”"
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html#torch.nn",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html#torch.nn",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "torch.nn",
    "text": "torch.nn"
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html#the-development-process-of-building-deep-neural-nets",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html#the-development-process-of-building-deep-neural-nets",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "the development process of building deep neural nets",
    "text": "the development process of building deep neural nets"
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html#going-forward",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html#going-forward",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "going forward",
    "text": "going forward"
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html#improve-on-my-loss-how-far-can-we-improve-a-wavenet-on-this-data",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html#improve-on-my-loss-how-far-can-we-improve-a-wavenet-on-this-data",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "improve on my loss! how far can we improve a WaveNet on this data?",
    "text": "improve on my loss! how far can we improve a WaveNet on this data?"
  },
  {
    "objectID": "blog/2024-12-06-nn-z2h-p6/index.html#performance-log",
    "href": "blog/2024-12-06-nn-z2h-p6/index.html#performance-log",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "performance log",
    "text": "performance log"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html",
    "href": "blog/2024-12-06-adv_sql/index.html",
    "title": "Advanced SQL",
    "section": "",
    "text": "This is my note/workaround for the article “25 SQL tips to level up your data engineering skills” by Start Data Engineering, authored by Joseph Machado."
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#qualify",
    "href": "blog/2024-12-06-adv_sql/index.html#qualify",
    "title": "Advanced SQL",
    "section": "#1.1 QUALIFY\n",
    "text": "#1.1 QUALIFY\n\nWe use QUALIFY to filter the output column of WINDOW function without creating more CTEs/Subqueries. It’s not available in many traditional/on-prem RDBMS (MySQL, Oracle, MSSQL, PostgreSQL) but available in mordern/cloud-based databases (Snowflake, BigQuery, Databricks, MS Fabric, Teradata).\n\nShow the codeSELECT\n    o_orderkey, \n    o_totalprice, \n    RANK() OVER (ORDER BY o_totalprice DESC) AS price_rank -- rank the order by `o_totalprice` in desc\nFROM orders\nQUALIFY price_rank &lt;= 10; -- filter top 10\n/* would need a CTE or Sub-query without QUALIFY */\n\n\n\n\noutput - first 10 rows"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#distinct-on",
    "href": "blog/2024-12-06-adv_sql/index.html#distinct-on",
    "title": "Advanced SQL",
    "section": "#1.2 DISTINCT ON\n",
    "text": "#1.2 DISTINCT ON\n\nOrginated from PostgreSQL, now support by some cloud-based like Snowflake but not in MySQL, SQL Server, Oracle, DISTINCT ON allows us to get 1 detailed row (first or last) for a particular partition.\n\nShow the codeSELECT DISTINCT ON (o_custkey) -- distinct only `o_custkey`\n    o_custkey, \n    o_orderdate, \n    o_totalprice\nFROM orders\nORDER BY o_custkey, o_orderdate DESC; -- make sure to get the latest order by `o_orderdate`\n\n\n\n\noutput - first 10 rows"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#struct_pack",
    "href": "blog/2024-12-06-adv_sql/index.html#struct_pack",
    "title": "Advanced SQL",
    "section": "#1.4 STRUCT_PACK()\n",
    "text": "#1.4 STRUCT_PACK()\n\nSTRUCT_PACK() is a function primarily associated with DuckDB:\n\nCreates a compact, binary representation of multiple values;\nAllows you to pack different data types into a single column;\nUseful for data compression and efficient storage.\n\nBelow picture depicts how struct works:\n\n\nstruct packing\n\n\nShow the codeWITH order_struct AS (\n    SELECT \n        o_orderkey,\n        STRUCT_PACK(o_orderdate, o_totalprice, o_orderkey) AS order_info\n    FROM orders\n)\nSELECT \n    MIN(order_info) AS min_order_date, -- get min of information from left to right\n    MAX(order_info) AS max_order_date_price -- get of information from left to right, \n    -- if there are many txn in that latest day, get the transaction with max price\nFROM order_struct;\n\n\n\n\noutput"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#this-can-be-done-with-row_number-qualify",
    "href": "blog/2024-12-06-adv_sql/index.html#this-can-be-done-with-row_number-qualify",
    "title": "Advanced SQL",
    "section": "#1.3 This can be done with ROW_NUMBER() + QUALIFY:",
    "text": "#1.3 This can be done with ROW_NUMBER() + QUALIFY:\n\nShow the codeSELECT \n    o_custkey, \n    o_orderdate, \n    o_totalprice,\n    ROW_NUMBER() OVER (PARTITION BY o_custkey ORDER BY o_orderdate DESC) AS rn\nFROM orders\nQUALIFY rn = 1\nORDER BY o_custkey DESC;\n\n\n\n\noutput - first 10 row"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#this-can-also-be-done-with-row_number-qualify",
    "href": "blog/2024-12-06-adv_sql/index.html#this-can-also-be-done-with-row_number-qualify",
    "title": "Advanced SQL",
    "section": "#1.3 This can also be done with ROW_NUMBER() + QUALIFY:",
    "text": "#1.3 This can also be done with ROW_NUMBER() + QUALIFY:\n\nShow the codeSELECT \n    o_custkey, \n    o_orderdate, \n    o_totalprice,\n    ROW_NUMBER() OVER (PARTITION BY o_custkey ORDER BY o_orderdate DESC) AS rn\nFROM orders\nQUALIFY rn = 1\nORDER BY o_custkey DESC; -- ROW_NUMBER() shuffle the order of the data\n\n\n\n\noutput - first 10 rows"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#bool_or-bool_and",
    "href": "blog/2024-12-06-adv_sql/index.html#bool_or-bool_and",
    "title": "Advanced SQL",
    "section": "#1.5 BOOL_OR() & BOOL_AND()\n",
    "text": "#1.5 BOOL_OR() & BOOL_AND()\n\nBOOL_OR() & BOOL_AND() allows you to check a logical statement along all rows of a columns, supported in PostgreSQL, Snowflake, DuckDB, BigQuery, Databricks:\n\nShow the codeSELECT \n    o_custkey, \n    BOOL_OR(cast(o_shippriority as boolean)) AS has_atleast_one_priority_order, -- check whether AT LEAST 1 order of that customer has Is Priority = True\n    BOOL_AND(cast(o_shippriority as boolean)) AS has_all_priority_order -- check whether ALL orders of that customer has Is Priority = True\nFROM orders\nGROUP BY o_custkey;\n\n\n\n\noutput - first 10 rows"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#exclude",
    "href": "blog/2024-12-06-adv_sql/index.html#exclude",
    "title": "Advanced SQL",
    "section": "#1.6 EXCLUDE()\n",
    "text": "#1.6 EXCLUDE()\n\nWhen you want to select all (*) columns excet few ones:\n\nShow the codeSELECT * EXCLUDE (o_orderdate, o_totalprice)\nFROM orders;"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#group-by-all-saves-the-day",
    "href": "blog/2024-12-06-adv_sql/index.html#group-by-all-saves-the-day",
    "title": "Advanced SQL",
    "section": "#1.7 GROUP BY ALL saves the day",
    "text": "#1.7 GROUP BY ALL saves the day\nRepeating all the columns listed in the SELECT statement in GROUP BY is annoying, just use ALL:\n\nShow the codeSELECT \n    o_orderkey, \n    o_custkey, \n    o_orderstatus, \n    SUM(o_totalprice) AS total_price\nFROM orders\nGROUP BY ALL;"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#count_if",
    "href": "blog/2024-12-06-adv_sql/index.html#count_if",
    "title": "Advanced SQL",
    "section": "#1.8 COUNT_IF()\n",
    "text": "#1.8 COUNT_IF()\n\nFilter over rows in specific column:\n\nShow the codeSELECT \n    o_custkey, \n    COUNT_IF(o_totalprice &gt; 100000) AS high_value_orders, \n    -- equivalent to SUM(CASE WHEN o_totalprice &gt; 100000 THEN 1 ELSE 0 END)\n    COUNT(o_totalprice) as all_orders\nFROM orders\nGROUP BY o_custkey;\n\n\n\n\noutput - first 10 rows"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#string_agg",
    "href": "blog/2024-12-06-adv_sql/index.html#string_agg",
    "title": "Advanced SQL",
    "section": "#1.9 STRING_AGG()\n",
    "text": "#1.9 STRING_AGG()\n\nConcatenate rows of string in a GROUP BY statement:\n\nShow the codeSELECT STRING_AGG(c_name, ', ') AS customer_names\nFROM customer;"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#null-handling-with-coalesce",
    "href": "blog/2024-12-06-adv_sql/index.html#null-handling-with-coalesce",
    "title": "Advanced SQL",
    "section": "#1.10 Null handling with COALESCE()\n",
    "text": "#1.10 Null handling with COALESCE()\n\nHandling null value in a column with value from another column or default value:\n\nShow the codeWITH fake_orders AS (\n    SELECT 1 AS o_orderkey, 100 AS o_totalprice, NULL AS discount\n    UNION ALL\n    SELECT 2 AS o_orderkey, 200 AS o_totalprice, 20 AS discount\n    UNION ALL\n    SELECT 3 AS o_orderkey, 300 AS o_totalprice, NULL AS discount\n)\nSELECT \n    o_orderkey, \n    o_totalprice, \n    discount,\n    COALESCE(discount, o_totalprice * 0.10) AS final_discount\nFROM fake_orders;\n\n\n\n\noutput"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#generate_series",
    "href": "blog/2024-12-06-adv_sql/index.html#generate_series",
    "title": "Advanced SQL",
    "section": "#1.11 GENERATE_SERIES()\n",
    "text": "#1.11 GENERATE_SERIES()\n\nThis helps you generate a sequence/series of data over a range with an interval which facilitating data simulation or joining with other tables.\n\nShow the codeSELECT *\nFROM generate_series(1, 10);\n\n\n\n\noutput\n\n\nShow the codeSELECT *\nFROM generate_series('2024-01-01'::DATE, '2024-01-10'::DATE, INTERVAL 1 DAY);\n\n\n\n\noutput"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#unnest",
    "href": "blog/2024-12-06-adv_sql/index.html#unnest",
    "title": "Advanced SQL",
    "section": "#1.12 UNNEST()\n",
    "text": "#1.12 UNNEST()\n\nUnpacking data wrapped in JSON array or list [...] using UNNEST():\n\nShow the codeWITH nested_data AS (\n    SELECT 1 AS id, [10, 20, 30] AS values\n    UNION ALL\n    SELECT 2 AS id, [40, 50] AS values\n)\nSELECT \n    id, \n    UNNEST(values) AS flattened_value\nFROM nested_data;\n\n\n\n\noutput"
  },
  {
    "objectID": "jiu_jitsu_journal/index.html#december",
    "href": "jiu_jitsu_journal/index.html#december",
    "title": "My Jiu Jitsu Journal",
    "section": "December",
    "text": "December\n\n\n\n\n\n\n12.30.2024 | Promotion day\n\n\n\n\n\nHôm nay, sau tròn 10 tháng tập luyện (02/28/2024 - 12/30/2024), mình đã nhận được vạch đai trắng đầu tiên.\nMật độ tập luyện trung bình là khoảng 3 buổi/tuần, không tham gia thi đấu, tăng nhẹ khoảng 2,3 kg, có phát triển một chút cơ bắp, và một số bông tai súp lơ (cauliflower ear) đầu tiên.\nVề mặt kỹ thuật, đã xây dựng được ý thức và khả năng thực hiện các đòn đánh dưới một áp lực trung bình; khi áp lực tăng lên, ý thức kỹ thuật dần mất đi và thay vào đó là các hành vi bản năng. Về mặt chiến thuật, chỉ đang có sơ khai hình dung về cách đánh cho một vị trí cụ thể, trong khoảng 10 mấy giây tiếp theo; chưa có game plan, sẽ cần suy nghĩ và xây dựng trong năm thứ 2 tập luyện.\nCái mình được đánh giá tốt là khả năng absorb technique, và truyền đạt lại cho người khác (các bạn mới vào).\nCảm ơn sư phụ Tiến Sói, các anh/em đứng lớp Thông, Huân, Nhật, Hùng, cùng các thành viên Sói Jiu-jitsu. Oss!\n\n\n\nTiêu chuẩn đai xanh tại Sói BJJ, là mục tiêu mà mình cần nỗ lực trong 1, 2 năm tới\n\n\n\n\n\n\n\n\n\n\n\n12.23.2024 | Compressing the techniques\n\n\n\n\n\nTrong khoản gần 2 tháng thực hiện các ghi chú vừa rồi, rất rất nhiều các tiểu tiết kỹ thuật được truyền đạt. Thường xuyên thực hành, chăm chỉ tập luyện gần như là cách duy nhất để ghi nhớ các kỹ thuật, tư duy đó vào cơ bắp của chúng ta. Tuy nhiên, một bổ trợ cũng thường xuyên được sử dụng, chính là đơn giản hóa những nguyên tắc cốt lõi nhất thành các câu lệnh có điều kiện đơn giản, nén chúng lại vừa đủ để cache vào short term memory của chúng ta.\nVì thế mà ở lò Sói thường xuất hiện các cụm từ như: “xem đồng hồ”, “nghe điện thoại”, “ôm trán”, hay “nếu nhìn thấy …, thì …”. Một trong số đó là: “nếu nhìn thấy khuỷu tay và cổ tay đối phương ở trước mặt - ngang hai mắt, không đánh nó là một điều tội lỗi”.\nHai tuần nay tập luyện vẫn là bài đánh chân từ bottom + kiểm soát và tiến tới kimura một cách có hiệu quả. Chân chính là chi đầu tiên engage với đối thủ, kiểm soát được chân (cố định được một khớp như đầu gối, hay làm xoắn nó) là mục tiêu đầu tiên cần đạt được để làm đối thủ bận rộn, kiếm thêm thời gian và bảo toàn thể lực.\nĐánh kimura trong thực tế cũng sẽ không đơn giản như chỉ cần chồm lên, hai tay kết nối được thành cái vì kèo (4-shape - thứ có thể dễ dàng quan sát trên phần mái tôn của nhà thi đấu Nguyễn Tri Phương) là xong. Mà là quá trình từ grip đúng cách (vào cổ tay), kiểm soát được đầu kia của trục tay (vai), tạo áp lực vào khuỷu tay (giống như armbar), khiến cho đối thủ đưa tay cho chúng ta kimura trong lúc scramble. Luôn tạo một áp lực nhất định, bắt đối thủ chọn 1 trong hai phương án đều bất lợi, dần dà sẽ làm cho họ đưa tới cho ta một vị trí có lợi hơn.\n\nChúng ta không dùng sức để lấy được một vị trí thuận lợi, chúng ta tạo áp lực đúng cách để khiến đối thủ đưa vị trí đó cho chúng ta - Vũ Đình Tiến (đại ý).\n\nNghe giống như một mental model mà mình từng đọc qua - dilemma.\n  Oss! Ghi chú được ghi lại sau 2 ngày tập - ngày 25, đúng là:\n\nThời gian không ngừng, chỉ có con người mới lười ngừng - Thong Le.\n\n\n\n\n\n\n\n\n\n\n12.15.2024 | My 2nd open mat\n\n\n\n\n\nAfter 9 months of training, I finally had my 2nd open mat at Soi Jiu-jitsu :). Rolling with the real heavy hitters at the first time made me think that this open mat not suit for me. But anw I had a free Sunday morning this week so I decided to try again.\nDid better this time, rolled with a skillful 7xKG and had to bear his weight most of the time but still I held out until the end. I just rolled for one round, since I’ve been practicing for 4 days in a row, and have got a shoulder injury. Let’s improve next time!\n\n\n\nWhat a coincidence! Danaher gave a related advice today\n\n\n\n\n\n\n\n\n\n\n\n12.09.2024 | Week of Americana\n\n\n\n\n\n\nTheme of the class this week is to master the Americana.\n\nThe head coach continued with the final bonus of how to do Americana the right way at his Seminar “Bottom game for dummy” at the ACE community hosted by Saigon Saga, teaching Americana in various positions.\nRemember weekend’s lesson of bottom game, you’ll have good position for a sweep or a triangle. But before jump into finishing triangle, let’s explore some americana/straight armbar variants. Opponent will be trying to posture up to prevent triangle choke and escape, their arm will be around your hip, use your 2 hands to grip/hook into their armpits to control the distance - arms to pull, legs to push.\n\nIf their inside elbows expose to your eyes, you can go for a straight armbar;\nIf not, you can see their outside elbow, go hook, pull them out of the body and overhook -&gt; kimura;\nIf they move elbow to inside body, you can bury their elbows to our pelvis, using the thrust to create a fulcrum, keep pushing and rotating their hands to outside. This is a variant of Americana;\nFinally, none of above work, you can go a triangle choke, but to concrete the technique: you can creat 2 layer circle that threat the opponent’s neck. The first one is traditional leg position (1 arm and the neck), the second one is underhook the remaining arm, circle over the head. The choke thus would be more efficient!\n\nI have some more rounds rolling with Gi/Kimono today with anh Tu & anh Thong, pretty cool (but literally hot in the Gi). I’ve taught some basics of gi fighting: how to grip properly and legally, how to generating force, esp. in a pulling action.\nFinal quote of the day:\n\nLet’s start to think about the game plan! - Thong Le\n\n\n\n\n\n\n\n\n\n\n12.08.2024 | Vu Dinh Tien Seminar\n\n\n\n\n\nBottom games for dummies\n\n\n\nSeminar’s Banner\n\n\nLet’s forget everything about BJJ before this class.\n\nPhase 1: getting contact\n\nControl the lower body of the opponent: 6 control points, block with feet or hook with instep;\nWhen 1 leg of the opponent pass your hip, hands to engage;\nEnd up with closed guard or half guard or k-guard.\n\n\n\nPhase 2: contacting\nWhen contacting, we want to control at least 1 leg & 1 arm (upper body), both in 1 side is preferable - like hinge shaft of the door. So that we can flip it to sweep.\nThere are many ways to control upper body, as said in notes before. How we attack the leg is also mentioned in this journal.\n\n\nPhase 3: loosing control & transitioning\nOpponent certainly does not want to be passive in the game, they want to post up/scramble/initiate frame, etc … to get out or our control. The matter here is we should be aware of this and prepared. We can exploit the opponent’s traction force to sit up, creating a shin to shin position or transitioning to leg attack like k-guard.\nThe keys in bottom game I thinks is distance control, tracking their movement, keeping disturbing their comfort, get proper wedge, hook.\n\n\nQ&A\n\nQ1 - Huan: If the opponent hides their heels, how can we hook in and flare it?\nA - Soi: No one can dominate in every direction, when they hide feet in their butts, they can be easily pushed back. We can sit up and push.\n\n\nQ2 - Stranger: how do we not let opponents hand-cross our heads when they try to pass our guard?\nA - Soi: Put your palm on you forehead - just like you’re striking - creating a frame. The opponent’s hand just can only slip through it. Their armpit reveals and you’ll have chance to underhook with remaining arm, and even finish with straight arm lock.\n\n\n\nBonus - Americana the right way:\n\nUse your wrist as a blocking wedge instead of hand grip. It will leverage the lock and create discomfort for the opponent.\nWe dont want to push their elbow up, because:\n\nas long we push it up, we loose the pressure we made in chest to chest;\nelbow up is a natural movement to ease the pressure we made. so we need to pin the elbow on the mat - use your head, with the wedging above you’ll finish the Americana.\n\n\nHappy learning!\n\n\n\n\n\n\n\n\n\n\n12.06.2024 | Deep half\n\n\n\n\n\nBuổi học được dẫn dắt bởi Huân, tiếp nối chủ đề hôm trước từ bottom - half guard -&gt; under hook tay -&gt; hook lưng -&gt; sweep.\nNhưng hôm nay tiếp cận theo hướng ngược lại -&gt; “lao vào lòng” đối thủ và hook sâu xuống chân/Deep Half.\nTừ đây có thể mở ra nhiều cơ hội tấn công, ít nhất hai hướng sweep tùy theo body movement của đối thủ."
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#exists",
    "href": "blog/2024-12-06-adv_sql/index.html#exists",
    "title": "Advanced SQL",
    "section": "#2.1 EXISTS\n",
    "text": "#2.1 EXISTS\n\nEXISTS is used to test for the existence of rows in a subquery, return TRUE if any row in the subquery returned. I think it’s often used for logic check to correlate queries.\nIt checks the existence like INNER JOIN but does not join full rows or select columns from the subquery.\n\nShow the codeSELECT \n    c_custkey, \n    c_name\nFROM customer \nWHERE EXISTS (\n    SELECT o_orderkey\n    FROM orders\n    WHERE 1=1\n    -- AND o_totalprice &gt; 5000000 -- Return customer who has at least 1 order with total price &gt; 500k\n    AND o_custkey = c_custkey  -- Return customer who has at least 1 order\n);"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#intersect",
    "href": "blog/2024-12-06-adv_sql/index.html#intersect",
    "title": "Advanced SQL",
    "section": "#2.2 INTERSECT\n",
    "text": "#2.2 INTERSECT\n\nBelow query return all c_custkey that appears in both customer and orders table.\n\nShow the codeSELECT c_custkey \nFROM customer\nINTERSECT\nSELECT o_custkey \nFROM orders;"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#except",
    "href": "blog/2024-12-06-adv_sql/index.html#except",
    "title": "Advanced SQL",
    "section": "#2.3 EXCEPT\n",
    "text": "#2.3 EXCEPT\n\nAnd the below query return all c_custkey that appears customer but not in orders table.\n\nShow the codeSELECT c_custkey\nFROM customer\nEXCEPT\nSELECT o_custkey\nFROM orders;\n\n\nUsing this, we can perform delta/data diff checking!"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#asof-join",
    "href": "blog/2024-12-06-adv_sql/index.html#asof-join",
    "title": "Advanced SQL",
    "section": "#7.1 ASOF JOIN\n",
    "text": "#7.1 ASOF JOIN\n\nAn ASOF JOIN (As-Of Join) is a time-series specific join operation that matches rows based on the closest timestamp, typically used in financial and time-series data analysis.\n\n\nKey characteristics:\n\nMatches each row from one table to the most recent row in another table by time\nUseful for tracking historical state changes\nCommon in financial databases and time-series analysis\n\n\n\nExample use case:\n\nJoining stock prices with trading orders\nMatching historical prices at the nearest timestamp\nTracking changes in reference data over time\n\n\n\nFor eg. “give me the value of the property as of this time”:\n\nShow the codeWITH stock_prices AS (\n    SELECT 'APPL' AS ticker, TIMESTAMP '2001-01-01 00:00:00' AS \"when\", 1 AS price\n    UNION ALL\n    SELECT 'APPL', TIMESTAMP '2001-01-01 00:01:00', 2\n    UNION ALL\n    SELECT 'APPL', TIMESTAMP '2001-01-01 00:02:00', 3\n    UNION ALL\n    SELECT 'MSFT', TIMESTAMP '2001-01-01 00:00:00', 1\n    UNION ALL\n    SELECT 'MSFT', TIMESTAMP '2001-01-01 00:01:00', 2\n    UNION ALL\n    SELECT 'MSFT', TIMESTAMP '2001-01-01 00:02:00', 3\n    UNION ALL\n    SELECT 'GOOG', TIMESTAMP '2001-01-01 00:00:00', 1\n    UNION ALL\n    SELECT 'GOOG', TIMESTAMP '2001-01-01 00:01:00', 2\n    UNION ALL\n    SELECT 'GOOG', TIMESTAMP '2001-01-01 00:02:00', 3\n),\nportfolio_holdings AS (\n    SELECT 'APPL' AS ticker, TIMESTAMP '2000-12-31 23:59:30' AS \"when\", 5.16 AS shares\n    UNION ALL\n    SELECT 'APPL', TIMESTAMP '2001-01-01 00:00:30', 2.94\n    UNION ALL\n    SELECT 'APPL', TIMESTAMP '2001-01-01 00:01:30', 24.13\n    UNION ALL\n    SELECT 'GOOG', TIMESTAMP '2000-12-31 23:59:30', 9.33\n    UNION ALL\n    SELECT 'GOOG', TIMESTAMP '2001-01-01 00:00:30', 23.45\n    UNION ALL\n    SELECT 'GOOG', TIMESTAMP '2001-01-01 00:01:30', 10.58\n    UNION ALL\n    SELECT 'DATA', TIMESTAMP '2000-12-31 23:59:30', 6.65\n    UNION ALL\n    SELECT 'DATA', TIMESTAMP '2001-01-01 00:00:30', 17.95\n    UNION ALL\n    SELECT 'DATA', TIMESTAMP '2001-01-01 00:01:30', 18.37\n)\nSELECT h.ticker,\n    h.when,\n    p.when as stock_price_ts,\n    price,\n    shares,\n    price * shares AS value\nFROM portfolio_holdings h\nASOF JOIN stock_prices p\n       ON h.ticker = p.ticker\n      AND h.when &gt;= p.when\nORDER BY 1, 2;\n\n\n\n\noutput\n\nWithout ASOF JOIN, we must use a WINDOW function to achieve this."
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#anti-join",
    "href": "blog/2024-12-06-adv_sql/index.html#anti-join",
    "title": "Advanced SQL",
    "section": "#7.2 ANTI JOIN\n",
    "text": "#7.2 ANTI JOIN\n\nThe pattern is to get rows in one table that are not in another table:\n\nShow the codeSELECT c.c_custkey\nFROM customer c\nLEFT JOIN orders o\nON c.c_custkey = o.o_custkey\nWHERE o.o_custkey IS NULL\nORDER BY c.c_custkey\nLIMIT 5;\n\n\nSome DBs have native support for ANTI JOIN:\n\nShow the codeSELECT c.c_custkey\nFROM customer c\nANTI JOIN orders o\nON c.c_custkey = o.o_custkey\nORDER BY c.c_custkey\nLIMIT 5;"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#lateral-join",
    "href": "blog/2024-12-06-adv_sql/index.html#lateral-join",
    "title": "Advanced SQL",
    "section": "#7.3 LATERAL JOIN\n",
    "text": "#7.3 LATERAL JOIN\n\nLATERAL JOIN is a powerful SQL join type that allows a subquery in the FROM clause to reference columns from preceding tables in the same FROM clause.\n\nEnables per-row dynamic subqueries\nAllows correlated subqueries in the FROM clause\nSupported in PostgreSQL, BigQuery, some other modern databases\n\nFor e.g, for each row in the orders table (o), the subquery in the LATERAL JOIN selects line items (l) that match certain conditions.\n\nShow the codeSELECT \n    o.o_orderkey, \n    o.o_totalprice, \n    l.l_linenumber,\n    l.l_extendedprice\nFROM orders o,\nLATERAL (\n    SELECT l.l_linenumber,\n    l_extendedprice\n    FROM lineitem l\n    WHERE l.l_orderkey = o.o_orderkey\n    AND l.l_linenumber &lt;= 2\n    AND l.l_extendedprice &lt; (o.o_totalprice / 2)\n) AS l\nORDER BY 1, 3;\n\n\n\n\noutput - first 10 rows\n\nFor each row in the orders table (o), the subquery in the LATERAL JOIN counts the number of line items (lineitem_count) related to that order.\n\nShow the codeSELECT \n    o.o_orderkey, \n    o.o_totalprice, \n    l.lineitem_count\nFROM orders o,\nLATERAL (\n    SELECT COUNT(*) AS lineitem_count\n    FROM lineitem l\n    WHERE l.l_orderkey = o.o_orderkey\n) AS l;\n\n\n\n\noutput - first 10 rows"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#pivot",
    "href": "blog/2024-12-06-adv_sql/index.html#pivot",
    "title": "Advanced SQL",
    "section": "#8.1 PIVOT\n",
    "text": "#8.1 PIVOT\n\nWe often want to change a value of a column into individual columns, we can use CASE ... WHEN ...:\n\nShow the codeSELECT \n    o_custkey,\n    SUM(CASE WHEN o_orderstatus = 'F' THEN o_totalprice ELSE 0 END) AS fulfilled_total,\n    SUM(CASE WHEN o_orderstatus = 'O' THEN o_totalprice ELSE 0 END) AS open_total,\n    SUM(CASE WHEN o_orderstatus = 'P' THEN o_totalprice ELSE 0 END) AS pending_total\nFROM orders\nGROUP BY o_custkey\nORDER BY o_custkey;\n\n\nBut also leverage PIVOT:\n\nShow the codeFROM orders\nPIVOT (\n    sum(o_totalprice)\n    FOR\n        o_orderstatus IN ('F', 'O', 'P')\n    GROUP BY o_custkey\n)\n    ORDER BY o_custkey;\n\n\n\n\noutput - first 10 rows"
  },
  {
    "objectID": "blog/2024-12-06-adv_sql/index.html#cube",
    "href": "blog/2024-12-06-adv_sql/index.html#cube",
    "title": "Advanced SQL",
    "section": "#8.1 CUBE\n",
    "text": "#8.1 CUBE\n\n\nShow the codeSELECT \n    o_orderpriority,\n    o_orderstatus,\n    EXTRACT(YEAR FROM o_orderdate) AS order_year,\n    SUM(o_totalprice) AS total_sales\nFROM orders\nGROUP BY CUBE (o_orderpriority, o_orderstatus, order_year)\nORDER BY 1,2,3;\n\n\n\n\noutput - first 10 rows\n\n\n\nCUBE (o_custkey, o_orderstatus, order_year): This will group the data and calculate subtotals and grand totals for all possible combinations of o_custkey, o_orderstatus, and order_year.\nIt will generate all combinations of the grouping columns, including:\n\nGrouping by just o_custkey\nGrouping by just o_orderstatus\nGrouping by just order_year\nGrouping by all three together\nGrouping by pairs of columns\nA grand total (no grouping by any column)\n\nSUM(o_totalprice): For each combination of groupings, it sums the total order price.\n\n\n\nUse cases:\n\n\nOLAP Reporting: CUBE is commonly used in OLAP scenarios where you need to analyze data from multiple perspectives. For instance, you may want to generate reports that show total sales by customer, by order status, by year, and all possible combinations of these dimensions.\nSales Analysis: In sales analysis, CUBE can help create pivot-like summaries that show how different attributes (e.g., region, product, time period) contribute to the overall sales.\nFinancial Reports: Financial departments often use CUBE to calculate totals and subtotals across dimensions like departments, time periods, and account categories, making it easier to prepare comprehensive financial reports.\n\n\nHappy learning!"
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "",
    "text": "This is not orginal content!\n\n\n\nThis is my study notes / codes along with Andrej Karpathy’s “Neural Networks: Zero to Hero” series.\nCodes are executed in Colab, this calculation capacity exceeds my computer’s ability."
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#starter-code-walkthrough",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#starter-code-walkthrough",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "starter code walkthrough",
    "text": "starter code walkthrough\n\nimport libraries\n\n\nShow the code\nimport torch\nimport torch.nn.functional as F\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n\n\nreading data\n\n\nShow the code\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt\"\nwords = pd.read_csv(url, header=None).iloc[:, 0].tolist()\nwords[:8]\n\n# &gt;&gt;&gt; ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n\n\n\n\nbuilding vocab\n\n\nShow the code\n# build the vocabulary of characters and mapping to/from integer\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i, s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i: s for s, i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n\n# itos: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', \n# 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', \n# 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n# vocab_size: 27\n\n\n\n\ninitializing randomnization\n\n\nShow the code\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\n\n\n\n\ncreate train/dev/test splits\n\n\nShow the code\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n# build the dataset\ndef buid_dataset(words):\n    X, Y = [], []\n\n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix]\n\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\nXtr, Ytr = buid_dataset(words[:n1])        # 80#\nXdev, Ydev = buid_dataset(words[n1:n2])    # 10%\nXte, Yte = buid_dataset(words[n2:])        # 10%\n\n# torch.Size([182625, 3]) torch.Size([182625])\n# torch.Size([22655, 3]) torch.Size([22655])\n# torch.Size([22866, 3]) torch.Size([22866])\n\n\n\n\ninput and response preview\n\n\nShow the code\nfor x, y in zip(Xtr[:20], Ytr[:20]):\n  print(''.join(itos[ix.item()] for ix in x), '---&gt;', itos[y.item()])\n\n\n... ---&gt; y\n..y ---&gt; u\n.yu ---&gt; h\nyuh ---&gt; e\nuhe ---&gt; n\nhen ---&gt; g\neng ---&gt; .\n... ---&gt; d\n..d ---&gt; i\n.di ---&gt; o\ndio ---&gt; n\nion ---&gt; d\nond ---&gt; r\nndr ---&gt; e\ndre ---&gt; .\n... ---&gt; x\n..x ---&gt; a\n.xa ---&gt; v\nxav ---&gt; i\navi ---&gt; e\n\n\ninitializing objects in networks\nNear copy paste of the layers we have developed in Part 3, I added some docstring to the classes.\n\nclass Linear\n\n\nShow the code\nclass Linear:\n  \"\"\"    \n  Applies an affine linear transformation to the incoming data: y = xA^T + b.\n\n  This class implements a linear (fully connected) layer, which performs a linear \n  transformation on the input tensor. It is typically used in neural network architectures \n  to transform input features between layers.\n\n  Args:\n      fan_in (int): Number of input features (input dimension).\n      fan_out (int): Number of output features (output dimension).\n      bias (bool, optional): Whether to include a learnable bias term. \n          Defaults to True.\n\n  Attributes:\n      weight (torch.Tensor): Weight matrix of shape (fan_in, fan_out), \n          initialized using Kaiming initialization.\n      bias (torch.Tensor or None): Bias vector of shape (fan_out), \n          initialized to zeros if bias is True, otherwise None.\n\n  Methods:\n      __call__(x): Applies the linear transformation to the input tensor x.\n      parameters(): Returns a list of trainable parameters (weight and bias).\n\n  Example:\n      &gt;&gt;&gt; layer = Linear(10, 5)  # Creates a linear layer with 10 input features and 5 output features\n      &gt;&gt;&gt; x = torch.randn(3, 10)  # Input tensor with batch size 3 and 10 features\n      &gt;&gt;&gt; output = layer(x)  # Applies linear transformation\n      &gt;&gt;&gt; output.shape\n      torch.Size([3, 5])\n  \"\"\"\n\n  def __init__(self, fan_in, fan_out, bias=True):\n    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n    self.bias = torch.zeros(fan_out) if bias else None\n  \n  def __call__(self, x):\n    self.out = x @ self.weight\n    if self.bias is not None:\n      self.out += self.bias\n    return self.out\n  \n  def parameters(self):\n    return [self.weight] + ([] if self.bias is None else [self.bias])\n\n\n\n\nclass BatchNorm1d\n\n\nShow the code\nclass BatchNorm1d:\n  \"\"\"\n  Applies Batch Normalization to the input tensor, a technique to improve \n  training stability and performance in deep neural networks.\n\n  Batch Normalization normalizes the input across the batch dimension, \n  reducing internal covariate shift and allowing higher learning rates. \n  This implementation supports both training and inference modes.\n\n  Args:\n      dim (int): Number of features or channels to be normalized.\n      eps (float, optional): A small constant added to the denominator for \n          numerical stability to prevent division by zero. \n          Defaults to 1e-5.\n      momentum (float, optional): Momentum for updating running mean and \n          variance during training. Controls the degree of exponential \n          moving average. Defaults to 0.1.\n\n  Attributes:\n      eps (float): Epsilon value for numerical stability.\n      momentum (float): Momentum for running statistics update.\n      training (bool): Indicates whether the layer is in training or inference mode.\n      gamma (torch.Tensor): Learnable scale parameter of shape (dim,).\n      beta (torch.Tensor): Learnable shift parameter of shape (dim,).\n      running_mean (torch.Tensor): Exponential moving average of batch means.\n      running_var (torch.Tensor): Exponential moving average of batch variances.\n\n  Methods:\n      __call__(x): Applies batch normalization to the input tensor.\n      parameters(): Returns learnable parameters (gamma and beta).\n\n  Key Normalization Steps:\n  1. Compute batch mean and variance (in training mode)\n  2. Normalize input by subtracting mean and dividing by standard deviation\n  3. Apply learnable scale (gamma) and shift (beta) parameters\n  4. Update running statistics during training\n\n  Example:\n      &gt;&gt;&gt; batch_norm = BatchNorm1d(64)  # For 64-channel input\n      &gt;&gt;&gt; x = torch.randn(32, 64)  # Batch of 32 samples with 64 features\n      &gt;&gt;&gt; normalized_x = batch_norm(x)  # Apply batch normalization\n      &gt;&gt;&gt; normalized_x.shape\n      torch.Size([32, 64])\n\n  Note:\n      - Supports both 2D (batch, features) and 3D (batch, channels, sequence) input tensors\n      - During inference, uses running statistics instead of batch statistics\n  \"\"\"\n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.momentum = momentum\n    self.training = True\n    # parameters (trained with backprop)\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n    # buffers (trained with a running 'momentum update')\n    self.running_mean = torch.zeros(dim)\n    self.running_var = torch.ones(dim)\n  \n  def __call__(self, x):\n    # calculate the forward pass\n    if self.training:\n      xmean = x.mean(dim, keepdim=True) # batch mean\n      xvar = x.var(dim, keepdim=True) # batch variance\n    else:\n      xmean = self.running_mean\n      xvar = self.running_var\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    # update the buffers\n    if self.training:\n      with torch.no_grad():\n        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n    return self.out\n  \n  def parameters(self):\n    return [self.gamma, self.beta]\n\n\n\n\n\nclass Tanh\n\n\nShow the code\nclass Tanh:\n    \"\"\"\n    Hyperbolic Tangent (Tanh) Activation Function\n\n    Applies the hyperbolic tangent activation function element-wise to the input tensor. \n    Tanh maps input values to the range [-1, 1], providing a symmetric and non-linear \n    transformation that helps neural networks learn complex patterns.\n\n    Mathematical Definition:\n    tanh(x) = (e^x - e^-x) / (e^x + e^-x)\n    \n    Key Characteristics:\n    - Output Range: [-1, 1]\n    - Symmetric around the origin\n    - Gradient is always less than 1, which helps mitigate the vanishing gradient problem\n    - Commonly used in recurrent neural networks and hidden layers\n\n    Methods:\n        __call__(x): Applies the Tanh activation to the input tensor.\n        parameters(): Returns an empty list, as Tanh has no learnable parameters.\n\n    Attributes:\n        out (torch.Tensor): Stores the output of the most recent forward pass.\n\n    Example:\n        &gt;&gt;&gt; activation = Tanh()\n        &gt;&gt;&gt; x = torch.tensor([-2.0, 0.0, 2.0])\n        &gt;&gt;&gt; y = activation(x)\n        &gt;&gt;&gt; y\n        tensor([-0.9640, 0.0000, 0.9640])\n\n    Note:\n        This implementation is stateless and does not modify the input tensor.\n        The activation is applied element-wise, preserving the input tensor's shape.\n    \"\"\"\n\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n\n    def parameters(self):\n        return []\n\n\n\nrandom number generator\n\n\nShow the code\ntorch.manual_seed(42); # seed rng for reproducibility\n\n\n\n\nnetwork architecture\n\n\nShow the code\n# original network\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\nC = torch.rand((vocab_size, n_embd))\nlayers = [\n  Linear(n_embd * block_size, n_hidden, bias=False),\n  BatchNorm1d(n_hidden),\n  Tanh(),\n  Linear(n_hidden, vocab_size),\n]\n\n# parameter init\nwith torch.no_grad():\n  layers[-1].weight *= 0.1 # last layer make less confident\n\nparameters = [C] + [p for layer in layers for p in layer.parameters()]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n\n# model params: 12097\n\n\n\n\n\noptimization\n\n\nShow the code\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n  # minibatch construct\n  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n  \n  # forward pass\n  emb = C[Xb] # embed the characters into vectors   \n  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n  for layer in layers:\n    x = layer(x)\n  loss = F.cross_entropy(x, Yb) # loss function\n  \n  # backward pass\n  for p in parameters:\n    p.grad = None\n  loss.backward()\n  \n  # update: simple SGD\n  lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay\n  for p in parameters:\n    p.data += -lr * p.grad\n\n  # track stats\n  if i % 10000 == 0: # print every once in a while\n    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n  lossi.append(loss.log10().item())\n\n\n     0/ 200000: 3.2885\n  10000/ 200000: 2.3938\n  20000/ 200000: 2.1235\n  30000/ 200000: 1.9222\n  40000/ 200000: 2.2440\n  50000/ 200000: 2.1108\n  60000/ 200000: 2.0624\n  70000/ 200000: 2.0893\n  80000/ 200000: 2.4173\n  90000/ 200000: 1.9744\n 100000/ 200000: 2.0883\n 110000/ 200000: 2.4538\n 120000/ 200000: 1.9535\n 130000/ 200000: 1.8980\n 140000/ 200000: 2.1196\n 150000/ 200000: 2.3550\n 160000/ 200000: 2.2957\n 170000/ 200000: 2.0286\n 180000/ 200000: 2.2379\n 190000/ 200000: 2.3866\n\n\nobserve training process/evaluation\n\n\nShow the code\nplt.plot(lossi)\n\n\n\n\n\nlossi plot at the beginning\n\n\n\n\ncalibrate the batchnorm after training\nWe should be using the running mean/variance of the whole dataset splits rather than the last mini-batch.\n\n\nShow the code\n# put layers into eval mode (needed for batchnorm especially)\nfor layer in model.layers:\n  layer.training = False\n\n\n\n\ncalculate on whole training and validation splits\n\n\nShow the code\n# evaluate the loss\n@torch.no_grad() # this decorator disables gradient tracking inside pytorch\ndef split_loss(split):\n  x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n  }[split]\n  logits = model(x)\n  loss = F.cross_entropy(logits, y)\n  print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\n\nPretty loss but there are still room for improve:\ntrain 2.0467958450317383\nval 2.0989298820495605\n\n\nsample from the model\nHere are Names generated by the model till now, we have relatively name-like results that do not exist in the training set.\nliz.\nlayah.\ndan.\nhilon.\navani.\nkorron.\naua.\nnoon.\nbethalyn.\nthia.\nbote.\njereanail.\nvitorien.\nzarashivonna.\nyakurrren.\njovon.\nmalynn.\nvanna.\ncaparmana.\nshantymonse."
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#lets-fix-the-learning-rate-plot",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#lets-fix-the-learning-rate-plot",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "let’s fix the learning rate plot",
    "text": "let’s fix the learning rate plot\nThe plot for lossi looks very crazy, it’s because the batch size of 32 is way too few so this time we got lucky, and next time we got unlucky. And the mini-batch loss splashed too much. We should probably fix it.\nWe pivot to a row for every 1000 observations of lossi and calculate the mean, we end up have 200 observations which is easier to see.\n\n\nShow the code\nplt.plot(torch.tensor(lossi).view(-1, 1000).mean(1))\n\n\nWe can also observe the learning rate decay at 150k training loops.\n\n\n\nlossi plot fixed"
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#pytorchifying-our-code-layers-containers-torch.nn-fun-bugs",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#pytorchifying-our-code-layers-containers-torch.nn-fun-bugs",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "pytorchifying our code: layers, containers, torch.nn, fun bugs",
    "text": "pytorchifying our code: layers, containers, torch.nn, fun bugs\nNow we notice that we still have the embedding operation lying outside the pytorch-ified layers. It basically creating a lookup table C, embedding it with our data Y (or Yb), then stretching out to row with view() which is very cheap in PyTorch as no more memory creation is needed.\nWe modulize this by constructing 2 classes:\n\n\nShow the code\nclass Embedding:\n  \n  def __init__(self, num_embeddings, embedding_dim):\n    self.weight = torch.randn((num_embeddings, embedding_dim))\n    \n  def __call__(self, IX):\n    self.out = self.weight[IX]\n    return self.out\n  \n  def parameters(self):\n    return [self.weight]\n\n# -----------------------------------------------------------------------------------------------\nclass Flatten:\n\n  def __call__(self, x):\n    self.out = x.view(x.shape[0], -1)\n    return self.out\n  \n  def parameters(self):\n    return []\n\n\nNow we can re-define the layers like this:\n\n\nShow the code\nlayers = [\n  Embedding(vocab_size, n_embd),\n  Flatten(),\n  Linear(n_embd * block_size, n_hidden, bias=False),\n  BatchNorm1d(n_hidden),\n  Tanh(),\n  Linear(n_hidden, vocab_size),\n]\n\n\nand also remove the C, emb definition in the forward pass construction. Going futher, we will be not only pytorchifying the elements of layers only, but also the layers itself. In PyTorch, we have term containers, which specifying how we organize the layers in a network. And what are we doing here is constructing layers sequentially, which is equivalent to Sequential in the containers:\n\n\nShow the code\nclass Sequential:\n  \n  def __init__(self, layers):\n    self.layers = layers\n  \n  def __call__(self, x):\n    for layer in self.layers:\n      x = layer(x)\n    self.out = x\n    return self.out\n  \n  def parameters(self):\n    # get parameters of all layers and stretch them out into one list\n    return [p for layer in self.layers for p in layer.parameters()]\n\n\nand wrapp the layers into our model:\n\n\nShow the code\nmodel = Sequential([\n  Embedding(vocab_size, n_embd),\n  Flatten(),\n  Linear(n_embd * block_size, n_hidden, bias=False),\n  BatchNorm1d(n_hidden),\n  Tanh(),\n  Linear(n_hidden, vocab_size),\n])"
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#overview-wavenet",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#overview-wavenet",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "overview: WaveNet",
    "text": "overview: WaveNet\n\n\n\nVisualization of the WaveNet idea - Progressive Fusion"
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#dataset-bump-the-context-size-to-8",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#dataset-bump-the-context-size-to-8",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "dataset bump the context size to 8",
    "text": "dataset bump the context size to 8\nfirst we change the block_size into 8 and now our dataset looks like:\n........ ---&gt; y\n.......y ---&gt; u\n......yu ---&gt; h\n.....yuh ---&gt; e\n....yuhe ---&gt; n\n...yuhen ---&gt; g\n..yuheng ---&gt; .\n........ ---&gt; d\n.......d ---&gt; i\n......di ---&gt; o\n.....dio ---&gt; n\n....dion ---&gt; d\n...diond ---&gt; r\n..diondr ---&gt; e\n.diondre ---&gt; .\n........ ---&gt; x\n.......x ---&gt; a\n......xa ---&gt; v\n.....xav ---&gt; i\n....xavi ---&gt; e\nThe model size now bumps up to 22k."
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#re-running-baseline-code-on-block_size-8",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#re-running-baseline-code-on-block_size-8",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "re-running baseline code on block_size = 8",
    "text": "re-running baseline code on block_size = 8\nJust by lazily extending the context size to 8, we can already improve the model a little bit, the loss on validation split now is around 2.045. The names generated now look prettier:\nzamari.\nbrennis.\nshavia.\nwililke.\nobalyid.\nleenoluja.\nrianny.\njordanoe.\nyuvalfue.\nozleega.\njemirene.\npolton.\njawi.\nmeyah.\ngekiniq.\nangelinne.\ntayler.\ncatrician.\nkyearie.\nanderias.\nLet’s deem this as a baseline then we can start to implement WaveNet and see how far we can go!"
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#implementing-wavenet-1",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#implementing-wavenet-1",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "implementing WaveNet",
    "text": "implementing WaveNet\nFirst, let’s revisit the shape of the tensors along the way of the forward pass in our neural net:\n\n\nShow the code\n# Look at a batch of just 4 examples\nix = torch.randint(0, Xtr.shape[0], (4,))\nXb, Yb = Xtr[ix], Ytr[ix]\nlogits = model(Xb)\nprint(Xb.shape)\nXb\n\n# &gt; torch.Size([4, 8]) # because the context length is now 8\n# &gt; tensor([[ 0,  0,  0,  0,  0,  0, 13,  9],\n#         [ 0,  0,  0,  0,  0,  0,  0,  0],\n#         [ 0,  0,  0, 11,  5, 18, 15, 12],\n#         [ 0,  0,  4, 15, 13,  9, 14,  9]])\n\n# Output of the embedding layer, each input is translated\n# to 10 dimensional vector\nmodel.layers[0].out.shape\n# &gt; torch.Size([4, 8, 10])\n\n# Output of Flatten layer, each 10-dim vector is concatenated\n# to each other for all 8-dim context vectors\nmodel.layers[1].out.shape\n# &gt; torch.Size([4, 80])\n\n# Output of the Linear layer, take 80 and create 200 channels,\n# just via matrix mult\nmodel.layers[2].out.shape\n# &gt; torch.Size([4, 200])\n\n\nNow look into the Linear layer, which take the input x in the forward pass, multiply by weight and add the bias in (there is broadcasting here). So the transformation in this layer looks like:\n\n\nShow the code\n(torch.randn(4, 80) @ torch.randn(80, 200) + torch.randn(200)).shape\n\n# &gt; torch.Size([4, 200])\n\n\nInput x matrix here does not need to be 2 dimensional array. The matrix multiplication in PyTorch is quite powerfull, you can pass more than 2 dimensional array. And all dimensions will be preserved except the last one. Like this:\n\n\nShow the code\n(torch.randn(4, 5, 80) @ torch.randn(80, 200) + torch.randn(200)).shape\n\n# &gt; torch.Size([4, 5, 200])\n\n\nWhich we want to improve now is not just flatten the 8 characters input too fast at the beginning, we want to group them pair by pair to process them in parallel.\n(x1 x2) (x3 x4) (x5 x6) (x7 x8)\nParticularly for 8 characters block size we want to divide it into 4 groups ~ 4 pair of bigrams. We are increasing the dimensions of the batch.\n\n\nShow the code\n(torch.randn(4, 4, 20) @ torch.randn(20, 200) + torch.randn(200)).shape\n\n# &gt; torch.Size([4, 4, 200])\n\n\nHow can we achieve this in PyTorch, we can index the odd and even indexes then pair them up.\n\n\nShow the code\ne = torch.randn(4, 8, 10) # 4 examples, 8 chars context size, 10-d embedding\n# goal: want this to be (4, 4, 20) where consecutive 10-d vectors get concatenated\n\nexplicit = torch.cat([e[:, ::2, :], e[:, 1::2, :]], dim=2) # cat in the third dim\nexplicit.shape \n# &gt; torch.Size([4, 4, 20])\n\n\nOf course, PyTorch provide a more efficient way to do this, using view():\n\n\nShow the code\n(e.view(4, 4, 20) == explicit).all()\n# &gt; tensor(True)\n\n\nWe are going to modulize the FlattenConsecutive:\n\n\nShow the code\nclass FlattenConsecutive:\n  \n  def __init__(self, n):\n    self.n = n\n    \n  def __call__(self, x):\n    B, T, C = x.shape\n    x = x.view(B, T//self.n, C*self.n)\n    if x.shape[1] == 1: # spurious tensor, if it's 1, squeeze it\n      x = x.squeeze(1)\n    self.out = x\n    return self.out\n  \n  def parameters(self):\n    return []\n\n\nand update the flatten layer to FlattenConsecutive(block_size) (8) in our model. We can observe the dimension of tensors in all layers:\n\n\nShow the code\nfor layer in model.layers:\n  print(layer.__class__.__name__,\": \", tuple(layer.out.shape))\n\n# Embedding :  (4, 8, 10)\n# Flatten :  (4, 80)\n# Linear :  (4, 200)\n# BatchNorm1d :  (4, 200)\n# Tanh :  (4, 200)\n# Linear :  (4, 27)\n\n\nThis is what we have currently. But as said, we dont want to flatten too fast, so we gonna flatten by 2 character, here is the update of the model - 3 layers present the consecutive flatten 4 -&gt; 2 -&gt; 1:\n\n\nShow the code\nmodel = Sequential([\n  Embedding(vocab_size, n_embd),\n  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(n_hidden, vocab_size),\n])\n\n\nand here is tensors dimension flowing in forward pass:\nEmbedding :  (4, 8, 10)\nFlattenConsecutive :  (4, 4, 20)\nLinear :  (4, 4, 200)\nBatchNorm1d :  (4, 4, 200)\nTanh :  (4, 4, 200)\nFlattenConsecutive :  (4, 2, 400)\nLinear :  (4, 2, 200)\nBatchNorm1d :  (4, 2, 200)\nTanh :  (4, 2, 200)\nFlattenConsecutive :  (4, 400)\nLinear :  (4, 200)\nBatchNorm1d :  (4, 200)\nTanh :  (4, 200)\nLinear :  (4, 27)\nThat’s is, we have successfully implemented the WaveNet."
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#training-the-wavenet-first-pass",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#training-the-wavenet-first-pass",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "training the WaveNet: first pass",
    "text": "training the WaveNet: first pass\nNow assume we use the same size of network (number of neurons), let’s see if the loss can be improved. We change the n_hidden = 68, so that the total parameters of our network remain 22k. Below is update tensor dims for a batch (32):\nEmbedding :  (32, 8, 10)\nFlattenConsecutive :  (32, 4, 20)\nLinear :  (32, 4, 68)\nBatchNorm1d :  (32, 4, 68)\nTanh :  (32, 4, 68)\nFlattenConsecutive :  (32, 2, 136)\nLinear :  (32, 2, 68)\nBatchNorm1d :  (32, 2, 68)\nTanh :  (32, 2, 68)\nFlattenConsecutive :  (32, 136)\nLinear :  (32, 68)\nBatchNorm1d :  (32, 68)\nTanh :  (32, 68)\nLinear :  (32, 27)\nIt turns out that we got almost identical result. There are 2 things:\n\nWe just constructed the architecture of WaveNet but not tortured the model enough to find best set of hyperparameters; and\nWe may have a bug in BatchNorm1d layer, let’s take a look into this."
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#fixing-batchnorm1d-bug",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#fixing-batchnorm1d-bug",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "fixing batchnorm1d bug",
    "text": "fixing batchnorm1d bug\nLet’s look at the BatchNorm1d happen in the first flatten layer:\n\n\nShow the code\ne = torch.randn(32, 4 , 68)\nemean = e.mean(0, keepdim = True) # 1, 4, 68\nevar = e.var(0, keepdim = True) # 1, 4, 68\n\nehat = (e - emean) / torch.sqrt(evar + 1e-5)\nehat.shape\n\n# &gt; torch.Size([32, 4, 68])\n\n\nFor ehat, everything is calculated properly, mean and variance are calculated to the batch and the 2nd dim 4 is preserved. But for the running_mean:\n\n\nShow the code\nmodel.layers[3].running_mean.shape\n\n# &gt; torch.Size([1, 4, 68])\n\n\nWe see it is (1, 4, 68) while we are expected it’s 1 dimensional only which is defined in the init method (torch.zeros(dim)). We are maintaining the batch norm in parallel over 4 x 68 channels individually and independently instead of just 68 channels. We want to treat this 4 just like a batch norm dimension, ie everaging of 32 * 4 numbers for 68 channels. Fortunately PyTorch mean() method offer the reducing dimension not only for integer but also tuple.\n\n\nShow the code\ne = torch.randn(32, 4 , 68)\nemean = e.mean((0,1), keepdim = True) # 1, 1, 68\nevar = e.var((0,1), keepdim = True) # 1, 1, 68\n\nehat = (e - emean) / torch.sqrt(evar + 1e-5)\nehat.shape\n\n# &gt; torch.Size([32, 4, 68])\n\nmodel.layers[3].running_mean.shape\n\n# &gt; torch.Size([1, 1, 68])\n\n\nWe now modify the BatchNorm1d definition accordingly, only the training mean/var in the __call__ method:\n\n\nShow the code\n# ---- remains the same\n  def __call__(self, x):\n    # calculate the forward pass\n    if self.training:\n      if x.ndim == 2:\n        dim = 0\n      elif x.ndim == 3:\n        dim = (0,1)\n      xmean = x.mean(dim, keepdim=True) # batch mean\n      xvar = x.var(dim, keepdim=True) # batch variance\n    else:\n      xmean = self.running_mean\n      xvar = self.running_var\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n# ---- remaind the same"
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#re-training-wavenet-with-bug-fix",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#re-training-wavenet-with-bug-fix",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "re-training WaveNet with bug fix",
    "text": "re-training WaveNet with bug fix\nNow retraining the network with bug fixed, we obtain a slightly better loss of 2.022. We just fixed the normalization term inside the network so they did not thrush too much so a little improvement only is expected."
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#scaling-up-our-wavenet",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#scaling-up-our-wavenet",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "scaling up our WaveNet",
    "text": "scaling up our WaveNet\nNow we’re ready to scale up our network and retrain everything, the model now have roughly 76k paramters. We finally passed the 2.0 threshold and achieved the loss of 1.99 on the validation split.\n\n\nShow the code\nn_embd = 24\nn_hidden = 128\n\n\nAnd here is final loss plot:\n\n\n\nlossi final"
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#performance-log",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#performance-log",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "performance log",
    "text": "performance log\n\nLoss logs\n\n\n\n\n\n\n\nStep\nWhat we did\nLoss we got (accum)\n\n\n\n\n1\noriginal (3 character context + 200 hidden neurons, 12K params)\ntrain 2.0467958450317383\nval 2.0989298820495605\n\n\n2\ncontext: 3 -&gt; 8 (22K params)\ntrain 1.9028635025024414\nval 2.044949769973755\n\n\n3\nflat -&gt; hierarchical (22K params)\ntrain 1.9366059303283691\nval 2.017268419265747\n\n\n4\nfix bug in batchnorm1d\ntrain 1.9156142473220825\nval 2.0228867530822754\n\n\n5\nscale up the network: n_embd 24, n_hidden 128 (76K params)\ntrain 1.7680459022521973\nval 1.994154691696167"
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#experimental-harness",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#experimental-harness",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "experimental harness",
    "text": "experimental harness\nThe “harness” metaphor is apt because it’s like a structured support system that allows researchers to systematically explore and optimize neural network configurations, much like a harness helps guide and support an athlete during training.\n\n\n\n\n\n\nNote\n\n\n\nAn experimental harness typically includes several key components:\n\nHyperparameter Search Space Definition: This involves specifying the range of hyperparameters to be explored, such as:\n\n\nLearning rates\nBatch sizes\nNetwork architecture depths\nActivation functions\nRegularization techniques\nDropout rates\n\n\nSearch Strategy: Methods for exploring the hyperparameter space, which can include:\n\n\nGrid search\nRandom search\nBayesian optimization\nEvolutionary algorithms\nGradient-based optimization techniques\n\n\nEvaluation Metrics: Predefined metrics to assess model performance, such as:\n\n\nValidation accuracy\nLoss function values\nPrecision and recall\nF1 score\nComputational efficiency\n\n\nAutomated Experiment Management: Tools and scripts that can:\n\n\nAutomatically generate and run different model configurations\nLog results\nTrack experiments\nCompare performance across different hyperparameter settings\n\n\nReproducibility Mechanisms: Ensuring that experiments can be repeated and validated, which includes:\n\n\nFixed random seeds\nConsistent data splitting\nVersioning of datasets and configurations"
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#wavenet-but-with-dilated-causal-convolutions",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#wavenet-but-with-dilated-causal-convolutions",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "WaveNet but with “dilated causal convolutions”",
    "text": "WaveNet but with “dilated causal convolutions”\n\nConvolution is a “for loop” applying a linear filter over space of some input sequence;\nNot happen only in Python but also in Kernel"
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#torch.nn",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#torch.nn",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "torch.nn",
    "text": "torch.nn\nWe have implement alot of concepts in torch.nn:\n\ncontainers: Sequential\nLinear, BatchNorm1d, Tanh, FlattenConsecutive, …"
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#the-development-process-of-building-deep-neural-nets",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#the-development-process-of-building-deep-neural-nets",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "the development process of building deep neural nets",
    "text": "the development process of building deep neural nets"
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#going-forward",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#going-forward",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "going forward",
    "text": "going forward"
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#improve-on-my-loss-how-far-can-we-improve-a-wavenet-on-this-data",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#improve-on-my-loss-how-far-can-we-improve-a-wavenet-on-this-data",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "improve on my loss! how far can we improve a WaveNet on this data?",
    "text": "improve on my loss! how far can we improve a WaveNet on this data?"
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#starter-code-walk-through",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#starter-code-walk-through",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "starter code walk through",
    "text": "starter code walk through\n\nimport libraries\n\n\nShow the code\nimport torch\nimport torch.nn.functional as F\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n\n\nreading data\n\n\nShow the code\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt\"\nwords = pd.read_csv(url, header=None).iloc[:, 0].tolist()\nwords[:8]\n\n# &gt;&gt;&gt; ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n\n\n\n\nbuilding vocab\n\n\nShow the code\n# build the vocabulary of characters and mapping to/from integer\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i, s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i: s for s, i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n\n# itos: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', \n# 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', \n# 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n# vocab_size: 27\n\n\n\n\ninitializing randomization\n\n\nShow the code\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\n\n\n\n\ncreate train/dev/test splits\n\n\nShow the code\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n# build the dataset\ndef buid_dataset(words):\n    X, Y = [], []\n\n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix]\n\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\nXtr, Ytr = buid_dataset(words[:n1])        # 80#\nXdev, Ydev = buid_dataset(words[n1:n2])    # 10%\nXte, Yte = buid_dataset(words[n2:])        # 10%\n\n# torch.Size([182625, 3]) torch.Size([182625])\n# torch.Size([22655, 3]) torch.Size([22655])\n# torch.Size([22866, 3]) torch.Size([22866])\n\n\n\n\ninput and response preview\n\n\nShow the code\nfor x, y in zip(Xtr[:20], Ytr[:20]):\n  print(''.join(itos[ix.item()] for ix in x), '---&gt;', itos[y.item()])\n\n\n... ---&gt; y\n..y ---&gt; u\n.yu ---&gt; h\nyuh ---&gt; e\nuhe ---&gt; n\nhen ---&gt; g\neng ---&gt; .\n... ---&gt; d\n..d ---&gt; i\n.di ---&gt; o\ndio ---&gt; n\nion ---&gt; d\nond ---&gt; r\nndr ---&gt; e\ndre ---&gt; .\n... ---&gt; x\n..x ---&gt; a\n.xa ---&gt; v\nxav ---&gt; i\navi ---&gt; e\n\n\ninitializing objects in networks\nNear copy paste of the layers we have developed in Part 3, I added some docstring to the classes.\n\nclass Linear\n\n\nShow the code\nclass Linear:\n  \"\"\"    \n  Applies an affine linear transformation to the incoming data: y = xA^T + b.\n\n  This class implements a linear (fully connected) layer, which performs a linear \n  transformation on the input tensor. It is typically used in neural network architectures \n  to transform input features between layers.\n\n  Args:\n      fan_in (int): Number of input features (input dimension).\n      fan_out (int): Number of output features (output dimension).\n      bias (bool, optional): Whether to include a learnable bias term. \n          Defaults to True.\n\n  Attributes:\n      weight (torch.Tensor): Weight matrix of shape (fan_in, fan_out), \n          initialized using Kaiming initialization.\n      bias (torch.Tensor or None): Bias vector of shape (fan_out), \n          initialized to zeros if bias is True, otherwise None.\n\n  Methods:\n      __call__(x): Applies the linear transformation to the input tensor x.\n      parameters(): Returns a list of trainable parameters (weight and bias).\n\n  Example:\n      &gt;&gt;&gt; layer = Linear(10, 5)  # Creates a linear layer with 10 input features and 5 output features\n      &gt;&gt;&gt; x = torch.randn(3, 10)  # Input tensor with batch size 3 and 10 features\n      &gt;&gt;&gt; output = layer(x)  # Applies linear transformation\n      &gt;&gt;&gt; output.shape\n      torch.Size([3, 5])\n  \"\"\"\n\n  def __init__(self, fan_in, fan_out, bias=True):\n    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n    self.bias = torch.zeros(fan_out) if bias else None\n  \n  def __call__(self, x):\n    self.out = x @ self.weight\n    if self.bias is not None:\n      self.out += self.bias\n    return self.out\n  \n  def parameters(self):\n    return [self.weight] + ([] if self.bias is None else [self.bias])\n\n\n\n\nclass BatchNorm1d\n\n\nShow the code\nclass BatchNorm1d:\n  \"\"\"\n  Applies Batch Normalization to the input tensor, a technique to improve \n  training stability and performance in deep neural networks.\n\n  Batch Normalization normalizes the input across the batch dimension, \n  reducing internal covariate shift and allowing higher learning rates. \n  This implementation supports both training and inference modes.\n\n  Args:\n      dim (int): Number of features or channels to be normalized.\n      eps (float, optional): A small constant added to the denominator for \n          numerical stability to prevent division by zero. \n          Defaults to 1e-5.\n      momentum (float, optional): Momentum for updating running mean and \n          variance during training. Controls the degree of exponential \n          moving average. Defaults to 0.1.\n\n  Attributes:\n      eps (float): Epsilon value for numerical stability.\n      momentum (float): Momentum for running statistics update.\n      training (bool): Indicates whether the layer is in training or inference mode.\n      gamma (torch.Tensor): Learnable scale parameter of shape (dim,).\n      beta (torch.Tensor): Learnable shift parameter of shape (dim,).\n      running_mean (torch.Tensor): Exponential moving average of batch means.\n      running_var (torch.Tensor): Exponential moving average of batch variances.\n\n  Methods:\n      __call__(x): Applies batch normalization to the input tensor.\n      parameters(): Returns learnable parameters (gamma and beta).\n\n  Key Normalization Steps:\n  1. Compute batch mean and variance (in training mode)\n  2. Normalize input by subtracting mean and dividing by standard deviation\n  3. Apply learnable scale (gamma) and shift (beta) parameters\n  4. Update running statistics during training\n\n  Example:\n      &gt;&gt;&gt; batch_norm = BatchNorm1d(64)  # For 64-channel input\n      &gt;&gt;&gt; x = torch.randn(32, 64)  # Batch of 32 samples with 64 features\n      &gt;&gt;&gt; normalized_x = batch_norm(x)  # Apply batch normalization\n      &gt;&gt;&gt; normalized_x.shape\n      torch.Size([32, 64])\n\n  Note:\n      - Supports both 2D (batch, features) and 3D (batch, channels, sequence) input tensors\n      - During inference, uses running statistics instead of batch statistics\n  \"\"\"\n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.momentum = momentum\n    self.training = True\n    # parameters (trained with backprop)\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n    # buffers (trained with a running 'momentum update')\n    self.running_mean = torch.zeros(dim)\n    self.running_var = torch.ones(dim)\n  \n  def __call__(self, x):\n    # calculate the forward pass\n    if self.training:\n      xmean = x.mean(dim, keepdim=True) # batch mean\n      xvar = x.var(dim, keepdim=True) # batch variance\n    else:\n      xmean = self.running_mean\n      xvar = self.running_var\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    # update the buffers\n    if self.training:\n      with torch.no_grad():\n        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n    return self.out\n  \n  def parameters(self):\n    return [self.gamma, self.beta]\n\n\n\n\n\nclass Tanh\n\n\nShow the code\nclass Tanh:\n    \"\"\"\n    Hyperbolic Tangent (Tanh) Activation Function\n\n    Applies the hyperbolic tangent activation function element-wise to the input tensor. \n    Tanh maps input values to the range [-1, 1], providing a symmetric and non-linear \n    transformation that helps neural networks learn complex patterns.\n\n    Mathematical Definition:\n    tanh(x) = (e^x - e^-x) / (e^x + e^-x)\n    \n    Key Characteristics:\n    - Output Range: [-1, 1]\n    - Symmetric around the origin\n    - Gradient is always less than 1, which helps mitigate the vanishing gradient problem\n    - Commonly used in recurrent neural networks and hidden layers\n\n    Methods:\n        __call__(x): Applies the Tanh activation to the input tensor.\n        parameters(): Returns an empty list, as Tanh has no learnable parameters.\n\n    Attributes:\n        out (torch.Tensor): Stores the output of the most recent forward pass.\n\n    Example:\n        &gt;&gt;&gt; activation = Tanh()\n        &gt;&gt;&gt; x = torch.tensor([-2.0, 0.0, 2.0])\n        &gt;&gt;&gt; y = activation(x)\n        &gt;&gt;&gt; y\n        tensor([-0.9640, 0.0000, 0.9640])\n\n    Note:\n        This implementation is stateless and does not modify the input tensor.\n        The activation is applied element-wise, preserving the input tensor's shape.\n    \"\"\"\n\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n\n    def parameters(self):\n        return []\n\n\n\nrandom number generator\n\n\nShow the code\ntorch.manual_seed(42); # seed rng for reproducibility\n\n\n\n\nnetwork architecture\n\n\nShow the code\n# original network\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\nC = torch.rand((vocab_size, n_embd))\nlayers = [\n  Linear(n_embd * block_size, n_hidden, bias=False),\n  BatchNorm1d(n_hidden),\n  Tanh(),\n  Linear(n_hidden, vocab_size),\n]\n\n# parameter init\nwith torch.no_grad():\n  layers[-1].weight *= 0.1 # last layer make less confident\n\nparameters = [C] + [p for layer in layers for p in layer.parameters()]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n\n# model params: 12097\n\n\n\n\n\noptimization\n\n\nShow the code\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n  # minibatch construct\n  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n  \n  # forward pass\n  emb = C[Xb] # embed the characters into vectors   \n  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n  for layer in layers:\n    x = layer(x)\n  loss = F.cross_entropy(x, Yb) # loss function\n  \n  # backward pass\n  for p in parameters:\n    p.grad = None\n  loss.backward()\n  \n  # update: simple SGD\n  lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay\n  for p in parameters:\n    p.data += -lr * p.grad\n\n  # track stats\n  if i % 10000 == 0: # print every once in a while\n    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n  lossi.append(loss.log10().item())\n\n\n     0/ 200000: 3.2885\n  10000/ 200000: 2.3938\n  20000/ 200000: 2.1235\n  30000/ 200000: 1.9222\n  40000/ 200000: 2.2440\n  50000/ 200000: 2.1108\n  60000/ 200000: 2.0624\n  70000/ 200000: 2.0893\n  80000/ 200000: 2.4173\n  90000/ 200000: 1.9744\n 100000/ 200000: 2.0883\n 110000/ 200000: 2.4538\n 120000/ 200000: 1.9535\n 130000/ 200000: 1.8980\n 140000/ 200000: 2.1196\n 150000/ 200000: 2.3550\n 160000/ 200000: 2.2957\n 170000/ 200000: 2.0286\n 180000/ 200000: 2.2379\n 190000/ 200000: 2.3866\n\n\nobserve training process/evaluation\n\n\nShow the code\nplt.plot(lossi)\n\n\n\n\n\nlossi plot at the beginning\n\n\n\n\ncalibrate the batchnorm after training\nWe should be using the running mean/variance of the whole dataset splits rather than the last mini-batch.\n\n\nShow the code\n# put layers into eval mode (needed for batchnorm especially)\nfor layer in model.layers:\n  layer.training = False\n\n\n\n\ncalculate on whole training and validation splits\n\n\nShow the code\n# evaluate the loss\n@torch.no_grad() # this decorator disables gradient tracking inside pytorch\ndef split_loss(split):\n  x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n  }[split]\n  logits = model(x)\n  loss = F.cross_entropy(logits, y)\n  print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\n\nPretty loss but there are still room for improve:\ntrain 2.0467958450317383\nval 2.0989298820495605\n\n\nsample from the model\nHere are Names generated by the model till now, we have relatively name-like results that do not exist in the training set.\nliz.\nlayah.\ndan.\nhilon.\navani.\nkorron.\naua.\nnoon.\nbethalyn.\nthia.\nbote.\njereanail.\nvitorien.\nzarashivonna.\nyakurrren.\njovon.\nmalynn.\nvanna.\ncaparmana.\nshantymonse."
  },
  {
    "objectID": "blog/2024-12-09-nn-z2h-p6/index.html#the-development-process-of-building-deep-neural-nets-going-forward",
    "href": "blog/2024-12-09-nn-z2h-p6/index.html#the-development-process-of-building-deep-neural-nets-going-forward",
    "title": "NN-Z2H Lesson 6: Building makemore part 5 - Building a WaveNet",
    "section": "the development process of building deep neural nets & going forward",
    "text": "the development process of building deep neural nets & going forward\n\nSpending a ton of time exploring PyTorch documentation, unfortunately it’s not a good one;\nTon of time to make the shapes work: fan in, fan out, NLC or NLC, broadcasting, viewing, etc;\nWhat we:\n\ndone: implemented dilated causal convoluntional network;\nto be explores: residual and skip connections;\nto be explores: experimental harness;\nmore mordern networks: RNN, LSTM, Transformer."
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html",
    "href": "blog/2024-12-12-intro-llm/index.html",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "",
    "text": "AI-assisted content\n\n\n\nThis summary is conducted with the help of “Gemini 2.0 Flash Experimental” in Google AI Studio."
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#intro-large-language-model-llm-talk",
    "href": "blog/2024-12-12-intro-llm/index.html#intro-large-language-model-llm-talk",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "Intro: Large Language Model (LLM) Talk",
    "text": "Intro: Large Language Model (LLM) Talk\nKarpathy begins by explaining the motivation for re-recording his talk, emphasizing its popularity. He frames it as a “busy person’s intro” to large language models, aiming to provide a concise yet informative overview. He quickly dives into the core of what constitutes an LLM."
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#llm-inference-the-essence-of-llms",
    "href": "blog/2024-12-12-intro-llm/index.html#llm-inference-the-essence-of-llms",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "LLM Inference: The Essence of LLMs",
    "text": "LLM Inference: The Essence of LLMs\nIn its most basic form, a large language model is just two files:\n\na “parameters” file; and\na “run” file.\n\nUsing the example of the Llama 2 70B model, released by Meta AI, he clarifies these points. The parameters file contains the model’s weights, essentially a vast list of numbers representing the trained neural network. These parameters, stored as 2-byte float16 numbers, for the 70B model, come up to about 140 GB (because each param is 2 bytes). The “run” file is the code (often in C or Python) that executes the neural network using the parameters. This code is surprisingly compact, requiring approximately 500 lines of C code without external dependencies. This is the essence of the model itself – a self-contained package requiring no internet connectivity once compiled.\nKarpathy uses the example of providing text to the model (“write a poem about scale AI”), which generates a poem, to demonstrate what inference (running the model) looks like. He emphasizes that the computational complexity comes in obtaining those parameters via training, not running the model itself. The demonstration is not actually the 70B model in real time but a much smaller 7B model, running about 10 times faster for illustrative purposes.\nThe magic lies under the parameters. But how can we obtain them?"
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#llm-training-the-heavy-lifting",
    "href": "blog/2024-12-12-intro-llm/index.html#llm-training-the-heavy-lifting",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "LLM Training: The Heavy Lifting",
    "text": "LLM Training: The Heavy Lifting\nThe bulk of the computational effort and cost is tied to training the model. This involves creating the parameters, a process far more complex than running it (inference). The training is conceptualized as a compression process of the internet.\nThe resources required for training Llama 2 70B:\n\nroughly 10 terabytes of text from a crawl of the internet,\nprocessed by 6,000 GPUs over 12 days,\nat a cost of around $2 million.\n\nThis process compresses the massive text dataset into the 140GB parameters file, which he refers to as “like a zip file of the internet” with a compression ratio of approximately 100x. Importantly, this is a lossy compression, meaning the model doesn’t store an exact copy of the text, but a generalized representation, a kind of “Gestalt.” Karpathy points out that the numbers associated with Llama 2 70B, while significant, are “rookie numbers” compared to the training efforts for state-of-the-art models used in ChatGPT or Claude which are of magnitude 10x or more and costing tens or hundreds of millions of dollars.\nAfter being trained (and fine-tuned - which is discussed later), running the LLM model which is neural network is faily computationally cheap. So what is neural network is really doing?"
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#llm-dreams-text-generation-hallucination",
    "href": "blog/2024-12-12-intro-llm/index.html#llm-dreams-text-generation-hallucination",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "LLM Dreams: Text Generation & Hallucination",
    "text": "LLM Dreams: Text Generation & Hallucination\nFundamentally, a language model’s primary task during training is to predict the next word in a sequence. For example, from a sequence “Cat sat on a”, the neural network will predict what word comes next, e.g. “mat” with a 97% probability. Karpathy explains that this prediction task has a very close mathematical relationship to compression. This simple prediction is a powerful objective, forcing the model to learn vast amounts of world knowledge. He uses a sample from Wikipedia about Ruth Handler to highlight how even seemingly mundane text is full of information that the model internalizes.\nAfter training, running the model (inference) involves iterative text generation. The model predicts a word, feeds it back in, predicts the next, and so on, in a process Karpathy describes as “dreaming internet documents.” He shows examples of generated text resembling Java code, Amazon product listings, and Wikipedia articles. This generated text is often plausible but hallucinated – not necessarily true or derived directly from the training data. He gives examples of made up ISBNs and text referencing an obscure fish species, demonstrating that the model is not simply memorizing the training set.\n\n\n\nAll of these are dreamed by neural networks"
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#how-do-they-work-the-transformer-architecture",
    "href": "blog/2024-12-12-intro-llm/index.html#how-do-they-work-the-transformer-architecture",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "How Do They Work? The Transformer Architecture",
    "text": "How Do They Work? The Transformer Architecture\nInto the inner workings of LLMs, they are based on a neural network architecture called the “Transformer”. The full details of the mathematical operations and architecture are available. The challenge is that parameters are spread across the network and the exact nature of their interactions and contributions to the whole is unknown. While we know how to iteratively adjust the parameters to improve next-word prediction accuracy, we don’t truly understand how these parameters collaborate to produce specific outputs. We do have some models suggesting they build up knowledge databases but this knowledge is imperfect and strange. Karpathy illustrates the weirdness of LLM knowledge using the viral “reversal course” example of Chat GPT’s inability to recognize “Tom Cruise” as “Merily Feifer’s son,” showing the knowledge is one-dimensional and directionally dependent. He summarizes that LLMs should be viewed as “mostly inscrutable artifacts”, unlike engineered systems (like cars). He emphasizes that they are empirical artifacts that necessitate sophisticated evaluation methods.\n\n\n\nReversal Course"
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#fine-tuning-into-an-assistant-from-document-generators-to-chatbots",
    "href": "blog/2024-12-12-intro-llm/index.html#fine-tuning-into-an-assistant-from-document-generators-to-chatbots",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "Fine-tuning into an Assistant: From Document Generators to Chatbots",
    "text": "Fine-tuning into an Assistant: From Document Generators to Chatbots\nThe first training phase results in an internet document generator, which is not that helpful. The second major step is fine-tuning, the process of transforming this into a helpful “assistant model”. This transformation is achieved by switching from training on a large collection of internet text to training on a smaller, carefully curated dataset of conversations.\nThis dataset is typically created manually by human labelers who are provided with labeling instructions to generate questions and model answers. An example is provided of a question, with the correct assistant answer. The pre-training stage prioritizes data quantity but low quality, while the fine-tuning stage favors high-quality, carefully labelled Q&A documents. A dataset of 100,000 high-quality Q&A documents would be more than sufficient for fine tuning. The fine tuning stage leverages the knowledge from pre-training and reconfigures the LLM to answer questions in a helpful manner. He gives the example of the prompt “can you help me with this code” with the corresponding helpful response. The model is able to “format” itself into an assistant that knows how to answer and responds to these kinds of questions, by learning patterns from the training data, and generating text word by word."
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#summary-so-far-pre-training-vs.-fine-tuning",
    "href": "blog/2024-12-12-intro-llm/index.html#summary-so-far-pre-training-vs.-fine-tuning",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "Summary So Far: Pre-training vs. Fine-tuning",
    "text": "Summary So Far: Pre-training vs. Fine-tuning\nKarpathy summarizes the two-stage process: * Pre-training: training on a massive internet dataset, resulting in a base model with world knowledge, this is computationally very expensive (millions of dollars) and done infrequently. * Fine-tuning: training on high-quality Q&A data, resulting in an assistant model, this is computationally cheaper and done more frequently (daily or weekly).\nThe Llama 2 series included both base and assistant models. The base model, without fine tuning, is not directly usable because it just samples documents rather than responding with an answer. Meta performed the pre-training and released the result. This allows others to do their own fine-tuning, providing tremendous freedom.\n\n\n\nHow to train your dragon ChatGPT"
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#appendix-comparisons-labeling-docs-rlhf-synthetic-data-leaderboard",
    "href": "blog/2024-12-12-intro-llm/index.html#appendix-comparisons-labeling-docs-rlhf-synthetic-data-leaderboard",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "Appendix: Comparisons, Labeling Docs, RLHF, Synthetic Data, Leaderboard",
    "text": "Appendix: Comparisons, Labeling Docs, RLHF, Synthetic Data, Leaderboard\nThis section expands on finer aspects of training and evaluation. Fine-tuning is followed by an optional third stage that leverages comparison labels. It is typically easier for a human labeler to rank options vs. generating the content. The labelers are given example Haikus and they are asked to pick the best one. These comparisons are used to further fine-tune the model via Reinforcement Learning from Human Feedback (RLHF). Karpathy shows an example of labeling instructions from the InstructGPT paper asking labelers to be “helpful, truthful, and harmless”. He notes that these instructions can be tens or hundreds of pages long and very complicated.\nHe then explains that human labelers are used, although increasingly, machine assisted. Models can be used to sample answers and a human can do the cherry picking to improve efficiency. He then displays the Chatbot Arena leaderboard where Language Models are ranked according to their ELO ratings (just like chess). This ranking system determines which model does better on user queries. He observes the closed, proprietary models like those from OpenAI and Anthropic generally perform better. However, open-weights models (Llama series, Zephyr) are available and can be fine-tuned and used by everyone. The open-source models may be less accurate, but good enough for many applications."
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#llm-scaling-laws-the-path-to-better-performance",
    "href": "blog/2024-12-12-intro-llm/index.html#llm-scaling-laws-the-path-to-better-performance",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "LLM Scaling Laws: The Path to Better Performance",
    "text": "LLM Scaling Laws: The Path to Better Performance\nKarpathy highlights the importance of scaling laws: the performance of LLMs in terms of next-word prediction accuracy is a predictable function of:\n\nthe number of parameters (N), and\nthe amount of training data (D).\n\nIncreased parameters and increased dataset always result in better performance (predictability), so algorithmic progress is a bonus. It’s not necessarily needed for improvement, which creates a gold rush, because bigger and better clusters can provide better results. He emphasizes this is correlated with performance across many downstream tasks.\n\n\n\nFree scaling!"
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#tool-use-expanding-capabilities",
    "href": "blog/2024-12-12-intro-llm/index.html#tool-use-expanding-capabilities",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "Tool Use: Expanding Capabilities",
    "text": "Tool Use: Expanding Capabilities\nHe uses a concrete example to demonstrate the tool use capabilities of LLMs using ChatGPT, by providing a query to find information on Scale AI’s funding rounds. ChatGPT can browse the internet using search tools, it then takes the information from search results, and uses that to answer the prompt. The model then can use a calculator tool to perform arithmetic tasks, and generates an estimation based on ratios from data it previously received. It can then use a python interpreter with the matplotlib library to generate graphs and visualizations to the data. This allows the language model to solve problems just as humans might, using external tools to aid in problem solving. It can also use DALL-E for image generation. The LLM orchestrates the various tools to solve the prompt and return the result to the user."
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#multimodality-beyond-text",
    "href": "blog/2024-12-12-intro-llm/index.html#multimodality-beyond-text",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "Multimodality: Beyond Text",
    "text": "Multimodality: Beyond Text\nThe next major aspect is multimodality, showing how LLMs can interact beyond text with images, audio, etc. Karpathy gives the example of ChatGPT analyzing a hand-sketched website diagram, generating functional HTML and JavaScript. ChatGPT can also hear and speak and thus enable speech-to-speech interactions.\n\n\n\nMultimodality"
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#thinking-system-12-reasoning-and-planning",
    "href": "blog/2024-12-12-intro-llm/index.html#thinking-system-12-reasoning-and-planning",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "Thinking, System 1/2: Reasoning and Planning",
    "text": "Thinking, System 1/2: Reasoning and Planning\nKarpathy introduces the concept of System 1 and System 2 thinking, inspired by Daniel Kahneman’s work. System 1 is fast, instinctive thinking (the answer is cached already), whereas System 2 is slower, more deliberate reasoning.\n\n\n\n2 systems of thinking\n\n\nLLMs currently only operate in System 1 mode, sampling word by word without deep planning. A major future direction is to enable System 2 thinking, where models can spend more time to improve quality. The idea is to make a trade off between compute time and accuracy. Karpathy notes how this is not possible today, where the LLM just goes chunk chunk chunk x3.14 and sampling words in the sequence without “thinking” through it. He emphasizes the importance of converting time into accuracy using tree-of-thought and other techniques.\n\n\n\nTowarding the Tree of Thoughts"
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#self-improvement-going-beyond-human-limits",
    "href": "blog/2024-12-12-intro-llm/index.html#self-improvement-going-beyond-human-limits",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "Self-improvement: Going Beyond Human Limits",
    "text": "Self-improvement: Going Beyond Human Limits\nHe references DeepMind’s AlphaGo, which initially learned by imitating human players but later surpassed human performance through self-improvement. Karpathy argues that current LLMs are only in the imitation stage, and the next challenge is to achieve self-improvement for them, similar to the AlphaGo step two. The lack of a general reward function, unlike the win/loss criteria of the go game, is the main challenge. In narrow domains though, this may be achievable.\n\n\n\nSelf-improvement is key for AlphaGo to surpass the best human in 40 days"
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#llm-customization-expert-systems",
    "href": "blog/2024-12-12-intro-llm/index.html#llm-customization-expert-systems",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "LLM Customization: Expert Systems",
    "text": "LLM Customization: Expert Systems\nKarpathy explains the need for customization to allow LLMs to become specialized in specific tasks. This is especially important due to the diversity of needs in the real world. He provides the example of the GPT Store by OpenAI, which allows users to create custom GPTs using custom instructions, and retrieval augmented generation. Fine-tuning LLMs may be needed to have experts in specific areas.\nThis is what people are hyped doing nowaday 😪."
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#llm-os-a-new-paradigm",
    "href": "blog/2024-12-12-intro-llm/index.html#llm-os-a-new-paradigm",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "LLM OS: A New Paradigm",
    "text": "LLM OS: A New Paradigm\nKarpathy proposes that LLMs should not be seen as mere chatbots, but rather the kernel process of an emerging operating system. He explains that the LLM is coordinating a lot of resources for problem solving, acting like the kernel of an operating system. He then goes to show an analog of a computer’s components, and what they might correspond to in LLM space. He outlines the LLM OS’s capabilities, such as internet browsing, local file referencing, access to software tools, ability to generate/interpret images, videos, and audio, long-term “thinking”, customization, and self-improvement. This maps directly to what we expect an operating system to do. He makes an analogy to a computer’s memory hierarchy with hard disks (internet/local files), random access memory (context window). He also draws a parallel between the diverse OS landscape of today with the proprietary desktop OS like Windows and MacOS vs open-source alternatives. In the LLM space, similarly there’s GPT/Claude/Bard and the open-source landscape built around Llama.\n\n\n\nEmbed the LLMs into the kernel"
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#llm-security-intro-the-new-battleground",
    "href": "blog/2024-12-12-intro-llm/index.html#llm-security-intro-the-new-battleground",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "LLM Security Intro: The New Battleground",
    "text": "LLM Security Intro: The New Battleground\nKarpathy then switches gears to security challenges, arguing that the new paradigm of LLMs introduce new security vulnerabilities. He emphasizes that cat and mouse security game that we are familiar with in computer security also exists in LLM security."
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#jailbreaks-bypassing-safety-restrictions",
    "href": "blog/2024-12-12-intro-llm/index.html#jailbreaks-bypassing-safety-restrictions",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "Jailbreaks: Bypassing Safety Restrictions",
    "text": "Jailbreaks: Bypassing Safety Restrictions\nHe starts with jailbreak attacks, demonstrating how prompts can circumvent safety restrictions and elicit harmful information from LLMs. One example given is the grandmother-role-play where the LLM was tricked to give instructions on how to make Napalm. There are many ways to trick a system into providing unsafe responses. He gives the example of base64 encoding a dangerous request to elicit unsafe information because training for safety often focuses on English text. The universal transferable suffix is another attack vector: adding the suffix will bypass safety filters. Image noise based jailbreaks are also possible by carefully crafting the noise pattern.\n\n\n\nExample of a jailbreak using base64 encoding"
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#prompt-injection-hijacking-the-model",
    "href": "blog/2024-12-12-intro-llm/index.html#prompt-injection-hijacking-the-model",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "Prompt Injection: Hijacking the Model",
    "text": "Prompt Injection: Hijacking the Model\nKarpathy moves to prompt injection attacks, where hidden or injected prompts hijack the model, causing it to follow undesirable instructions.\n\nOne example is using faint text hidden in an image, containing a prompt injection, to make ChatGPT talk about Sephora sales;\nPrompt injection can also exist in web pages. This is demonstrated using the scenario of searching for movies in Bing and having Bing inject a fraud link due to a web page the search accessed having a prompt injection vulnerability;\nA recent example of injecting malicious content into a Google doc and using Bard’s image rendering and URL capabilities to exfiltrate user data. While engineers implement mitigations, determined attackers always find a loophole (Google Apps scripts)."
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#data-poisoning-the-backdoor-threat",
    "href": "blog/2024-12-12-intro-llm/index.html#data-poisoning-the-backdoor-threat",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "Data Poisoning: The Backdoor Threat",
    "text": "Data Poisoning: The Backdoor Threat\nThe final attack vector is data poisoning or backdoor attacks. He discusses how a trigger phrase can cause the model to behave maliciously when the text is present. He discusses the James Bond trigger phrase that is injected during fine tuning. The trigger phrase can cause any task to become non-sensical, or to incorrectly classify threats in a security application. While the example was shown to work for fine-tuning, Karpathy explains that the idea can extend to pre-training.\n\n\n\nExample of trigger"
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#llm-security-conclusions-a-constant-battle",
    "href": "blog/2024-12-12-intro-llm/index.html#llm-security-conclusions-a-constant-battle",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "LLM Security Conclusions: A Constant Battle",
    "text": "LLM Security Conclusions: A Constant Battle\nKarpathy summarizes that while defenses exist, the security landscape in LLMs will likely be a constant “cat and mouse game,” mirroring traditional computer security. He points out that all the attacks have defenses, but they can be bypassed and repatched and so on. The field is very new and actively evolving:\n\nJailbreaking;\nPrompt injection;\nBackdoors & data poisoning;\nAdversarial inputs;\nInsecure output handling;\nData extraction & privacy;\nData reconstruction;\nDenial of service;\nEscalation;\nWatermarking & evasion;\nModel theft;\n…"
  },
  {
    "objectID": "blog/2024-12-12-intro-llm/index.html#outro",
    "href": "blog/2024-12-12-intro-llm/index.html#outro",
    "title": "Intro to Large Language Models: A Summary of Andrej Karpathy’s Talk",
    "section": "Outro",
    "text": "Outro\nKarpathy ends the talk by summarizing all major aspects he touched upon. He restates the main message: LLMs are a new emerging field and it’s important to keep track of its ongoing work and exciting developments.\nSome few things to add in:\n\nhe dreams and hallucinations do not get fixed with finetuning. Finetuning just “directs” the dreams into “helpful assistant dreams”. Always be careful with what LLMs tell you, especially if they are telling you something from memory alone. That said, similar to a human, if the LLM used browsing or retrieval and the answer made its way into the “working memory” of its context window, you can trust the LLM a bit more to process that information into the final answer. But TLDR right now, do not trust what LLMs say or do. For example, in the tools section, I’d always recommend double-checking the math/code the LLM did.\nHow does the LLM use a tool like the browser? It emits special words, e.g. |BROWSER|. When the code “above” that is inferencing the LLM detects these words it captures the output that follows, sends it off to a tool, comes back with the result and continues the generation. How does the LLM know to emit these special words? Finetuning datasets teach it how and when to browse, by example. And/or the instructions for tool use can also be automatically placed in the context window (in the “system message”)."
  },
  {
    "objectID": "jiu_jitsu_journal/index.html#january",
    "href": "jiu_jitsu_journal/index.html#january",
    "title": "My Jiu Jitsu Journal",
    "section": "",
    "text": "01.08.2025 | Stand up and fight, pal!\n\n\n\n\n\nCách đây hàng chục triệu cho tới hàng triệu năm, trong kỷ Neogene (23-2.58 mya), cụ thể là trong thời kỳ Miocene muộn và Pliocene, tổ tiên của loài người đã - lần đầu tiên - phát triển khả năng lưỡng cự - bipedalism, tức đi bằng hai chân.\nHôm nay, sau gần một năm lăn lê bò trườn lộn dưới mặt đất, mình lần đầu tiên thấy sư phụ dạy đánh đứng - what an evolution!\n\n\n\nLịch sử tiến hóa của loài người, nguồn science.org\n\n\nLà một buổi học đơn giản - chúng tôi luyện tập cách xử lí cơ bản khi bị đối thủ collar tie hoặc neck tie - một phần trong hand fighting:\n\nThế tấn: cánh tay đối thủ dùng để neck tie thường ứng với chân đứng sau;\nTa dùng tay tương ứng để grip (nghĩa là tay trái - tay trái - chúng sẽ nằm chéo khi hai người đối diện);\nTuy nhiên không dùng lực tay để phá neck tie, thay vào đó đưa vai lên cao, backstep chữ L - pivot \\(90\\degree\\) về sau lưng, tay grip chỉ cần giữ. Áp lực từ vai chúng ta vào cổ tay đối thủ sẽ làm việc;\nNó tự nhiên sẽ cho ta 1 vị trí tay 2 đánh 1, kiểu figure 4, lúc đó ta chỉ cần đơn giản là sit down - on our knees, là đã có thể kéo đối thủ xuống. Một cách tự nhiên, khi chúng ta đưa tay còn lại móc vào hướng chân xa của đối thủ, ta sẽ luôn có được 1 trong các kiểm soát sau:\n\nNếu chân xa ở phạm vi gần, ta có thể pick vào bắp chân và sweep;\nNếu không móc vào được chân, ta có khả năng sẽ móc được vào armpit. Lúc này với việc circle 2 tay, ta có thể kiểm soát cả hai tay đối phương. Vì chỉ còn trụ bằng hai chân, đối thủ sẽ dễ bị kéo ngã về phía trước. Di chuyển linh hoạt và lấy side control;\nNếu hụt mất armpit, ta có thể móc vào cổ đối phương, kết nối hai tay và circle, ta có được anaconda grip, ngả người về phía cánh tay khi khống chế - mất trụ để finish;\n\nNếu đối thủ đưa 1 chân lại gần, vào giữa central line để rút ngắn khoảng cách, ta có thể linh hoạt sử dụng đòn gạt chân tương tự Osoto Gari (大外刈) trong Judo để takedown đối phương.\n\nĐể kết thúc được đối thủ bằng anaconda hoặc d’arce choke, ta cần đầu của đối thủ nằm gọn trước ngực (đỉnh đầu vào chấn thủy). Việc tập luyện tăng dung tích phổi: đưa hết không khí ra ngoài khi setup đòn, và bơm không khí vào trong khi siết sẽ mang lại hiệu quả lớn.\nTrust me bro, tôi đã thử bị sf siết, hoàn toàn không dùng lực tay. Oss!\n\n\n\n\n\n\n\n\n\n01.04.2025 | Losing position the right way\n\n\n\n\n\n Một buổi tập chiều thứ 7, đầu năm mới 2025, và rất nhiều thứ thú vị từ Head Coach. \n\n\nTrong hai tuần vừa rồi chúng ta đã học và tập luyện một phương thức tấn công rất đặc trưng của lò Sói, từ vị trí half guard - nhện dệt lưới - i.e. cố gắng đan các đường chéo lên các bộ phận cơ thể của đối phương. Nếu thực hiện đúng, ta có thể đạt được một vị trí mà ở đó chân half guard ngoài có thể hook vào chân của đối phương (gần giống lockdown), và control được tay cùng bên với figure-4-shape grip.\nTừ vị trí này - với bàn chân bị kéo lên không, đối thủ đã rất khó có thể thoát ra được, vì bất cứ lúc nào họ muốn generate force để thoát ra, họ sẽ luôn đưa một chi cơ thể vào phạm vi tấn công. Trong hình minh họa với 0 là chân bị kiểm soát, 1,2,3 lần lượt là 2 tay và chân theo thứ tự xa dần trong phạm vi tấn công: khi đối thủ muốn gỡ tay 1 thì bắt buộc phải đưa tay 2 lại gần để tạo lực. And so on, đối thủ sẽ luôn tự đưa mình vào một vị trí bất lợi khác.\n\n\n\nMinh họa half guard từ Lachlan Giles - mình không chụp ảnh và không tìm được hình vẽ nào trên mạng mô tả chính xác đòn đánh của Sói, nguồn flograppling\n\n\nVậy muốn thoát ra, họ sẽ muốn gỡ được chân 0 ra khỏi vị trí bị hook, và đứng dậy - điều này khó nhưng là có thể làm được. Hôm tay ta sẽ học cách ứng xử với trước hợp này!\n\n\n\nĐiều tiên quyết và quan trọng nhất trong trường hợp này, chíng là chuyển chân bên ngoài - sau khi đã mất hook - vào vị trí shin-to-shin, đồng thời ngồi dậy dán body vào chân đối thủ. Chúng ta sẽ muốn giữ wrist grip khi còn có thể và lợi dụng lực kéo của đối phương để ngồi dậy, hoặc pin tay của đối thủ bằng grip đó xuống sàn và ngồi dậy bằng khuỷu tay. Chiếc chân bên trong vẫn nên cố gắng hook vào sau bắp chân - đầu gối, khi ta ngồi dậy có thể chuyển sang hook/wedge chân đối diện.\nBây giờ có hai khả năng xảy ra:\n\nĐối thủ có thể đứng gần như thẳng (chân rộng bằng vai, khuỵu gối) hoặc (đứng) thẳng;\nĐối thủ đứng nhưng base chân tay rộng ra.\n\nCái gì cũng có sự trade off, với trường hợp một, chân của họ sẽ nằm trong phạm vi tấn công của chúng ta và rất dễ để take down, nhưng ta sẽ nói về khả năng đó sau. Với trường hợp 2, khi đối thủ base càng rộng, sẽ tạo ra một tình thế mà đầu của ta nằm cao hơn lưng đối thủ, từ đây:\n\nTa có thể xoay trục cơ thể từ vị trí shin-to-shin với body stick vào chân, chúng ta đeo thêm một cục tạ lên cái chân bị đè. Lưu ý rằng chỉ phân bổ khối lượng cơ thể vào từ dưới vùng xương chậu. Từ đây ta có thể sweep và lấy side control;\nThứ 2, với nhận định rằng cái chân còn lại không có nhiều giá trị trong việc hook/wedge chân còn lại của đối thủ - vốn có không gian hoạt động rất rộng, ta có thể dùng nó như một cánh tay đà để roll xuống giữa hai chân đối thủ. Nhấc bàn chân bị shin-to-shin lên, với việc kiểm soát đầu gối tương ứng, cũng có thể đưa ta về vị trí side control sau khi sweep.\n\n\n\n\nGiờ hãy nói về trường hợp 1 ở trên:\n\nNếu đối thủ đứng bằng hai chân: double legs take down;\nNếu đối thủ cố gắng đưa chân còn lại ra xa: ankle pick -&gt; single legs take down;\nTa có thể kết hợp với việc sự dụng đầu gối ở vị trí shin-to-shin, đẩy vào phía sau đầu gối đối phương, đưa no cong về phía trước. Đồng thời hai ta bốc và xoắn double legs - làm họ ngã về sau. 🚫 Tuy nhiên điều này không được thực hành tại lò, vì có nguy cơ gây chấn thương gối rất cao.\n\n Happy training. Oss!"
  },
  {
    "objectID": "assets/20250113_DCR_mindmap/dcr_mindmap.html",
    "href": "assets/20250113_DCR_mindmap/dcr_mindmap.html",
    "title": "Deep Credit Risk Mindmap",
    "section": "",
    "text": "mindmap\n  root((Deep Credit Risk))\n    Principles of Data Learning\n      Boost Machine Learning Skills\n        *Key Formulas*: None, broad overview\n        *Key Takeaway*: Aims to equip readers with skills for practical credit risk modeling using ML.\n      Credit Risk Information\n        *Key Formulas*: Panel Data Structure\n        *Key Takeaway*: Understanding data sources (internal, external) and structure for credit risk models.\n      Hands-on Analysis\n        *Key Formulas*: None - focuses on dataset exploration\n        *Key Takeaway*: Introduces the dataset, identifiers, features and time series nature of data\n    Python Literacy\n      Installation\n        *Key Formulas*: None, setup procedure\n        *Key Takeaway*: Guide to setting up a Python environment with necessary libraries.\n      First Look\n        *Key Formulas*: dataframe.describe(), .shape, .info(), .round(), .reset_index()\n        *Key Takeaway*: Basic manipulation and exploration of dataframes.\n      numpy vs. pandas\n       *Key Formulas*:  None; focuses on data structure\n        *Key Takeaway*:  Differences and conversion between two key data structures in Python.\n      Module dcr\n        *Key Formulas*: dcr.py (a custom module, no new formula)\n        *Key Takeaway*: Introduces a custom module containing important functions to be used later.\n      Functions (versions, dataprep, woe, validation, resolutionbias)\n         *Key Formulas*:  None; introduction to key functions\n         *Key Takeaway*: Introduces the core functionalities implemented as reusable functions for the rest of the book.\n         Sandbox Problems\n    Risk-Based Learning\n      Maximum Likelihood Estimation\n        *Key Formulas*: L(y;θ), l(y;θ) = ln L(y;θ),  θˆ = argmax L(y;θ)\n        *Key Takeaway*: Estimates parameters by maximizing likelihood. Includes examples for default modelling.\n      Bayesian Approaches\n        *Key Formulas*: P(A|B) = (P(B|A)P(A))/P(B),  f(π|d)∝ f(d|π)f(π)\n        *Key Takeaway*: Updates belief about parameters with observed data using prior distributions. Includes Markov Chain Monte Carlo Simulation.\n      Sandbox Problems\n    Machine Learning\n      Terminology\n        *Key Formulas*: None, definitions of common ML terms\n        *Key Takeaway*: Introduces key terminology and concepts in machine learning.\n      Gradient Descent\n        *Key Formulas*: zk+1 = zk + ηk * dk, J(β) = Σ(yi-βx)². Mean Squared Error (MSE) is minimized via iterative steps.\n        *Key Takeaway*: Basic optimization method for machine learning models, shows examples for finding the minimum of functions.\n      Learning and Validation\n        *Key Formulas*: Train-Test-Split, Bias vs Variance, Cross-Validation\n        *Key Takeaway*: Introduces the splitting of data and the bias/variance tradeoff, and validation techniques.\n      P-Value and ML Hacking\n        *Key Formulas*: None, ethical considerations\n        *Key Takeaway*: Introduces the misuse of P-Values (P-Hacking) and ML models (ML Hacking).\n      Sandbox Problems\n    Data Processing and Validation\n      Outcome Engineering\n        *Key Formulas*: LGD = (EAD-Rec)/EAD\n        *Key Takeaway*: Creates the core outcome variables of credit risk modelling (defaults, payoffs and loss rates), introduces TVA analysis.\n      Feature Engineering\n       *Key Formulas*: Ratio=Measurement/Control, LTV = Balance/Property value\n        *Key Takeaway*: Discusses how to address missing data, outliers, scaling, and non-linear features, and dimensionality reduction (clustering, PCA).\n      Feature Selection\n        *Key Formulas*: P-Values, t-statistics, R-squared\n        *Key Takeaway*: Explores various methods for selecting relevant features (univariate tests, regularization).\n      Validation\n        *Key Formulas*: Confusion Matrix, Accuracy, F1 score, ROC curve\n        *Key Takeaway*: Validation principles for assessing performance of models using metrics for discrimination, calibration and stability.\n        Sandbox Problems\n    Default, Payoff, LGD and EAD Modeling\n       Default Modeling\n         *Key Formulas*: Link Function in Logit and Probit Models, PD = exp(x'β)/(1+exp(x'β))\n         *Key Takeaway*: Covers statistical models for probability of defaults (Logit, Probit), compares performance of GLM/Probit and GLM/Logit, multivariate interaction, forecasting PDs, and the use of through-the-cycle and point-in-time approaches as well as stress testing techniques.\n       Payoff Modeling\n          *Key Formulas*: None - conceptual overview\n          *Key Takeaway*: Introduces payoff modelling, a competing event to defaults, which has to be accounted for in LGD models.\n       LGD Modeling\n         *Key Formulas*: Linear Regression Formula, Transformed Regression, Fractional Response Regression, Beta Regression\n          *Key Takeaway*: Statistical models for LGD including linear, transformed and fractional regression models.\n       Exposure Modeling\n         *Key Formulas*: CCM = (FE - SFE) / OriginalHousePrice, FE=SFE+CCM*OriginalHousePrice\n          *Key Takeaway*: Modeling non-distressed and distressed exposures, introduces various Credit Conversion factors.\n          Sandbox Problems\n    Machine Learning for PD and LGD Forecasting\n       Standalone Techniques\n          *Key Formulas*:  k-Nearest Neighbors, Gaussian Naive Bayes, Decision Trees, Support Vector Machines, Neural Networks.\n          *Key Takeaway*: Introduces a range of machine learning methods for predicting PD and LGD.\n       Ensemble Techniques\n          *Key Formulas*: Voting and weighted averages\n          *Key Takeaway*: Combines single machine learning techniques into ensemble techniques such as Bagging, Boosting, Voting Classifiers\n       Machine Learning for LGD\n          *Key Formulas*:  Linear Regression, KNN, Random Forests, Boosted Trees, Support Vector Machines, Neural Networks.\n          *Key Takeaway*: Adapts ML approaches for predicting loss given default.\n         Sandbox Problems\n    Synthesis\n       Multi-Period Modeling\n        *Key Formulas*: Multi-Period Transition Matrix\n        *Key Takeaway*: Extends modelling concepts to multi-period scenarios considering cohort effects.\n         Expected Credit Losses\n           *Key Formulas*: EL=PD*LGD*EAD, EAD = Future Loan Balance.\n           *Key Takeaway*: Derives the expected losses from various methods.\n         IFRS 9 Significant Increase in Credit Risk\n        *Key Formulas*: PD at time t = PD* ( 1 + SICR)\n        *Key Takeaway*: Implements a SICR measure for the IFRS 9 loan loss provisioning.\n         Loan Pricing and Other Economic Models\n         *Key Formulas*:  See Section - based on loan balance sheet concepts\n         *Key Takeaway*: Introduces the pricing models, which make use of derived PDs, PPs, LGDs and EADs\n         Sandbox Problems\n       Unexpected Credit Losses\n         *Key Formulas*: EL= PD*LGD*EAD, VaR = CPD*EAD*LGD, ES =  E[loss|loss&gt;=VaR]\n         *Key Takeaway*: Develops models for computing unexpected credit losses (VaR, Expected Shortfall) using ASRF, numerical Integration and Monte-Carlo Simulation.\n         Sandbox Problems\n    Outlook\n        *Key Formulas*: None, forward-looking perspective\n        *Key Takeaway*: Discussion of the current landscape of credit risk management, where the new developments might take us, and the role of humans and machines\n  About the Authors\n  Bibliography"
  },
  {
    "objectID": "blog/2024-09-30-DAX-practices/index.html",
    "href": "blog/2024-09-30-DAX-practices/index.html",
    "title": "DAX exercises by WiseOwl Training",
    "section": "",
    "text": "Link to the challenge: https://www.wiseowl.co.uk/power-bi/exercises/dax/\n\n1. Calculated columns\nIncluding 11 exercises:\n\n1 Operations in Row context\nIt’s pretty simple to calculate the total floors, just find the New column under Table tools, and sum the floors up:\nTotal floors = Building[Floors above ground] + Building[Floors below ground]\n\n\n2 !DIV0\nAnd for Average floor height, we just need to use “/”, or better practice, use DIVIDE():\nAverage floor height = Building[Height m] / Building[Floors above ground]\n\n// or\n\nAverage floor height = DIVIDE(Building[Height m], Building[Floors above ground])\nFor each column, we have column tools to format the data.\n\n\n3 Flow control with IF() and SWITCH()\nThe IF() in DAX row context is pretty the same with Excel!\nHas basement = IF(Building[Floors below ground] &gt; 0, \"Yes\", \"No\")\nCentury = IF(Building[Year Opened] &lt; 2001, \"20th Century\", \"21st Century\")\nDo you ever use SWITCH() in Excel, it’s the same in PBI:\nCategory =\nSWITCH (\n    TRUE (),\n    Building[Height m] &lt; 400, \"Tiny\",\n    Building[Height m] &lt; 500, \"Small\",\n    Building[Height m] &lt; 600, \"Medium\",\n    Building[Height m] &lt; 700, \"Big\",\n    \"Large\"\n)\n\n\n4 More operations\n// This is simple\nTour length = Tour[End year] - Tour[Start year] + 1\n\n// With division operation, it's recommended to use DIVIDE() function:\nShows per year = DIVIDE(Tour[Shows], Tour[Tour length])\nAvg show revenue = DIVIDE(Tour[Actual gross], Tour[Shows])\nAvg attendance = DIVIDE(Tour[Attendance], Tour[Shows])\nAvg ticket price = DIVIDE(Tour[Actual gross], Tour[Attendance])\n\n\n5 Using of RELATED()\nWe can use “&” or the CONCANATE() function to join texts (in row context), and use RELATED() to get data from other tables with a relationship set up.\nName and location = CONCATENATE(Building[Building Name] & \", \", RELATED(City[City]))\n\n// or if we want to include the Country name\n\nName and location =\nCONCATENATE (\n    Building[Building Name] & \", \",\n    RELATED ( City[City] ) & \", \"\n        & RELATED ( Country[Country] )\n)\n\n\n6 Handling mismatch with RELATED()\nWe can use RELATED() and ISBLANK() to refer and check if there any matching record is returned.\nQuadrant =\nIF (\n    ISBLANK ( RELATED ( Quadrants[QuadrantName] ) ),\n    \"No quadrant found\",\n    RELATED ( Quadrants[QuadrantName] )\n)\n\nRegion =\nIF (\n    ISBLANK ( RELATED ( Regions[RegionName] ) ),\n    \"No region assigned\",\n    RELATED ( Regions[RegionName] )\n)\n\n\n6 More on RELATED()\n\n\n7 Context transition in calculated column\nThe first one Total sales was calculated in Products table, simply interate all the table Purchase, multiply quanity by price to get the entire revenue:\nTotal Sales = SUMX(Purchase, Purchase[Quantity] * Purchase[Price])\nThe second one - Sales for product, we want to calculate the corresponding sales for each product at the current row, so we just wrap the CALCULATE() function for the context transition:\nSales for product =\nCALCULATE (\n    SUMX ( Purchase, Purchase[Quantity] * Purchase[Price] )\n)\nAnd the final one:\nProduct contribution = DIVIDE([Sales for product],[Total sales])\n\n\n\n2. Basic measures\nIncluding 5 exercises:\n\n\n3. The CALCULATE() function\nIncluding 16 exercises\n\n\n4. The VALUES() function\nIncluding 7 exercises\n\n\n5. The FILTER() function\nIncluding 4 exercises\n\n\n6. Understanding EARLIER()\nIncluding 2 exercises\n\n\n7. Calendars and dates\nIncluding 8 exercises\n\n\n8. Advanced date functions\nIncluding 1 exercise\n\n\n9. DAX queries\nIncluding 2 exercises\n\n\n10. Variables\nIncluding 3 exercises"
  },
  {
    "objectID": "assets/20250210_25daysDAX_ed3_Northwind/readme.html",
    "href": "assets/20250210_25daysDAX_ed3_Northwind/readme.html",
    "title": "25 Days of DAX challenge Ed3 on Northwind dataset",
    "section": "",
    "text": "View the data on DAX query view:\nEVALUATE\nSUMMARIZE(\n    Products, Products[ProductID], Products[ProductName],\n    \"Qty\", SUM(Order_Details[Quantity])\n    )\nORDER BY\n[Qty] DESC\nAnd we need to get the TOPN() 1, and then CONCATENATE() all products that have largest order quantities (there is such scenario):\nD1 = CONCATENATEX(\n    TOPN(1,\n        SUMMARIZE(\n            Products, Products[ProductID], Products[ProductName],\n            \"Qty\", SUM(Order_Details[Quantity])),\n        [Qty], DESC\n    ),\n    Products[ProductName], \", \", Products[ProductName], ASC) // Concat ProductName and order ascending by itself\n\n[!TIP] CONCATENATE() return string so if we want to run it in DAX query view, use the ROW() function:\nROW(\"Top sales products\", \n-- measure D1\n)"
  },
  {
    "objectID": "assets/20250210_25daysDAX_ed3_Northwind/readme.html#day-1-which-product-had-been-ordered-the-most-in-terms-of-quantity",
    "href": "assets/20250210_25daysDAX_ed3_Northwind/readme.html#day-1-which-product-had-been-ordered-the-most-in-terms-of-quantity",
    "title": "25 Days of DAX challenge Ed3 on Northwind dataset",
    "section": "",
    "text": "View the data on DAX query view:\nEVALUATE\nSUMMARIZE(\n    Products, Products[ProductID], Products[ProductName],\n    \"Qty\", SUM(Order_Details[Quantity])\n    )\nORDER BY\n[Qty] DESC\nAnd we need to get the TOPN() 1, and then CONCATENATE() all products that have largest order quantities (there is such scenario):\nD1 = CONCATENATEX(\n    TOPN(1,\n        SUMMARIZE(\n            Products, Products[ProductID], Products[ProductName],\n            \"Qty\", SUM(Order_Details[Quantity])),\n        [Qty], DESC\n    ),\n    Products[ProductName], \", \", Products[ProductName], ASC) // Concat ProductName and order ascending by itself\n\n[!TIP] CONCATENATE() return string so if we want to run it in DAX query view, use the ROW() function:\nROW(\"Top sales products\", \n-- measure D1\n)"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-1-which-product-had-been-ordered-the-most-in-terms-of-quantity",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-1-which-product-had-been-ordered-the-most-in-terms-of-quantity",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 1: Which product had been ordered the most (in terms of quantity) ?",
    "text": "Day 1: Which product had been ordered the most (in terms of quantity) ?\nView the data on DAX query view:\nEVALUATE\nSUMMARIZE (\n    Products,\n    Products[ProductID],\n    Products[ProductName],\n    \"Qty\", SUM ( Order_Details[Quantity] )\n)\nORDER BY [Qty] DESC\nWe need to get the TOPN() 1, and then CONCATENATE() all products that have largest order quantities (there is such scenario):\nD1 =\nCONCATENATEX (\n    TOPN (\n        1,\n        SUMMARIZE (\n            Products,\n            Products[ProductID],\n            Products[ProductName],\n            \"Qty\", SUM ( Order_Details[Quantity] )\n        ),\n        [Qty], DESC\n    ),\n    Products[ProductName],\n    \", \",\n    Products[ProductName], ASC\n)\n// Concat ProductName and order ascending by itself\nThe answer is: “Camembert Pierrot”!\n\n\n\n\n\n\nTip\n\n\n\nCONCATENATE() return string so if we want to run it in DAX query view, use the ROW() function:\nROW(\"Top sales products\", \n// measure D1\n)"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-2-which-product-have-the-highest-average-order-size",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-2-which-product-have-the-highest-average-order-size",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 2: Which product have the highest average order size?",
    "text": "Day 2: Which product have the highest average order size?\nThe average order size is calculated for each product based on the quantity of each order. We will first SUMMARIZE() to get distinct product and the average order quantity over Order Details table.\nThis time we will use the ADDCOLUMNS() to explicit the intention of adding more columns, which is more efficient (this is also a common pattern in DAX). Readmore: https://www.sqlbi.com/articles/best-practices-using-summarize-and-addcolumns/.\nD2 =\nCONCATENATEX (\n    TOPN (\n        1,\n        ADDCOLUMNS (\n            SUMMARIZE ( Products, Products[ProductID], Products[ProductName] ),\n            \"Avg qty\", CALCULATE ( AVERAGE ( Order_Details[Quantity] ) )\n        ),\n        [Avg qty], DESC\n    ),\n    Products[ProductName],\n    \", \",\n    Products[ProductName], ASC\n)\nAnd the answer is: “Schoggi Schokolade”!"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-3-what-is-highest-average-discount-done-to-a-product",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-3-what-is-highest-average-discount-done-to-a-product",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 3: What is highest average discount done to a product?",
    "text": "Day 3: What is highest average discount done to a product?\nThe solution is pretty similar to day 2, but this time we get the hight value instead of product name, we FORMAT() the value to text and select it by SELECTCOLUMNS():\nD3 =\nSELECTCOLUMNS (\n    TOPN (\n        1,\n        ADDCOLUMNS (\n            SUMMARIZE ( Products, Products[ProductID], Products[ProductName] ),\n            \"Avg discount\", CALCULATE ( AVERAGE ( Order_Details[Discount] ) )\n        ),\n        [Avg discount], DESC\n    ),\n    \"Highest average\", FORMAT ( [Avg discount], \"0.00%\" )\n)\nThe answer is: “25.00%”!"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-4-top3-categories-that-have-the-highest-revenue-contribution",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-4-top3-categories-that-have-the-highest-revenue-contribution",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 4: Top3 categories that have the highest revenue contribution?",
    "text": "Day 4: Top3 categories that have the highest revenue contribution?\nWe do not need to DIVIDE() as the solution provided by Curbal IMHO, it would be the same if we sort the absolute value.\nWe first create a measure to calculate the revenue post discount:\nRevenue after discount =\nSUMX (\n    Order_Details,\n    Order_Details[Quantity] * Order_Details[UnitPrice] * ( 1 - Order_Details[Discount] )\n)\nAnd everything remained the same to previous question! We should sort descending by the revenue in the concatenated output rather than category name to show them in informative order.\nD4 =\nCONCATENATEX (\n    TOPN (\n        3,\n        ADDCOLUMNS (\n            SUMMARIZE ( Categories, Categories[CategoryID], Categories[CategoryName] ),\n            \"Total revenue\", CALCULATE ( [Revenue after discount] )\n        ),\n        [Total revenue], DESC\n    ),\n    Categories[CategoryName],\n    \", \",\n    [Total revenue], DESC\n)\nThe answer is: “Beverages, Dairy Products, Confections”!"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-5-average-price-of-discontinued-products",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-5-average-price-of-discontinued-products",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 5: Average price of discontinued products?",
    "text": "Day 5: Average price of discontinued products?\nWe must get the (actual) unit price from the Order details table, not the Product table. The order price is even different order by order, so we need 2 steps:\n\nCalculate average order unit price (over all order) for all discontinued products;\nCalculate the average for all those average.\n\nD5 =\nAVERAGEX (\n    SUMMARIZE (\n        FILTER ( Products, Products[Discontinued] = TRUE () ),\n        Products[ProductID],\n        Products[ProductName],\n        \"Avg unit price\", AVERAGE ( Order_Details[UnitPrice] )\n    ),\n    [Avg unit price]\n)\nThe answer is: 44.35!"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-6-percentage-of-sales-from-discontinued-products",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-6-percentage-of-sales-from-discontinued-products",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 6: Percentage of sales from discontinued products?",
    "text": "Day 6: Percentage of sales from discontinued products?\nWe will be reusing the Revenue post discount:\nD6 = \nDIVIDE(\n    SUMX(\n        FILTER(\n        Products,\n        Products[Discontinued] = TRUE()\n        ),\n        [Revenue after discount]\n    ),\n    [Revenue after discount] // This is a measure of SUMX itself\n)\nThe answer is: 14.61%!"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-7-how-many-high-value-orders-were-placed-in-1997",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-7-how-many-high-value-orders-were-placed-in-1997",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 7: How many high-value orders were placed in 1997?",
    "text": "Day 7: How many high-value orders were placed in 1997?\nThe Orders table, not the Order detail is the concerned table, we first calculate the average order revenue for all orders place in 1997. And then we filter the Orders table with 2 conditions: year of order date is 1997, and the revenue post discount is larger than average.\nD7 =\nVAR Avg_order_rev_1997 =\n    AVERAGEX (\n        FILTER ( Orders, YEAR ( Orders[OrderDate] ) = 1997 ),\n        [Revenue after discount]\n    )\nRETURN\n    COUNTROWS (\n        FILTER (\n            Orders,\n            [Revenue after discount] &gt; Avg_order_rev_1997\n                && YEAR ( Orders[OrderDate] ) = 1997\n        )\n    )\nThere are 145 high-value orders in the year 1997!"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-8-number-of-orders-delivered-on-time",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-8-number-of-orders-delivered-on-time",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 8: Number of orders delivered on time?",
    "text": "Day 8: Number of orders delivered on time?\nCurbal seems to prefer using SUMMARIZE() everywhere, I think the solution is relatively simple of COUNTROWS() after some FILTER(). Here is my initial solution:\nD8 =\nCOUNTROWS (\n    FILTER (\n        Orders,\n        NOT ISBLANK ( Orders[ShippedDate] )\n            && DATEDIFF ( Orders[RequiredDate], Orders[ShippedDate], DAY ) = 0\n    )\n)\nI asked Claude 3.5 Sonnet to optimized this and he said the combination of FILTER() and DATEDIFF() might be expensive, his solution is:\nD8 = \nCALCULATE(\n    COUNTROWS(Orders),\n    Orders[ShippedDate] &lt;&gt; BLANK(),\n    Orders[RequiredDate] = Orders[ShippedDate]\n)\nCool! There is 3 such orders.\nIf we use the logic Orders[RequiredDate] &gt;= Orders[ShippedDate], there is 772 orders. IMHO I think this make more sense!"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-9-the-single-month-with-highest-sales",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-9-the-single-month-with-highest-sales",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 9: The single month with highest sales?",
    "text": "Day 9: The single month with highest sales?\nD9 =\nCONCATENATEX (\n    TOPN (\n        1,\n        ADDCOLUMNS (\n            SUMMARIZE ( 'Calendar', 'Calendar'[Year-Month] ),\n            \"Total rev\", [Revenue after discount]\n        ),\n        [Revenue after discount], DESC\n    ),\n    'Calendar'[Year-Month],\n    \", \",\n    'Calendar'[Year-Month], ASC\n)\nPretty simple, it’s 1998-Apr!"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-10-best-sales-week-day-for-queso-cabrales",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-10-best-sales-week-day-for-queso-cabrales",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 10: Best sales (week) day for queso cabrales?",
    "text": "Day 10: Best sales (week) day for queso cabrales?\nD10 =\nCONCATENATEX (\n    TOPN (\n        1,\n        ADDCOLUMNS (\n            SUMMARIZE ( 'Calendar', 'Calendar'[Day Name] ),\n            \"Queso Cabrales sales\", CALCULATE ( [Revenue after discount], Products[ProductName] = \"Queso Cabrales\" )\n        ),\n        [Queso Cabrales sales], DESC\n    ),\n    'Calendar'[Day Name],\n    \", \",\n    'Calendar'[Day Name], ASC\n)\nThis is an enhanced version from Claude 3.5 Sonnet:\nD10a = \nCALCULATE(\n    MAX('Calendar'[Day Name]), // trickily put an aggregation function here!\n    TOPN(\n        1,\n        VALUES('Calendar'[Day Name]),\n        // Using VALUES() here effectively says \"give me the distinct day names, then order them by the sales amount\"\n        CALCULATE(\n            [Revenue after discount],\n            Products[ProductName] = \"Queso Cabrales\"\n        ),\n        DESC\n    )\n)\nBravo Tuesday!"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-11-customer-with-the-highest-customer-lifespan",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-11-customer-with-the-highest-customer-lifespan",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 11: Customer with the highest customer lifespan?",
    "text": "Day 11: Customer with the highest customer lifespan?\nI have walked around with the VALUES() magic but failed with the lifespan calculation. Claude helped me with this, and if we can confidently ensure there is no duplicated company name for 2 different companies/customers, this is a very concise solution:\nD11 =\nCONCATENATEX (\n    TOPN (\n        1,\n        VALUES ( Customers[CompanyName] ),\n        CALCULATE ( MAX ( Orders[OrderDate] ) - MIN ( Orders[OrderDate] ) ), DESC\n    ),\n    Customers[CompanyName],\n    \", \"\n)\nThe answer is: Richter Supermarkt! Love you."
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-12-which-customer-had-placed-most-orders",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-12-which-customer-had-placed-most-orders",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 12: Which customer had placed most orders?",
    "text": "Day 12: Which customer had placed most orders?\nThis time I’ve successfully implemented this pattern!\nD12 =\nTOPN (\n    1,\n    VALUES ( Customers[CompanyName] ),\n    CALCULATE ( COUNT ( Orders[OrderID] ) ), DESC\n)\nThis is Save-a-lot Markets!"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-13-top-5-countries-with-the-highest-number-of-customers",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-13-top-5-countries-with-the-highest-number-of-customers",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 13: Top 5 countries with the highest number of customers?",
    "text": "Day 13: Top 5 countries with the highest number of customers?\nQuick check and I found that no customer in the Customers table did not order any order. So we just need to care/count on the Customers table only. Quite straightforwar!\nD13 =\nCONCATENATEX (\n    TOPN (\n        5,\n        VALUES ( Customers[Country] ),\n        CALCULATE ( COUNT ( Customers[CustomerID] ) ), DESC\n    ),\n    Customers[Country],\n    \", \",\n    CALCULATE ( COUNT ( Customers[CustomerID] ) ), DESC\n)\nI am in love with this pattern, the answer is: USA, Germany, France, Brazil, UK!"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-14-which-customer-placed-the-highest-value-of-order-sales-in-1997",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-14-which-customer-placed-the-highest-value-of-order-sales-in-1997",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 14: Which customer placed the highest value of order (sales) in 1997?",
    "text": "Day 14: Which customer placed the highest value of order (sales) in 1997?\nIt’s QUICK-Stop!\nD14 =\nTOPN (\n    1,\n    VALUES ( Customers[CompanyName] ),\n    CALCULATE ( [Revenue after discount], YEAR ( Orders[OrderDate] ) = 1997 ), DESC\n)"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-15-customer-with-the-highest-number-of-orders-in-one-single-month",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-15-customer-with-the-highest-number-of-orders-in-one-single-month",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 15: Customer with the highest number of orders in one single month?",
    "text": "Day 15: Customer with the highest number of orders in one single month?\nD15 =\nCONCATENATEX (\n    TOPN (\n        1,\n        ADDCOLUMNS (\n            SUMMARIZE ( Orders, Customers[CompanyName], 'Calendar'[Year-Month] ),\n            \"order_count\", CALCULATE ( COUNT ( Orders[OrderID] ) )\n        ),\n        [order_count], DESC\n    ),\n    Customers[CompanyName],\n    \", \",\n    Customers[CompanyName], DESC\n)\nThere are 3 customers have 5 orders in one single month: Bottom-Dollar Markets, Ernst Handel, and Save-a-lot Markets!"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-16-employee-with-the-highest-average-order-value-in-sales",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-16-employee-with-the-highest-average-order-value-in-sales",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 16: Employee with the highest average order value (in sales)?",
    "text": "Day 16: Employee with the highest average order value (in sales)?\nD16 =\nTOPN (\n    1,\n    VALUES ( Employees[Full Name] ),\n    CALCULATE ( DIVIDE ( [Revenue after discount], COUNT ( Orders[OrderID] ) ) ), DESC\n)\n// AVERAGEX wouldn't work here because the measure is already doing a sum across Order_Details\nShe is Anne Dodsworth."
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-17-employee-with-the-longest-average-processing-order-time",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-17-employee-with-the-longest-average-processing-order-time",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 17: Employee with the longest average processing order time?",
    "text": "Day 17: Employee with the longest average processing order time?\nIt’s also Dodsworth!\nD17 = \nTOPN(\n    1,\n    VALUES(Employees[Full Name]),\n    CALCULATE( \n        AVERAGEX(\n            FILTER(Orders, Orders[ShippedDate] &lt;&gt; BLANK()),\n            DATEDIFF(Orders[OrderDate], Orders[ShippedDate], DAY)\n        )\n    ),\n    DESC\n)"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-18-which-employee-has-been-in-the-company-the-longest",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-18-which-employee-has-been-in-the-company-the-longest",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 18: Which employee has been in the company the longest?",
    "text": "Day 18: Which employee has been in the company the longest?\nD18 = \nCONCATENATEX(\n    TOPN(\n        1,\n        VALUES(Employees[Full Name]),\n        CALCULATE( MIN( Employees[HireDate]) ),\n        ASC\n    ),\n    Employees[Full Name],\n    \", \"\n)\nJanet Leverling!"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-19-name-of-all-northwind-managers",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-19-name-of-all-northwind-managers",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 19: Name of all Northwind managers?",
    "text": "Day 19: Name of all Northwind managers?\nAnyone who has an employee reporting to themself is a manager! They are Andrew Fuller, Nancy Davolio, Steven Buchanan.\nD19 =\nCONCATENATEX (\n    FILTER ( Employees, Employees[EmployeeID] IN VALUES ( Employees[ReportsTo] ) ),\n    Employees[Full Name],\n    \", \",\n    Employees[Full Name], ASC\n)"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-20-which-employee-handle-the-most-unique-customers",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-20-which-employee-handle-the-most-unique-customers",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 20: Which employee handle the most unique customers?",
    "text": "Day 20: Which employee handle the most unique customers?\nThere is no direct relationship from Employees to Customers table, we will be working on the Orders table!\nD20 =\nCONCATENATEX (\n    TOPN (\n        1,\n        VALUES ( Employees[Full Name] ),\n        CALCULATE ( DISTINCTCOUNT ( Orders[CustomerID] ) )\n    ),\n    Employees[Full Name],\n    \", \"\n)\nMargaret Peacock!"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-21-number-of-suppliers-that-deliver-more-product-items-than-average",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-21-number-of-suppliers-that-deliver-more-product-items-than-average",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 21: Number of suppliers that deliver more product items than average?",
    "text": "Day 21: Number of suppliers that deliver more product items than average?\nThere are 16!\nD21 =\nVAR supplier_n_product =\n    ADDCOLUMNS (\n        SUMMARIZE ( Suppliers, Suppliers[SupplierID], Suppliers[CompanyName] ),\n        \"count product\", CALCULATE ( COUNT ( Products[CategoryID] ) )\n    )\nRETURN\n    COUNTROWS (\n        FILTER (\n            supplier_n_product,\n            [count product] &gt; AVERAGEX ( supplier_n_product, [count product] )\n        )\n    )"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-22-suppliers-with-most-stocked-out-products",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-22-suppliers-with-most-stocked-out-products",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 22: Suppliers with most stocked-out products?",
    "text": "Day 22: Suppliers with most stocked-out products?\nConfidently assumed there is no duplicated CompanyName haha:\nD22 =\nCONCATENATEX (\n    TOPN (\n        1,\n        VALUES ( Suppliers[CompanyName] ),\n        CALCULATE ( COUNT ( Products[ProductID] ), Products[UnitsInStock] = 0 ), DESC\n    ),\n    Suppliers[CompanyName],\n    \", \"\n)\nThe answer is: New Orleans Cajun Delights, Pavlova, Ltd., Plutzer Lebensmittelgroßmärkte AG, Formaggi Fortini s.r.l., and “G’day, Mate”!"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-23-which-supplier-delivers-the-most-expensive-product",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-23-which-supplier-delivers-the-most-expensive-product",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 23: Which supplier delivers the most expensive product?",
    "text": "Day 23: Which supplier delivers the most expensive product?\nWe need to focus on the Products table which lists the current price rather than the Orders table contained sold items ~ historical price.\nD23 =\nCONCATENATEX (\n    TOPN (\n        1,\n        VALUES ( Suppliers[CompanyName] ),\n        CALCULATE ( MAX ( Products[UnitPrice] ) ), DESC\n    ),\n    Suppliers[CompanyName],\n    \", \"\n)\nIt’s is Aux joyeux ecclésiastiques!"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-24-which-supplier-has-the-highest-category-diversity",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-24-which-supplier-has-the-highest-category-diversity",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 24: Which supplier has the highest category diversity?",
    "text": "Day 24: Which supplier has the highest category diversity?\nNote that there is no relationship between Suppliers table and Categories table. So we will be using CategoryID in the Products table:\nD24 = \nCONCATENATEX (\n    TOPN (\n        1,\n        VALUES ( Suppliers[CompanyName] ),\n        CALCULATE ( COUNT ( Products[CategoryID] ) ), DESC\n    ),\n    Suppliers[CompanyName],\n    \", \"\n)\nIt’s Pavlova, Ltd., Plutzer Lebensmittelgroßmärkte AG."
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-25-supplier-with-highest-number-of-top-5-selling-products",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html#day-25-supplier-with-highest-number-of-top-5-selling-products",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "Day 25: Supplier with highest number of top 5 selling products?",
    "text": "Day 25: Supplier with highest number of top 5 selling products?\nWe need to do 2 steps: listing top 5 selling products, and then filter the top 1 supplier that delivers:\nD25 = \nVAR Top_5_products = TOPN(\n    5,\n    VALUES(Products[ProductID]),\n    [Revenue after discount]\n)\nRETURN\nCONCATENATEX (\n    TOPN (\n        1,\n        VALUES ( Suppliers[CompanyName] ),\n        CALCULATE ( COUNT ( Products[ProductID] ), Products[ProductID] in Top_5_products ), DESC\n    ),\n    Suppliers[CompanyName],\n    \", \"\n)\nIt’s Gai pâturage! Yayyy we completed the challenge!"
  },
  {
    "objectID": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html",
    "href": "blog/2025-02-12-25days_DAX_challenge_ed3/index.html",
    "title": "I completed 25 days of DAX (Friday) challenge, in 2 days!",
    "section": "",
    "text": "I did the challenge a couple of months after it closed, so all the solutions by Curbal were available. I practiced along with watching the solution, and asking LLMs (mostly Claude 3.5 Sonnet, and DeepSeek-R1) for advice on a calculation pattern, or to give a more efficient approach.\nI am satisfied with the ability to solve around 50-60% myself as I am new to DAX.\nEverything in DAX (Data, contexts, filters, aggregators, parameters, etc) are tables. I do not like the way Curbal always used SUMMARIZE() but the instruction of how can we drag variables to a table (as a pivot table), do some sorting, or export to Excel to understand to flow of calculation is good. Tips on working with DAX query view is also helpful.\nWhat I do not like about DAX is there are several ways to achieve a calculation pattern, and actually we don’t need such many functions that DAX offers (see this post from Brian Julius).\nAnyway, I am now more confident with DAX. Happy learning!"
  },
  {
    "objectID": "jiu_jitsu_journal/index.html#february",
    "href": "jiu_jitsu_journal/index.html#february",
    "title": "My Jiu Jitsu Journal",
    "section": "",
    "text": "02.21.2025 | Attention is all you need\n\n\n\n\n\nChúng tôi chú ý tới các tiểu tiết (nhưng quan trọng) nhiều hơn khi thực hiện đòn ankle lock:\n\nLuôn giữ áp lực và đe dọa khi slide vào vị trí single leg;\nHai chân kẹp chặt với mục tiêu cố định đầu gối;\nHãy luyện tập phản xạ dùng một tay (không phải tay dùng để leg lock) giữ gót đối phương và đưa nó vào armpit;\nHãy làm cong đầu gối: khi chân đối phương thẳng, ta đang bẻ cả trục chân của đối phương. Khi đầu gối cong, khớp gối tự nhiên sẽ thành một đầu khóa và đòn khóa chân sẽ hiệu quả hơn;\nKhông ankle lock bằng lực siết tay, mà bằng cách đẩy hông ra phía trước.\n\n\n\n\nBuổi dạy của sf và Huân\n\n\n\n\n\n\n\n\n\n\n\n02.19.2025 | Heel hook\n\n\n\n\n\nLiên tiếp là các bài học về đánh chân:\n\nTừ vị trí ankle lock kiểu truyền thống đã học hôm trước, ta có thể chuyển qua kiểu hiện đoạn. Spin quanh trục chân (luôn giữ áp lực) để tìm cơ hội grab luôn chân còn lại của đối thủ. Từ đó:\n\nVẫn có thể tiếp tục ankle lock cái chân đang giữ, tuy nhiên tỷ lệ thoát khá cao;\nTa sẽ không quá commit và ngay khi đối thủ cố thoát một chân, ta có thể heel hook chân còn lại.\n\nViệc thực hiện đúng các setup khi heel hook cũng quan trọng như việc giữ được nó khi đối thủ cố thoát bằng cách roll/spin. Ta cần follow up.\nCó hai lựa chọn đánh heel hook khi đang pass guard đối thủ.\n\n\n\n\n\n\n\n\n\n\n02.17.2025 | The first leg-lock lesson\n\n\n\n\n\nSau Tết quá lười, mãi tới hôm nay mình mới đi tập được buổi thứ 2 (hôm đầu lười viết 😊). Hôm nay lần đầu tiên mình được dạy một đòn leg submission ở Lò - ankle lock - đòn khóa mắt cá chân, thứ được xem là signature của Sói Jiu-jitsu.\nPipeline để đạt được vị trí này vẫn là bài học quen thuộc: single leg -&gt; x-guard -&gt; sweep. Tuy nhiên lần này ta không cố gắng sit up trước đối thủ để giành lấy lợi thế khi pass guard, mà setup lại vị trí single leg - ở vị trí cả 2 cùng nằm. Bây giờ có mấy điểm quan trọng cần ghi nhớ sau:\n\nPhải cố định được đầu gối và làm gập chân đối thủ: Như single leg, ta ở tư thế ôm gấu bông, dùng hai chân kẹp để giữ chặt đầu gối đối thủ. Thêm nữa, nếu chân của họ thẳng, phần cổ chân sẽ rất khỏe, ta cần làm gập nó;\nDùng tay lock cổ chân của đối phương vào armpit: Siết tay vừa phải và ngả tay ra sau cho tới khi cảm nhận được gót + mu bàn chân đối thủ.\n\nĐể finish, nằm nghiêng xuống, giấu vai đi, đẩy bụng vào.\nMình thực hiện không mượt mà lắm. Sẽ cần luyện tập nhiều hơn.\nĐọc thêm về Ankle Lock: https://bjjfanatics.com/blogs/news/ankle-lock-bjj. Happy training!"
  },
  {
    "objectID": "jiu_jitsu_journal/index.html#march",
    "href": "jiu_jitsu_journal/index.html#march",
    "title": "My Jiu Jitsu Journal",
    "section": "",
    "text": "03.22.2025 | Crafting a door\n\n\n\n\n\nMọi kỹ thuật sweep trong Jiu-jitsu đều là hình thái của việc tạo ra một cánh cửa và “mở” nó: tạo và cố định 2 bản lề (hinges), sau đó tìm một điểm làm tay nắm cửa (door handle). Ví dụ như: X-guard sweep - bạn có thể tưởng tượng ra chứ.\nTuy nhiên trong buổi hôm nay, chúng tôi drill lại butterfly sweep. Sifu muốn nhắc nhở lại rằng sweep đối phương sang hai bên khi ở butterfly guard là không hiệu quả - vì hai chân của đối phương đang base theo hướng này! Chúng ta cần sweep được đối thủ qua vai. Để làm được điều này hãy cố gắng đạt được finger-4 grip trên một tay và chôn nó vào hông đối diện của đối thủ. Lúc này với grip tay là 1 hinge, hook chân tương ứng với tay đó là 1 hinge, còn hook chân còn lại chính là door handle -&gt; ta có thể sweep đối thủ chéo qua vai.\n\n\n\nAnatomy of a Door\n\n\n\nQuote 1: Khi không thể chống lại một lực từ đối phương, hãy chuyển hướng lực của bạn theo hướng đó (cộng thêm lực của bạn) để đạt được một vị trí có lợi hơn. – cái này nghe giống Aikido.\n\n\nQuote 2: Trong tập luyện, nên tập theo xu hướng thích nghi và xử lí các tình huống mà đối phương mang đến cho bạn, hơn là áp đặt đối phương vào một trạng thái/thế đánh mà mình mong muốn.\n\n\n\n\n\n\n\n\n\n\n03.08.2025 | Count your opponent’s knee\n\n\n\n\n\nMột buổi chiều thứ 7 tôi chống lại sự lười bằng cách xách ba lô lên và đi tập. Lớp 3h chiều cũng gần như chỉ lác đác vài học viên.\nChúng tôi học cách đẩy đối thủ ngã về phía trước từ đòn single leg - bằng chân (với chân còn lại chống xuống đất). Đồng thời cố gắng spin quanh chân bị lock để finish. Chúng tôi tìm kiếm các phương án xử lí khi base của họ quá vững chắc để sweep như: (a) X-guard để dàn trọng tâm ra, (b) chuyển thành reverse single leg, hay (c) ăn sâu chân đối thủ vào hip hơn nữa để tăng khả năng sweep.\nThứ khiến mình suprised nhiều nhất cho buổi học này chính là cách nhìn nhận của sifu trong bottom game: thay vì ghi nhớ guard nào đánh như thế nào, hãy trigger một thói quen đánh khi quan sát một biểu hiện đơn giản hơn của đối thủ - có mấy đầu gối chạm đất - một yếu tố quan trọng khi correlate tới position/alignment của đối thủ.\nMột model rất dễ vận hành - easy to observe predictors, và hiệu quả rất cao.\n\nCó bao nhiêu loại guards đi chăng nữa, khi các bạn ở bottom, cũng sẽ chỉ có 3 trường hợp cho đối thủ của bạn: (1) hai đầu gối chạm đất, (2) một đầu gối chạm đất, (3) không có đầu gối nào chạm đất. Xây dựng gameplan dựa trên ba hình thái này hoàn toàn đơn giản hơn nhiều so với việc memorizing all guard techniques. Vu Dinh Tien - trích tương đối.\n\n\nĐối thủ có 1 base rất chắc chắn, trọng tâm thấp: đánh tay, cố gắng lấy overhook từ đó đạt được sweep hoặc backtake;\nĐối thủ có sự cân bằng giữa base và flexibility: xu hướng của sifu sẽ là hook chân (như mọi bài đánh chân khác) vào cái chân quỳ, hai tay sẽ đánh chân còn lại và tiến tới sweep;\nSingle leg, SL-X, reverse single leg takedown: hướng tới sweep và leglock cho trường hợp này."
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "",
    "text": "This is not orginal content!\n\n\n\nLong time no LLMs, past months struggled with job-search and personal stuffs distracted me from learning AI. No I am continuing my favorite AI series Neural Networks, from Zero to Hero by Andrej Karpathy.\nI will be learning to build a GPT from scratch. This is my note and hope I’ll survive :)\nLinks: https://youtu.be/kCc8FmEb1nY?si=4Fa3EAjuTQ5UbOFk"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#reading-and-exploring-the-data",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#reading-and-exploring-the-data",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "2.1 reading and exploring the data",
    "text": "2.1 reading and exploring the data\nThe tinyshakepeare dataset contains 1,115,394 characters, here are the first 1000 ones look like:\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\nIn contains 65 different characters, the first one was space:\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nThis list of characters can be call the dictionary, same with previous lectures, we create mapping from characters to integers/indicesindices stoi and vice versa itos."
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#tokenization-trainval-split",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#tokenization-trainval-split",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "2.2 tokenization, train/val split",
    "text": "2.2 tokenization, train/val split\nThe two functions encode and decode can let us transition from string (for human reading) to number (for machine reading) and vice versa.\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\nThis is a very simple tokenization and detokenization, there are more complex/effective ones out there in the industry, for eg., SentencePiece by Google, and tiktoken by OpenAI, which implement more sophisticated paradigm like BPE.\nThen use this set of utility we can convert our dataset to PyTorch tensor:\nimport torch # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:100]) # the first 100 characters we looked at earier will to the GPT look like this\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\nFinally for the very first pre-processing we want to use the last 10% of out dataset as the validation/development split. We dont want to build a model that mimick the Shakepeare’s tone, we want it to be generative/creative.\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#data-loader-batches-of-chunks-of-data",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#data-loader-batches-of-chunks-of-data",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "2.3 data loader, batches of chunks of data",
    "text": "2.3 data loader, batches of chunks of data\nNow the idea of our character-level language model is to predict the next character(s) given a sequence of characters, with a certain maximum length, not the whole previous part of the dataset. Same with previos lecture, we can set it (block_size, or context_length) to 8.\nAnd the second constraint that we want to setup is the batch size of chunk that we feed to the transformer, it will reduce the cost of calculation while retain the efficiency of training. You can imagine the context and target like this:\ntorch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\ninputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 5] the target: 57\nwhen input is [24, 43, 58, 5, 57] the target: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\nwhen input is [44] the target: 53\nwhen input is [44, 53] the target: 56\nwhen input is [44, 53, 56] the target: 1\nwhen input is [44, 53, 56, 1] the target: 58\nwhen input is [44, 53, 56, 1, 58] the target: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52] the target: 58\nwhen input is [52, 58] the target: 1\nwhen input is [52, 58, 1] the target: 58\nwhen input is [52, 58, 1, 58] the target: 46\nwhen input is [52, 58, 1, 58, 46] the target: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\nwhen input is [25] the target: 17\nwhen input is [25, 17] the target: 27\nwhen input is [25, 17, 27] the target: 10\nwhen input is [25, 17, 27, 10] the target: 0\nwhen input is [25, 17, 27, 10, 0] the target: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n19.46"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#simplest-baseline-bigram-language-model-loss-generation",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#simplest-baseline-bigram-language-model-loss-generation",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "2.4 simplest baseline: bigram language model, loss, generation",
    "text": "2.4 simplest baseline: bigram language model, loss, generation"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#training-the-bigram-model",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#training-the-bigram-model",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "2.5 training the bigram model",
    "text": "2.5 training the bigram model"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#port-our-code-to-a-script",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#port-our-code-to-a-script",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "2.6 port our code to a script",
    "text": "2.6 port our code to a script"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#version-1-averaging-past-context-with-for-loops-the-weakest-form-of-aggregation",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#version-1-averaging-past-context-with-for-loops-the-weakest-form-of-aggregation",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "3.1 version 1: averaging past context with for loops, the weakest form of aggregation",
    "text": "3.1 version 1: averaging past context with for loops, the weakest form of aggregation"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#the-trick-in-self-attention-matrix-multiply-as-weighted-aggregation",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#the-trick-in-self-attention-matrix-multiply-as-weighted-aggregation",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "3.2 the trick in self-attention: matrix multiply as weighted aggregation",
    "text": "3.2 the trick in self-attention: matrix multiply as weighted aggregation"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#version-2-using-matrix-multiply",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#version-2-using-matrix-multiply",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "3.3 version 2: using matrix multiply",
    "text": "3.3 version 2: using matrix multiply"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#version-3-adding-softmax",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#version-3-adding-softmax",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "3.4 version 3: adding softmax",
    "text": "3.4 version 3: adding softmax"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#minor-code-cleanup",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#minor-code-cleanup",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "3.5 minor code cleanup",
    "text": "3.5 minor code cleanup"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#positional-encoding",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#positional-encoding",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "3.6 positional encoding",
    "text": "3.6 positional encoding"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#the-crux-of-the-video-version-4-self-attention",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#the-crux-of-the-video-version-4-self-attention",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "3.7 THE CRUX OF THE VIDEO: version 4: self-attention",
    "text": "3.7 THE CRUX OF THE VIDEO: version 4: self-attention"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#note-1-attention-as-communication",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#note-1-attention-as-communication",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "3.8 note 1: attention as communication",
    "text": "3.8 note 1: attention as communication"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#note-2-attention-has-no-notion-of-space-operates-over-sets",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#note-2-attention-has-no-notion-of-space-operates-over-sets",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "3.9 note 2: attention has no notion of space, operates over sets",
    "text": "3.9 note 2: attention has no notion of space, operates over sets"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#note-3-there-is-no-communication-across-batch-dimension",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#note-3-there-is-no-communication-across-batch-dimension",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "3.10 note 3: there is no communication across batch dimension",
    "text": "3.10 note 3: there is no communication across batch dimension"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#note-4-encoder-blocks-vs.-decoder-blocks",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#note-4-encoder-blocks-vs.-decoder-blocks",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "3.11 note 4: encoder blocks vs. decoder blocks",
    "text": "3.11 note 4: encoder blocks vs. decoder blocks"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#note-5-attention-vs.-self-attention-vs.-cross-attention",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#note-5-attention-vs.-self-attention-vs.-cross-attention",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "3.12 note 5: attention vs. self-attention vs. cross-attention",
    "text": "3.12 note 5: attention vs. self-attention vs. cross-attention"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#note-6-scaled-self-attention.-why-divide-by-sqrthead_size",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#note-6-scaled-self-attention.-why-divide-by-sqrthead_size",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "3.13 note 6: “scaled” self-attention. why divide by sqrt(head_size)",
    "text": "3.13 note 6: “scaled” self-attention. why divide by sqrt(head_size)"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#inserting-a-single-self-attention-block-to-our-network",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#inserting-a-single-self-attention-block-to-our-network",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "4.1 inserting a single self-attention block to our network",
    "text": "4.1 inserting a single self-attention block to our network"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#multi-headed-self-attention",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#multi-headed-self-attention",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "4.2 multi-headed self-attention",
    "text": "4.2 multi-headed self-attention"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#feedforward-layers-of-transformer-block",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#feedforward-layers-of-transformer-block",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "4.3 feedforward layers of transformer block",
    "text": "4.3 feedforward layers of transformer block"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#residual-connections",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#residual-connections",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "4.4 residual connections",
    "text": "4.4 residual connections"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#layernorm-and-its-relationship-to-our-previous-batchnorm",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#layernorm-and-its-relationship-to-our-previous-batchnorm",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "4.5 layernorm (and its relationship to our previous batchnorm)",
    "text": "4.5 layernorm (and its relationship to our previous batchnorm)"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#scaling-up-the-model-creating-a-few-variables.-adding-dropout",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#scaling-up-the-model-creating-a-few-variables.-adding-dropout",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "4.6 scaling up the model! creating a few variables. adding dropout",
    "text": "4.6 scaling up the model! creating a few variables. adding dropout"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#encoder-vs.-decoder-vs.-both-transformers",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#encoder-vs.-decoder-vs.-both-transformers",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "5.1 encoder vs. decoder vs. both (?) Transformers",
    "text": "5.1 encoder vs. decoder vs. both (?) Transformers"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#super-quick-walkthrough-of-nanogpt-batched-multi-headed-self-attention",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#super-quick-walkthrough-of-nanogpt-batched-multi-headed-self-attention",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "5.2 super quick walkthrough of nanoGPT, batched multi-headed self-attention",
    "text": "5.2 super quick walkthrough of nanoGPT, batched multi-headed self-attention"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#back-to-chatgpt-gpt-3-pretraining-vs.-finetuning-rlhf",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#back-to-chatgpt-gpt-3-pretraining-vs.-finetuning-rlhf",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "5.3 back to ChatGPT, GPT-3, pretraining vs. finetuning, RLHF",
    "text": "5.3 back to ChatGPT, GPT-3, pretraining vs. finetuning, RLHF"
  },
  {
    "objectID": "blog/2025-04-27-nn-z2h-p7/index.html#conclusions",
    "href": "blog/2025-04-27-nn-z2h-p7/index.html#conclusions",
    "title": "Let’s build GPT, in code, spelled out!",
    "section": "5.4 conclusions",
    "text": "5.4 conclusions"
  },
  {
    "objectID": "jiu_jitsu_journal/index.html#may",
    "href": "jiu_jitsu_journal/index.html#may",
    "title": "My Jiu Jitsu Journal",
    "section": "",
    "text": "There was no note for April haha :“&gt;\n\n\n\n\n\n\n05.05.2025 | Lockdown\n\n\n\n\n\n\nLockdown là một thế khóa chân rất hiệu quả trong việc kiểm soát một cách cứng nhắc một chân của đối thủ. Nó hiệu quả khi ngăn cho chúng ta không bị pass guard, nhưng cũng làm chúng ta bế tắc khi hai chân cũng tự khóa cứng;\nChìa khóa là sự linh hoạt của hông/hip, khi lockdown, chân của chúng ta sẽ dễ dàng đảo theo trục ngang - như con lắc đơn - hơn là co duỗi theo trục dọc. Ta có thể tận dụng nó để làm bánh đà xoay hông;\nVì thế luôn có hai lựa chọn đặt hông: (1) xoay vào trong như half guard, (2) hoặc xoay ra ngoài tiếp cận lưng của đối thủ;\nChúng tôi drill lockdown từ những vị trí bị động như (1) khi cố thoát mount, hoặc chủ động như (2) half guard. Mục tiêu của các bài tập vẫn là “Crafting a door”, xây dựng thêm một điểm bản lề phía upper - ví dụ chính là vai. Khi đó cùng với bản lề thứ 2 là chân lock down, ta có thể sweep được đối thủ;\nHôm nay có fighing pant mới hehe - 悟空-黑神话🙊🙉🙈🐵."
  }
]